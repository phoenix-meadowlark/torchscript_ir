# Unimplemented, as it would require downloading terabytes of model weights.

# Pretrained models as describe by
#   https://huggingface.co/transformers/pretrained_models.html
# Broken links are kept but commented out.
MODEL_SHORTCUTS = [
    "bert-base-uncased",
    "bert-large-uncased",
    "bert-base-cased",
    "bert-large-cased",
    "bert-base-multilingual-uncased",
    "bert-base-multilingual-cased",
    "bert-base-chinese",
    "bert-base-german-cased",
    "bert-large-uncased-whole-word-masking",
    "bert-large-cased-whole-word-masking",
    "bert-large-uncased-whole-word-masking-finetuned-squad",
    "bert-large-uncased-whole-word-masking",
    "bert-large-cased-whole-word-masking-finetuned-squad",
    "bert-large-cased-whole-word-masking",
    "bert-base-cased-finetuned-mrpc",
    "bert-base-cased",
    "bert-base-german-dbmdz-cased",
    "bert-base-german-dbmdz-uncased",
    "cl-tohoku/bert-base-japanese",
    "cl-tohoku/bert-base-japanese-whole-word-masking",
    "cl-tohoku/bert-base-japanese-char",
    "cl-tohoku/bert-base-japanese-char-whole-word-masking",
    "TurkuNLP/bert-base-finnish-cased-v1",
    "TurkuNLP/bert-base-finnish-uncased-v1",
    "wietsedv/bert-base-dutch-cased",
    "openai-gpt",
    "gpt2",
    "gpt2-medium",
    "gpt2-large",
    "gpt2-xl",
    "transfo-xl-wt103",
    "xlnet-base-cased",
    "xlnet-large-cased",
    "xlm-mlm-en-2048",
    "xlm-mlm-ende-1024",
    "xlm-mlm-enfr-1024",
    "xlm-mlm-enro-1024",
    "xlm-mlm-xnli15-1024",
    "xlm-mlm-tlm-xnli15-1024",
    "xlm-clm-enfr-1024",
    "xlm-clm-ende-1024",
    "xlm-mlm-17-1280",
    "xlm-mlm-100-1280",
    "roberta-base",
    "roberta-large",
    "roberta-large-mnli",
    "roberta-large",
    "distilroberta-base",
    "roberta-base-openai-detector",
    "roberta-base",
    "roberta-large-openai-detector",
    "roberta-large",
    "distilbert-base-uncased",
    "distilbert-base-uncased-distilled-squad",
    "distilbert-base-cased",
    "distilbert-base-cased-distilled-squad",
    "distilgpt2",
    "distilbert-base-german-cased",
    "distilbert-base-multilingual-cased",
    "ctrl",
    "camembert-base",
    "albert-base-v1",
    "albert-large-v1",
    "albert-xlarge-v1",
    "albert-xxlarge-v1",
    "albert-base-v2",
    "albert-large-v2",
    "albert-xlarge-v2",
    "albert-xxlarge-v2",
    "t5-small",
    "t5-base",
    "t5-large",
    # "t5-3B",
    # "t5-11B",
    "xlm-roberta-base",
    "xlm-roberta-large",
    "flaubert/flaubert_small_cased",
    "flaubert/flaubert_base_uncased",
    "flaubert/flaubert_base_cased",
    "flaubert/flaubert_large_cased",
    "facebook/bart-large",
    "facebook/bart-base",
    "facebook/bart-large-mnli",
    "facebook/bart-large-cnn",
    # "DialoGPT-small",
    # "DialoGPT-medium",
    # "DialoGPT-large",
    # "reformer-enwik8",
    # "reformer-crime-and-punishment",
    # "Helsinki-NLP/opus-mt-{src}-{tgt}",
    # "google/pegasus-{dataset}",
    "google/pegasus-xsum",
    "allenai/longformer-base-4096",
    "allenai/longformer-large-4096",
    "facebook/mbart-large-cc25",
    "facebook/mbart-large-en-ro",
    # "lxmert-base-uncased",
    "funnel-transformer/small",
    "funnel-transformer/small-base",
    "funnel-transformer/medium",
    "funnel-transformer/medium-base",
    "funnel-transformer/intermediate",
    "funnel-transformer/intermediate-base",
    "funnel-transformer/large",
    "funnel-transformer/large-base",
    "funnel-transformer/xlarge",
    "funnel-transformer/xlarge-base",
    "microsoft/layoutlm-base-uncased",
    "microsoft/layoutlm-large-uncased",
]
