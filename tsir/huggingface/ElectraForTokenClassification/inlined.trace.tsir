graph(%self.1 : __torch__.transformers.modeling_electra.ElectraForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_17359.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_17358.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_electra.___torch_mangle_17357.ElectraModel = prim::GetAttr[name="electra"](%self.1)
  %10 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %11 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %12 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %13 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %14 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %15 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %17 : int = prim::Constant[value=128](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %18 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %19 : Double() = prim::Constant[value={-10000}](), scope: __module.electra # transformers/modeling_utils.py:258:0
  %20 : float = prim::Constant[value=1.](), scope: __module.electra # torch/tensor.py:396:0
  %21 : None = prim::Constant(), scope: __module.electra
  %22 : int = prim::Constant[value=6](), scope: __module.electra # transformers/modeling_utils.py:257:0
  %23 : int = prim::Constant[value=3](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %24 : int = prim::Constant[value=2](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %26 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %27 : Device = prim::Constant[value="cpu"](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %28 : int = prim::Constant[value=4](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %29 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %30 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %31 : __torch__.transformers.modeling_electra.___torch_mangle_17356.ElectraEncoder = prim::GetAttr[name="encoder"](%5)
  %32 : __torch__.torch.nn.modules.linear.___torch_mangle_17150.Linear = prim::GetAttr[name="embeddings_project"](%5)
  %33 : __torch__.transformers.modeling_electra.___torch_mangle_17149.ElectraEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %34 : int = aten::size(%input_ids, %30), scope: __module.electra # transformers/modeling_electra.py:724:0
  %35 : int = aten::size(%input_ids, %29), scope: __module.electra # transformers/modeling_electra.py:724:0
  %36 : int[] = prim::ListConstruct(%34, %35), scope: __module.electra
  %input.2 : Long(17:13, 13:1) = aten::zeros(%36, %28, %30, %27, %26), scope: __module.electra # transformers/modeling_electra.py:735:0
  %38 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %30, %30, %25, %29), scope: __module.electra # transformers/modeling_utils.py:244:0
  %39 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%38, %29), scope: __module.electra # transformers/modeling_utils.py:244:0
  %40 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%39, %24), scope: __module.electra # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%40, %23, %30, %25, %29), scope: __module.electra # transformers/modeling_utils.py:244:0
  %42 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %22, %26, %26, %21), scope: __module.electra # transformers/modeling_utils.py:257:0
  %43 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%42, %20, %29), scope: __module.electra # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%43, %19), scope: __module.electra # transformers/modeling_utils.py:258:0
  %45 : __torch__.torch.nn.modules.normalization.___torch_mangle_17147.LayerNorm = prim::GetAttr[name="LayerNorm"](%33)
  %46 : __torch__.torch.nn.modules.sparse.___torch_mangle_17146.Embedding = prim::GetAttr[name="token_type_embeddings"](%33)
  %47 : __torch__.torch.nn.modules.sparse.___torch_mangle_17145.Embedding = prim::GetAttr[name="position_embeddings"](%33)
  %48 : __torch__.torch.nn.modules.sparse.___torch_mangle_17144.Embedding = prim::GetAttr[name="word_embeddings"](%33)
  %49 : Tensor = prim::GetAttr[name="position_ids"](%33)
  %50 : int = aten::size(%input_ids, %29), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:173:0
  %51 : Long(1:512, 512:1) = aten::slice(%49, %30, %30, %25, %29), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%51, %29, %30, %50, %29), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %53 : Tensor = prim::GetAttr[name="weight"](%48)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%53, %input_ids, %30, %26, %26), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %55 : Tensor = prim::GetAttr[name="weight"](%47)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%55, %input.1, %14, %26, %26), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %57 : Tensor = prim::GetAttr[name="weight"](%46)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%57, %input.2, %14, %26, %26), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %59 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %29), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%59, %token_type_embeddings, %29), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %61 : Tensor = prim::GetAttr[name="bias"](%45)
  %62 : Tensor = prim::GetAttr[name="weight"](%45)
  %63 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %63, %62, %61, %16, %15), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %18, %26), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %66 : Tensor = prim::GetAttr[name="bias"](%32)
  %67 : Tensor = prim::GetAttr[name="weight"](%32)
  %68 : Float(128:1, 256:128) = aten::t(%67), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %output.1 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.5, %68), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %input.6 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.1, %66, %29), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1678:0
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %72 : __torch__.transformers.modeling_electra.___torch_mangle_17354.ElectraLayer = prim::GetAttr[name="11"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %74 : __torch__.transformers.modeling_electra.___torch_mangle_17337.ElectraLayer = prim::GetAttr[name="10"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %76 : __torch__.transformers.modeling_electra.___torch_mangle_17320.ElectraLayer = prim::GetAttr[name="9"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %78 : __torch__.transformers.modeling_electra.___torch_mangle_17303.ElectraLayer = prim::GetAttr[name="8"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %80 : __torch__.transformers.modeling_electra.___torch_mangle_17286.ElectraLayer = prim::GetAttr[name="7"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %82 : __torch__.transformers.modeling_electra.___torch_mangle_17269.ElectraLayer = prim::GetAttr[name="6"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %84 : __torch__.transformers.modeling_electra.___torch_mangle_17252.ElectraLayer = prim::GetAttr[name="5"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %86 : __torch__.transformers.modeling_electra.___torch_mangle_17235.ElectraLayer = prim::GetAttr[name="4"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %88 : __torch__.transformers.modeling_electra.___torch_mangle_17218.ElectraLayer = prim::GetAttr[name="3"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %90 : __torch__.transformers.modeling_electra.___torch_mangle_17201.ElectraLayer = prim::GetAttr[name="2"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %92 : __torch__.transformers.modeling_electra.___torch_mangle_17184.ElectraLayer = prim::GetAttr[name="1"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_17355.ModuleList = prim::GetAttr[name="layer"](%31)
  %94 : __torch__.transformers.modeling_electra.___torch_mangle_17167.ElectraLayer = prim::GetAttr[name="0"](%93)
  %95 : __torch__.transformers.modeling_electra.___torch_mangle_17166.ElectraOutput = prim::GetAttr[name="output"](%94)
  %96 : __torch__.transformers.modeling_electra.___torch_mangle_17162.ElectraIntermediate = prim::GetAttr[name="intermediate"](%94)
  %97 : __torch__.transformers.modeling_electra.___torch_mangle_17160.ElectraAttention = prim::GetAttr[name="attention"](%94)
  %98 : __torch__.transformers.modeling_electra.___torch_mangle_17159.ElectraSelfOutput = prim::GetAttr[name="output"](%97)
  %99 : __torch__.transformers.modeling_electra.___torch_mangle_17155.ElectraSelfAttention = prim::GetAttr[name="self"](%97)
  %100 : __torch__.torch.nn.modules.linear.___torch_mangle_17153.Linear = prim::GetAttr[name="value"](%99)
  %101 : __torch__.torch.nn.modules.linear.___torch_mangle_17152.Linear = prim::GetAttr[name="key"](%99)
  %102 : __torch__.torch.nn.modules.linear.___torch_mangle_17151.Linear = prim::GetAttr[name="query"](%99)
  %103 : Tensor = prim::GetAttr[name="bias"](%102)
  %104 : Tensor = prim::GetAttr[name="weight"](%102)
  %105 : Float(256:1, 256:256) = aten::t(%104), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %105), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.2, %103, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %108 : Tensor = prim::GetAttr[name="bias"](%101)
  %109 : Tensor = prim::GetAttr[name="weight"](%101)
  %110 : Float(256:1, 256:256) = aten::t(%109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %110), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.3, %108, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %113 : Tensor = prim::GetAttr[name="bias"](%100)
  %114 : Tensor = prim::GetAttr[name="weight"](%100)
  %115 : Float(256:1, 256:256) = aten::t(%114), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %115), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.4, %113, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %118 : int = aten::size(%x.1, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %119 : int = aten::size(%x.1, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %120 : int[] = prim::ListConstruct(%118, %119, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.1, %120), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %122 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.2, %122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %124 : int = aten::size(%x.3, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %125 : int = aten::size(%x.3, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %126 : int[] = prim::ListConstruct(%124, %125, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.3, %126), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %128 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.4, %128), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %130 : int = aten::size(%x.5, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %131 : int = aten::size(%x.5, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %132 : int[] = prim::ListConstruct(%130, %131, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.5, %132), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %134 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.6, %134), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %136 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.1, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %136), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %input.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
  %input.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.7, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.8, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:265:0
  %143 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %144 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.1, %143), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%144, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %146 : int = aten::size(%context_layer.2, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %147 : int = aten::size(%context_layer.2, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %148 : int[] = prim::ListConstruct(%146, %147, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %input.9 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.2, %148), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %150 : __torch__.torch.nn.modules.normalization.___torch_mangle_17157.LayerNorm = prim::GetAttr[name="LayerNorm"](%98)
  %151 : __torch__.torch.nn.modules.linear.___torch_mangle_17156.Linear = prim::GetAttr[name="dense"](%98)
  %152 : Tensor = prim::GetAttr[name="bias"](%151)
  %153 : Tensor = prim::GetAttr[name="weight"](%151)
  %154 : Float(256:1, 256:256) = aten::t(%153), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.9, %154), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.5, %152, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.10, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.1, %input.6, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
  %159 : Tensor = prim::GetAttr[name="bias"](%150)
  %160 : Tensor = prim::GetAttr[name="weight"](%150)
  %161 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.11, %161, %160, %159, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %163 : __torch__.torch.nn.modules.linear.___torch_mangle_17161.Linear = prim::GetAttr[name="dense"](%96)
  %164 : Tensor = prim::GetAttr[name="bias"](%163)
  %165 : Tensor = prim::GetAttr[name="weight"](%163)
  %166 : Float(256:1, 1024:256) = aten::t(%165), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.1, %166), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %164, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %170 : __torch__.torch.nn.modules.normalization.___torch_mangle_17164.LayerNorm = prim::GetAttr[name="LayerNorm"](%95)
  %171 : __torch__.torch.nn.modules.linear.___torch_mangle_17163.Linear = prim::GetAttr[name="dense"](%95)
  %172 : Tensor = prim::GetAttr[name="bias"](%171)
  %173 : Tensor = prim::GetAttr[name="weight"](%171)
  %174 : Float(1024:1, 256:1024) = aten::t(%173), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.7 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.13, %174), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.14 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.7, %172, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.14, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.2, %input_tensor.1, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output # transformers/modeling_electra.py:365:0
  %179 : Tensor = prim::GetAttr[name="bias"](%170)
  %180 : Tensor = prim::GetAttr[name="weight"](%170)
  %181 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm
  %input.16 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.15, %181, %180, %179, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %183 : __torch__.transformers.modeling_electra.___torch_mangle_17183.ElectraOutput = prim::GetAttr[name="output"](%92)
  %184 : __torch__.transformers.modeling_electra.___torch_mangle_17179.ElectraIntermediate = prim::GetAttr[name="intermediate"](%92)
  %185 : __torch__.transformers.modeling_electra.___torch_mangle_17177.ElectraAttention = prim::GetAttr[name="attention"](%92)
  %186 : __torch__.transformers.modeling_electra.___torch_mangle_17176.ElectraSelfOutput = prim::GetAttr[name="output"](%185)
  %187 : __torch__.transformers.modeling_electra.___torch_mangle_17172.ElectraSelfAttention = prim::GetAttr[name="self"](%185)
  %188 : __torch__.torch.nn.modules.linear.___torch_mangle_17170.Linear = prim::GetAttr[name="value"](%187)
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_17169.Linear = prim::GetAttr[name="key"](%187)
  %190 : __torch__.torch.nn.modules.linear.___torch_mangle_17168.Linear = prim::GetAttr[name="query"](%187)
  %191 : Tensor = prim::GetAttr[name="bias"](%190)
  %192 : Tensor = prim::GetAttr[name="weight"](%190)
  %193 : Float(256:1, 256:256) = aten::t(%192), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.8 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %193), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.8, %191, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %196 : Tensor = prim::GetAttr[name="bias"](%189)
  %197 : Tensor = prim::GetAttr[name="weight"](%189)
  %198 : Float(256:1, 256:256) = aten::t(%197), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.9 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %198), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.9, %196, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %201 : Tensor = prim::GetAttr[name="bias"](%188)
  %202 : Tensor = prim::GetAttr[name="weight"](%188)
  %203 : Float(256:1, 256:256) = aten::t(%202), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.10 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %203), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.10, %201, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %206 : int = aten::size(%x.7, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %207 : int = aten::size(%x.7, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %208 : int[] = prim::ListConstruct(%206, %207, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.7, %208), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %210 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.8, %210), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %212 : int = aten::size(%x.9, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %213 : int = aten::size(%x.9, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %214 : int[] = prim::ListConstruct(%212, %213, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.9, %214), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %216 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.10, %216), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %218 : int = aten::size(%x.11, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %219 : int = aten::size(%x.11, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %220 : int[] = prim::ListConstruct(%218, %219, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.11, %220), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %222 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.12, %222), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %224 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.2, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %224), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
  %input.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
  %input.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.17, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.18, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:265:0
  %231 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %232 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.3, %231), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%232, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %234 : int = aten::size(%context_layer.4, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %235 : int = aten::size(%context_layer.4, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %236 : int[] = prim::ListConstruct(%234, %235, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %input.19 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.4, %236), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
  %238 : __torch__.torch.nn.modules.normalization.___torch_mangle_17174.LayerNorm = prim::GetAttr[name="LayerNorm"](%186)
  %239 : __torch__.torch.nn.modules.linear.___torch_mangle_17173.Linear = prim::GetAttr[name="dense"](%186)
  %240 : Tensor = prim::GetAttr[name="bias"](%239)
  %241 : Tensor = prim::GetAttr[name="weight"](%239)
  %242 : Float(256:1, 256:256) = aten::t(%241), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.19, %242), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.11, %240, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.20, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.21 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.3, %input.16, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
  %247 : Tensor = prim::GetAttr[name="bias"](%238)
  %248 : Tensor = prim::GetAttr[name="weight"](%238)
  %249 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.21, %249, %248, %247, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %251 : __torch__.torch.nn.modules.linear.___torch_mangle_17178.Linear = prim::GetAttr[name="dense"](%184)
  %252 : Tensor = prim::GetAttr[name="bias"](%251)
  %253 : Tensor = prim::GetAttr[name="weight"](%251)
  %254 : Float(256:1, 1024:256) = aten::t(%253), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.2, %254), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %252, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %258 : __torch__.torch.nn.modules.normalization.___torch_mangle_17181.LayerNorm = prim::GetAttr[name="LayerNorm"](%183)
  %259 : __torch__.torch.nn.modules.linear.___torch_mangle_17180.Linear = prim::GetAttr[name="dense"](%183)
  %260 : Tensor = prim::GetAttr[name="bias"](%259)
  %261 : Tensor = prim::GetAttr[name="weight"](%259)
  %262 : Float(1024:1, 256:1024) = aten::t(%261), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.13 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.23, %262), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.24 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.13, %260, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.24, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.25 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.4, %input_tensor.2, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output # transformers/modeling_electra.py:365:0
  %267 : Tensor = prim::GetAttr[name="bias"](%258)
  %268 : Tensor = prim::GetAttr[name="weight"](%258)
  %269 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm
  %input.26 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.25, %269, %268, %267, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %271 : __torch__.transformers.modeling_electra.___torch_mangle_17200.ElectraOutput = prim::GetAttr[name="output"](%90)
  %272 : __torch__.transformers.modeling_electra.___torch_mangle_17196.ElectraIntermediate = prim::GetAttr[name="intermediate"](%90)
  %273 : __torch__.transformers.modeling_electra.___torch_mangle_17194.ElectraAttention = prim::GetAttr[name="attention"](%90)
  %274 : __torch__.transformers.modeling_electra.___torch_mangle_17193.ElectraSelfOutput = prim::GetAttr[name="output"](%273)
  %275 : __torch__.transformers.modeling_electra.___torch_mangle_17189.ElectraSelfAttention = prim::GetAttr[name="self"](%273)
  %276 : __torch__.torch.nn.modules.linear.___torch_mangle_17187.Linear = prim::GetAttr[name="value"](%275)
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_17186.Linear = prim::GetAttr[name="key"](%275)
  %278 : __torch__.torch.nn.modules.linear.___torch_mangle_17185.Linear = prim::GetAttr[name="query"](%275)
  %279 : Tensor = prim::GetAttr[name="bias"](%278)
  %280 : Tensor = prim::GetAttr[name="weight"](%278)
  %281 : Float(256:1, 256:256) = aten::t(%280), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.14 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %281), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.14, %279, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %284 : Tensor = prim::GetAttr[name="bias"](%277)
  %285 : Tensor = prim::GetAttr[name="weight"](%277)
  %286 : Float(256:1, 256:256) = aten::t(%285), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.15 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %286), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.15, %284, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %289 : Tensor = prim::GetAttr[name="bias"](%276)
  %290 : Tensor = prim::GetAttr[name="weight"](%276)
  %291 : Float(256:1, 256:256) = aten::t(%290), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.16 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %291), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.16, %289, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %294 : int = aten::size(%x.13, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %295 : int = aten::size(%x.13, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %296 : int[] = prim::ListConstruct(%294, %295, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.13, %296), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %298 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.14, %298), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %300 : int = aten::size(%x.15, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %301 : int = aten::size(%x.15, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %302 : int[] = prim::ListConstruct(%300, %301, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.15, %302), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %304 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.16, %304), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %306 : int = aten::size(%x.17, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %307 : int = aten::size(%x.17, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %308 : int[] = prim::ListConstruct(%306, %307, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.17, %308), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %310 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.18, %310), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %312 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.3, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %312), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
  %input.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
  %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.27, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.28, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:265:0
  %319 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %320 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.5, %319), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%320, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %322 : int = aten::size(%context_layer.6, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %323 : int = aten::size(%context_layer.6, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %324 : int[] = prim::ListConstruct(%322, %323, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %input.29 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.6, %324), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
  %326 : __torch__.torch.nn.modules.normalization.___torch_mangle_17191.LayerNorm = prim::GetAttr[name="LayerNorm"](%274)
  %327 : __torch__.torch.nn.modules.linear.___torch_mangle_17190.Linear = prim::GetAttr[name="dense"](%274)
  %328 : Tensor = prim::GetAttr[name="bias"](%327)
  %329 : Tensor = prim::GetAttr[name="weight"](%327)
  %330 : Float(256:1, 256:256) = aten::t(%329), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.29, %330), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.17, %328, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.30, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.5, %input.26, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
  %335 : Tensor = prim::GetAttr[name="bias"](%326)
  %336 : Tensor = prim::GetAttr[name="weight"](%326)
  %337 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.31, %337, %336, %335, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %339 : __torch__.torch.nn.modules.linear.___torch_mangle_17195.Linear = prim::GetAttr[name="dense"](%272)
  %340 : Tensor = prim::GetAttr[name="bias"](%339)
  %341 : Tensor = prim::GetAttr[name="weight"](%339)
  %342 : Float(256:1, 1024:256) = aten::t(%341), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.3, %342), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %340, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %346 : __torch__.torch.nn.modules.normalization.___torch_mangle_17198.LayerNorm = prim::GetAttr[name="LayerNorm"](%271)
  %347 : __torch__.torch.nn.modules.linear.___torch_mangle_17197.Linear = prim::GetAttr[name="dense"](%271)
  %348 : Tensor = prim::GetAttr[name="bias"](%347)
  %349 : Tensor = prim::GetAttr[name="weight"](%347)
  %350 : Float(1024:1, 256:1024) = aten::t(%349), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.19 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.33, %350), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.34 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.19, %348, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.34, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.35 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.6, %input_tensor.3, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output # transformers/modeling_electra.py:365:0
  %355 : Tensor = prim::GetAttr[name="bias"](%346)
  %356 : Tensor = prim::GetAttr[name="weight"](%346)
  %357 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm
  %input.36 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.35, %357, %356, %355, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %359 : __torch__.transformers.modeling_electra.___torch_mangle_17217.ElectraOutput = prim::GetAttr[name="output"](%88)
  %360 : __torch__.transformers.modeling_electra.___torch_mangle_17213.ElectraIntermediate = prim::GetAttr[name="intermediate"](%88)
  %361 : __torch__.transformers.modeling_electra.___torch_mangle_17211.ElectraAttention = prim::GetAttr[name="attention"](%88)
  %362 : __torch__.transformers.modeling_electra.___torch_mangle_17210.ElectraSelfOutput = prim::GetAttr[name="output"](%361)
  %363 : __torch__.transformers.modeling_electra.___torch_mangle_17206.ElectraSelfAttention = prim::GetAttr[name="self"](%361)
  %364 : __torch__.torch.nn.modules.linear.___torch_mangle_17204.Linear = prim::GetAttr[name="value"](%363)
  %365 : __torch__.torch.nn.modules.linear.___torch_mangle_17203.Linear = prim::GetAttr[name="key"](%363)
  %366 : __torch__.torch.nn.modules.linear.___torch_mangle_17202.Linear = prim::GetAttr[name="query"](%363)
  %367 : Tensor = prim::GetAttr[name="bias"](%366)
  %368 : Tensor = prim::GetAttr[name="weight"](%366)
  %369 : Float(256:1, 256:256) = aten::t(%368), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.20 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %369), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.20, %367, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %372 : Tensor = prim::GetAttr[name="bias"](%365)
  %373 : Tensor = prim::GetAttr[name="weight"](%365)
  %374 : Float(256:1, 256:256) = aten::t(%373), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.21 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %374), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.21, %372, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %377 : Tensor = prim::GetAttr[name="bias"](%364)
  %378 : Tensor = prim::GetAttr[name="weight"](%364)
  %379 : Float(256:1, 256:256) = aten::t(%378), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.22 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %379), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.22, %377, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %382 : int = aten::size(%x.19, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %383 : int = aten::size(%x.19, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %384 : int[] = prim::ListConstruct(%382, %383, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.19, %384), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %386 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.20, %386), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %388 : int = aten::size(%x.21, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %389 : int = aten::size(%x.21, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %390 : int[] = prim::ListConstruct(%388, %389, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.21, %390), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %392 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.22, %392), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %394 : int = aten::size(%x.23, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %395 : int = aten::size(%x.23, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %396 : int[] = prim::ListConstruct(%394, %395, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.24 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.23, %396), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %398 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.24, %398), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %400 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.4, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %400), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
  %input.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
  %input.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.37, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.38, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:265:0
  %407 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %408 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.7, %407), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%408, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %410 : int = aten::size(%context_layer.8, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %411 : int = aten::size(%context_layer.8, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %412 : int[] = prim::ListConstruct(%410, %411, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %input.39 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.8, %412), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
  %414 : __torch__.torch.nn.modules.normalization.___torch_mangle_17208.LayerNorm = prim::GetAttr[name="LayerNorm"](%362)
  %415 : __torch__.torch.nn.modules.linear.___torch_mangle_17207.Linear = prim::GetAttr[name="dense"](%362)
  %416 : Tensor = prim::GetAttr[name="bias"](%415)
  %417 : Tensor = prim::GetAttr[name="weight"](%415)
  %418 : Float(256:1, 256:256) = aten::t(%417), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.39, %418), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.23, %416, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.40, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.7, %input.36, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
  %423 : Tensor = prim::GetAttr[name="bias"](%414)
  %424 : Tensor = prim::GetAttr[name="weight"](%414)
  %425 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.41, %425, %424, %423, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %427 : __torch__.torch.nn.modules.linear.___torch_mangle_17212.Linear = prim::GetAttr[name="dense"](%360)
  %428 : Tensor = prim::GetAttr[name="bias"](%427)
  %429 : Tensor = prim::GetAttr[name="weight"](%427)
  %430 : Float(256:1, 1024:256) = aten::t(%429), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.4, %430), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %428, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.42), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %434 : __torch__.torch.nn.modules.normalization.___torch_mangle_17215.LayerNorm = prim::GetAttr[name="LayerNorm"](%359)
  %435 : __torch__.torch.nn.modules.linear.___torch_mangle_17214.Linear = prim::GetAttr[name="dense"](%359)
  %436 : Tensor = prim::GetAttr[name="bias"](%435)
  %437 : Tensor = prim::GetAttr[name="weight"](%435)
  %438 : Float(1024:1, 256:1024) = aten::t(%437), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.25 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.43, %438), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.44 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.25, %436, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.44, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.8, %input_tensor.4, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output # transformers/modeling_electra.py:365:0
  %443 : Tensor = prim::GetAttr[name="bias"](%434)
  %444 : Tensor = prim::GetAttr[name="weight"](%434)
  %445 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm
  %input.46 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.45, %445, %444, %443, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %447 : __torch__.transformers.modeling_electra.___torch_mangle_17234.ElectraOutput = prim::GetAttr[name="output"](%86)
  %448 : __torch__.transformers.modeling_electra.___torch_mangle_17230.ElectraIntermediate = prim::GetAttr[name="intermediate"](%86)
  %449 : __torch__.transformers.modeling_electra.___torch_mangle_17228.ElectraAttention = prim::GetAttr[name="attention"](%86)
  %450 : __torch__.transformers.modeling_electra.___torch_mangle_17227.ElectraSelfOutput = prim::GetAttr[name="output"](%449)
  %451 : __torch__.transformers.modeling_electra.___torch_mangle_17223.ElectraSelfAttention = prim::GetAttr[name="self"](%449)
  %452 : __torch__.torch.nn.modules.linear.___torch_mangle_17221.Linear = prim::GetAttr[name="value"](%451)
  %453 : __torch__.torch.nn.modules.linear.___torch_mangle_17220.Linear = prim::GetAttr[name="key"](%451)
  %454 : __torch__.torch.nn.modules.linear.___torch_mangle_17219.Linear = prim::GetAttr[name="query"](%451)
  %455 : Tensor = prim::GetAttr[name="bias"](%454)
  %456 : Tensor = prim::GetAttr[name="weight"](%454)
  %457 : Float(256:1, 256:256) = aten::t(%456), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.26 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %457), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.26, %455, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %460 : Tensor = prim::GetAttr[name="bias"](%453)
  %461 : Tensor = prim::GetAttr[name="weight"](%453)
  %462 : Float(256:1, 256:256) = aten::t(%461), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.27 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %462), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.27, %460, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %465 : Tensor = prim::GetAttr[name="bias"](%452)
  %466 : Tensor = prim::GetAttr[name="weight"](%452)
  %467 : Float(256:1, 256:256) = aten::t(%466), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.28 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %467), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.28, %465, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %470 : int = aten::size(%x.25, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %471 : int = aten::size(%x.25, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %472 : int[] = prim::ListConstruct(%470, %471, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.26 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.25, %472), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %474 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.26, %474), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %476 : int = aten::size(%x.27, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %477 : int = aten::size(%x.27, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %478 : int[] = prim::ListConstruct(%476, %477, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.28 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.27, %478), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %480 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.28, %480), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %482 : int = aten::size(%x.29, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %483 : int = aten::size(%x.29, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %484 : int[] = prim::ListConstruct(%482, %483, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.30 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.29, %484), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %486 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.30, %486), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %488 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.5, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %488), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
  %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
  %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:265:0
  %495 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %496 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.9, %495), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%496, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %498 : int = aten::size(%context_layer.10, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %499 : int = aten::size(%context_layer.10, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %500 : int[] = prim::ListConstruct(%498, %499, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %input.49 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.10, %500), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
  %502 : __torch__.torch.nn.modules.normalization.___torch_mangle_17225.LayerNorm = prim::GetAttr[name="LayerNorm"](%450)
  %503 : __torch__.torch.nn.modules.linear.___torch_mangle_17224.Linear = prim::GetAttr[name="dense"](%450)
  %504 : Tensor = prim::GetAttr[name="bias"](%503)
  %505 : Tensor = prim::GetAttr[name="weight"](%503)
  %506 : Float(256:1, 256:256) = aten::t(%505), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.49, %506), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.29, %504, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.50, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.9, %input.46, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
  %511 : Tensor = prim::GetAttr[name="bias"](%502)
  %512 : Tensor = prim::GetAttr[name="weight"](%502)
  %513 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.51, %513, %512, %511, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %515 : __torch__.torch.nn.modules.linear.___torch_mangle_17229.Linear = prim::GetAttr[name="dense"](%448)
  %516 : Tensor = prim::GetAttr[name="bias"](%515)
  %517 : Tensor = prim::GetAttr[name="weight"](%515)
  %518 : Float(256:1, 1024:256) = aten::t(%517), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.5, %518), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %516, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %522 : __torch__.torch.nn.modules.normalization.___torch_mangle_17232.LayerNorm = prim::GetAttr[name="LayerNorm"](%447)
  %523 : __torch__.torch.nn.modules.linear.___torch_mangle_17231.Linear = prim::GetAttr[name="dense"](%447)
  %524 : Tensor = prim::GetAttr[name="bias"](%523)
  %525 : Tensor = prim::GetAttr[name="weight"](%523)
  %526 : Float(1024:1, 256:1024) = aten::t(%525), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.31 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.53, %526), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.54 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.31, %524, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.54, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.10, %input_tensor.5, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output # transformers/modeling_electra.py:365:0
  %531 : Tensor = prim::GetAttr[name="bias"](%522)
  %532 : Tensor = prim::GetAttr[name="weight"](%522)
  %533 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm
  %input.56 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.55, %533, %532, %531, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %535 : __torch__.transformers.modeling_electra.___torch_mangle_17251.ElectraOutput = prim::GetAttr[name="output"](%84)
  %536 : __torch__.transformers.modeling_electra.___torch_mangle_17247.ElectraIntermediate = prim::GetAttr[name="intermediate"](%84)
  %537 : __torch__.transformers.modeling_electra.___torch_mangle_17245.ElectraAttention = prim::GetAttr[name="attention"](%84)
  %538 : __torch__.transformers.modeling_electra.___torch_mangle_17244.ElectraSelfOutput = prim::GetAttr[name="output"](%537)
  %539 : __torch__.transformers.modeling_electra.___torch_mangle_17240.ElectraSelfAttention = prim::GetAttr[name="self"](%537)
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_17238.Linear = prim::GetAttr[name="value"](%539)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_17237.Linear = prim::GetAttr[name="key"](%539)
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_17236.Linear = prim::GetAttr[name="query"](%539)
  %543 : Tensor = prim::GetAttr[name="bias"](%542)
  %544 : Tensor = prim::GetAttr[name="weight"](%542)
  %545 : Float(256:1, 256:256) = aten::t(%544), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %545), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.32, %543, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %548 : Tensor = prim::GetAttr[name="bias"](%541)
  %549 : Tensor = prim::GetAttr[name="weight"](%541)
  %550 : Float(256:1, 256:256) = aten::t(%549), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %550), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.33, %548, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %553 : Tensor = prim::GetAttr[name="bias"](%540)
  %554 : Tensor = prim::GetAttr[name="weight"](%540)
  %555 : Float(256:1, 256:256) = aten::t(%554), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %555), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.34, %553, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %558 : int = aten::size(%x.31, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %559 : int = aten::size(%x.31, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %560 : int[] = prim::ListConstruct(%558, %559, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.32 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.31, %560), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %562 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.32, %562), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %564 : int = aten::size(%x.33, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %565 : int = aten::size(%x.33, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %566 : int[] = prim::ListConstruct(%564, %565, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.34 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.33, %566), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %568 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.34, %568), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %570 : int = aten::size(%x.35, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %571 : int = aten::size(%x.35, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %572 : int[] = prim::ListConstruct(%570, %571, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.36 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.35, %572), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %574 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.36, %574), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %576 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.6, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %576), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
  %input.57 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
  %input.58 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.57, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.58, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:265:0
  %583 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %584 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.11, %583), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%584, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %586 : int = aten::size(%context_layer.12, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %587 : int = aten::size(%context_layer.12, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %588 : int[] = prim::ListConstruct(%586, %587, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %input.59 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.12, %588), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
  %590 : __torch__.torch.nn.modules.normalization.___torch_mangle_17242.LayerNorm = prim::GetAttr[name="LayerNorm"](%538)
  %591 : __torch__.torch.nn.modules.linear.___torch_mangle_17241.Linear = prim::GetAttr[name="dense"](%538)
  %592 : Tensor = prim::GetAttr[name="bias"](%591)
  %593 : Tensor = prim::GetAttr[name="weight"](%591)
  %594 : Float(256:1, 256:256) = aten::t(%593), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.59, %594), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.35, %592, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.60, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.61 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.11, %input.56, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
  %599 : Tensor = prim::GetAttr[name="bias"](%590)
  %600 : Tensor = prim::GetAttr[name="weight"](%590)
  %601 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.61, %601, %600, %599, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_17246.Linear = prim::GetAttr[name="dense"](%536)
  %604 : Tensor = prim::GetAttr[name="bias"](%603)
  %605 : Tensor = prim::GetAttr[name="weight"](%603)
  %606 : Float(256:1, 1024:256) = aten::t(%605), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.6, %606), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %604, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.62), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %610 : __torch__.torch.nn.modules.normalization.___torch_mangle_17249.LayerNorm = prim::GetAttr[name="LayerNorm"](%535)
  %611 : __torch__.torch.nn.modules.linear.___torch_mangle_17248.Linear = prim::GetAttr[name="dense"](%535)
  %612 : Tensor = prim::GetAttr[name="bias"](%611)
  %613 : Tensor = prim::GetAttr[name="weight"](%611)
  %614 : Float(1024:1, 256:1024) = aten::t(%613), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.37 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.63, %614), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.64 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.37, %612, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.64, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.12, %input_tensor.6, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output # transformers/modeling_electra.py:365:0
  %619 : Tensor = prim::GetAttr[name="bias"](%610)
  %620 : Tensor = prim::GetAttr[name="weight"](%610)
  %621 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm
  %input.66 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.65, %621, %620, %619, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %623 : __torch__.transformers.modeling_electra.___torch_mangle_17268.ElectraOutput = prim::GetAttr[name="output"](%82)
  %624 : __torch__.transformers.modeling_electra.___torch_mangle_17264.ElectraIntermediate = prim::GetAttr[name="intermediate"](%82)
  %625 : __torch__.transformers.modeling_electra.___torch_mangle_17262.ElectraAttention = prim::GetAttr[name="attention"](%82)
  %626 : __torch__.transformers.modeling_electra.___torch_mangle_17261.ElectraSelfOutput = prim::GetAttr[name="output"](%625)
  %627 : __torch__.transformers.modeling_electra.___torch_mangle_17257.ElectraSelfAttention = prim::GetAttr[name="self"](%625)
  %628 : __torch__.torch.nn.modules.linear.___torch_mangle_17255.Linear = prim::GetAttr[name="value"](%627)
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_17254.Linear = prim::GetAttr[name="key"](%627)
  %630 : __torch__.torch.nn.modules.linear.___torch_mangle_17253.Linear = prim::GetAttr[name="query"](%627)
  %631 : Tensor = prim::GetAttr[name="bias"](%630)
  %632 : Tensor = prim::GetAttr[name="weight"](%630)
  %633 : Float(256:1, 256:256) = aten::t(%632), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.38 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %633), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.38, %631, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %636 : Tensor = prim::GetAttr[name="bias"](%629)
  %637 : Tensor = prim::GetAttr[name="weight"](%629)
  %638 : Float(256:1, 256:256) = aten::t(%637), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.39 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %638), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.39, %636, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %641 : Tensor = prim::GetAttr[name="bias"](%628)
  %642 : Tensor = prim::GetAttr[name="weight"](%628)
  %643 : Float(256:1, 256:256) = aten::t(%642), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.40 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %643), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.40, %641, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %646 : int = aten::size(%x.37, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %647 : int = aten::size(%x.37, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %648 : int[] = prim::ListConstruct(%646, %647, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.38 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.37, %648), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %650 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.38, %650), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %652 : int = aten::size(%x.39, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %653 : int = aten::size(%x.39, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %654 : int[] = prim::ListConstruct(%652, %653, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.40 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.39, %654), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %656 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.40, %656), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %658 : int = aten::size(%x.41, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %659 : int = aten::size(%x.41, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %660 : int[] = prim::ListConstruct(%658, %659, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.42 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.41, %660), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %662 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.42, %662), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %664 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.7, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %664), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
  %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
  %input.68 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.67, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.68, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:265:0
  %671 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %672 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.13, %671), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%672, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %674 : int = aten::size(%context_layer.14, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %675 : int = aten::size(%context_layer.14, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %676 : int[] = prim::ListConstruct(%674, %675, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %input.69 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.14, %676), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
  %678 : __torch__.torch.nn.modules.normalization.___torch_mangle_17259.LayerNorm = prim::GetAttr[name="LayerNorm"](%626)
  %679 : __torch__.torch.nn.modules.linear.___torch_mangle_17258.Linear = prim::GetAttr[name="dense"](%626)
  %680 : Tensor = prim::GetAttr[name="bias"](%679)
  %681 : Tensor = prim::GetAttr[name="weight"](%679)
  %682 : Float(256:1, 256:256) = aten::t(%681), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.69, %682), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.41, %680, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.70, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.71 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.13, %input.66, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
  %687 : Tensor = prim::GetAttr[name="bias"](%678)
  %688 : Tensor = prim::GetAttr[name="weight"](%678)
  %689 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.71, %689, %688, %687, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %691 : __torch__.torch.nn.modules.linear.___torch_mangle_17263.Linear = prim::GetAttr[name="dense"](%624)
  %692 : Tensor = prim::GetAttr[name="bias"](%691)
  %693 : Tensor = prim::GetAttr[name="weight"](%691)
  %694 : Float(256:1, 1024:256) = aten::t(%693), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.7, %694), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %692, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.72), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %698 : __torch__.torch.nn.modules.normalization.___torch_mangle_17266.LayerNorm = prim::GetAttr[name="LayerNorm"](%623)
  %699 : __torch__.torch.nn.modules.linear.___torch_mangle_17265.Linear = prim::GetAttr[name="dense"](%623)
  %700 : Tensor = prim::GetAttr[name="bias"](%699)
  %701 : Tensor = prim::GetAttr[name="weight"](%699)
  %702 : Float(1024:1, 256:1024) = aten::t(%701), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.73, %702), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.74 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.43, %700, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.74, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.75 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.14, %input_tensor.7, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output # transformers/modeling_electra.py:365:0
  %707 : Tensor = prim::GetAttr[name="bias"](%698)
  %708 : Tensor = prim::GetAttr[name="weight"](%698)
  %709 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm
  %input.76 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.75, %709, %708, %707, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %711 : __torch__.transformers.modeling_electra.___torch_mangle_17285.ElectraOutput = prim::GetAttr[name="output"](%80)
  %712 : __torch__.transformers.modeling_electra.___torch_mangle_17281.ElectraIntermediate = prim::GetAttr[name="intermediate"](%80)
  %713 : __torch__.transformers.modeling_electra.___torch_mangle_17279.ElectraAttention = prim::GetAttr[name="attention"](%80)
  %714 : __torch__.transformers.modeling_electra.___torch_mangle_17278.ElectraSelfOutput = prim::GetAttr[name="output"](%713)
  %715 : __torch__.transformers.modeling_electra.___torch_mangle_17274.ElectraSelfAttention = prim::GetAttr[name="self"](%713)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_17272.Linear = prim::GetAttr[name="value"](%715)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_17271.Linear = prim::GetAttr[name="key"](%715)
  %718 : __torch__.torch.nn.modules.linear.___torch_mangle_17270.Linear = prim::GetAttr[name="query"](%715)
  %719 : Tensor = prim::GetAttr[name="bias"](%718)
  %720 : Tensor = prim::GetAttr[name="weight"](%718)
  %721 : Float(256:1, 256:256) = aten::t(%720), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.44 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %721), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.44, %719, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %724 : Tensor = prim::GetAttr[name="bias"](%717)
  %725 : Tensor = prim::GetAttr[name="weight"](%717)
  %726 : Float(256:1, 256:256) = aten::t(%725), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %726), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.45, %724, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %729 : Tensor = prim::GetAttr[name="bias"](%716)
  %730 : Tensor = prim::GetAttr[name="weight"](%716)
  %731 : Float(256:1, 256:256) = aten::t(%730), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %731), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.46, %729, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %734 : int = aten::size(%x.43, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %735 : int = aten::size(%x.43, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %736 : int[] = prim::ListConstruct(%734, %735, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.44 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.43, %736), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %738 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.44, %738), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %740 : int = aten::size(%x.45, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %741 : int = aten::size(%x.45, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %742 : int[] = prim::ListConstruct(%740, %741, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.46 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.45, %742), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %744 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.46, %744), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %746 : int = aten::size(%x.47, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %747 : int = aten::size(%x.47, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %748 : int[] = prim::ListConstruct(%746, %747, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.48 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.47, %748), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %750 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.48, %750), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %752 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.8, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %752), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
  %input.77 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
  %input.78 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.77, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.78, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:265:0
  %759 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %760 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.15, %759), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%760, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %762 : int = aten::size(%context_layer.16, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %763 : int = aten::size(%context_layer.16, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %764 : int[] = prim::ListConstruct(%762, %763, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %input.79 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.16, %764), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
  %766 : __torch__.torch.nn.modules.normalization.___torch_mangle_17276.LayerNorm = prim::GetAttr[name="LayerNorm"](%714)
  %767 : __torch__.torch.nn.modules.linear.___torch_mangle_17275.Linear = prim::GetAttr[name="dense"](%714)
  %768 : Tensor = prim::GetAttr[name="bias"](%767)
  %769 : Tensor = prim::GetAttr[name="weight"](%767)
  %770 : Float(256:1, 256:256) = aten::t(%769), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.79, %770), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.47, %768, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.80, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.81 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.15, %input.76, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
  %775 : Tensor = prim::GetAttr[name="bias"](%766)
  %776 : Tensor = prim::GetAttr[name="weight"](%766)
  %777 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.81, %777, %776, %775, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_17280.Linear = prim::GetAttr[name="dense"](%712)
  %780 : Tensor = prim::GetAttr[name="bias"](%779)
  %781 : Tensor = prim::GetAttr[name="weight"](%779)
  %782 : Float(256:1, 1024:256) = aten::t(%781), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.8, %782), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %780, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %786 : __torch__.torch.nn.modules.normalization.___torch_mangle_17283.LayerNorm = prim::GetAttr[name="LayerNorm"](%711)
  %787 : __torch__.torch.nn.modules.linear.___torch_mangle_17282.Linear = prim::GetAttr[name="dense"](%711)
  %788 : Tensor = prim::GetAttr[name="bias"](%787)
  %789 : Tensor = prim::GetAttr[name="weight"](%787)
  %790 : Float(1024:1, 256:1024) = aten::t(%789), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.49 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.83, %790), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.84 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.49, %788, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.84, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.85 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.16, %input_tensor.8, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output # transformers/modeling_electra.py:365:0
  %795 : Tensor = prim::GetAttr[name="bias"](%786)
  %796 : Tensor = prim::GetAttr[name="weight"](%786)
  %797 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm
  %input.86 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.85, %797, %796, %795, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %799 : __torch__.transformers.modeling_electra.___torch_mangle_17302.ElectraOutput = prim::GetAttr[name="output"](%78)
  %800 : __torch__.transformers.modeling_electra.___torch_mangle_17298.ElectraIntermediate = prim::GetAttr[name="intermediate"](%78)
  %801 : __torch__.transformers.modeling_electra.___torch_mangle_17296.ElectraAttention = prim::GetAttr[name="attention"](%78)
  %802 : __torch__.transformers.modeling_electra.___torch_mangle_17295.ElectraSelfOutput = prim::GetAttr[name="output"](%801)
  %803 : __torch__.transformers.modeling_electra.___torch_mangle_17291.ElectraSelfAttention = prim::GetAttr[name="self"](%801)
  %804 : __torch__.torch.nn.modules.linear.___torch_mangle_17289.Linear = prim::GetAttr[name="value"](%803)
  %805 : __torch__.torch.nn.modules.linear.___torch_mangle_17288.Linear = prim::GetAttr[name="key"](%803)
  %806 : __torch__.torch.nn.modules.linear.___torch_mangle_17287.Linear = prim::GetAttr[name="query"](%803)
  %807 : Tensor = prim::GetAttr[name="bias"](%806)
  %808 : Tensor = prim::GetAttr[name="weight"](%806)
  %809 : Float(256:1, 256:256) = aten::t(%808), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %809), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.50, %807, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %812 : Tensor = prim::GetAttr[name="bias"](%805)
  %813 : Tensor = prim::GetAttr[name="weight"](%805)
  %814 : Float(256:1, 256:256) = aten::t(%813), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %814), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.51, %812, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %817 : Tensor = prim::GetAttr[name="bias"](%804)
  %818 : Tensor = prim::GetAttr[name="weight"](%804)
  %819 : Float(256:1, 256:256) = aten::t(%818), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %819), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.52, %817, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %822 : int = aten::size(%x.49, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %823 : int = aten::size(%x.49, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %824 : int[] = prim::ListConstruct(%822, %823, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.50 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.49, %824), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %826 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.50, %826), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %828 : int = aten::size(%x.51, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %829 : int = aten::size(%x.51, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %830 : int[] = prim::ListConstruct(%828, %829, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.52 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.51, %830), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %832 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.52, %832), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %834 : int = aten::size(%x.53, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %835 : int = aten::size(%x.53, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %836 : int[] = prim::ListConstruct(%834, %835, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.54 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.53, %836), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %838 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.54, %838), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %840 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.9, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %840), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
  %input.87 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
  %input.88 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.87, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.88, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:265:0
  %847 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %848 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.17, %847), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%848, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %850 : int = aten::size(%context_layer.18, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %851 : int = aten::size(%context_layer.18, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %852 : int[] = prim::ListConstruct(%850, %851, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %input.89 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.18, %852), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
  %854 : __torch__.torch.nn.modules.normalization.___torch_mangle_17293.LayerNorm = prim::GetAttr[name="LayerNorm"](%802)
  %855 : __torch__.torch.nn.modules.linear.___torch_mangle_17292.Linear = prim::GetAttr[name="dense"](%802)
  %856 : Tensor = prim::GetAttr[name="bias"](%855)
  %857 : Tensor = prim::GetAttr[name="weight"](%855)
  %858 : Float(256:1, 256:256) = aten::t(%857), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.89, %858), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.53, %856, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.90, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.91 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.17, %input.86, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
  %863 : Tensor = prim::GetAttr[name="bias"](%854)
  %864 : Tensor = prim::GetAttr[name="weight"](%854)
  %865 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.91, %865, %864, %863, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %867 : __torch__.torch.nn.modules.linear.___torch_mangle_17297.Linear = prim::GetAttr[name="dense"](%800)
  %868 : Tensor = prim::GetAttr[name="bias"](%867)
  %869 : Tensor = prim::GetAttr[name="weight"](%867)
  %870 : Float(256:1, 1024:256) = aten::t(%869), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.9, %870), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %868, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %874 : __torch__.torch.nn.modules.normalization.___torch_mangle_17300.LayerNorm = prim::GetAttr[name="LayerNorm"](%799)
  %875 : __torch__.torch.nn.modules.linear.___torch_mangle_17299.Linear = prim::GetAttr[name="dense"](%799)
  %876 : Tensor = prim::GetAttr[name="bias"](%875)
  %877 : Tensor = prim::GetAttr[name="weight"](%875)
  %878 : Float(1024:1, 256:1024) = aten::t(%877), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.93, %878), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.94 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.55, %876, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.94, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.95 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.18, %input_tensor.9, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output # transformers/modeling_electra.py:365:0
  %883 : Tensor = prim::GetAttr[name="bias"](%874)
  %884 : Tensor = prim::GetAttr[name="weight"](%874)
  %885 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm
  %input.96 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.95, %885, %884, %883, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %887 : __torch__.transformers.modeling_electra.___torch_mangle_17319.ElectraOutput = prim::GetAttr[name="output"](%76)
  %888 : __torch__.transformers.modeling_electra.___torch_mangle_17315.ElectraIntermediate = prim::GetAttr[name="intermediate"](%76)
  %889 : __torch__.transformers.modeling_electra.___torch_mangle_17313.ElectraAttention = prim::GetAttr[name="attention"](%76)
  %890 : __torch__.transformers.modeling_electra.___torch_mangle_17312.ElectraSelfOutput = prim::GetAttr[name="output"](%889)
  %891 : __torch__.transformers.modeling_electra.___torch_mangle_17308.ElectraSelfAttention = prim::GetAttr[name="self"](%889)
  %892 : __torch__.torch.nn.modules.linear.___torch_mangle_17306.Linear = prim::GetAttr[name="value"](%891)
  %893 : __torch__.torch.nn.modules.linear.___torch_mangle_17305.Linear = prim::GetAttr[name="key"](%891)
  %894 : __torch__.torch.nn.modules.linear.___torch_mangle_17304.Linear = prim::GetAttr[name="query"](%891)
  %895 : Tensor = prim::GetAttr[name="bias"](%894)
  %896 : Tensor = prim::GetAttr[name="weight"](%894)
  %897 : Float(256:1, 256:256) = aten::t(%896), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %897), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.56, %895, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %900 : Tensor = prim::GetAttr[name="bias"](%893)
  %901 : Tensor = prim::GetAttr[name="weight"](%893)
  %902 : Float(256:1, 256:256) = aten::t(%901), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %902), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.57, %900, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %905 : Tensor = prim::GetAttr[name="bias"](%892)
  %906 : Tensor = prim::GetAttr[name="weight"](%892)
  %907 : Float(256:1, 256:256) = aten::t(%906), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %907), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.58, %905, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %910 : int = aten::size(%x.55, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %911 : int = aten::size(%x.55, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %912 : int[] = prim::ListConstruct(%910, %911, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.56 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.55, %912), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %914 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.56, %914), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %916 : int = aten::size(%x.57, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %917 : int = aten::size(%x.57, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %918 : int[] = prim::ListConstruct(%916, %917, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.58 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.57, %918), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %920 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.58, %920), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %922 : int = aten::size(%x.59, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %923 : int = aten::size(%x.59, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %924 : int[] = prim::ListConstruct(%922, %923, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.60 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.59, %924), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %926 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.60, %926), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %928 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.10, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %928), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
  %input.97 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
  %input.98 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.97, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.98, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:265:0
  %935 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %936 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.19, %935), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%936, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %938 : int = aten::size(%context_layer.20, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %939 : int = aten::size(%context_layer.20, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %940 : int[] = prim::ListConstruct(%938, %939, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %input.99 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.20, %940), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
  %942 : __torch__.torch.nn.modules.normalization.___torch_mangle_17310.LayerNorm = prim::GetAttr[name="LayerNorm"](%890)
  %943 : __torch__.torch.nn.modules.linear.___torch_mangle_17309.Linear = prim::GetAttr[name="dense"](%890)
  %944 : Tensor = prim::GetAttr[name="bias"](%943)
  %945 : Tensor = prim::GetAttr[name="weight"](%943)
  %946 : Float(256:1, 256:256) = aten::t(%945), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.99, %946), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.59, %944, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.100, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.19, %input.96, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
  %951 : Tensor = prim::GetAttr[name="bias"](%942)
  %952 : Tensor = prim::GetAttr[name="weight"](%942)
  %953 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.101, %953, %952, %951, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %955 : __torch__.torch.nn.modules.linear.___torch_mangle_17314.Linear = prim::GetAttr[name="dense"](%888)
  %956 : Tensor = prim::GetAttr[name="bias"](%955)
  %957 : Tensor = prim::GetAttr[name="weight"](%955)
  %958 : Float(256:1, 1024:256) = aten::t(%957), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.10, %958), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %956, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %962 : __torch__.torch.nn.modules.normalization.___torch_mangle_17317.LayerNorm = prim::GetAttr[name="LayerNorm"](%887)
  %963 : __torch__.torch.nn.modules.linear.___torch_mangle_17316.Linear = prim::GetAttr[name="dense"](%887)
  %964 : Tensor = prim::GetAttr[name="bias"](%963)
  %965 : Tensor = prim::GetAttr[name="weight"](%963)
  %966 : Float(1024:1, 256:1024) = aten::t(%965), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.61 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.103, %966), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.104 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.61, %964, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.104, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.20, %input_tensor.10, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output # transformers/modeling_electra.py:365:0
  %971 : Tensor = prim::GetAttr[name="bias"](%962)
  %972 : Tensor = prim::GetAttr[name="weight"](%962)
  %973 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm
  %input.106 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.105, %973, %972, %971, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %975 : __torch__.transformers.modeling_electra.___torch_mangle_17336.ElectraOutput = prim::GetAttr[name="output"](%74)
  %976 : __torch__.transformers.modeling_electra.___torch_mangle_17332.ElectraIntermediate = prim::GetAttr[name="intermediate"](%74)
  %977 : __torch__.transformers.modeling_electra.___torch_mangle_17330.ElectraAttention = prim::GetAttr[name="attention"](%74)
  %978 : __torch__.transformers.modeling_electra.___torch_mangle_17329.ElectraSelfOutput = prim::GetAttr[name="output"](%977)
  %979 : __torch__.transformers.modeling_electra.___torch_mangle_17325.ElectraSelfAttention = prim::GetAttr[name="self"](%977)
  %980 : __torch__.torch.nn.modules.linear.___torch_mangle_17323.Linear = prim::GetAttr[name="value"](%979)
  %981 : __torch__.torch.nn.modules.linear.___torch_mangle_17322.Linear = prim::GetAttr[name="key"](%979)
  %982 : __torch__.torch.nn.modules.linear.___torch_mangle_17321.Linear = prim::GetAttr[name="query"](%979)
  %983 : Tensor = prim::GetAttr[name="bias"](%982)
  %984 : Tensor = prim::GetAttr[name="weight"](%982)
  %985 : Float(256:1, 256:256) = aten::t(%984), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.62 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %985), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.62, %983, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %988 : Tensor = prim::GetAttr[name="bias"](%981)
  %989 : Tensor = prim::GetAttr[name="weight"](%981)
  %990 : Float(256:1, 256:256) = aten::t(%989), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.63 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %990), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.63, %988, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %993 : Tensor = prim::GetAttr[name="bias"](%980)
  %994 : Tensor = prim::GetAttr[name="weight"](%980)
  %995 : Float(256:1, 256:256) = aten::t(%994), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.64 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %995), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.64, %993, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %998 : int = aten::size(%x.61, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %999 : int = aten::size(%x.61, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1000 : int[] = prim::ListConstruct(%998, %999, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.62 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.61, %1000), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1002 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.62, %1002), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1004 : int = aten::size(%x.63, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1005 : int = aten::size(%x.63, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1006 : int[] = prim::ListConstruct(%1004, %1005, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.64 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.63, %1006), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1008 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.64, %1008), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1010 : int = aten::size(%x.65, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1011 : int = aten::size(%x.65, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1012 : int[] = prim::ListConstruct(%1010, %1011, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.66 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.65, %1012), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1014 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.66, %1014), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1016 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.11, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1016), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
  %input.107 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
  %input.108 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.107, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.108, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:265:0
  %1023 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %1024 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.21, %1023), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1024, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %1026 : int = aten::size(%context_layer.22, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1027 : int = aten::size(%context_layer.22, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1028 : int[] = prim::ListConstruct(%1026, %1027, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %input.109 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.22, %1028), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
  %1030 : __torch__.torch.nn.modules.normalization.___torch_mangle_17327.LayerNorm = prim::GetAttr[name="LayerNorm"](%978)
  %1031 : __torch__.torch.nn.modules.linear.___torch_mangle_17326.Linear = prim::GetAttr[name="dense"](%978)
  %1032 : Tensor = prim::GetAttr[name="bias"](%1031)
  %1033 : Tensor = prim::GetAttr[name="weight"](%1031)
  %1034 : Float(256:1, 256:256) = aten::t(%1033), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.109, %1034), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.65, %1032, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.110, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.111 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.21, %input.106, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
  %1039 : Tensor = prim::GetAttr[name="bias"](%1030)
  %1040 : Tensor = prim::GetAttr[name="weight"](%1030)
  %1041 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.111, %1041, %1040, %1039, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1043 : __torch__.torch.nn.modules.linear.___torch_mangle_17331.Linear = prim::GetAttr[name="dense"](%976)
  %1044 : Tensor = prim::GetAttr[name="bias"](%1043)
  %1045 : Tensor = prim::GetAttr[name="weight"](%1043)
  %1046 : Float(256:1, 1024:256) = aten::t(%1045), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.11, %1046), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1044, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.112), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1050 : __torch__.torch.nn.modules.normalization.___torch_mangle_17334.LayerNorm = prim::GetAttr[name="LayerNorm"](%975)
  %1051 : __torch__.torch.nn.modules.linear.___torch_mangle_17333.Linear = prim::GetAttr[name="dense"](%975)
  %1052 : Tensor = prim::GetAttr[name="bias"](%1051)
  %1053 : Tensor = prim::GetAttr[name="weight"](%1051)
  %1054 : Float(1024:1, 256:1024) = aten::t(%1053), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.67 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.113, %1054), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.67, %1052, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.114, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.22, %input_tensor.11, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output # transformers/modeling_electra.py:365:0
  %1059 : Tensor = prim::GetAttr[name="bias"](%1050)
  %1060 : Tensor = prim::GetAttr[name="weight"](%1050)
  %1061 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm
  %input.116 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.115, %1061, %1060, %1059, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1063 : __torch__.transformers.modeling_electra.___torch_mangle_17353.ElectraOutput = prim::GetAttr[name="output"](%72)
  %1064 : __torch__.transformers.modeling_electra.___torch_mangle_17349.ElectraIntermediate = prim::GetAttr[name="intermediate"](%72)
  %1065 : __torch__.transformers.modeling_electra.___torch_mangle_17347.ElectraAttention = prim::GetAttr[name="attention"](%72)
  %1066 : __torch__.transformers.modeling_electra.___torch_mangle_17346.ElectraSelfOutput = prim::GetAttr[name="output"](%1065)
  %1067 : __torch__.transformers.modeling_electra.___torch_mangle_17342.ElectraSelfAttention = prim::GetAttr[name="self"](%1065)
  %1068 : __torch__.torch.nn.modules.linear.___torch_mangle_17340.Linear = prim::GetAttr[name="value"](%1067)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_17339.Linear = prim::GetAttr[name="key"](%1067)
  %1070 : __torch__.torch.nn.modules.linear.___torch_mangle_17338.Linear = prim::GetAttr[name="query"](%1067)
  %1071 : Tensor = prim::GetAttr[name="bias"](%1070)
  %1072 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1073 : Float(256:1, 256:256) = aten::t(%1072), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.68 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1073), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.68, %1071, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1076 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1077 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1078 : Float(256:1, 256:256) = aten::t(%1077), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.69 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1078), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.69, %1076, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1081 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1082 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1083 : Float(256:1, 256:256) = aten::t(%1082), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.70 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1083), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.70, %1081, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1086 : int = aten::size(%x.67, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1087 : int = aten::size(%x.67, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1088 : int[] = prim::ListConstruct(%1086, %1087, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.68 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.67, %1088), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1090 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %query_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.68, %1090), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1092 : int = aten::size(%x.69, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1093 : int = aten::size(%x.69, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1094 : int[] = prim::ListConstruct(%1092, %1093, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.70 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.69, %1094), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1096 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %key_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.70, %1096), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1098 : int = aten::size(%x.71, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1099 : int = aten::size(%x.71, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1100 : int[] = prim::ListConstruct(%1098, %1099, %28, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.71, %1100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1102 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %value_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x, %1102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1104 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer, %14, %12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %1104), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
  %input.117 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
  %input.118 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.117, %14, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.118, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:265:0
  %1111 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %1112 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.23, %1111), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %context_layer : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1112, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %1114 : int = aten::size(%context_layer, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1115 : int = aten::size(%context_layer, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1116 : int[] = prim::ListConstruct(%1114, %1115, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %input.119 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer, %1116), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
  %1118 : __torch__.torch.nn.modules.normalization.___torch_mangle_17344.LayerNorm = prim::GetAttr[name="LayerNorm"](%1066)
  %1119 : __torch__.torch.nn.modules.linear.___torch_mangle_17343.Linear = prim::GetAttr[name="dense"](%1066)
  %1120 : Tensor = prim::GetAttr[name="bias"](%1119)
  %1121 : Tensor = prim::GetAttr[name="weight"](%1119)
  %1122 : Float(256:1, 256:256) = aten::t(%1121), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.119, %1122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.71, %1120, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.120, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.121 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.23, %input.116, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
  %1127 : Tensor = prim::GetAttr[name="bias"](%1118)
  %1128 : Tensor = prim::GetAttr[name="weight"](%1118)
  %1129 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.121, %1129, %1128, %1127, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1131 : __torch__.torch.nn.modules.linear.___torch_mangle_17348.Linear = prim::GetAttr[name="dense"](%1064)
  %1132 : Tensor = prim::GetAttr[name="bias"](%1131)
  %1133 : Tensor = prim::GetAttr[name="weight"](%1131)
  %1134 : Float(256:1, 1024:256) = aten::t(%1133), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor, %1134), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1132, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1138 : __torch__.torch.nn.modules.normalization.___torch_mangle_17351.LayerNorm = prim::GetAttr[name="LayerNorm"](%1063)
  %1139 : __torch__.torch.nn.modules.linear.___torch_mangle_17350.Linear = prim::GetAttr[name="dense"](%1063)
  %1140 : Tensor = prim::GetAttr[name="bias"](%1139)
  %1141 : Tensor = prim::GetAttr[name="weight"](%1139)
  %1142 : Float(1024:1, 256:1024) = aten::t(%1141), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.123, %1142), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.124 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.73, %1140, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.124, %18, %26), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.125 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states, %input_tensor, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output # transformers/modeling_electra.py:365:0
  %1147 : Tensor = prim::GetAttr[name="bias"](%1138)
  %1148 : Tensor = prim::GetAttr[name="weight"](%1138)
  %1149 : int[] = prim::ListConstruct(%10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm
  %input.126 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.125, %1149, %1148, %1147, %16, %15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1151 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1152 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.126, %1152, %1151), scope: __module.dropout # torch/nn/functional.py:973:0
  %1154 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1155 : Tensor = prim::GetAttr[name="bias"](%3)
  %1156 : Tensor = prim::GetAttr[name="weight"](%3)
  %1157 : Float(256:1, 2:256) = aten::t(%1156), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1157), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1159 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1155, %1154), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1159)
  return (%9)
