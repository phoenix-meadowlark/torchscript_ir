MobileBertModel(
  (embeddings): MobileBertEmbeddings(
    (word_embeddings): Embedding(30522, 128, padding_idx=0)
    (position_embeddings): Embedding(512, 512)
    (token_type_embeddings): Embedding(2, 512)
    (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)
    (LayerNorm): NoNorm()
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): MobileBertEncoder(
    (layer): ModuleList(
      (0): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (1): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (2): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (3): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (4): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (5): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (6): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (7): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (8): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (9): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (10): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (11): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (12): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (13): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (14): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (15): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (16): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (17): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (18): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (19): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (20): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (21): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (22): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
      (23): MobileBertLayer(
        (attention): MobileBertAttention(
          (self): MobileBertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): MobileBertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (intermediate): MobileBertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): MobileBertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): NoNorm()
          (bottleneck): OutputBottleneck(
            (dense): Linear(in_features=128, out_features=512, bias=True)
            (LayerNorm): NoNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (bottleneck): Bottleneck(
          (input): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
          (attention): BottleneckLayer(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): NoNorm()
          )
        )
        (ffn): ModuleList(
          (0): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (1): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
          (2): FFNLayer(
            (intermediate): MobileBertIntermediate(
              (dense): Linear(in_features=128, out_features=512, bias=True)
            )
            (output): FFNOutput(
              (dense): Linear(in_features=512, out_features=128, bias=True)
              (LayerNorm): NoNorm()
            )
          )
        )
      )
    )
  )
  (pooler): MobileBertPooler(
    (dense): Linear(in_features=512, out_features=512, bias=True)
  )
)

MobileBertModel._actual_script_module
MobileBertModel.forward
  graph(%self.1 : __torch__.transformers.modeling_mobilebert.MobileBertModel,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %11912 : __torch__.transformers.modeling_mobilebert.MobileBertPooler = prim::GetAttr[name="pooler"](%self.1)
    %11908 : __torch__.transformers.modeling_mobilebert.MobileBertEncoder = prim::GetAttr[name="encoder"](%self.1)
    %9651 : __torch__.transformers.modeling_mobilebert.MobileBertEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
    %3300 : int = prim::Constant[value=0]() # transformers/modeling_mobilebert.py:871:0
    %3301 : int = aten::size(%input_ids, %3300) # transformers/modeling_mobilebert.py:871:0
    %3302 : Long() = prim::NumToTensor(%3301)
    %3306 : int = aten::Int(%3302)
    %3303 : int = prim::Constant[value=1]() # transformers/modeling_mobilebert.py:871:0
    %3304 : int = aten::size(%input_ids, %3303) # transformers/modeling_mobilebert.py:871:0
    %3305 : Long() = prim::NumToTensor(%3304)
    %3307 : int = aten::Int(%3305)
    %3308 : int[] = prim::ListConstruct(%3306, %3307)
    %3309 : int = prim::Constant[value=4]() # transformers/modeling_mobilebert.py:882:0
    %3310 : int = prim::Constant[value=0]() # transformers/modeling_mobilebert.py:882:0
    %3311 : Device = prim::Constant[value="cpu"]() # transformers/modeling_mobilebert.py:882:0
    %3312 : bool = prim::Constant[value=0]() # transformers/modeling_mobilebert.py:882:0
    %input.5 : Long(17:13, 13:1) = aten::zeros(%3308, %3309, %3310, %3311, %3312) # transformers/modeling_mobilebert.py:882:0
    %3314 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %3315 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %3316 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
    %3317 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %3318 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %3314, %3315, %3316, %3317) # transformers/modeling_utils.py:244:0
    %3319 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %3320 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%3318, %3319) # transformers/modeling_utils.py:244:0
    %3321 : int = prim::Constant[value=2]() # transformers/modeling_utils.py:244:0
    %3322 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3320, %3321) # transformers/modeling_utils.py:244:0
    %3323 : int = prim::Constant[value=3]() # transformers/modeling_utils.py:244:0
    %3324 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %3325 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
    %3326 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%3322, %3323, %3324, %3325, %3326) # transformers/modeling_utils.py:244:0
    %3328 : int = prim::Constant[value=6]() # transformers/modeling_utils.py:257:0
    %3329 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
    %3330 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
    %3331 : None = prim::Constant()
    %3332 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %3328, %3329, %3330, %3331) # transformers/modeling_utils.py:257:0
    %3333 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
    %3334 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
    %3335 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%3332, %3333, %3334) # torch/tensor.py:396:0
    %3336 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_utils.py:258:0
    %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%3335, %3336) # transformers/modeling_utils.py:258:0
    %13099 : Tensor = prim::CallMethod[name="forward"](%9651, %input_ids, %input.5)
    %13100 : Tensor = prim::CallMethod[name="forward"](%11908, %13099, %attention_mask)
    %13101 : Tensor = prim::CallMethod[name="forward"](%11912, %13100)
    %8570 : (Float(17:6656, 13:512, 512:1), Float(17:512, 512:1)) = prim::TupleConstruct(%13100, %13101)
    return (%8570)

MobileBertModel.embeddings
MobileBertEmbeddings._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_mobilebert.MobileBertEmbeddings,
        %input_ids : Long(17:13, 13:1),
        %input.5 : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.2)
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.2)
    %3 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="token_type_embeddings"](%self.2)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.2)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="embedding_transformation"](%self.2)
    %6 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.2)
    %7 : Tensor = prim::GetAttr[name="position_ids"](%self.2)
    %12 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:185:0
    %13 : int = aten::size(%input_ids, %12), scope: __module.embeddings # transformers/modeling_mobilebert.py:185:0
    %seq_length : Long() = prim::NumToTensor(%13), scope: __module.embeddings
    %15 : int = aten::Int(%seq_length), scope: __module.embeddings
    %16 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %17 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %18 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %19 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %20 : Long(1:512, 512:1) = aten::slice(%7, %16, %17, %18, %19), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %21 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %22 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %23 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %input.4 : Long(1:512, 13:1) = aten::slice(%20, %21, %22, %15, %23), scope: __module.embeddings # transformers/modeling_mobilebert.py:192:0
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %input_ids)
    %26 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %27 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %28 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %29 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %30 : Float(17:1664, 13:128, 128:1) = aten::slice(%77, %26, %27, %28, %29), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %31 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %32 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %33 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %34 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %input.1 : Float(17:1664, 12:128, 128:1) = aten::slice(%30, %31, %32, %33, %34), scope: __module.embeddings # transformers/modeling_mobilebert.py:209:0
    %36 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %37 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %38 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %39 : int = prim::Constant[value=1](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %40 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %41 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %42 : int[] = prim::ListConstruct(%36, %37, %38, %39, %40, %41), scope: __module.embeddings
    %43 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %44 : Float(17:1664, 13:128, 128:1) = aten::constant_pad_nd(%input.1, %42, %43), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %45 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %46 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %47 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %48 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %49 : Float(17:1664, 13:128, 128:1) = aten::slice(%77, %45, %46, %47, %48), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %50 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %51 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %52 : int = prim::Constant[value=-1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %53 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %input.2 : Float(17:1664, 12:128, 128:1) = aten::slice(%49, %50, %51, %52, %53), scope: __module.embeddings # transformers/modeling_mobilebert.py:211:0
    %55 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %56 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %57 : int = prim::Constant[value=1](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %58 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %59 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %60 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %61 : int[] = prim::ListConstruct(%55, %56, %57, %58, %59, %60), scope: __module.embeddings
    %62 : int = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %63 : Float(17:1664, 13:128, 128:1) = aten::constant_pad_nd(%input.2, %61, %62), scope: __module.embeddings # torch/nn/functional.py:3552:0
    %64 : Tensor[] = prim::ListConstruct(%44, %77, %63), scope: __module.embeddings
    %65 : int = prim::Constant[value=2](), scope: __module.embeddings # transformers/modeling_mobilebert.py:207:0
    %input.3 : Float(17:4992, 13:384, 384:1) = aten::cat(%64, %65), scope: __module.embeddings # transformers/modeling_mobilebert.py:207:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %input.3)
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %input.4)
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.5)
    %71 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:222:0
    %72 : Float(17:6656, 13:512, 512:1) = aten::add(%78, %79, %71), scope: __module.embeddings # transformers/modeling_mobilebert.py:222:0
    %73 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_mobilebert.py:222:0
    %input_tensor.1 : Float(17:6656, 13:512, 512:1) = aten::add(%72, %80, %73), scope: __module.embeddings # transformers/modeling_mobilebert.py:222:0
    %81 : Tensor = prim::CallMethod[name="forward"](%2, %input_tensor.1)
    %82 : Tensor = prim::CallMethod[name="forward"](%1, %81)
    return (%82)

MobileBertModel.encoder
MobileBertEncoder._actual_script_module
  graph(%self.9 : __torch__.transformers.modeling_mobilebert.MobileBertEncoder,
        %50 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %2 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="23"](%1)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="22"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="21"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %8 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="20"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %10 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="19"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %12 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="18"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %14 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="17"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %16 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="16"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %18 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="15"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %20 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="14"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %22 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="13"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %24 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="12"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %26 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="11"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %28 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="10"](%27)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %30 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="9"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %32 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="8"](%31)
    %33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %34 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="7"](%33)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %36 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="6"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %38 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="5"](%37)
    %39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %40 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="4"](%39)
    %41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %42 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="3"](%41)
    %43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %44 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="2"](%43)
    %45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %46 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="1"](%45)
    %47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.9)
    %48 : __torch__.transformers.modeling_mobilebert.MobileBertLayer = prim::GetAttr[name="0"](%47)
    %75 : Tensor = prim::CallMethod[name="forward"](%48, %50, %attention_mask)
    %76 : Tensor = prim::CallMethod[name="forward"](%46, %75, %attention_mask)
    %77 : Tensor = prim::CallMethod[name="forward"](%44, %76, %attention_mask)
    %78 : Tensor = prim::CallMethod[name="forward"](%42, %77, %attention_mask)
    %79 : Tensor = prim::CallMethod[name="forward"](%40, %78, %attention_mask)
    %80 : Tensor = prim::CallMethod[name="forward"](%38, %79, %attention_mask)
    %81 : Tensor = prim::CallMethod[name="forward"](%36, %80, %attention_mask)
    %82 : Tensor = prim::CallMethod[name="forward"](%34, %81, %attention_mask)
    %83 : Tensor = prim::CallMethod[name="forward"](%32, %82, %attention_mask)
    %84 : Tensor = prim::CallMethod[name="forward"](%30, %83, %attention_mask)
    %85 : Tensor = prim::CallMethod[name="forward"](%28, %84, %attention_mask)
    %86 : Tensor = prim::CallMethod[name="forward"](%26, %85, %attention_mask)
    %87 : Tensor = prim::CallMethod[name="forward"](%24, %86, %attention_mask)
    %88 : Tensor = prim::CallMethod[name="forward"](%22, %87, %attention_mask)
    %89 : Tensor = prim::CallMethod[name="forward"](%20, %88, %attention_mask)
    %90 : Tensor = prim::CallMethod[name="forward"](%18, %89, %attention_mask)
    %91 : Tensor = prim::CallMethod[name="forward"](%16, %90, %attention_mask)
    %92 : Tensor = prim::CallMethod[name="forward"](%14, %91, %attention_mask)
    %93 : Tensor = prim::CallMethod[name="forward"](%12, %92, %attention_mask)
    %94 : Tensor = prim::CallMethod[name="forward"](%10, %93, %attention_mask)
    %95 : Tensor = prim::CallMethod[name="forward"](%8, %94, %attention_mask)
    %96 : Tensor = prim::CallMethod[name="forward"](%6, %95, %attention_mask)
    %97 : Tensor = prim::CallMethod[name="forward"](%4, %96, %attention_mask)
    %98 : Tensor = prim::CallMethod[name="forward"](%2, %97, %attention_mask)
    return (%98)

MobileBertModel.pooler
MobileBertPooler._actual_script_module
  graph(%self.1066 : __torch__.transformers.modeling_mobilebert.MobileBertPooler,
        %7 : Float(17:6656, 13:512, 512:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1066)
    %2 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %3 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %4 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %5 : int = prim::Constant[value=1](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %6 : Float(17:6656, 13:512, 512:1) = aten::slice(%7, %2, %3, %4, %5), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %8 : int = prim::Constant[value=1](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %9 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %input : Float(17:6656, 512:1) = aten::select(%6, %8, %9), scope: __module.pooler # transformers/modeling_mobilebert.py:603:0
    %13 : Tensor = prim::CallMethod[name="forward"](%1, %input)
    %12 : Float(17:512, 512:1) = aten::tanh(%13), scope: __module.pooler # transformers/modeling_mobilebert.py:608:0
    return (%12)

MobileBertEmbeddings.LayerNorm
NoNorm._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.7)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.1, %3), scope: __module.embeddings/__module.embeddings.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.6 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.embeddings/__module.embeddings.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.6)

MobileBertEmbeddings.dropout
Dropout._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    %input.7 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    return (%input.7)

MobileBertEmbeddings.embedding_transformation
Linear._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.linear.Linear,
        %input.3 : Float(17:4992, 13:384, 384:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.4)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %4 : Float(384:1, 512:384) = aten::t(%3), scope: __module.embeddings/__module.embeddings.embedding_transformation # torch/nn/functional.py:1676:0
    %output.1 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.3, %4), scope: __module.embeddings/__module.embeddings.embedding_transformation # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.embedding_transformation # torch/nn/functional.py:1678:0
    %inputs_embeds : Float(17:6656, 13:512, 512:1) = aten::add_(%output.1, %2, %6), scope: __module.embeddings/__module.embeddings.embedding_transformation # torch/nn/functional.py:1678:0
    return (%inputs_embeds)

MobileBertEmbeddings.position_embeddings
Embedding._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.4 : Long(1:512, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %3 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %position_embeddings : Float(1:6656, 13:512, 512:1) = aten::embedding(%2, %input.4, %3, %4, %5), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    return (%position_embeddings)

MobileBertEmbeddings.token_type_embeddings
Embedding._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.5 : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %3 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %token_type_embeddings : Float(17:6656, 13:512, 512:1) = aten::embedding(%2, %input.5, %3, %4, %5), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    return (%token_type_embeddings)

MobileBertEmbeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %3 : int = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds.1 : Float(17:1664, 13:128, 128:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds.1)

ModuleList.*
  module had no methods with graph attrs.

MobileBertLayer._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.10)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.10)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.10)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.10)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.10)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.10)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.10)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.18 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.18)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.18)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.11)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.11)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.45 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.45)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.23 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.23)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.47 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.47)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.47)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.47)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.8 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.8)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.24 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.24)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.24)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.4 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.4)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.19 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.19)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.19)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.19)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.19)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %x.2 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %query_layer.1 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.2, %30), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %x.4 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %key_layer.1 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.4, %51), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %x.6 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %value_layer.1 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.6, %72), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.1, %74, %75), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %76), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %78), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %input.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.9, %82, %83), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.10)
    %context_layer.1 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.1, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.2 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.2, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.2, %99), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %input.11 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.2, %110), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.11)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.10 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.10, %2, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.1)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %output.5 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    %x.3 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.5, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.3)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %output.4 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    %x.1 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.4, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.1)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %output.6 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    %x.5 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.6, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.5)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.26 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.4 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.4, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.12 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.12)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %output.7 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.1 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.7, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.1)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.14 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.22 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.14, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.22)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.49 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.8 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.8, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.24 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.24)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.50 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.50)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.50)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.50)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.9 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.9)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %output.15 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    %layer_output.1 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.15, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.1)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.53 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.9 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.9, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.26 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.26)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.51)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.51)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.16 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.25 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.16, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.25)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.5 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.bottleneck/__module.encoder.layer.0.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.5)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.15 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.15)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.15)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.12 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.12)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.12)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.14 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.1 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.1)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.2 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.2 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.2, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.input/__module.encoder.layer.0.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.2)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.17)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.17)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.8 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.8)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.3 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.3 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.3, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.bottleneck/__module.encoder.layer.0.bottleneck.attention/__module.encoder.layer.0.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.3)

FFNLayer._actual_script_module
  graph(%self.27 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.27)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.27)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.28)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.14 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.14)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.30 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.30)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.30)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.5 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.5)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.intermediate/__module.encoder.layer.0.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.8 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.intermediate/__module.encoder.layer.0.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.intermediate/__module.encoder.layer.0.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.13 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.8, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.intermediate/__module.encoder.layer.0.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.13)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.32 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.5 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.5, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.15 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.15)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.9 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.2 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.9, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.0/__module.encoder.layer.0.ffn.0.output/__module.encoder.layer.0.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.2)

FFNLayer._actual_script_module
  graph(%self.33 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.33)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.33)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.34 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.34)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.17 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.17)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.36 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.36)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.36)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.6 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.6)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.35)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.35)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.intermediate/__module.encoder.layer.0.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.10 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.intermediate/__module.encoder.layer.0.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.intermediate/__module.encoder.layer.0.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.16 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.10, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.intermediate/__module.encoder.layer.0.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.16)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.38 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.6 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.6, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.18 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.18)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.11 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.3 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.11, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.1/__module.encoder.layer.0.ffn.1.output/__module.encoder.layer.0.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.3)

FFNLayer._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.39)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.39)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.40 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.40)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.20 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.20)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.42 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.42)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.42)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.7 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.7)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.41)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.41)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.intermediate/__module.encoder.layer.0.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.12 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.intermediate/__module.encoder.layer.0.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.intermediate/__module.encoder.layer.0.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.19 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.12, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.intermediate/__module.encoder.layer.0.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.19)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.44 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.7 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.7, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.21 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.21)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.13 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.4 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.13, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.ffn.2/__module.encoder.layer.0.ffn.2.output/__module.encoder.layer.0.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.4)

MobileBertLayer._actual_script_module
  graph(%self.54 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.54)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.54)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.54)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.54)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.54)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.54)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.54)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.62)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.62)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.55)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.55)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.89 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.89)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.42 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.42)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.91 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.91)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.91)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.91)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.16 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.16)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.68 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.68)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.68)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.12 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.12)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.63 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.63)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.63)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.63)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.63)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %x.8 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %query_layer.2 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.8, %30), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %x.10 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %key_layer.2 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.10, %51), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %x.12 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %value_layer.2 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.12, %72), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.2, %74, %75), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %76), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %78), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %input.29 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.28, %82, %83), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.29)
    %context_layer.3 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.3, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.4 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.4, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.4, %99), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %input.30 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.4, %110), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.30)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.29 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.29, %2, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.2)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %output.20 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    %x.9 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.20, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.9)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %output.19 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    %x.7 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.19, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.7)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %output.21 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    %x.11 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.21, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.11)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.70 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.12 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.12, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.31 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.31)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %output.22 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.6 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.22, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.6)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.29 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.41 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.29, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.41)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.93 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.16 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.16, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.43 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.43)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.94)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.94)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.94)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.17 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.17)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %output.30 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    %layer_output.2 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.30, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.2)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.97 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.17 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.17, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.45 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.45)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.95)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.95)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.31 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.44 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.31, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.44)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.10 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.bottleneck/__module.encoder.layer.1.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.10)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.59 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.59)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.59)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.56)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.56)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.58 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.2 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.2)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.17 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.10 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.17, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.input/__module.encoder.layer.1.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.10)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.27 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.27)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.18 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.11 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.18, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.bottleneck/__module.encoder.layer.1.bottleneck.attention/__module.encoder.layer.1.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.11)

FFNLayer._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.71)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.71)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.72)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.33 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.33)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.74 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.74)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.74)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.13 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.13)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.intermediate/__module.encoder.layer.1.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.23 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.intermediate/__module.encoder.layer.1.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.intermediate/__module.encoder.layer.1.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.32 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.23, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.intermediate/__module.encoder.layer.1.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.32)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.76 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.13 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.13, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.34 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.34)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.75)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.75)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.24 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.7 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.24, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.0/__module.encoder.layer.1.ffn.0.output/__module.encoder.layer.1.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.7)

FFNLayer._actual_script_module
  graph(%self.77 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.77)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.77)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.78 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.78)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.36 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.36)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.80 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.80)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.80)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.14 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.14)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.intermediate/__module.encoder.layer.1.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.25 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.intermediate/__module.encoder.layer.1.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.intermediate/__module.encoder.layer.1.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.35 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.25, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.intermediate/__module.encoder.layer.1.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.35)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.82 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.14 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.14, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.37 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.37)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.26 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.8 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.26, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.1/__module.encoder.layer.1.ffn.1.output/__module.encoder.layer.1.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.8)

FFNLayer._actual_script_module
  graph(%self.83 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.83)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.83)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.84 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.84)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.39 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.39)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.86 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.86)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.86)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.15 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.15)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.85)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.85)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.intermediate/__module.encoder.layer.1.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.27 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.intermediate/__module.encoder.layer.1.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.intermediate/__module.encoder.layer.1.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.38 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.27, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.intermediate/__module.encoder.layer.1.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.38)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.15 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.15, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.40 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.40)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.28 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.9 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.28, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.ffn.2/__module.encoder.layer.1.ffn.2.output/__module.encoder.layer.1.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.9)

MobileBertLayer._actual_script_module
  graph(%self.98 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.98)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.98)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.98)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.98)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.98)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.98)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.98)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.106 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.106)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.106)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.99 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.99)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.99)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.133 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.133)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.61 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.61)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.135 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.135)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.135)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.135)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.24 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.24)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.112 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.112)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.112)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.20 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.20)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.107)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.107)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.107)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.107)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %x.14 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %query_layer.3 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.14, %30), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %x.16 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %key_layer.3 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.16, %51), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %x.18 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %value_layer.3 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.18, %72), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.3, %74, %75), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %76), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %78), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %82, %83), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.48)
    %context_layer.5 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.5, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.6 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.6, %95), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.6, %99), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %input.49 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.6, %110), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.49)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.48 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %2, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.3)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %output.35 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    %x.15 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.35, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.15)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %output.34 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    %x.13 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.34, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.13)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %output.36 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    %x.17 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.36, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.17)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.114 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.20 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.20, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.50 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.50)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %output.37 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.11 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.37, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.11)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.44 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.60 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.44, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.60)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.137 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.24 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.137)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.137)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.24, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.62 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.62)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.138)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.138)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.138)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.25 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.25)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %output.45 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    %layer_output.3 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.45, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.3)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.141 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.25 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.141)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.141)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.25, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.64 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.64)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.139 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.139)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.139)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.46 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.63 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.46, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.63)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.15 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.bottleneck/__module.encoder.layer.2.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.15)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.103 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.103)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.103)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.100 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.100)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.100)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.102 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.3 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.3)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.101)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.101)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.32 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.18 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.32, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.input/__module.encoder.layer.2.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.18)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.105)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.105)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.46 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.46)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.33 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.19 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.33, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.bottleneck/__module.encoder.layer.2.bottleneck.attention/__module.encoder.layer.2.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.19)

FFNLayer._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.115)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.115)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.116)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.52 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.52)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.118 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.118)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.118)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.21 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.21)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.117)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.intermediate/__module.encoder.layer.2.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.38 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.intermediate/__module.encoder.layer.2.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.intermediate/__module.encoder.layer.2.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.51 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.38, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.intermediate/__module.encoder.layer.2.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.51)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.120 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.21 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.21, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.53 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.53)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.39 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.12 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.39, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.0/__module.encoder.layer.2.ffn.0.output/__module.encoder.layer.2.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.12)

FFNLayer._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.121)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.121)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.122 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.122)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.55 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.55)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.124 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.124)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.124)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.22 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.22)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.intermediate/__module.encoder.layer.2.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.40 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.intermediate/__module.encoder.layer.2.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.intermediate/__module.encoder.layer.2.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.54 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.40, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.intermediate/__module.encoder.layer.2.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.54)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.126 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.22 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.126)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.126)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.22, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.56 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.56)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.41 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.13 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.41, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.1/__module.encoder.layer.2.ffn.1.output/__module.encoder.layer.2.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.13)

FFNLayer._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.127)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.127)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.128)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.58 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.58)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.130 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.130)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.130)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.23 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.23)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.129)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.129)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.intermediate/__module.encoder.layer.2.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.42 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.intermediate/__module.encoder.layer.2.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.intermediate/__module.encoder.layer.2.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.57 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.42, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.intermediate/__module.encoder.layer.2.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.57)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.132 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.23 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.132)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.132)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.23, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.59 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.59)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.131 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.131)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.131)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.43 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.14 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.43, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.ffn.2/__module.encoder.layer.2.ffn.2.output/__module.encoder.layer.2.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.14)

MobileBertLayer._actual_script_module
  graph(%self.142 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.142)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.142)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.142)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.142)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.142)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.142)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.142)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.150 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.150)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.150)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.143)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.143)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.177 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.177)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.80 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # torch/nn/functional.py:1119:0
    return (%input.80)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.179 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.179)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.179)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.179)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.32 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.32)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.156 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.156)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.156)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.28 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.28)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.151 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.151)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.151)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.151)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.151)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %x.20 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %query_layer.4 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.20, %30), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %x.22 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %key_layer.4 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.22, %51), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %x.24 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %value_layer.4 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.24, %72), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.4, %74, %75), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %76), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %78), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.66 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.66, %82, %83), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.67)
    %context_layer.7 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.7, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.8 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.8, %95), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.8, %99), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %input.68 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.8, %110), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.68)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.67 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.67, %2, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.4)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %output.50 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    %x.21 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.50, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.21)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.152)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.152)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %output.49 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    %x.19 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.49, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.19)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.154 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.154)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.154)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %output.51 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    %x.23 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.51, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.23)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.158 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.28 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.158)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.158)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.28, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.69 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.69)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %output.52 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.16 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.52, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.16)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %output.59 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    %input.79 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.59, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.79)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.32 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.181)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.181)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.32, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.81 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.81)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.182)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.182)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.182)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.33 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.33)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.180)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.180)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %output.60 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    %layer_output.4 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.60, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.4)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.185 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.33 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.185)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.33, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.83 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.83)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.183)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.183)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.61 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.82 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.61, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.82)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.20 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.bottleneck/__module.encoder.layer.3.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.20)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.147 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.147)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.147)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.144 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.144)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.144)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.146 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.4 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.4)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.145)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.145)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.47 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.26 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.47, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.input/__module.encoder.layer.3.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.26)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.149)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.149)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.65 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.65)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.48 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.27 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.48, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.bottleneck/__module.encoder.layer.3.bottleneck.attention/__module.encoder.layer.3.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.27)

FFNLayer._actual_script_module
  graph(%self.159 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.159)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.159)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.160)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.71 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.71)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.162 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.162)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.162)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.29 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.29)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.161)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.161)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.intermediate/__module.encoder.layer.3.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.53 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.intermediate/__module.encoder.layer.3.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.intermediate/__module.encoder.layer.3.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.70 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.53, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.intermediate/__module.encoder.layer.3.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.70)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.164 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.29 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.164)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.29, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.72 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.72)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.163 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.163)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.163)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.54 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.17 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.54, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.0/__module.encoder.layer.3.ffn.0.output/__module.encoder.layer.3.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.17)

FFNLayer._actual_script_module
  graph(%self.165 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.165)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.165)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.166 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.166)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.74 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.74)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.168 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.168)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.168)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.30 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.30)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.167)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.intermediate/__module.encoder.layer.3.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.55 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.intermediate/__module.encoder.layer.3.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.intermediate/__module.encoder.layer.3.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.73 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.55, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.intermediate/__module.encoder.layer.3.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.73)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.170 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.30 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.170)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.30, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.75 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.75)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.169 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.169)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.169)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.56 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.18 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.56, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.1/__module.encoder.layer.3.ffn.1.output/__module.encoder.layer.3.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.18)

FFNLayer._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.171)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.171)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.172 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.172)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.77 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.77)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.174 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.174)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.174)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.31 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.31)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.173)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.173)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.intermediate/__module.encoder.layer.3.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.57 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.intermediate/__module.encoder.layer.3.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.intermediate/__module.encoder.layer.3.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.76 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.57, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.intermediate/__module.encoder.layer.3.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.76)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.176 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.31 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.31, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.78 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.78)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.175 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.175)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.175)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.58 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.19 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.58, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.ffn.2/__module.encoder.layer.3.ffn.2.output/__module.encoder.layer.3.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.19)

MobileBertLayer._actual_script_module
  graph(%self.186 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.186)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.186)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.186)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.186)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.186)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.186)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.186)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.194 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.194)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.194)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.187 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.187)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.187)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.221 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.221)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.99 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # torch/nn/functional.py:1119:0
    return (%input.99)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.223 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.223)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.223)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.223)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.40 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.40)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.200 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.200)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.200)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.36 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.36)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.195 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.195)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.195)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.195)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.195)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %x.26 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %query_layer.5 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.26, %30), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %x.28 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %key_layer.5 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.28, %51), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %x.30 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %value_layer.5 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.30, %72), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.5, %74, %75), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %76), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %78), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.85 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %input.86 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.85, %82, %83), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.86)
    %context_layer.9 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.9, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.10 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.10, %95), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.10, %99), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %input.87 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.10, %110), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.87)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.86 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.86, %2, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.5)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %output.65 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    %x.27 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.65, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.27)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.196)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.196)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %output.64 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    %x.25 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.64, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.25)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.198 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.198)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.198)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %output.66 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    %x.29 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.66, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.29)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.202 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.36 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.202)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.202)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.36, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.88 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.88)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %output.67 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.21 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.67, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.21)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.222)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.222)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %output.74 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    %input.98 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.74, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.98)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.225 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.40 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.225)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.40, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.100 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.100)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.226 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.226)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.226)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.226)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.41 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.41)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.224 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.224)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.224)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %output.75 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    %layer_output.5 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.75, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.5)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.229 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.41 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.229)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.229)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.41, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.102 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.102)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.227 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.227)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.227)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.76 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.101 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.76, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.101)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.228 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.25 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.bottleneck/__module.encoder.layer.4.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.25)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.191 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.191)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.191)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.188 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.188)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.188)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.190 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.190)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.190)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.5 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.5)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.62 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.34 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.62, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.input/__module.encoder.layer.4.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.34)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.193 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.193)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.193)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.84 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.84)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.192)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.192)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.63 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.35 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.63, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.bottleneck/__module.encoder.layer.4.bottleneck.attention/__module.encoder.layer.4.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.35)

FFNLayer._actual_script_module
  graph(%self.203 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.203)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.203)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.204 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.204)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.90 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.90)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.206 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.206)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.206)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.37 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.37)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.205 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.205)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.205)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.intermediate/__module.encoder.layer.4.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.68 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.intermediate/__module.encoder.layer.4.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.intermediate/__module.encoder.layer.4.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.89 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.68, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.intermediate/__module.encoder.layer.4.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.89)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.208 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.37 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.37, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.91 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.91)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.207 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.207)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.207)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.69 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.22 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.69, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.0/__module.encoder.layer.4.ffn.0.output/__module.encoder.layer.4.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.22)

FFNLayer._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.209)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.209)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.210 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.210)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.93 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.93)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.212 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.212)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.212)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.38 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.38)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.211)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.211)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.intermediate/__module.encoder.layer.4.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.70 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.intermediate/__module.encoder.layer.4.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.intermediate/__module.encoder.layer.4.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.92 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.70, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.intermediate/__module.encoder.layer.4.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.92)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.214 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.38 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.38, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.94 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.94)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.213)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.213)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.71 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.23 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.71, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.1/__module.encoder.layer.4.ffn.1.output/__module.encoder.layer.4.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.23)

FFNLayer._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.215)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.215)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.216 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.216)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.96 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.96)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.218 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.218)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.218)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.39 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.39)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.217)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.217)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.intermediate/__module.encoder.layer.4.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.72 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.intermediate/__module.encoder.layer.4.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.intermediate/__module.encoder.layer.4.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.95 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.72, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.intermediate/__module.encoder.layer.4.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.95)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.220 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.39 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.220)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.220)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.39, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.97 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.97)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.219)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.219)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.73 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.24 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.73, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.ffn.2/__module.encoder.layer.4.ffn.2.output/__module.encoder.layer.4.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.24)

MobileBertLayer._actual_script_module
  graph(%self.230 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.230)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.230)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.230)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.230)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.230)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.230)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.230)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.238 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.238)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.238)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.231 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.231)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.231)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.265 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.265)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.118 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # torch/nn/functional.py:1119:0
    return (%input.118)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.267 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.267)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.267)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.267)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.48 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.48)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.244 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.244)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.244)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.44 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.44)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.239 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.239)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.239)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.239)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.239)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %x.32 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %query_layer.6 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.32, %30), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %x.34 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %key_layer.6 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.34, %51), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %x.36 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %value_layer.6 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.36, %72), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.6, %74, %75), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %76), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %78), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.104 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %input.105 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.104, %82, %83), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.105)
    %context_layer.11 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.11, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.12 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.12, %95), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.12, %99), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %input.106 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.12, %110), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.106)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.243 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.105 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.105, %2, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.6)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.241 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.241)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.241)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %output.80 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    %x.33 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.80, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.33)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.240 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.240)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.240)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %output.79 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    %x.31 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.79, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.31)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.242 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.242)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.242)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %output.81 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    %x.35 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.81, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.35)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.246 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.44 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.246)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.246)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.44, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.107 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.107)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.245)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.245)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %output.82 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.26 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.82, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.26)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.266 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.266)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.266)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %output.89 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    %input.117 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.89, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.117)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.269 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.48 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.269)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.269)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.48, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.119 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.119)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.270 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.270)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.270)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.270)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.49 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.49)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.268 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.268)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.268)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %output.90 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    %layer_output.6 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.90, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.6)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.273 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.49 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.273)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.273)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.49, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.121 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.121)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.271 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.271)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.271)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.91 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.120 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.91, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.120)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.272 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.30 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.bottleneck/__module.encoder.layer.5.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.30)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.235 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.235)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.235)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.232 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.232)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.232)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.234 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.234)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.234)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.6 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.6)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.233)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.233)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.77 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.42 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.77, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.input/__module.encoder.layer.5.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.42)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.237 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.237)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.237)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.103 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.103)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.236)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.236)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.78 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.43 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.78, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.bottleneck/__module.encoder.layer.5.bottleneck.attention/__module.encoder.layer.5.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.43)

FFNLayer._actual_script_module
  graph(%self.247 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.247)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.247)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.248)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.109 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.109)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.250 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.250)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.250)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.45 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.45)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.249 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.249)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.249)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.intermediate/__module.encoder.layer.5.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.83 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.intermediate/__module.encoder.layer.5.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.intermediate/__module.encoder.layer.5.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.108 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.83, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.intermediate/__module.encoder.layer.5.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.108)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.252 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.45 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.252)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.252)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.45, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.110 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.110)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.251 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.251)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.251)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.84 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.27 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.84, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.0/__module.encoder.layer.5.ffn.0.output/__module.encoder.layer.5.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.27)

FFNLayer._actual_script_module
  graph(%self.253 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.253)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.253)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.254 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.254)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.112 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.112)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.256 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.256)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.256)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.46 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.46)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.255)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.255)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.intermediate/__module.encoder.layer.5.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.85 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.intermediate/__module.encoder.layer.5.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.intermediate/__module.encoder.layer.5.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.111 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.85, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.intermediate/__module.encoder.layer.5.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.111)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.258 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.46 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.258)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.258)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.46, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.113 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.113)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.257 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.257)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.257)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.86 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.28 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.86, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.1/__module.encoder.layer.5.ffn.1.output/__module.encoder.layer.5.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.28)

FFNLayer._actual_script_module
  graph(%self.259 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.259)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.259)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.260 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.260)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.115 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.115)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.262 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.262)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.262)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.47 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.47)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.261 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.261)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.261)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.intermediate/__module.encoder.layer.5.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.87 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.intermediate/__module.encoder.layer.5.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.intermediate/__module.encoder.layer.5.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.114 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.87, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.intermediate/__module.encoder.layer.5.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.114)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.264 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.47 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.264)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.264)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.47, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.116 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.116)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.263 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.263)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.263)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.88 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.29 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.88, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.ffn.2/__module.encoder.layer.5.ffn.2.output/__module.encoder.layer.5.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.29)

MobileBertLayer._actual_script_module
  graph(%self.274 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.274)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.274)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.274)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.274)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.274)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.274)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.274)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.282 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.282)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.282)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.275 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.275)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.275)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.309 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.309)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.137 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # torch/nn/functional.py:1119:0
    return (%input.137)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.311 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.311)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.311)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.311)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.56 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.56)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.288 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.288)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.288)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.52 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.52)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.283 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.283)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.283)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.283)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.283)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %x.38 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %query_layer.7 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.38, %30), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %x.40 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %key_layer.7 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.40, %51), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %x.42 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %value_layer.7 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.42, %72), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.7, %74, %75), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %76), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %78), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.123 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %input.124 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.123, %82, %83), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.124)
    %context_layer.13 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.13, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.14 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.14, %95), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.14, %99), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %input.125 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.14, %110), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.125)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.287 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.124 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.124, %2, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.7)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.285 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.285)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.285)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %output.95 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    %x.39 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.95, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.39)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.284 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.284)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.284)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %output.94 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    %x.37 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.94, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.37)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.286 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.286)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.286)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %output.96 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    %x.41 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.96, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.41)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.290 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.52 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.290)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.290)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.52, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.126 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.126)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.289 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.289)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.289)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %output.97 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.31 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.97, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.31)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.310 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.310)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.310)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %output.104 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    %input.136 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.104, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.136)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.313 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.56 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.313)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.313)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.56, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.138 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.138)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.314 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.314)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.314)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.314)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.57 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.57)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.312 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.312)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.312)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %output.105 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    %layer_output.7 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.105, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.7)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.317 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.57 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.317)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.317)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.57, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.140 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.140)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.315 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.315)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.315)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.106 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.139 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.106, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.139)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.316 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.35 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.bottleneck/__module.encoder.layer.6.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.35)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.279 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.279)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.279)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.276 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.276)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.276)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.278 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.278)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.278)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.7 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.7)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.277 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.277)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.277)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.92 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.50 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.92, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.input/__module.encoder.layer.6.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.50)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.281 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.281)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.281)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.122 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.122)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.280 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.280)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.280)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.93 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.51 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.93, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.bottleneck/__module.encoder.layer.6.bottleneck.attention/__module.encoder.layer.6.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.51)

FFNLayer._actual_script_module
  graph(%self.291 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.291)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.291)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.292 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.292)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.128 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.128)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.294 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.294)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.294)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.53 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.53)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.293 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.293)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.293)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.intermediate/__module.encoder.layer.6.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.98 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.intermediate/__module.encoder.layer.6.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.intermediate/__module.encoder.layer.6.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.127 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.98, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.intermediate/__module.encoder.layer.6.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.127)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.296 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.53 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.296)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.296)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.53, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.129 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.129)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.295 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.295)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.295)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.99 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.32 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.99, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.0/__module.encoder.layer.6.ffn.0.output/__module.encoder.layer.6.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.32)

FFNLayer._actual_script_module
  graph(%self.297 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.297)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.297)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.298 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.298)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.131 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.131)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.300 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.300)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.300)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.54 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.54)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.299 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.299)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.299)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.intermediate/__module.encoder.layer.6.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.100 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.intermediate/__module.encoder.layer.6.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.intermediate/__module.encoder.layer.6.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.130 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.100, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.intermediate/__module.encoder.layer.6.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.130)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.302 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.54 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.302)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.302)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.54, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.132 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.132)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.301 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.301)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.301)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.101 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.33 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.101, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.1/__module.encoder.layer.6.ffn.1.output/__module.encoder.layer.6.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.33)

FFNLayer._actual_script_module
  graph(%self.303 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.303)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.303)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.304 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.304)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.134 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.134)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.306 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.306)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.306)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.55 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.55)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.305 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.305)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.305)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.intermediate/__module.encoder.layer.6.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.102 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.intermediate/__module.encoder.layer.6.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.intermediate/__module.encoder.layer.6.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.133 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.102, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.intermediate/__module.encoder.layer.6.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.133)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.308 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.55 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.308)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.308)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.55, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.135 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.135)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.307 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.307)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.307)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.103 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.34 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.103, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.ffn.2/__module.encoder.layer.6.ffn.2.output/__module.encoder.layer.6.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.34)

MobileBertLayer._actual_script_module
  graph(%self.318 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.318)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.318)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.318)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.318)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.318)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.318)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.318)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.326 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.326)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.326)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.319 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.319)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.319)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.353 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.353)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.156 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # torch/nn/functional.py:1119:0
    return (%input.156)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.355 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.355)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.355)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.355)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.64 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.64)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.332 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.332)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.332)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.60 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.60)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.327 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.327)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.327)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.327)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.327)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %x.44 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %query_layer.8 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.44, %30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %x.46 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %key_layer.8 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.46, %51), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %x.48 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %value_layer.8 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.48, %72), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.8, %74, %75), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %76), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %78), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.142 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %input.143 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.142, %82, %83), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.143)
    %context_layer.15 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.15, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.16 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.16, %95), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.16, %99), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %input.144 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.16, %110), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.144)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.331 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.143 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.143, %2, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.8)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.329 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.329)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.329)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %output.110 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    %x.45 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.110, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.45)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.328 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.328)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.328)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %output.109 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    %x.43 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.109, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.43)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.330 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.330)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.330)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %output.111 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    %x.47 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.111, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.47)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.334 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.60 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.334)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.334)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.60, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.145 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.145)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.333 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.333)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.333)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %output.112 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.36 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.112, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.36)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.354 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.354)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.354)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %output.119 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    %input.155 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.119, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.155)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.357 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.64 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.357)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.357)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.64, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.157 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.157)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.358 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.358)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.358)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.358)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.65 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.65)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.356 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.356)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.356)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %output.120 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    %layer_output.8 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.120, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.8)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.361 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.65 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.361)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.361)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.65, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.159 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.159)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.359 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.359)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.359)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.121 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.158 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.121, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.158)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.360 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.40 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.bottleneck/__module.encoder.layer.7.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.40)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.323 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.323)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.323)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.320 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.320)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.320)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.322 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.322)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.322)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.8 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.8)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.321 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.321)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.321)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.107 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.58 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.107, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.input/__module.encoder.layer.7.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.58)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.325 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.325)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.325)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.141 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.141)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.324 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.324)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.324)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.108 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.59 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.108, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.bottleneck/__module.encoder.layer.7.bottleneck.attention/__module.encoder.layer.7.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.59)

FFNLayer._actual_script_module
  graph(%self.335 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.335)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.335)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.336 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.336)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.147 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.147)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.338 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.338)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.338)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.61 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.61)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.337 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.337)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.337)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.intermediate/__module.encoder.layer.7.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.113 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.intermediate/__module.encoder.layer.7.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.intermediate/__module.encoder.layer.7.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.146 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.113, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.intermediate/__module.encoder.layer.7.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.146)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.340 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.61 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.340)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.340)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.61, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.148 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.148)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.339 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.339)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.339)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.114 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.37 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.114, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.0/__module.encoder.layer.7.ffn.0.output/__module.encoder.layer.7.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.37)

FFNLayer._actual_script_module
  graph(%self.341 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.341)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.341)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.342 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.342)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.150 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.150)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.344 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.344)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.344)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.62 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.62)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.343 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.343)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.343)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.intermediate/__module.encoder.layer.7.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.115 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.intermediate/__module.encoder.layer.7.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.intermediate/__module.encoder.layer.7.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.149 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.115, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.intermediate/__module.encoder.layer.7.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.149)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.346 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.62 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.346)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.346)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.62, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.151 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.151)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.345 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.345)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.345)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.116 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.38 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.116, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.1/__module.encoder.layer.7.ffn.1.output/__module.encoder.layer.7.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.38)

FFNLayer._actual_script_module
  graph(%self.347 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.347)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.347)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.348 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.348)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.153 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.153)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.350 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.350)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.350)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.63 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.63)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.349 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.349)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.349)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.intermediate/__module.encoder.layer.7.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.117 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.intermediate/__module.encoder.layer.7.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.intermediate/__module.encoder.layer.7.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.152 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.117, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.intermediate/__module.encoder.layer.7.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.152)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.352 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.63 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.352)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.352)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.63, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.154 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.154)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.351 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.351)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.351)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.118 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.39 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.118, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.ffn.2/__module.encoder.layer.7.ffn.2.output/__module.encoder.layer.7.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.39)

MobileBertLayer._actual_script_module
  graph(%self.362 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.362)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.362)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.362)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.362)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.362)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.362)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.362)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.370 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.370)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.370)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.363 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.363)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.363)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.397 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.397)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.175 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # torch/nn/functional.py:1119:0
    return (%input.175)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.399 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.399)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.399)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.399)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.72 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.72)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.376 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.376)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.376)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.68 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.68)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.371 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.371)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.371)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.371)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.371)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %x.50 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %query_layer.9 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.50, %30), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %x.52 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %key_layer.9 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.52, %51), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %x.54 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %value_layer.9 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.54, %72), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.9, %74, %75), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %76), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %78), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.161 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %input.162 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.161, %82, %83), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.162)
    %context_layer.17 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.17, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.18 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.18, %95), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.18, %99), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %input.163 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.18, %110), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.163)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.375 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.162 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.162, %2, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.9)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.373 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.373)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.373)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %output.125 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    %x.51 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.125, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.51)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.372 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.372)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.372)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %output.124 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    %x.49 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.124, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.49)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.374 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.374)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.374)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %output.126 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    %x.53 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.126, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.53)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.378 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.68 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.378)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.378)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.68, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.164 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.164)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.377 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.377)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.377)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %output.127 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.41 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.127, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.41)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.398 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.398)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.398)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %output.134 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    %input.174 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.134, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.174)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.401 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.72 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.401)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.401)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.72, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.176 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.176)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.402 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.402)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.402)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.402)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.73 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.73)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.400 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.400)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.400)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %output.135 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    %layer_output.9 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.135, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.9)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.405 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.73 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.405)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.405)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.73, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.178 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.178)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.403 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.403)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.403)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.136 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.177 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.136, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.177)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.404 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.45 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.bottleneck/__module.encoder.layer.8.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.45)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.367 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.367)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.367)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.364 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.364)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.364)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.366 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.366)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.366)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.9 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.9)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.365 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.365)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.365)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.122 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.66 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.122, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.input/__module.encoder.layer.8.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.66)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.369 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.369)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.369)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.160 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.160)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.368 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.368)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.368)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.123 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.67 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.123, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.bottleneck/__module.encoder.layer.8.bottleneck.attention/__module.encoder.layer.8.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.67)

FFNLayer._actual_script_module
  graph(%self.379 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.379)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.379)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.380 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.380)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.166 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.166)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.382 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.382)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.382)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.69 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.69)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.381 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.381)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.381)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.intermediate/__module.encoder.layer.8.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.128 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.intermediate/__module.encoder.layer.8.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.intermediate/__module.encoder.layer.8.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.165 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.128, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.intermediate/__module.encoder.layer.8.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.165)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.384 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.69 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.384)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.384)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.69, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.167 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.167)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.383 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.383)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.383)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.129 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.42 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.129, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.0/__module.encoder.layer.8.ffn.0.output/__module.encoder.layer.8.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.42)

FFNLayer._actual_script_module
  graph(%self.385 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.385)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.385)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.386 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.386)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.169 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.169)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.388 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.388)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.388)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.70 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.70)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.387 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.387)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.387)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.intermediate/__module.encoder.layer.8.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.130 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.intermediate/__module.encoder.layer.8.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.intermediate/__module.encoder.layer.8.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.168 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.130, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.intermediate/__module.encoder.layer.8.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.168)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.390 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.70 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.390)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.390)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.70, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.170 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.170)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.389 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.389)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.389)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.131 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.43 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.131, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.1/__module.encoder.layer.8.ffn.1.output/__module.encoder.layer.8.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.43)

FFNLayer._actual_script_module
  graph(%self.391 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.391)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.391)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.392 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.392)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.172 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.172)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.394 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.394)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.394)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.71 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.71)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.393 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.393)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.393)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.intermediate/__module.encoder.layer.8.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.132 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.intermediate/__module.encoder.layer.8.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.intermediate/__module.encoder.layer.8.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.171 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.132, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.intermediate/__module.encoder.layer.8.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.171)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.396 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.71 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.396)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.396)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.71, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.173 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.173)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.395 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.395)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.395)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.133 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.44 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.133, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.ffn.2/__module.encoder.layer.8.ffn.2.output/__module.encoder.layer.8.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.44)

MobileBertLayer._actual_script_module
  graph(%self.406 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.406)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.406)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.406)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.406)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.406)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.406)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.406)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.414 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.414)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.414)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.407 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.407)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.407)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.441 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.441)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.194 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # torch/nn/functional.py:1119:0
    return (%input.194)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.443 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.443)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.443)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.443)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.80 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.80)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.420 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.420)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.420)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.76 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.76)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.415 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.415)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.415)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.415)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.415)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %x.56 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %query_layer.10 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.56, %30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %x.58 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %key_layer.10 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.58, %51), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %x.60 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %value_layer.10 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.60, %72), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.10, %74, %75), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %76), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %78), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.180 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %input.181 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.180, %82, %83), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.181)
    %context_layer.19 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.19, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.20 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.20, %95), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.20, %99), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %input.182 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.20, %110), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.182)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.419 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.181 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.181, %2, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.10)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.417 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.417)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.417)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %output.140 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    %x.57 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.140, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.57)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.416 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.416)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.416)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %output.139 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    %x.55 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.139, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.55)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.418 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.418)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.418)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %output.141 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    %x.59 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.141, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.59)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.422 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.76 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.422)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.422)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.76, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.183 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.183)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.421 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.421)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.421)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %output.142 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.46 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.142, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.46)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.442 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.442)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.442)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %output.149 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    %input.193 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.149, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.193)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.445 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.80 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.445)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.445)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.80, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.195 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.195)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.446 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.446)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.446)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.446)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.81 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.81)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.444 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.444)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.444)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %output.150 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    %layer_output.10 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.150, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.10)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.449 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.81 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.449)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.449)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.81, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.197 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.197)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.447 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.447)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.447)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.151 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.196 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.151, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.196)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.448 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.50 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.bottleneck/__module.encoder.layer.9.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.50)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.411 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.411)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.411)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.408 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.408)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.408)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.410 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.410)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.410)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.10 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.10)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.409 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.409)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.409)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.137 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.74 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.137, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.input/__module.encoder.layer.9.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.74)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.413 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.413)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.413)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.179 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.179)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.412 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.412)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.412)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.138 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.75 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.138, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.bottleneck/__module.encoder.layer.9.bottleneck.attention/__module.encoder.layer.9.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.75)

FFNLayer._actual_script_module
  graph(%self.423 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.423)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.423)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.424 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.424)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.185 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.185)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.426 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.426)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.426)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.77 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.77)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.425 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.425)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.425)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.intermediate/__module.encoder.layer.9.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.143 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.intermediate/__module.encoder.layer.9.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.intermediate/__module.encoder.layer.9.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.184 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.143, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.intermediate/__module.encoder.layer.9.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.184)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.428 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.77 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.428)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.428)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.77, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.186 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.186)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.427 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.427)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.427)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.144 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.47 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.144, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.0/__module.encoder.layer.9.ffn.0.output/__module.encoder.layer.9.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.47)

FFNLayer._actual_script_module
  graph(%self.429 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.429)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.429)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.430 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.430)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.188 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.188)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.432 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.432)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.432)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.78 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.78)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.431 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.431)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.431)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.intermediate/__module.encoder.layer.9.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.145 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.intermediate/__module.encoder.layer.9.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.intermediate/__module.encoder.layer.9.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.187 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.145, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.intermediate/__module.encoder.layer.9.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.187)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.434 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.78 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.434)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.434)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.78, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.189 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.189)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.433 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.433)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.433)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.146 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.48 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.146, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.1/__module.encoder.layer.9.ffn.1.output/__module.encoder.layer.9.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.48)

FFNLayer._actual_script_module
  graph(%self.435 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.435)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.435)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.436 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.436)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.191 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.191)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.438 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.438)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.438)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.79 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.79)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.437 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.437)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.437)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.intermediate/__module.encoder.layer.9.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.147 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.intermediate/__module.encoder.layer.9.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.intermediate/__module.encoder.layer.9.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.190 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.147, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.intermediate/__module.encoder.layer.9.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.190)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.440 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.79 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.440)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.440)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.79, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.192 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.192)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.439 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.439)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.439)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.148 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.49 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.148, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.ffn.2/__module.encoder.layer.9.ffn.2.output/__module.encoder.layer.9.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.49)

MobileBertLayer._actual_script_module
  graph(%self.450 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.450)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.450)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.450)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.450)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.450)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.450)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.450)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.458 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.458)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.458)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.451 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.451)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.451)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.485 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.485)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.213 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # torch/nn/functional.py:1119:0
    return (%input.213)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.487 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.487)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.487)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.487)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.88 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.88)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.464 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.464)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.464)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.84 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.84)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.459 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.459)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.459)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.459)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.459)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %x.62 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %query_layer.11 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.62, %30), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %x.64 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %key_layer.11 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.64, %51), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %x.66 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %value_layer.11 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.66, %72), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.11, %74, %75), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %76), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %78), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.199 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %input.200 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.199, %82, %83), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.200)
    %context_layer.21 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.21, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.22 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.22, %95), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.22, %99), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %input.201 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.22, %110), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.201)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.463 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.200 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.200, %2, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.11)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.461 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.461)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.461)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %output.155 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    %x.63 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.155, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.63)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.460 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.460)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.460)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %output.154 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    %x.61 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.154, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.61)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.462 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.462)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.462)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %output.156 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    %x.65 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.156, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.65)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.466 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.84 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.466)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.466)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.84, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.202 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.202)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.465 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.465)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.465)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %output.157 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.51 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.157, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.51)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.486 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.486)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.486)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %output.164 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    %input.212 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.164, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.212)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.489 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.88 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.489)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.489)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.88, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.214 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.214)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.490 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.490)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.490)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.490)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.89 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.89)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.488 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.488)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.488)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %output.165 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    %layer_output.11 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.165, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.11)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.493 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.89 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.493)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.493)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.89, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.216 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.216)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.491 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.491)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.491)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.166 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.215 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.166, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.215)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.492 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.55 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.bottleneck/__module.encoder.layer.10.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.55)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.455 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.455)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.455)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.452 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.452)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.452)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.454 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.454)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.454)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.11 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.11)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.453 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.453)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.453)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.152 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.82 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.152, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.input/__module.encoder.layer.10.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.82)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.457 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.457)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.457)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.198 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.198)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.456 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.456)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.456)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.153 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.83 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.153, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.bottleneck/__module.encoder.layer.10.bottleneck.attention/__module.encoder.layer.10.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.83)

FFNLayer._actual_script_module
  graph(%self.467 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.467)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.467)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.468 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.468)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.204 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.204)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.470 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.470)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.470)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.85 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.85)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.469 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.469)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.469)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.intermediate/__module.encoder.layer.10.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.158 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.intermediate/__module.encoder.layer.10.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.intermediate/__module.encoder.layer.10.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.203 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.158, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.intermediate/__module.encoder.layer.10.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.203)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.472 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.85 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.472)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.472)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.85, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.205 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.205)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.471 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.471)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.471)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.159 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.52 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.159, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.0/__module.encoder.layer.10.ffn.0.output/__module.encoder.layer.10.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.52)

FFNLayer._actual_script_module
  graph(%self.473 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.473)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.473)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.474 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.474)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.207 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.207)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.476 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.476)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.476)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.86 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.86)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.475 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.475)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.475)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.intermediate/__module.encoder.layer.10.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.160 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.intermediate/__module.encoder.layer.10.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.intermediate/__module.encoder.layer.10.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.206 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.160, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.intermediate/__module.encoder.layer.10.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.206)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.478 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.86 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.478)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.478)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.86, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.208 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.208)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.477 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.477)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.477)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.161 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.53 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.161, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.1/__module.encoder.layer.10.ffn.1.output/__module.encoder.layer.10.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.53)

FFNLayer._actual_script_module
  graph(%self.479 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.479)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.479)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.480 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.480)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.210 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.210)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.482 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.482)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.482)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.87 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.87)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.481 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.481)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.481)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.intermediate/__module.encoder.layer.10.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.162 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.intermediate/__module.encoder.layer.10.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.intermediate/__module.encoder.layer.10.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.209 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.162, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.intermediate/__module.encoder.layer.10.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.209)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.484 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.87 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.484)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.484)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.87, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.211 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.211)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.483 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.483)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.483)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.163 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.54 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.163, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.ffn.2/__module.encoder.layer.10.ffn.2.output/__module.encoder.layer.10.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.54)

MobileBertLayer._actual_script_module
  graph(%self.494 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.494)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.494)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.494)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.494)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.494)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.494)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.494)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.502 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.502)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.502)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.495 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.495)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.495)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.529 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.529)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.232 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # torch/nn/functional.py:1119:0
    return (%input.232)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.531 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.531)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.531)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.531)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.96 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.96)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.508 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.508)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.508)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.92 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.92)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.503 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.503)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.503)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.503)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.503)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %x.68 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %query_layer.12 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.68, %30), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %x.70 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %key_layer.12 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.70, %51), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %x.72 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %value_layer.12 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.72, %72), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.12, %74, %75), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.12, %76), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.24 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %78), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.218 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.24, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %input.219 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.218, %82, %83), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.219)
    %context_layer.23 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.12), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.23, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.24 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.24, %95), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.24, %99), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %input.220 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.24, %110), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.220)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.507 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.219 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.219, %2, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.12)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.505 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.505)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.505)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %output.170 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    %x.69 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.170, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.69)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.504 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.504)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.504)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %output.169 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    %x.67 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.169, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.67)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.506 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.506)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.506)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %output.171 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    %x.71 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.171, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.71)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.510 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.92 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.510)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.510)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.92, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.221 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.221)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.509 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.509)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.509)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %output.172 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.56 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.172, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.56)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.530 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.530)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.530)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %output.179 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    %input.231 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.179, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.231)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.533 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.96 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.533)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.533)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.96, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.233 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.233)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.534 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.534)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.534)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.534)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.97 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.97)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.532 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.532)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.532)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %output.180 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    %layer_output.12 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.180, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.12)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.537 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.97 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.537)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.537)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.97, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.235 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.235)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.535 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.535)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.535)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.181 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.234 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.181, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.234)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.536 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.60 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.bottleneck/__module.encoder.layer.11.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.60)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.499 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.499)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.499)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.496 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.496)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.496)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.498 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.498)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.498)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.12 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.12)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.497 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.497)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.497)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.167 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.90 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.167, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.input/__module.encoder.layer.11.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.90)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.501 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.501)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.501)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.217 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.217)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.500 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.500)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.500)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.168 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.91 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.168, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.bottleneck/__module.encoder.layer.11.bottleneck.attention/__module.encoder.layer.11.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.91)

FFNLayer._actual_script_module
  graph(%self.511 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.511)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.511)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.512 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.512)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.223 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.223)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.514 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.514)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.514)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.93 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.93)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.513 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.513)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.513)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.intermediate/__module.encoder.layer.11.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.173 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.intermediate/__module.encoder.layer.11.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.intermediate/__module.encoder.layer.11.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.222 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.173, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.intermediate/__module.encoder.layer.11.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.222)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.516 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.93 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.516)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.516)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.93, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.224 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.224)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.515 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.515)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.515)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.174 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.57 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.174, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.0/__module.encoder.layer.11.ffn.0.output/__module.encoder.layer.11.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.57)

FFNLayer._actual_script_module
  graph(%self.517 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.517)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.517)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.518 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.518)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.226 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.226)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.520 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.520)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.520)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.94 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.94)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.519 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.519)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.519)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.intermediate/__module.encoder.layer.11.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.175 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.intermediate/__module.encoder.layer.11.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.intermediate/__module.encoder.layer.11.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.225 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.175, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.intermediate/__module.encoder.layer.11.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.225)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.522 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.94 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.522)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.522)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.94, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.227 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.227)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.521 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.521)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.521)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.176 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.58 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.176, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.1/__module.encoder.layer.11.ffn.1.output/__module.encoder.layer.11.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.58)

FFNLayer._actual_script_module
  graph(%self.523 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.523)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.523)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.524 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.524)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.229 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.229)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.526 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.526)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.526)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.95 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.95)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.525 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.525)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.525)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.intermediate/__module.encoder.layer.11.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.177 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.intermediate/__module.encoder.layer.11.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.intermediate/__module.encoder.layer.11.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.228 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.177, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.intermediate/__module.encoder.layer.11.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.228)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.528 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.95 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.528)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.528)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.95, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.230 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.230)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.527 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.527)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.527)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.178 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.59 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.178, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.ffn.2/__module.encoder.layer.11.ffn.2.output/__module.encoder.layer.11.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.59)

MobileBertLayer._actual_script_module
  graph(%self.538 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.538)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.538)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.538)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.538)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.538)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.538)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.538)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.546 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.546)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.546)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.539 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.539)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.539)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.573 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.573)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.251 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate # torch/nn/functional.py:1119:0
    return (%input.251)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.575 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.575)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.575)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.575)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.104 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.104)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.552 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.552)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.552)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.100 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.100)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.547 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.547)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.547)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.547)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.547)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %x.74 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %query_layer.13 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.74, %30), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %x.76 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %key_layer.13 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.76, %51), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %x.78 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %value_layer.13 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.78, %72), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.13, %74, %75), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.25 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.13, %76), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.26 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.25, %78), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.237 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.26, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %input.238 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.237, %82, %83), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.238)
    %context_layer.25 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.13), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.25, %91), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.26 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.26, %95), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.26, %99), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
    %input.239 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.26, %110), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.239)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.551 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.238 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.238, %2, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.13)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.549 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.549)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.549)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
    %output.185 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
    %x.75 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.185, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.75)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.548 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.548)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.548)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
    %output.184 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
    %x.73 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.184, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.73)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.550 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.550)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.550)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
    %output.186 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
    %x.77 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.186, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.77)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.554 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.100 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.554)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.554)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.100, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.240 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.240)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.553 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.553)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.553)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
    %output.187 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.61 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.187, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.61)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.574 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.574)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.574)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
    %output.194 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
    %input.250 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.194, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.250)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.577 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.104 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.577)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.577)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.104, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.252 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.252)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.578 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.578)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.578)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.578)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.105 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.105)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.576 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.576)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.576)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
    %output.195 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
    %layer_output.13 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.195, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.13)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.581 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.105 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.581)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.581)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.105, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.254 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.254)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.579 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.579)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.579)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.196 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.253 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.196, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.253)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.580 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.65 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.bottleneck/__module.encoder.layer.12.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.65)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.543 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.543)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.543)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.540 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.540)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.540)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.542 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.542)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.542)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.13 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.13)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.541 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.541)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.541)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.182 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.98 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.182, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.input/__module.encoder.layer.12.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.98)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.545 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.545)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.545)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.236 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.236)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.544 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.544)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.544)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.183 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.99 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.183, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.bottleneck/__module.encoder.layer.12.bottleneck.attention/__module.encoder.layer.12.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.99)

FFNLayer._actual_script_module
  graph(%self.555 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.555)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.555)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.556 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.556)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.242 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.242)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.558 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.558)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.558)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.101 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.101)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.557 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.557)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.557)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.intermediate/__module.encoder.layer.12.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.188 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.intermediate/__module.encoder.layer.12.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.intermediate/__module.encoder.layer.12.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.241 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.188, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.intermediate/__module.encoder.layer.12.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.241)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.560 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.101 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.560)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.560)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.101, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.243 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.243)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.559 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.559)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.559)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.189 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.62 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.189, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.0/__module.encoder.layer.12.ffn.0.output/__module.encoder.layer.12.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.62)

FFNLayer._actual_script_module
  graph(%self.561 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.561)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.561)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.562 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.562)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.245 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.245)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.564 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.564)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.564)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.102 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.102)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.563 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.563)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.563)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.intermediate/__module.encoder.layer.12.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.190 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.intermediate/__module.encoder.layer.12.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.intermediate/__module.encoder.layer.12.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.244 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.190, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.intermediate/__module.encoder.layer.12.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.244)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.566 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.102 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.566)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.566)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.102, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.246 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.246)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.565 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.565)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.565)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.191 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.63 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.191, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.1/__module.encoder.layer.12.ffn.1.output/__module.encoder.layer.12.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.63)

FFNLayer._actual_script_module
  graph(%self.567 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.567)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.567)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.568 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.568)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.248 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.248)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.570 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.570)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.570)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.103 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.103)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.569 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.569)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.569)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.intermediate/__module.encoder.layer.12.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.192 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.intermediate/__module.encoder.layer.12.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.intermediate/__module.encoder.layer.12.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.247 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.192, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.intermediate/__module.encoder.layer.12.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.247)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.572 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.103 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.572)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.572)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.103, %3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.249 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.249)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.571 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.571)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.571)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.193 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.64 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.193, %2, %6), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.ffn.2/__module.encoder.layer.12.ffn.2.output/__module.encoder.layer.12.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.64)

MobileBertLayer._actual_script_module
  graph(%self.582 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.582)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.582)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.582)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.582)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.582)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.582)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.582)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.590 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.590)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.590)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.583 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.583)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.583)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.617 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.617)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.270 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate # torch/nn/functional.py:1119:0
    return (%input.270)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.619 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.619)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.619)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.619)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.112 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.112)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.596 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.596)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.596)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.108 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.108)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.591 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.591)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.591)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.591)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.591)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %x.80 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %query_layer.14 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.80, %30), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %x.82 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %key_layer.14 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.82, %51), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %x.84 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %value_layer.14 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.84, %72), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.14, %74, %75), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.14, %76), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.27, %78), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.256 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.28, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %input.257 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.256, %82, %83), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.257)
    %context_layer.27 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.14), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.27, %91), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.28 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.28, %95), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.28, %99), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
    %input.258 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.28, %110), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.258)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.595 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.257 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.257, %2, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.14)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.593 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.593)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.593)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
    %output.200 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
    %x.81 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.200, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.81)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.592 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.592)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.592)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
    %output.199 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
    %x.79 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.199, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.79)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.594 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.594)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.594)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
    %output.201 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
    %x.83 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.201, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.83)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.598 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.108 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.598)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.598)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.108, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.259 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.259)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.597 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.597)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.597)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
    %output.202 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.66 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.202, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.66)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.618 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.618)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.618)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
    %output.209 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
    %input.269 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.209, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.269)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.621 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.112 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.621)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.621)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.112, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.271 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.271)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.622 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.622)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.622)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.622)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.113 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.113)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.620 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.620)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.620)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
    %output.210 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
    %layer_output.14 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.210, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.14)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.625 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.113 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.625)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.625)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.113, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.273 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.273)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.623 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.623)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.623)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.211 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.272 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.211, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.272)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.624 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.70 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.bottleneck/__module.encoder.layer.13.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.70)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.587 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.587)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.587)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.584 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.584)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.584)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.586 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.586)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.586)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.14 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.14)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.585 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.585)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.585)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.197 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.106 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.197, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.input/__module.encoder.layer.13.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.106)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.589 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.589)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.589)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.255 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.255)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.588 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.588)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.588)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.198 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.107 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.198, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.bottleneck/__module.encoder.layer.13.bottleneck.attention/__module.encoder.layer.13.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.107)

FFNLayer._actual_script_module
  graph(%self.599 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.599)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.599)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.600 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.600)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.261 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.261)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.602 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.602)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.602)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.109 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.109)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.601 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.601)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.601)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.intermediate/__module.encoder.layer.13.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.203 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.intermediate/__module.encoder.layer.13.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.intermediate/__module.encoder.layer.13.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.260 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.203, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.intermediate/__module.encoder.layer.13.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.260)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.604 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.109 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.604)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.604)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.109, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.262 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.262)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.603 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.603)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.603)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.204 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.67 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.204, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.0/__module.encoder.layer.13.ffn.0.output/__module.encoder.layer.13.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.67)

FFNLayer._actual_script_module
  graph(%self.605 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.605)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.605)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.606 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.606)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.264 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.264)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.608 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.608)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.608)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.110 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.110)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.607 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.607)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.607)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.intermediate/__module.encoder.layer.13.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.205 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.intermediate/__module.encoder.layer.13.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.intermediate/__module.encoder.layer.13.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.263 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.205, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.intermediate/__module.encoder.layer.13.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.263)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.610 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.110 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.610)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.610)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.110, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.265 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.265)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.609 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.609)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.609)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.206 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.68 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.206, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.1/__module.encoder.layer.13.ffn.1.output/__module.encoder.layer.13.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.68)

FFNLayer._actual_script_module
  graph(%self.611 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.611)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.611)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.612 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.612)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.267 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.267)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.614 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.614)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.614)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.111 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.111)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.613 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.613)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.613)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.intermediate/__module.encoder.layer.13.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.207 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.intermediate/__module.encoder.layer.13.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.intermediate/__module.encoder.layer.13.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.266 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.207, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.intermediate/__module.encoder.layer.13.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.266)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.616 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.111 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.616)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.616)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.111, %3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.268 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.268)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.615 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.615)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.615)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.208 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.69 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.208, %2, %6), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.ffn.2/__module.encoder.layer.13.ffn.2.output/__module.encoder.layer.13.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.69)

MobileBertLayer._actual_script_module
  graph(%self.626 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.626)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.626)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.626)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.626)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.626)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.626)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.626)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.634 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.634)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.634)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.627 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.627)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.627)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.661 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.661)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.289 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate # torch/nn/functional.py:1119:0
    return (%input.289)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.663 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.663)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.663)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.663)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.120 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.120)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.640 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.640)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.640)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.116 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.116)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.635 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.635)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.635)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.635)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.635)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %x.86 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %query_layer.15 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.86, %30), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %x.88 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %key_layer.15 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.88, %51), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %x.90 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %value_layer.15 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.90, %72), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.15, %74, %75), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.29 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.15, %76), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.30 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.29, %78), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.275 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.30, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %input.276 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.275, %82, %83), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.276)
    %context_layer.29 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.15), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.29, %91), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.30 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.30, %95), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.30, %99), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
    %input.277 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.30, %110), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.277)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.639 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.276 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.276, %2, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.15)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.637 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.637)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.637)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
    %output.215 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
    %x.87 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.215, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.87)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.636 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.636)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.636)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
    %output.214 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
    %x.85 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.214, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.85)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.638 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.638)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.638)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
    %output.216 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
    %x.89 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.216, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.89)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.642 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.116 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.642)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.642)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.116, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.278 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.278)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.641 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.641)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.641)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
    %output.217 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.71 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.217, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.71)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.662 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.662)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.662)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
    %output.224 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
    %input.288 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.224, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.288)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.665 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.120 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.665)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.665)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.120, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.290 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.290)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.666 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.666)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.666)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.666)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.121 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.121)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.664 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.664)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.664)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
    %output.225 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
    %layer_output.15 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.225, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.15)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.669 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.121 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.669)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.669)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.121, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.292 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.292)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.667 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.667)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.667)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.226 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.291 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.226, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.291)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.668 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.75 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.bottleneck/__module.encoder.layer.14.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.75)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.631 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.631)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.631)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.628 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.628)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.628)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.630 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.630)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.630)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.15 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.15)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.629 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.629)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.629)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.212 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.114 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.212, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.input/__module.encoder.layer.14.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.114)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.633 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.633)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.633)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.274 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.274)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.632 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.632)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.632)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.213 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.115 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.213, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.bottleneck/__module.encoder.layer.14.bottleneck.attention/__module.encoder.layer.14.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.115)

FFNLayer._actual_script_module
  graph(%self.643 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.643)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.643)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.644 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.644)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.280 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.280)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.646 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.646)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.646)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.117 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.117)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.645 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.645)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.645)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.intermediate/__module.encoder.layer.14.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.218 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.intermediate/__module.encoder.layer.14.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.intermediate/__module.encoder.layer.14.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.279 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.218, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.intermediate/__module.encoder.layer.14.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.279)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.648 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.117 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.648)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.648)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.117, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.281 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.281)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.647 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.647)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.647)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.219 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.72 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.219, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.0/__module.encoder.layer.14.ffn.0.output/__module.encoder.layer.14.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.72)

FFNLayer._actual_script_module
  graph(%self.649 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.649)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.649)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.650 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.650)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.283 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.283)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.652 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.652)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.652)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.118 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.118)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.651 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.651)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.651)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.intermediate/__module.encoder.layer.14.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.220 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.intermediate/__module.encoder.layer.14.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.intermediate/__module.encoder.layer.14.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.282 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.220, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.intermediate/__module.encoder.layer.14.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.282)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.654 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.118 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.654)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.654)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.118, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.284 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.284)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.653 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.653)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.653)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.221 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.73 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.221, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.1/__module.encoder.layer.14.ffn.1.output/__module.encoder.layer.14.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.73)

FFNLayer._actual_script_module
  graph(%self.655 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.655)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.655)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.656 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.656)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.286 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.286)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.658 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.658)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.658)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.119 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.119)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.657 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.657)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.657)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.intermediate/__module.encoder.layer.14.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.222 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.intermediate/__module.encoder.layer.14.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.intermediate/__module.encoder.layer.14.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.285 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.222, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.intermediate/__module.encoder.layer.14.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.285)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.660 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.119 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.660)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.660)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.119, %3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.287 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.287)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.659 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.659)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.659)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.223 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.74 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.223, %2, %6), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.ffn.2/__module.encoder.layer.14.ffn.2.output/__module.encoder.layer.14.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.74)

MobileBertLayer._actual_script_module
  graph(%self.670 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.670)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.670)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.670)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.670)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.670)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.670)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.670)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.678 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.678)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.678)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.671 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.671)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.671)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.705 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.705)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.308 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate # torch/nn/functional.py:1119:0
    return (%input.308)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.707 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.707)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.707)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.707)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.128 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.128)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.684 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.684)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.684)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.124 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.124)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.679 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.679)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.679)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.679)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.679)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %x.92 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %query_layer.16 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.92, %30), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %x.94 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %key_layer.16 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.94, %51), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %x.96 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %value_layer.16 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.96, %72), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.16, %74, %75), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.31 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.16, %76), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.32 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.31, %78), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.294 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.32, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %input.295 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.294, %82, %83), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.295)
    %context_layer.31 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.16), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.31, %91), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.32 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.32, %95), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.32, %99), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
    %input.296 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.32, %110), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.296)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.683 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.295 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.295, %2, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.16)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.681 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.681)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.681)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
    %output.230 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
    %x.93 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.230, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.93)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.680 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.680)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.680)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
    %output.229 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
    %x.91 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.229, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.91)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.682 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.682)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.682)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
    %output.231 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
    %x.95 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.231, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.95)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.686 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.124 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.686)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.686)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.124, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.297 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.297)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.685 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.685)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.685)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
    %output.232 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.76 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.232, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.76)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.706 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.706)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.706)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
    %output.239 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
    %input.307 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.239, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.307)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.709 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.128 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.709)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.709)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.128, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.309 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.309)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.710 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.710)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.710)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.710)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.129 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.129)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.708 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.708)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.708)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
    %output.240 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
    %layer_output.16 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.240, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.16)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.713 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.129 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.713)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.713)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.129, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.311 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.311)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.711 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.711)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.711)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.241 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.310 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.241, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.310)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.712 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.80 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.bottleneck/__module.encoder.layer.15.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.80)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.675 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.675)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.675)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.672 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.672)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.672)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.674 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.674)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.674)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.16 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.16)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.673 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.673)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.673)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.227 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.122 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.227, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.input/__module.encoder.layer.15.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.122)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.677 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.677)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.677)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.293 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.293)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.676 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.676)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.676)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.228 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.123 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.228, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.bottleneck/__module.encoder.layer.15.bottleneck.attention/__module.encoder.layer.15.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.123)

FFNLayer._actual_script_module
  graph(%self.687 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.687)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.687)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.688 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.688)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.299 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.299)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.690 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.690)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.690)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.125 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.125)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.689 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.689)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.689)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.intermediate/__module.encoder.layer.15.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.233 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.intermediate/__module.encoder.layer.15.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.intermediate/__module.encoder.layer.15.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.298 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.233, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.intermediate/__module.encoder.layer.15.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.298)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.692 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.125 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.692)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.692)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.125, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.300 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.300)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.691 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.691)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.691)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.234 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.77 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.234, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.0/__module.encoder.layer.15.ffn.0.output/__module.encoder.layer.15.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.77)

FFNLayer._actual_script_module
  graph(%self.693 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.693)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.693)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.694 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.694)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.302 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.302)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.696 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.696)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.696)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.126 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.126)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.695 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.695)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.695)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.intermediate/__module.encoder.layer.15.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.235 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.intermediate/__module.encoder.layer.15.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.intermediate/__module.encoder.layer.15.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.301 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.235, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.intermediate/__module.encoder.layer.15.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.301)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.698 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.126 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.698)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.698)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.126, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.303 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.303)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.697 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.697)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.697)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.236 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.78 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.236, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.1/__module.encoder.layer.15.ffn.1.output/__module.encoder.layer.15.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.78)

FFNLayer._actual_script_module
  graph(%self.699 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.699)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.699)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.700 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.700)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.305 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.305)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.702 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.702)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.702)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.127 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.127)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.701 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.701)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.701)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.intermediate/__module.encoder.layer.15.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.237 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.intermediate/__module.encoder.layer.15.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.intermediate/__module.encoder.layer.15.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.304 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.237, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.intermediate/__module.encoder.layer.15.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.304)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.704 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.127 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.704)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.704)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.127, %3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.306 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.306)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.703 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.703)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.703)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.238 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.79 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.238, %2, %6), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.ffn.2/__module.encoder.layer.15.ffn.2.output/__module.encoder.layer.15.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.79)

MobileBertLayer._actual_script_module
  graph(%self.714 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.714)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.714)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.714)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.714)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.714)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.714)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.714)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.722 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.722)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.722)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.715 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.715)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.715)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.749 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.749)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.327 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate # torch/nn/functional.py:1119:0
    return (%input.327)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.751 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.751)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.751)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.751)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.136 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.136)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.728 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.728)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.728)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.132 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.132)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.723 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.723)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.723)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.723)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.723)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %x.98 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %query_layer.17 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.98, %30), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %x.100 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %key_layer.17 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.100, %51), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %x.102 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %value_layer.17 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.102, %72), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.17, %74, %75), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.33 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.17, %76), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.34 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.33, %78), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.313 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.34, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %input.314 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.313, %82, %83), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.314)
    %context_layer.33 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.17), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.33, %91), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.34 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.34, %95), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.34, %99), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
    %input.315 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.34, %110), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.315)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.727 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.314 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.314, %2, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.17)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.725 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.725)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.725)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
    %output.245 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
    %x.99 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.245, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.99)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.724 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.724)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.724)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
    %output.244 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
    %x.97 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.244, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.97)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.726 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.726)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.726)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
    %output.246 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
    %x.101 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.246, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.101)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.730 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.132 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.730)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.730)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.132, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.316 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.316)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.729 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.729)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.729)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
    %output.247 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.81 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.247, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.81)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.750 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.750)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.750)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
    %output.254 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
    %input.326 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.254, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.326)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.753 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.136 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.753)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.753)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.136, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.328 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.328)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.754 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.754)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.754)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.754)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.137 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.137)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.752 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.752)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.752)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
    %output.255 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
    %layer_output.17 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.255, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.17)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.757 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.137 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.757)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.757)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.137, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.330 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.330)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.755 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.755)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.755)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.256 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.329 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.256, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.329)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.756 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.85 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.bottleneck/__module.encoder.layer.16.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.85)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.719 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.719)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.719)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.716 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.716)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.716)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.718 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.718)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.718)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.17 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.17)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.717 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.717)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.717)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.242 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.130 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.242, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.input/__module.encoder.layer.16.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.130)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.721 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.721)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.721)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.312 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.312)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.720 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.720)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.720)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.243 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.131 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.243, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.bottleneck/__module.encoder.layer.16.bottleneck.attention/__module.encoder.layer.16.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.131)

FFNLayer._actual_script_module
  graph(%self.731 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.731)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.731)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.732 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.732)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.318 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.318)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.734 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.734)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.734)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.133 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.133)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.733 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.733)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.733)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.intermediate/__module.encoder.layer.16.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.248 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.intermediate/__module.encoder.layer.16.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.intermediate/__module.encoder.layer.16.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.317 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.248, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.intermediate/__module.encoder.layer.16.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.317)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.736 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.133 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.736)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.736)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.133, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.319 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.319)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.735 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.735)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.735)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.249 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.82 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.249, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.0/__module.encoder.layer.16.ffn.0.output/__module.encoder.layer.16.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.82)

FFNLayer._actual_script_module
  graph(%self.737 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.737)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.737)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.738 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.738)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.321 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.321)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.740 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.740)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.740)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.134 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.134)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.739 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.739)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.739)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.intermediate/__module.encoder.layer.16.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.250 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.intermediate/__module.encoder.layer.16.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.intermediate/__module.encoder.layer.16.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.320 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.250, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.intermediate/__module.encoder.layer.16.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.320)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.742 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.134 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.742)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.742)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.134, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.322 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.322)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.741 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.741)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.741)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.251 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.83 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.251, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.1/__module.encoder.layer.16.ffn.1.output/__module.encoder.layer.16.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.83)

FFNLayer._actual_script_module
  graph(%self.743 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.743)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.743)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.744 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.744)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.324 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.324)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.746 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.746)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.746)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.135 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.135)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.745 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.745)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.745)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.intermediate/__module.encoder.layer.16.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.252 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.intermediate/__module.encoder.layer.16.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.intermediate/__module.encoder.layer.16.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.323 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.252, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.intermediate/__module.encoder.layer.16.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.323)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.748 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.135 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.748)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.748)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.135, %3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.325 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.325)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.747 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.747)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.747)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.253 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.84 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.253, %2, %6), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.ffn.2/__module.encoder.layer.16.ffn.2.output/__module.encoder.layer.16.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.84)

MobileBertLayer._actual_script_module
  graph(%self.758 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.758)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.758)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.758)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.758)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.758)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.758)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.758)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.766 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.766)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.766)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.759 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.759)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.759)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.793 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.793)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.346 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate # torch/nn/functional.py:1119:0
    return (%input.346)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.795 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.795)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.795)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.795)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.144 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.144)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.772 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.772)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.772)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.140 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.140)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.767 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.767)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.767)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.767)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.767)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %x.104 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %query_layer.18 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.104, %30), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %x.106 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %key_layer.18 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.106, %51), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %x.108 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %value_layer.18 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.108, %72), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.18, %74, %75), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.35 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.18, %76), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.36 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.35, %78), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.332 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.36, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %input.333 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.332, %82, %83), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.333)
    %context_layer.35 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.18), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.35, %91), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.36 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.36, %95), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.36, %99), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
    %input.334 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.36, %110), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.334)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.771 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.333 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.333, %2, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.18)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.769 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.769)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.769)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
    %output.260 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
    %x.105 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.260, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.105)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.768 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.768)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.768)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
    %output.259 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
    %x.103 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.259, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.103)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.770 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.770)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.770)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
    %output.261 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
    %x.107 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.261, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.107)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.774 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.140 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.774)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.774)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.140, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.335 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.335)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.773 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.773)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.773)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
    %output.262 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.86 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.262, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.86)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.794 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.794)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.794)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
    %output.269 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
    %input.345 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.269, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.345)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.797 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.144 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.797)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.797)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.144, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.347 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.347)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.798 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.798)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.798)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.798)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.145 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.145)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.796 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.796)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.796)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
    %output.270 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
    %layer_output.18 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.270, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.18)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.801 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.145 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.801)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.801)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.145, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.349 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.349)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.799 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.799)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.799)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.271 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.348 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.271, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.348)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.800 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.90 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.bottleneck/__module.encoder.layer.17.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.90)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.763 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.763)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.763)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.760 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.760)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.760)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.762 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.762)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.762)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.18 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.18)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.761 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.761)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.761)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.257 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.138 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.257, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.input/__module.encoder.layer.17.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.138)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.765 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.765)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.765)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.331 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.331)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.764 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.764)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.764)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.258 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.139 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.258, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.bottleneck/__module.encoder.layer.17.bottleneck.attention/__module.encoder.layer.17.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.139)

FFNLayer._actual_script_module
  graph(%self.775 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.775)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.775)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.776 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.776)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.337 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.337)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.778 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.778)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.778)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.141 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.141)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.777 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.777)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.777)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.intermediate/__module.encoder.layer.17.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.263 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.intermediate/__module.encoder.layer.17.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.intermediate/__module.encoder.layer.17.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.336 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.263, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.intermediate/__module.encoder.layer.17.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.336)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.780 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.141 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.780)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.780)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.141, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.338 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.338)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.779 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.779)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.779)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.264 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.87 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.264, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.0/__module.encoder.layer.17.ffn.0.output/__module.encoder.layer.17.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.87)

FFNLayer._actual_script_module
  graph(%self.781 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.781)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.781)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.782 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.782)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.340 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.340)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.784 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.784)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.784)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.142 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.142)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.783 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.783)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.783)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.intermediate/__module.encoder.layer.17.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.265 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.intermediate/__module.encoder.layer.17.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.intermediate/__module.encoder.layer.17.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.339 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.265, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.intermediate/__module.encoder.layer.17.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.339)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.786 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.142 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.786)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.786)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.142, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.341 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.341)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.785 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.785)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.785)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.266 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.88 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.266, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.1/__module.encoder.layer.17.ffn.1.output/__module.encoder.layer.17.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.88)

FFNLayer._actual_script_module
  graph(%self.787 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.787)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.787)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.788 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.788)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.343 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.343)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.790 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.790)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.790)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.143 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.143)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.789 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.789)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.789)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.intermediate/__module.encoder.layer.17.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.267 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.intermediate/__module.encoder.layer.17.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.intermediate/__module.encoder.layer.17.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.342 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.267, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.intermediate/__module.encoder.layer.17.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.342)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.792 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.143 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.792)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.792)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.143, %3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.344 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.344)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.791 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.791)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.791)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.268 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.89 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.268, %2, %6), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.ffn.2/__module.encoder.layer.17.ffn.2.output/__module.encoder.layer.17.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.89)

MobileBertLayer._actual_script_module
  graph(%self.802 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.802)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.802)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.802)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.802)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.802)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.802)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.802)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.810 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.810)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.810)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.803 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.803)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.803)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.837 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.837)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.365 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate # torch/nn/functional.py:1119:0
    return (%input.365)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.839 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.839)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.839)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.839)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.152 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.152)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.816 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.816)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.816)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.148 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.148)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.811 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.811)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.811)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.811)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.811)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %x.110 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %query_layer.19 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.110, %30), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %x.112 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %key_layer.19 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.112, %51), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %x.114 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %value_layer.19 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.114, %72), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.19, %74, %75), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.19, %76), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.37, %78), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.351 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.38, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %input.352 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.351, %82, %83), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.352)
    %context_layer.37 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.19), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.37, %91), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.38 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.38, %95), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.38, %99), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
    %input.353 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.38, %110), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.353)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.815 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.352 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.352, %2, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.19)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.813 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.813)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.813)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
    %output.275 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
    %x.111 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.275, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.111)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.812 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.812)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.812)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
    %output.274 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
    %x.109 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.274, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.109)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.814 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.814)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.814)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
    %output.276 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
    %x.113 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.276, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.113)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.818 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.148 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.818)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.818)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.148, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.354 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.354)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.817 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.817)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.817)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
    %output.277 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.91 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.277, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.91)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.838 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.838)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.838)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
    %output.284 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
    %input.364 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.284, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.364)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.841 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.152 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.841)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.841)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.152, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.366 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.366)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.842 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.842)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.842)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.842)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.153 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.153)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.840 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.840)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.840)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
    %output.285 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
    %layer_output.19 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.285, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.19)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.845 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.153 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.845)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.845)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.153, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.368 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.368)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.843 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.843)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.843)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.286 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.367 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.286, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.367)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.844 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.95 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.bottleneck/__module.encoder.layer.18.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.95)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.807 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.807)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.807)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.804 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.804)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.804)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.806 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.806)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.806)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.19 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.19)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.805 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.805)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.805)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.272 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.146 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.272, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.input/__module.encoder.layer.18.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.146)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.809 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.809)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.809)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.350 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.350)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.808 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.808)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.808)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.273 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.147 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.273, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.bottleneck/__module.encoder.layer.18.bottleneck.attention/__module.encoder.layer.18.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.147)

FFNLayer._actual_script_module
  graph(%self.819 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.819)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.819)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.820 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.820)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.356 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.356)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.822 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.822)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.822)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.149 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.149)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.821 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.821)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.821)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.intermediate/__module.encoder.layer.18.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.278 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.intermediate/__module.encoder.layer.18.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.intermediate/__module.encoder.layer.18.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.355 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.278, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.intermediate/__module.encoder.layer.18.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.355)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.824 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.149 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.824)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.824)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.149, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.357 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.357)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.823 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.823)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.823)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.279 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.92 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.279, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.0/__module.encoder.layer.18.ffn.0.output/__module.encoder.layer.18.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.92)

FFNLayer._actual_script_module
  graph(%self.825 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.825)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.825)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.826 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.826)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.359 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.359)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.828 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.828)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.828)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.150 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.150)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.827 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.827)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.827)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.intermediate/__module.encoder.layer.18.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.280 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.intermediate/__module.encoder.layer.18.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.intermediate/__module.encoder.layer.18.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.358 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.280, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.intermediate/__module.encoder.layer.18.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.358)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.830 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.150 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.830)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.830)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.150, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.360 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.360)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.829 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.829)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.829)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.281 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.93 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.281, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.1/__module.encoder.layer.18.ffn.1.output/__module.encoder.layer.18.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.93)

FFNLayer._actual_script_module
  graph(%self.831 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.831)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.831)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.832 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.832)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.362 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.362)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.834 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.834)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.834)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.151 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.151)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.833 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.833)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.833)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.intermediate/__module.encoder.layer.18.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.282 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.intermediate/__module.encoder.layer.18.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.intermediate/__module.encoder.layer.18.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.361 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.282, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.intermediate/__module.encoder.layer.18.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.361)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.836 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.151 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.836)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.836)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.151, %3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.363 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.363)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.835 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.835)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.835)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.283 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.94 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.283, %2, %6), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.ffn.2/__module.encoder.layer.18.ffn.2.output/__module.encoder.layer.18.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.94)

MobileBertLayer._actual_script_module
  graph(%self.846 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.846)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.846)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.846)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.846)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.846)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.846)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.846)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.854 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.854)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.854)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.847 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.847)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.847)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.881 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.881)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.384 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate # torch/nn/functional.py:1119:0
    return (%input.384)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.883 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.883)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.883)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.883)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.160 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.160)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.860 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.860)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.860)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.156 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.156)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.855 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.855)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.855)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.855)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.855)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %x.116 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %query_layer.20 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.116, %30), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %x.118 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %key_layer.20 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.118, %51), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %x.120 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %value_layer.20 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.120, %72), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.20, %74, %75), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.39 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.20, %76), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.40 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.39, %78), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.370 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.40, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %input.371 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.370, %82, %83), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.371)
    %context_layer.39 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.20), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.39, %91), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.40 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.40, %95), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.40, %99), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
    %input.372 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.40, %110), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.372)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.859 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.371 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.371, %2, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.20)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.857 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.857)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.857)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
    %output.290 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
    %x.117 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.290, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.117)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.856 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.856)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.856)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
    %output.289 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
    %x.115 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.289, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.115)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.858 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.858)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.858)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
    %output.291 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
    %x.119 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.291, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.119)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.862 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.156 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.862)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.862)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.156, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.373 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.373)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.861 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.861)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.861)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
    %output.292 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.96 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.292, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.96)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.882 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.882)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.882)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
    %output.299 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
    %input.383 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.299, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.383)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.885 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.160 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.885)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.885)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.160, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.385 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.385)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.886 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.886)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.886)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.886)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.161 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.161)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.884 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.884)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.884)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
    %output.300 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
    %layer_output.20 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.300, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.20)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.889 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.161 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.889)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.889)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.161, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.387 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.387)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.887 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.887)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.887)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.301 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.386 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.301, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.386)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.888 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.100 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.bottleneck/__module.encoder.layer.19.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.100)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.851 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.851)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.851)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.848 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.848)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.848)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.850 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.850)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.850)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.20 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.20)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.849 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.849)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.849)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.287 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.154 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.287, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.input/__module.encoder.layer.19.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.154)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.853 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.853)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.853)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.369 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.369)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.852 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.852)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.852)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.288 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.155 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.288, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.bottleneck/__module.encoder.layer.19.bottleneck.attention/__module.encoder.layer.19.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.155)

FFNLayer._actual_script_module
  graph(%self.863 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.863)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.863)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.864 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.864)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.375 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.375)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.866 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.866)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.866)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.157 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.157)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.865 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.865)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.865)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.intermediate/__module.encoder.layer.19.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.293 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.intermediate/__module.encoder.layer.19.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.intermediate/__module.encoder.layer.19.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.374 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.293, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.intermediate/__module.encoder.layer.19.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.374)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.868 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.157 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.868)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.868)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.157, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.376 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.376)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.867 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.867)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.867)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.294 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.97 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.294, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.0/__module.encoder.layer.19.ffn.0.output/__module.encoder.layer.19.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.97)

FFNLayer._actual_script_module
  graph(%self.869 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.869)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.869)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.870 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.870)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.378 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.378)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.872 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.872)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.872)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.158 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.158)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.871 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.871)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.871)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.intermediate/__module.encoder.layer.19.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.295 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.intermediate/__module.encoder.layer.19.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.intermediate/__module.encoder.layer.19.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.377 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.295, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.intermediate/__module.encoder.layer.19.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.377)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.874 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.158 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.874)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.874)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.158, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.379 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.379)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.873 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.873)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.873)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.296 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.98 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.296, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.1/__module.encoder.layer.19.ffn.1.output/__module.encoder.layer.19.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.98)

FFNLayer._actual_script_module
  graph(%self.875 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.875)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.875)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.876 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.876)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.381 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.381)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.878 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.878)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.878)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.159 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.159)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.877 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.877)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.877)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.intermediate/__module.encoder.layer.19.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.297 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.intermediate/__module.encoder.layer.19.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.intermediate/__module.encoder.layer.19.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.380 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.297, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.intermediate/__module.encoder.layer.19.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.380)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.880 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.159 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.880)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.880)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.159, %3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.382 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.382)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.879 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.879)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.879)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.298 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.99 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.298, %2, %6), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.ffn.2/__module.encoder.layer.19.ffn.2.output/__module.encoder.layer.19.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.99)

MobileBertLayer._actual_script_module
  graph(%self.890 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.890)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.890)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.890)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.890)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.890)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.890)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.890)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.898 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.898)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.898)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.891 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.891)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.891)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.925 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.925)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.403 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate # torch/nn/functional.py:1119:0
    return (%input.403)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.927 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.927)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.927)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.927)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.168 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.168)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.904 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.904)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.904)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.164 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.164)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.899 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.899)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.899)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.899)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.899)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %x.122 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %query_layer.21 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.122, %30), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %x.124 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %key_layer.21 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.124, %51), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %x.126 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %value_layer.21 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.126, %72), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.21, %74, %75), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.41 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.21, %76), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.42 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.41, %78), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.389 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.42, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %input.390 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.389, %82, %83), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.390)
    %context_layer.41 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.21), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.41, %91), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.42 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.42, %95), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.42, %99), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
    %input.391 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.42, %110), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.391)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.903 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.390 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.390, %2, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.21)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.901 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.901)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.901)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
    %output.305 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
    %x.123 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.305, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.123)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.900 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.900)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.900)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
    %output.304 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
    %x.121 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.304, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.121)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.902 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.902)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.902)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
    %output.306 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
    %x.125 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.306, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.125)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.906 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.164 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.906)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.906)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.164, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.392 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.392)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.905 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.905)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.905)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
    %output.307 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.101 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.307, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.101)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.926 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.926)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.926)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
    %output.314 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
    %input.402 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.314, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.402)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.929 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.168 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.929)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.929)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.168, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.404 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.404)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.930 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.930)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.930)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.930)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.169 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.169)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.928 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.928)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.928)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
    %output.315 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
    %layer_output.21 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.315, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.21)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.933 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.169 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.933)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.933)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.169, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.406 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.406)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.931 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.931)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.931)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.316 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.405 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.316, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.405)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.932 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.105 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.bottleneck/__module.encoder.layer.20.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.105)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.895 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.895)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.895)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.892 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.892)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.892)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.894 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.894)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.894)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.21 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.21)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.893 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.893)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.893)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.302 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.162 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.302, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.input/__module.encoder.layer.20.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.162)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.897 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.897)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.897)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.388 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.388)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.896 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.896)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.896)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.303 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.163 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.303, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.bottleneck/__module.encoder.layer.20.bottleneck.attention/__module.encoder.layer.20.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.163)

FFNLayer._actual_script_module
  graph(%self.907 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.907)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.907)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.908 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.908)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.394 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.394)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.910 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.910)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.910)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.165 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.165)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.909 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.909)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.909)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.intermediate/__module.encoder.layer.20.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.308 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.intermediate/__module.encoder.layer.20.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.intermediate/__module.encoder.layer.20.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.393 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.308, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.intermediate/__module.encoder.layer.20.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.393)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.912 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.165 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.912)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.912)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.165, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.395 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.395)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.911 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.911)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.911)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.309 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.102 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.309, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.0/__module.encoder.layer.20.ffn.0.output/__module.encoder.layer.20.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.102)

FFNLayer._actual_script_module
  graph(%self.913 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.913)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.913)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.914 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.914)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.397 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.397)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.916 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.916)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.916)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.166 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.166)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.915 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.915)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.915)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.intermediate/__module.encoder.layer.20.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.310 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.intermediate/__module.encoder.layer.20.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.intermediate/__module.encoder.layer.20.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.396 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.310, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.intermediate/__module.encoder.layer.20.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.396)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.918 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.166 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.918)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.918)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.166, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.398 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.398)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.917 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.917)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.917)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.311 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.103 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.311, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.1/__module.encoder.layer.20.ffn.1.output/__module.encoder.layer.20.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.103)

FFNLayer._actual_script_module
  graph(%self.919 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.919)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.919)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.920 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.920)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.400 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.400)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.922 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.922)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.922)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.167 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.167)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.921 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.921)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.921)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.intermediate/__module.encoder.layer.20.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.312 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.intermediate/__module.encoder.layer.20.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.intermediate/__module.encoder.layer.20.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.399 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.312, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.intermediate/__module.encoder.layer.20.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.399)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.924 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.167 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.924)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.924)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.167, %3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.401 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.401)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.923 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.923)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.923)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.313 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.104 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.313, %2, %6), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.ffn.2/__module.encoder.layer.20.ffn.2.output/__module.encoder.layer.20.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.104)

MobileBertLayer._actual_script_module
  graph(%self.934 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.934)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.934)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.934)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.934)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.934)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.934)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.934)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.942 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.942)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.942)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.935 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.935)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.935)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.969 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.969)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.422 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate # torch/nn/functional.py:1119:0
    return (%input.422)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.971 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.971)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.971)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.971)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.176 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.176)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.948 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.948)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.948)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.172 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.172)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.943 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.943)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.943)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.943)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.943)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %x.128 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %query_layer.22 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.128, %30), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %x.130 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %key_layer.22 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.130, %51), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %x.132 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %value_layer.22 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.132, %72), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.22, %74, %75), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.43 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.22, %76), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.44 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.43, %78), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.408 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.44, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %input.409 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.408, %82, %83), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.409)
    %context_layer.43 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.22), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.43, %91), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.44 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.44, %95), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.44, %99), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
    %input.410 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.44, %110), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.410)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.947 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.409 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.409, %2, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.22)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.945 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.945)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.945)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
    %output.320 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
    %x.129 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.320, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.129)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.944 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.944)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.944)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
    %output.319 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
    %x.127 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.319, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.127)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.946 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.946)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.946)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
    %output.321 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
    %x.131 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.321, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.131)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.950 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.172 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.950)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.950)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.172, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.411 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.411)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.949 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.949)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.949)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
    %output.322 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.106 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.322, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.106)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.970 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.970)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.970)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
    %output.329 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
    %input.421 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.329, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.421)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.973 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.176 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.973)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.973)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.176, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.423 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.423)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.974 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.974)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.974)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.974)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.177 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.177)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.972 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.972)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.972)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
    %output.330 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
    %layer_output.22 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.330, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.22)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.977 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.177 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.977)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.977)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.177, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.425 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.425)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.975 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.975)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.975)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.331 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.424 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.331, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.424)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.976 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.110 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.bottleneck/__module.encoder.layer.21.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.110)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.939 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.939)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.939)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.936 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.936)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.936)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.938 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.938)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.938)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.22 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.22)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.937 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.937)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.937)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.317 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.170 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.317, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.input/__module.encoder.layer.21.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.170)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.941 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.941)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.941)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.407 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.407)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.940 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.940)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.940)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.318 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.171 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.318, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.bottleneck/__module.encoder.layer.21.bottleneck.attention/__module.encoder.layer.21.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.171)

FFNLayer._actual_script_module
  graph(%self.951 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.951)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.951)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.952 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.952)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.413 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.413)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.954 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.954)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.954)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.173 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.173)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.953 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.953)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.953)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.intermediate/__module.encoder.layer.21.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.323 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.intermediate/__module.encoder.layer.21.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.intermediate/__module.encoder.layer.21.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.412 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.323, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.intermediate/__module.encoder.layer.21.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.412)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.956 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.173 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.956)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.956)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.173, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.414 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.414)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.955 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.955)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.955)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.324 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.107 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.324, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.0/__module.encoder.layer.21.ffn.0.output/__module.encoder.layer.21.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.107)

FFNLayer._actual_script_module
  graph(%self.957 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.957)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.957)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.958 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.958)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.416 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.416)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.960 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.960)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.960)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.174 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.174)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.959 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.959)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.959)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.intermediate/__module.encoder.layer.21.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.325 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.intermediate/__module.encoder.layer.21.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.intermediate/__module.encoder.layer.21.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.415 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.325, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.intermediate/__module.encoder.layer.21.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.415)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.962 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.174 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.962)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.962)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.174, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.417 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.417)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.961 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.961)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.961)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.326 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.108 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.326, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.1/__module.encoder.layer.21.ffn.1.output/__module.encoder.layer.21.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.108)

FFNLayer._actual_script_module
  graph(%self.963 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.963)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.963)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.964 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.964)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.419 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.419)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.966 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.966)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.966)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.175 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.175)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.965 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.965)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.965)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.intermediate/__module.encoder.layer.21.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.327 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.intermediate/__module.encoder.layer.21.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.intermediate/__module.encoder.layer.21.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.418 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.327, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.intermediate/__module.encoder.layer.21.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.418)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.968 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.175 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.968)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.968)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.175, %3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.420 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.420)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.967 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.967)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.967)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.328 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.109 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.328, %2, %6), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.ffn.2/__module.encoder.layer.21.ffn.2.output/__module.encoder.layer.21.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.109)

MobileBertLayer._actual_script_module
  graph(%self.978 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.978)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.978)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.978)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.978)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.978)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.978)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.978)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.986 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.986)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.986)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.979 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.979)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.979)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1013 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1013)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.441 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate # torch/nn/functional.py:1119:0
    return (%input.441)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.1015 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.1015)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1015)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1015)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.184 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.184)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.992 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.992)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.992)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.180 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.180)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.987 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.987)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.987)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.987)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.987)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %x.134 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %query_layer.23 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.134, %30), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %x.136 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %key_layer.23 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.136, %51), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %x.138 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %value_layer.23 : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.138, %72), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer.23, %74, %75), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.45 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.23, %76), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores.46 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.45, %78), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.427 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.46, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %input.428 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.427, %82, %83), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.428)
    %context_layer.45 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer.23), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.45, %91), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer.46 : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer.46, %95), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer.46, %99), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
    %input.429 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer.46, %110), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.429)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.991 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.428 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.428, %2, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.23)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.989 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.989)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.989)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
    %output.335 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
    %x.135 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.335, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.135)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.988 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.988)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.988)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
    %output.334 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
    %x.133 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.334, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.133)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.990 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.990)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.990)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
    %output.336 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
    %x.137 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.336, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.137)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.994 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.180 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.994)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.994)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.180, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.430 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.430)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.993 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.993)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.993)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
    %output.337 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.111 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.337, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.111)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1014 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1014)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1014)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
    %output.344 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
    %input.440 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.344, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.440)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1017 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.184 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1017)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1017)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.184, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.442 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.442)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.1018 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1018)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.1018)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1018)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor.185 : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.185)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.1016 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1016)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1016)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
    %output.345 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
    %layer_output.23 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.345, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output.23)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.1021 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.185 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1021)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1021)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor.185, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.444 : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.444)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.1019 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1019)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1019)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output.346 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.443 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.346, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.443)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.1020 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs.115 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.bottleneck/__module.encoder.layer.22.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs.115)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.983 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.983)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.983)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.980 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.980)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.980)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.982 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.982)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.982)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor.23 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor.23)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.981 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.981)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.981)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.332 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.178 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.332, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.input/__module.encoder.layer.22.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.178)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.985 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.985)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.985)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.426 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.426)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.984 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.984)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.984)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.333 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.179 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.333, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.bottleneck/__module.encoder.layer.22.bottleneck.attention/__module.encoder.layer.22.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.179)

FFNLayer._actual_script_module
  graph(%self.995 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.995)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.995)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.996 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.996)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.432 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.432)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.998 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.998)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.998)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.181 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.181)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.997 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.997)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.997)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.intermediate/__module.encoder.layer.22.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.338 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.intermediate/__module.encoder.layer.22.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.intermediate/__module.encoder.layer.22.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.431 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.338, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.intermediate/__module.encoder.layer.22.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.431)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1000 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.181 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1000)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1000)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.181, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.433 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.433)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.999 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.999)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.999)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.339 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.112 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.339, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.0/__module.encoder.layer.22.ffn.0.output/__module.encoder.layer.22.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.112)

FFNLayer._actual_script_module
  graph(%self.1001 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.1001)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1001)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1002 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1002)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.435 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.435)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.1004 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1004)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1004)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.182 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.182)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1003 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1003)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1003)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.intermediate/__module.encoder.layer.22.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.340 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.intermediate/__module.encoder.layer.22.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.intermediate/__module.encoder.layer.22.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.434 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.340, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.intermediate/__module.encoder.layer.22.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.434)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1006 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.182 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1006)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1006)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.182, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.436 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.436)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.1005 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1005)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1005)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.341 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.113 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.341, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.1/__module.encoder.layer.22.ffn.1.output/__module.encoder.layer.22.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.113)

FFNLayer._actual_script_module
  graph(%self.1007 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.1007)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1007)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1008 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1008)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.438 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.438)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.1010 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1010)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1010)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.183 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.183)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1009 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1009)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1009)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.intermediate/__module.encoder.layer.22.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.342 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.intermediate/__module.encoder.layer.22.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.intermediate/__module.encoder.layer.22.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.437 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.342, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.intermediate/__module.encoder.layer.22.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.437)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1012 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.183 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1012)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1012)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.183, %3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.439 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.439)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.1011 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1011)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1011)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.343 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.114 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.343, %2, %6), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.ffn.2/__module.encoder.layer.22.ffn.2.output/__module.encoder.layer.22.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.114)

MobileBertLayer._actual_script_module
  graph(%self.1022 : __torch__.transformers.modeling_mobilebert.MobileBertLayer,
        %1 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertOutput = prim::GetAttr[name="output"](%self.1022)
    %4 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1022)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.1022)
    %6 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.1022)
    %8 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffn"](%self.1022)
    %10 : __torch__.transformers.modeling_mobilebert.FFNLayer = prim::GetAttr[name="0"](%9)
    %11 : __torch__.transformers.modeling_mobilebert.MobileBertAttention = prim::GetAttr[name="attention"](%self.1022)
    %12 : __torch__.transformers.modeling_mobilebert.Bottleneck = prim::GetAttr[name="bottleneck"](%self.1022)
    %30 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %1)
    %14 : Float(17:1664, 13:128, 128:1), %15 : Float(17:1664, 13:128, 128:1) = prim::TupleUnpack(%30)
    %31 : Tensor = prim::CallMethod[name="forward"](%11, %14, %1, %attention_mask, %15)
    %32 : Tensor = prim::CallMethod[name="forward"](%10, %31)
    %33 : Tensor = prim::CallMethod[name="forward"](%8, %32)
    %34 : Tensor = prim::CallMethod[name="forward"](%6, %33)
    %35 : Tensor = prim::CallMethod[name="forward"](%4, %34)
    %36 : Tensor = prim::CallMethod[name="forward"](%3, %35, %34, %1)
    return (%36)

MobileBertLayer.attention
MobileBertAttention._actual_script_module
  graph(%self.1030 : __torch__.transformers.modeling_mobilebert.MobileBertAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1),
        %4 : Float(17:1664, 13:128, 128:1)):
    %5 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput = prim::GetAttr[name="output"](%self.1030)
    %6 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention = prim::GetAttr[name="self"](%self.1030)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2, %attention_mask)
    %10 : Tensor = prim::CallMethod[name="forward"](%5, %9, %4)
    return (%10)

MobileBertLayer.bottleneck
Bottleneck._actual_script_module
  graph(%self.1023 : __torch__.transformers.modeling_mobilebert.Bottleneck,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="attention"](%self.1023)
    %3 : __torch__.transformers.modeling_mobilebert.BottleneckLayer = prim::GetAttr[name="input"](%self.1023)
    %7 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %8 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %6 : (Float(17:1664, 13:128, 128:1), Float(17:1664, 13:128, 128:1)) = prim::TupleConstruct(%8, %7)
    return (%6)

MobileBertLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1057 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1057)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.460 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate # torch/nn/functional.py:1119:0
    return (%input.460)

MobileBertLayer.output
MobileBertOutput._actual_script_module
  graph(%self.1059 : __torch__.transformers.modeling_mobilebert.MobileBertOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1),
        %3 : Float(17:6656, 13:512, 512:1)):
    %4 : __torch__.transformers.modeling_mobilebert.OutputBottleneck = prim::GetAttr[name="bottleneck"](%self.1059)
    %5 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1059)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1059)
    %12 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output # transformers/modeling_mobilebert.py:405:0
    %input_tensor.192 : Float(17:1664, 13:128, 128:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output # transformers/modeling_mobilebert.py:405:0
    %13 : Tensor = prim::CallMethod[name="forward"](%5, %input_tensor.192)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %13, %3)
    return (%14)

MobileBertAttention.output
MobileBertSelfOutput._actual_script_module
  graph(%self.1036 : __torch__.transformers.modeling_mobilebert.MobileBertSelfOutput,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1036)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1036)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output # transformers/modeling_mobilebert.py:301:0
    %input_tensor.188 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output # transformers/modeling_mobilebert.py:301:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.188)
    return (%10)

MobileBertAttention.self
MobileBertSelfAttention._actual_script_module
  graph(%self.1031 : __torch__.transformers.modeling_mobilebert.MobileBertSelfAttention,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.1031)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.1031)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.1031)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.1031)
    %112 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %114 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %11 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %12 : int = aten::size(%112, %11), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %13 : Long() = prim::NumToTensor(%12), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %14 : int = aten::Int(%13), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %16 : int = aten::size(%112, %15), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %17 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %18 : int = aten::Int(%17), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %22 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %23 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %24 : int[] = prim::ListConstruct(%14, %18, %22, %23), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %x.140 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%112, %24), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %26 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %28 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %29 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %30 : int[] = prim::ListConstruct(%26, %27, %28, %29), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %query_layer : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.140, %30), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %32 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %33 : int = aten::size(%113, %32), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %34 : Long() = prim::NumToTensor(%33), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %35 : int = aten::Int(%34), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %36 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %37 : int = aten::size(%113, %36), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %38 : Long() = prim::NumToTensor(%37), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %39 : int = aten::Int(%38), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %43 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %44 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %45 : int[] = prim::ListConstruct(%35, %39, %43, %44), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %x.142 : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%113, %45), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %47 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %49 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %51 : int[] = prim::ListConstruct(%47, %48, %49, %50), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %key_layer : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x.142, %51), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %53 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %54 : int = aten::size(%114, %53), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %55 : Long() = prim::NumToTensor(%54), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %56 : int = aten::Int(%55), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %57 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %58 : int = aten::size(%114, %57), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:243:0
    %59 : Long() = prim::NumToTensor(%58), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %60 : int = aten::Int(%59), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %64 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %65 : int = prim::Constant[value=32](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %66 : int[] = prim::ListConstruct(%56, %60, %64, %65), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %x : Float(17:1664, 13:128, 4:32, 32:1) = aten::view(%114, %66), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:244:0
    %68 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %69 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %70 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %71 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %72 : int[] = prim::ListConstruct(%68, %69, %70, %71), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %value_layer : Float(17:1664, 4:32, 13:128, 32:1) = aten::permute(%x, %72), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:245:0
    %74 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:267:0
    %75 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:267:0
    %76 : Float(17:1664, 4:32, 32:1, 13:128) = aten::transpose(%key_layer, %74, %75), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:267:0
    %attention_scores.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %76), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:267:0
    %78 : Double() = prim::Constant[value={5.65685}](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:268:0
    %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.47, %78), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:268:0
    %80 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:271:0
    %input.446 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %80), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:271:0
    %82 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
    %83 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %input.447 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.446, %82, %83), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
    %115 : Tensor = prim::CallMethod[name="forward"](%4, %input.447)
    %context_layer.47 : Float(17:1664, 4:416, 13:32, 32:1) = aten::matmul(%115, %value_layer), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:280:0
    %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %88 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %89 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %90 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %91 : int[] = prim::ListConstruct(%87, %88, %89, %90), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %92 : Float(17:1664, 13:32, 4:416, 32:1) = aten::permute(%context_layer.47, %91), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %93 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %context_layer : Float(17:1664, 13:128, 4:32, 32:1) = aten::contiguous(%92, %93), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:281:0
    %95 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:282:0
    %96 : int = aten::size(%context_layer, %95), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:282:0
    %97 : Long() = prim::NumToTensor(%96), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %98 : int = aten::Int(%97), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %99 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:282:0
    %100 : int = aten::size(%context_layer, %99), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:282:0
    %101 : Long() = prim::NumToTensor(%100), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %102 : int = aten::Int(%101), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %109 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:283:0
    %110 : int[] = prim::ListConstruct(%98, %102, %109), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
    %input.448 : Float(17:1664, 13:128, 128:1) = aten::view(%context_layer, %110), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_mobilebert.py:283:0
    return (%input.448)

MobileBertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.1035 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.447 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.447, %2, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs)

MobileBertSelfAttention.key
Linear._actual_script_module
  graph(%self.1033 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1033)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1033)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
    %output.350 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
    %x.141 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.350, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.141)

MobileBertSelfAttention.query
Linear._actual_script_module
  graph(%self.1032 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1032)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1032)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
    %output.349 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
    %x.139 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.349, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.139)

MobileBertSelfAttention.value
Linear._actual_script_module
  graph(%self.1034 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1034)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1034)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
    %output.351 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
    %x.143 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.351, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.143)

MobileBertSelfOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1038 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.188 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1038)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1038)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.188, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.449 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.449)

MobileBertSelfOutput.dense
Linear._actual_script_module
  graph(%self.1037 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1037)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1037)
    %4 : Float(128:1, 128:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
    %output.352 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.116 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.352, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.116)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1058 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1058)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1058)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
    %output.359 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
    %input.459 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.359, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.459)

MobileBertOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1061 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.192 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1061)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1061)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.192, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.461 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.461)

MobileBertOutput.bottleneck
OutputBottleneck._actual_script_module
  graph(%self.1062 : __torch__.transformers.modeling_mobilebert.OutputBottleneck,
        %1 : Float(17:1664, 13:128, 128:1),
        %2 : Float(17:6656, 13:512, 512:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1062)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.1062)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1062)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %input_tensor : Float(17:6656, 13:512, 512:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck # transformers/modeling_mobilebert.py:384:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor)
    return (%13)

MobileBertOutput.dense
Linear._actual_script_module
  graph(%self.1060 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1060)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1060)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
    %output.360 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
    %layer_output : Float(17:1664, 13:128, 128:1) = aten::add_(%output.360, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
    return (%layer_output)

OutputBottleneck.LayerNorm
NoNorm._actual_script_module
  graph(%self.1065 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1065)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1065)
    %4 : Float(17:6656, 13:512, 512:1) = aten::mul(%input_tensor, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %hidden_states : Float(17:6656, 13:512, 512:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%hidden_states)

OutputBottleneck.dense
Linear._actual_script_module
  graph(%self.1063 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1063)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1063)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %output : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dense # torch/nn/functional.py:1678:0
    %input.462 : Float(17:6656, 13:512, 512:1) = aten::add_(%output, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dense # torch/nn/functional.py:1678:0
    return (%input.462)

OutputBottleneck.dropout
Dropout._actual_script_module
  graph(%self.1064 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dropout # torch/nn/functional.py:973:0
    %layer_outputs : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.bottleneck/__module.encoder.layer.23.output.bottleneck.dropout # torch/nn/functional.py:973:0
    return (%layer_outputs)

Bottleneck.attention
BottleneckLayer._actual_script_module
  graph(%self.1027 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1027)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1027)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

Bottleneck.input
BottleneckLayer._actual_script_module
  graph(%self.1024 : __torch__.transformers.modeling_mobilebert.BottleneckLayer,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1024)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1024)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6)
    return (%7)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.1026 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1026)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1026)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %residual_tensor : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%residual_tensor)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.1025 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1025)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1025)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %output.347 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.dense # torch/nn/functional.py:1678:0
    %input_tensor.186 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.347, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.input/__module.encoder.layer.23.bottleneck.input.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.186)

BottleneckLayer.LayerNorm
NoNorm._actual_script_module
  graph(%self.1029 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1029)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1029)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%1, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.445 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.445)

BottleneckLayer.dense
Linear._actual_script_module
  graph(%self.1028 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1028)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1028)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %output.348 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    %input_tensor.187 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.348, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.bottleneck/__module.encoder.layer.23.bottleneck.attention/__module.encoder.layer.23.bottleneck.attention.dense # torch/nn/functional.py:1678:0
    return (%input_tensor.187)

FFNLayer._actual_script_module
  graph(%self.1039 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.1039)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1039)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1040 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1040)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.451 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.intermediate # torch/nn/functional.py:1119:0
    return (%input.451)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.1042 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1042)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1042)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.189 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.189)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1041 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1041)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1041)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.intermediate/__module.encoder.layer.23.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.353 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.intermediate/__module.encoder.layer.23.ffn.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.intermediate/__module.encoder.layer.23.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.450 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.353, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.intermediate/__module.encoder.layer.23.ffn.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.450)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1044 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.189 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1044)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1044)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.189, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.452 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.452)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.1043 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1043)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1043)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %output.354 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.117 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.354, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.0/__module.encoder.layer.23.ffn.0.output/__module.encoder.layer.23.ffn.0.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.117)

FFNLayer._actual_script_module
  graph(%self.1045 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.1045)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1045)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1046 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1046)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.454 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.intermediate # torch/nn/functional.py:1119:0
    return (%input.454)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.1048 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1048)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1048)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.190 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.190)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1047 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1047)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1047)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.intermediate/__module.encoder.layer.23.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.355 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.intermediate/__module.encoder.layer.23.ffn.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.intermediate/__module.encoder.layer.23.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.453 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.355, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.intermediate/__module.encoder.layer.23.ffn.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.453)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1050 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.190 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1050)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1050)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.190, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.455 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.455)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.1049 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1049)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1049)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %output.356 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.118 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.356, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.1/__module.encoder.layer.23.ffn.1.output/__module.encoder.layer.23.ffn.1.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.118)

FFNLayer._actual_script_module
  graph(%self.1051 : __torch__.transformers.modeling_mobilebert.FFNLayer,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.transformers.modeling_mobilebert.FFNOutput = prim::GetAttr[name="output"](%self.1051)
    %3 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate = prim::GetAttr[name="intermediate"](%self.1051)
    %6 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %7 : Tensor = prim::CallMethod[name="forward"](%2, %6, %1)
    return (%7)

FFNLayer.intermediate
MobileBertIntermediate._actual_script_module
  graph(%self.1052 : __torch__.transformers.modeling_mobilebert.MobileBertIntermediate,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1052)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.457 : Float(17:6656, 13:512, 512:1) = aten::relu(%5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.intermediate # torch/nn/functional.py:1119:0
    return (%input.457)

FFNLayer.output
FFNOutput._actual_script_module
  graph(%self.1054 : __torch__.transformers.modeling_mobilebert.FFNOutput,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1664, 13:128, 128:1)):
    %3 : __torch__.transformers.modeling_mobilebert.NoNorm = prim::GetAttr[name="LayerNorm"](%self.1054)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.1054)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %input_tensor.191 : Float(17:1664, 13:128, 128:1) = aten::add(%9, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output # transformers/modeling_mobilebert.py:466:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input_tensor.191)
    return (%10)

MobileBertIntermediate.dense
Linear._actual_script_module
  graph(%self.1053 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1053)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1053)
    %4 : Float(128:1, 512:128) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.intermediate/__module.encoder.layer.23.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.357 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.intermediate/__module.encoder.layer.23.ffn.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.intermediate/__module.encoder.layer.23.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.456 : Float(17:6656, 13:512, 512:1) = aten::add_(%output.357, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.intermediate/__module.encoder.layer.23.ffn.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.456)

FFNOutput.LayerNorm
NoNorm._actual_script_module
  graph(%self.1056 : __torch__.transformers.modeling_mobilebert.NoNorm,
        %input_tensor.191 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1056)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1056)
    %4 : Float(17:1664, 13:128, 128:1) = aten::mul(%input_tensor.191, %3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %5 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    %input.458 : Float(17:1664, 13:128, 128:1) = aten::add(%4, %2, %5), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.LayerNorm # transformers/modeling_mobilebert.py:154:0
    return (%input.458)

FFNOutput.dense
Linear._actual_script_module
  graph(%self.1055 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.1055)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.1055)
    %4 : Float(512:1, 128:512) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %output.358 : Float(17:1664, 13:128, 128:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.dense # torch/nn/functional.py:1678:0
    %layer_outputs.119 : Float(17:1664, 13:128, 128:1) = aten::add_(%output.358, %2, %6), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.ffn.2/__module.encoder.layer.23.ffn.2.output/__module.encoder.layer.23.ffn.2.output.dense # torch/nn/functional.py:1678:0
    return (%layer_outputs.119)

MobileBertPooler.dense
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %input : Float(17:6656, 512:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self)
    %3 : Tensor = prim::GetAttr[name="weight"](%self)
    %4 : Float(512:1, 512:512) = aten::t(%3), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %6 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %pooled_output : Float(17:512, 512:1) = aten::addmm(%2, %input, %4, %5, %6), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    return (%pooled_output)

