graph(%self.1 : __torch__.transformers.modeling_bert.BertLMHeadModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_bert.___torch_mangle_5922.BertOnlyMLMHead = prim::GetAttr[name="cls"](%self.1)
  %4 : __torch__.transformers.modeling_bert.___torch_mangle_5916.BertModel = prim::GetAttr[name="bert"](%self.1)
  %8 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %9 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %10 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %11 : int = prim::Constant[value=12](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %12 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %13 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %14 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %15 : int = prim::Constant[value=768](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %17 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
  %18 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
  %19 : None = prim::Constant(), scope: __module.bert
  %20 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %21 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %22 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %24 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %25 : Device = prim::Constant[value="cpu"](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %26 : int = prim::Constant[value=4](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %27 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %28 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %29 : __torch__.transformers.modeling_bert.___torch_mangle_5915.BertEncoder = prim::GetAttr[name="encoder"](%4)
  %30 : __torch__.transformers.modeling_bert.___torch_mangle_5709.BertEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %31 : int = aten::size(%input_ids, %28), scope: __module.bert # transformers/modeling_bert.py:795:0
  %32 : int = aten::size(%input_ids, %27), scope: __module.bert # transformers/modeling_bert.py:795:0
  %33 : int[] = prim::ListConstruct(%31, %32), scope: __module.bert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%33, %26, %28, %25, %24), scope: __module.bert # transformers/modeling_bert.py:806:0
  %35 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %28, %28, %23, %27), scope: __module.bert # transformers/modeling_utils.py:244:0
  %36 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%35, %27), scope: __module.bert # transformers/modeling_utils.py:244:0
  %37 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%36, %22), scope: __module.bert # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%37, %21, %28, %23, %27), scope: __module.bert # transformers/modeling_utils.py:244:0
  %39 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %20, %24, %24, %19), scope: __module.bert # transformers/modeling_utils.py:257:0
  %40 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%39, %18, %27), scope: __module.bert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%40, %17), scope: __module.bert # transformers/modeling_utils.py:258:0
  %42 : __torch__.torch.nn.modules.normalization.___torch_mangle_5707.LayerNorm = prim::GetAttr[name="LayerNorm"](%30)
  %43 : __torch__.torch.nn.modules.sparse.___torch_mangle_5706.Embedding = prim::GetAttr[name="token_type_embeddings"](%30)
  %44 : __torch__.torch.nn.modules.sparse.___torch_mangle_5705.Embedding = prim::GetAttr[name="position_embeddings"](%30)
  %45 : __torch__.torch.nn.modules.sparse.___torch_mangle_5704.Embedding = prim::GetAttr[name="word_embeddings"](%30)
  %46 : Tensor = prim::GetAttr[name="position_ids"](%30)
  %47 : int = aten::size(%input_ids, %27), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:184:0
  %48 : Long(1:512, 512:1) = aten::slice(%46, %28, %28, %23, %27), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%48, %27, %28, %47, %27), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %50 : Tensor = prim::GetAttr[name="weight"](%45)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%50, %input_ids, %28, %24, %24), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %52 : Tensor = prim::GetAttr[name="weight"](%44)
  %position_embeddings : Float(1:9984, 13:768, 768:1) = aten::embedding(%52, %input.1, %12, %24, %24), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %54 : Tensor = prim::GetAttr[name="weight"](%43)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%54, %input.2, %12, %24, %24), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %56 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %27), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%56, %token_type_embeddings, %27), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %58 : Tensor = prim::GetAttr[name="bias"](%42)
  %59 : Tensor = prim::GetAttr[name="weight"](%42)
  %60 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %60, %59, %58, %14, %13), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %16, %24), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %64 : __torch__.transformers.modeling_bert.___torch_mangle_5913.BertLayer = prim::GetAttr[name="11"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %66 : __torch__.transformers.modeling_bert.___torch_mangle_5896.BertLayer = prim::GetAttr[name="10"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %68 : __torch__.transformers.modeling_bert.___torch_mangle_5879.BertLayer = prim::GetAttr[name="9"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %70 : __torch__.transformers.modeling_bert.___torch_mangle_5862.BertLayer = prim::GetAttr[name="8"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %72 : __torch__.transformers.modeling_bert.___torch_mangle_5845.BertLayer = prim::GetAttr[name="7"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %74 : __torch__.transformers.modeling_bert.___torch_mangle_5828.BertLayer = prim::GetAttr[name="6"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %76 : __torch__.transformers.modeling_bert.___torch_mangle_5811.BertLayer = prim::GetAttr[name="5"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %78 : __torch__.transformers.modeling_bert.___torch_mangle_5794.BertLayer = prim::GetAttr[name="4"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %80 : __torch__.transformers.modeling_bert.___torch_mangle_5777.BertLayer = prim::GetAttr[name="3"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %82 : __torch__.transformers.modeling_bert.___torch_mangle_5760.BertLayer = prim::GetAttr[name="2"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %84 : __torch__.transformers.modeling_bert.___torch_mangle_5743.BertLayer = prim::GetAttr[name="1"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_5914.ModuleList = prim::GetAttr[name="layer"](%29)
  %86 : __torch__.transformers.modeling_bert.___torch_mangle_5726.BertLayer = prim::GetAttr[name="0"](%85)
  %87 : __torch__.transformers.modeling_bert.___torch_mangle_5725.BertOutput = prim::GetAttr[name="output"](%86)
  %88 : __torch__.transformers.modeling_bert.___torch_mangle_5721.BertIntermediate = prim::GetAttr[name="intermediate"](%86)
  %89 : __torch__.transformers.modeling_bert.___torch_mangle_5719.BertAttention = prim::GetAttr[name="attention"](%86)
  %90 : __torch__.transformers.modeling_bert.___torch_mangle_5718.BertSelfOutput = prim::GetAttr[name="output"](%89)
  %91 : __torch__.transformers.modeling_bert.___torch_mangle_5714.BertSelfAttention = prim::GetAttr[name="self"](%89)
  %92 : __torch__.torch.nn.modules.linear.___torch_mangle_5712.Linear = prim::GetAttr[name="value"](%91)
  %93 : __torch__.torch.nn.modules.linear.___torch_mangle_5711.Linear = prim::GetAttr[name="key"](%91)
  %94 : __torch__.torch.nn.modules.linear.___torch_mangle_5710.Linear = prim::GetAttr[name="query"](%91)
  %95 : Tensor = prim::GetAttr[name="bias"](%94)
  %96 : Tensor = prim::GetAttr[name="weight"](%94)
  %97 : Float(768:1, 768:768) = aten::t(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %97), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %95, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %100 : Tensor = prim::GetAttr[name="bias"](%93)
  %101 : Tensor = prim::GetAttr[name="weight"](%93)
  %102 : Float(768:1, 768:768) = aten::t(%101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %102), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %100, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %105 : Tensor = prim::GetAttr[name="bias"](%92)
  %106 : Tensor = prim::GetAttr[name="weight"](%92)
  %107 : Float(768:1, 768:768) = aten::t(%106), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %107), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %105, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %110 : int = aten::size(%x.1, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %111 : int = aten::size(%x.1, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %112 : int[] = prim::ListConstruct(%110, %111, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %112), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %114 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %114), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %116 : int = aten::size(%x.3, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %117 : int = aten::size(%x.3, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %118 : int[] = prim::ListConstruct(%116, %117, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %118), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %120 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %122 : int = aten::size(%x.5, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %123 : int = aten::size(%x.5, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %124 : int[] = prim::ListConstruct(%122, %123, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %124), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %126 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %126), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %128 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %128), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %135 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %136 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %135), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%136, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %138 : int = aten::size(%context_layer.2, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %139 : int = aten::size(%context_layer.2, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %140 : int[] = prim::ListConstruct(%138, %139, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %140), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %142 : __torch__.torch.nn.modules.normalization.___torch_mangle_5716.LayerNorm = prim::GetAttr[name="LayerNorm"](%90)
  %143 : __torch__.torch.nn.modules.linear.___torch_mangle_5715.Linear = prim::GetAttr[name="dense"](%90)
  %144 : Tensor = prim::GetAttr[name="bias"](%143)
  %145 : Tensor = prim::GetAttr[name="weight"](%143)
  %146 : Float(768:1, 768:768) = aten::t(%145), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %146), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %144, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %151 : Tensor = prim::GetAttr[name="bias"](%142)
  %152 : Tensor = prim::GetAttr[name="weight"](%142)
  %153 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %153, %152, %151, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %155 : __torch__.torch.nn.modules.linear.___torch_mangle_5720.Linear = prim::GetAttr[name="dense"](%88)
  %156 : Tensor = prim::GetAttr[name="bias"](%155)
  %157 : Tensor = prim::GetAttr[name="weight"](%155)
  %158 : Float(768:1, 3072:768) = aten::t(%157), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %158), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %156, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %162 : __torch__.torch.nn.modules.normalization.___torch_mangle_5723.LayerNorm = prim::GetAttr[name="LayerNorm"](%87)
  %163 : __torch__.torch.nn.modules.linear.___torch_mangle_5722.Linear = prim::GetAttr[name="dense"](%87)
  %164 : Tensor = prim::GetAttr[name="bias"](%163)
  %165 : Tensor = prim::GetAttr[name="weight"](%163)
  %166 : Float(3072:1, 768:3072) = aten::t(%165), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %166), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %164, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %171 : Tensor = prim::GetAttr[name="bias"](%162)
  %172 : Tensor = prim::GetAttr[name="weight"](%162)
  %173 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %173, %172, %171, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %175 : __torch__.transformers.modeling_bert.___torch_mangle_5742.BertOutput = prim::GetAttr[name="output"](%84)
  %176 : __torch__.transformers.modeling_bert.___torch_mangle_5738.BertIntermediate = prim::GetAttr[name="intermediate"](%84)
  %177 : __torch__.transformers.modeling_bert.___torch_mangle_5736.BertAttention = prim::GetAttr[name="attention"](%84)
  %178 : __torch__.transformers.modeling_bert.___torch_mangle_5735.BertSelfOutput = prim::GetAttr[name="output"](%177)
  %179 : __torch__.transformers.modeling_bert.___torch_mangle_5731.BertSelfAttention = prim::GetAttr[name="self"](%177)
  %180 : __torch__.torch.nn.modules.linear.___torch_mangle_5729.Linear = prim::GetAttr[name="value"](%179)
  %181 : __torch__.torch.nn.modules.linear.___torch_mangle_5728.Linear = prim::GetAttr[name="key"](%179)
  %182 : __torch__.torch.nn.modules.linear.___torch_mangle_5727.Linear = prim::GetAttr[name="query"](%179)
  %183 : Tensor = prim::GetAttr[name="bias"](%182)
  %184 : Tensor = prim::GetAttr[name="weight"](%182)
  %185 : Float(768:1, 768:768) = aten::t(%184), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %185), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %183, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %188 : Tensor = prim::GetAttr[name="bias"](%181)
  %189 : Tensor = prim::GetAttr[name="weight"](%181)
  %190 : Float(768:1, 768:768) = aten::t(%189), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %190), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %188, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %193 : Tensor = prim::GetAttr[name="bias"](%180)
  %194 : Tensor = prim::GetAttr[name="weight"](%180)
  %195 : Float(768:1, 768:768) = aten::t(%194), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %195), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %193, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %198 : int = aten::size(%x.7, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %199 : int = aten::size(%x.7, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %200 : int[] = prim::ListConstruct(%198, %199, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %200), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %202 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %202), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %204 : int = aten::size(%x.9, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %205 : int = aten::size(%x.9, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %206 : int[] = prim::ListConstruct(%204, %205, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %206), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %208 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %208), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %210 : int = aten::size(%x.11, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %211 : int = aten::size(%x.11, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %212 : int[] = prim::ListConstruct(%210, %211, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %212), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %214 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %214), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %216 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %216), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %223 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %224 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %223), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%224, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %226 : int = aten::size(%context_layer.4, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %227 : int = aten::size(%context_layer.4, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %228 : int[] = prim::ListConstruct(%226, %227, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %228), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %230 : __torch__.torch.nn.modules.normalization.___torch_mangle_5733.LayerNorm = prim::GetAttr[name="LayerNorm"](%178)
  %231 : __torch__.torch.nn.modules.linear.___torch_mangle_5732.Linear = prim::GetAttr[name="dense"](%178)
  %232 : Tensor = prim::GetAttr[name="bias"](%231)
  %233 : Tensor = prim::GetAttr[name="weight"](%231)
  %234 : Float(768:1, 768:768) = aten::t(%233), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %234), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %232, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %239 : Tensor = prim::GetAttr[name="bias"](%230)
  %240 : Tensor = prim::GetAttr[name="weight"](%230)
  %241 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %241, %240, %239, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %243 : __torch__.torch.nn.modules.linear.___torch_mangle_5737.Linear = prim::GetAttr[name="dense"](%176)
  %244 : Tensor = prim::GetAttr[name="bias"](%243)
  %245 : Tensor = prim::GetAttr[name="weight"](%243)
  %246 : Float(768:1, 3072:768) = aten::t(%245), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %246), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %244, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %250 : __torch__.torch.nn.modules.normalization.___torch_mangle_5740.LayerNorm = prim::GetAttr[name="LayerNorm"](%175)
  %251 : __torch__.torch.nn.modules.linear.___torch_mangle_5739.Linear = prim::GetAttr[name="dense"](%175)
  %252 : Tensor = prim::GetAttr[name="bias"](%251)
  %253 : Tensor = prim::GetAttr[name="weight"](%251)
  %254 : Float(3072:1, 768:3072) = aten::t(%253), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %254), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %252, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %259 : Tensor = prim::GetAttr[name="bias"](%250)
  %260 : Tensor = prim::GetAttr[name="weight"](%250)
  %261 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %261, %260, %259, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %263 : __torch__.transformers.modeling_bert.___torch_mangle_5759.BertOutput = prim::GetAttr[name="output"](%82)
  %264 : __torch__.transformers.modeling_bert.___torch_mangle_5755.BertIntermediate = prim::GetAttr[name="intermediate"](%82)
  %265 : __torch__.transformers.modeling_bert.___torch_mangle_5753.BertAttention = prim::GetAttr[name="attention"](%82)
  %266 : __torch__.transformers.modeling_bert.___torch_mangle_5752.BertSelfOutput = prim::GetAttr[name="output"](%265)
  %267 : __torch__.transformers.modeling_bert.___torch_mangle_5748.BertSelfAttention = prim::GetAttr[name="self"](%265)
  %268 : __torch__.torch.nn.modules.linear.___torch_mangle_5746.Linear = prim::GetAttr[name="value"](%267)
  %269 : __torch__.torch.nn.modules.linear.___torch_mangle_5745.Linear = prim::GetAttr[name="key"](%267)
  %270 : __torch__.torch.nn.modules.linear.___torch_mangle_5744.Linear = prim::GetAttr[name="query"](%267)
  %271 : Tensor = prim::GetAttr[name="bias"](%270)
  %272 : Tensor = prim::GetAttr[name="weight"](%270)
  %273 : Float(768:1, 768:768) = aten::t(%272), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %273), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %271, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %276 : Tensor = prim::GetAttr[name="bias"](%269)
  %277 : Tensor = prim::GetAttr[name="weight"](%269)
  %278 : Float(768:1, 768:768) = aten::t(%277), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %278), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %276, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %281 : Tensor = prim::GetAttr[name="bias"](%268)
  %282 : Tensor = prim::GetAttr[name="weight"](%268)
  %283 : Float(768:1, 768:768) = aten::t(%282), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %283), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %281, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %286 : int = aten::size(%x.13, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %287 : int = aten::size(%x.13, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %288 : int[] = prim::ListConstruct(%286, %287, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %288), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %290 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %290), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %292 : int = aten::size(%x.15, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %293 : int = aten::size(%x.15, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %294 : int[] = prim::ListConstruct(%292, %293, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %294), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %296 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %296), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %298 : int = aten::size(%x.17, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %299 : int = aten::size(%x.17, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %300 : int[] = prim::ListConstruct(%298, %299, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %300), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %302 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %302), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %304 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %304), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %311 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %312 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %311), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%312, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %314 : int = aten::size(%context_layer.6, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %315 : int = aten::size(%context_layer.6, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %316 : int[] = prim::ListConstruct(%314, %315, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %316), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %318 : __torch__.torch.nn.modules.normalization.___torch_mangle_5750.LayerNorm = prim::GetAttr[name="LayerNorm"](%266)
  %319 : __torch__.torch.nn.modules.linear.___torch_mangle_5749.Linear = prim::GetAttr[name="dense"](%266)
  %320 : Tensor = prim::GetAttr[name="bias"](%319)
  %321 : Tensor = prim::GetAttr[name="weight"](%319)
  %322 : Float(768:1, 768:768) = aten::t(%321), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %322), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %320, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %327 : Tensor = prim::GetAttr[name="bias"](%318)
  %328 : Tensor = prim::GetAttr[name="weight"](%318)
  %329 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %329, %328, %327, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %331 : __torch__.torch.nn.modules.linear.___torch_mangle_5754.Linear = prim::GetAttr[name="dense"](%264)
  %332 : Tensor = prim::GetAttr[name="bias"](%331)
  %333 : Tensor = prim::GetAttr[name="weight"](%331)
  %334 : Float(768:1, 3072:768) = aten::t(%333), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %334), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %332, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %338 : __torch__.torch.nn.modules.normalization.___torch_mangle_5757.LayerNorm = prim::GetAttr[name="LayerNorm"](%263)
  %339 : __torch__.torch.nn.modules.linear.___torch_mangle_5756.Linear = prim::GetAttr[name="dense"](%263)
  %340 : Tensor = prim::GetAttr[name="bias"](%339)
  %341 : Tensor = prim::GetAttr[name="weight"](%339)
  %342 : Float(3072:1, 768:3072) = aten::t(%341), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %342), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %340, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %347 : Tensor = prim::GetAttr[name="bias"](%338)
  %348 : Tensor = prim::GetAttr[name="weight"](%338)
  %349 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %349, %348, %347, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %351 : __torch__.transformers.modeling_bert.___torch_mangle_5776.BertOutput = prim::GetAttr[name="output"](%80)
  %352 : __torch__.transformers.modeling_bert.___torch_mangle_5772.BertIntermediate = prim::GetAttr[name="intermediate"](%80)
  %353 : __torch__.transformers.modeling_bert.___torch_mangle_5770.BertAttention = prim::GetAttr[name="attention"](%80)
  %354 : __torch__.transformers.modeling_bert.___torch_mangle_5769.BertSelfOutput = prim::GetAttr[name="output"](%353)
  %355 : __torch__.transformers.modeling_bert.___torch_mangle_5765.BertSelfAttention = prim::GetAttr[name="self"](%353)
  %356 : __torch__.torch.nn.modules.linear.___torch_mangle_5763.Linear = prim::GetAttr[name="value"](%355)
  %357 : __torch__.torch.nn.modules.linear.___torch_mangle_5762.Linear = prim::GetAttr[name="key"](%355)
  %358 : __torch__.torch.nn.modules.linear.___torch_mangle_5761.Linear = prim::GetAttr[name="query"](%355)
  %359 : Tensor = prim::GetAttr[name="bias"](%358)
  %360 : Tensor = prim::GetAttr[name="weight"](%358)
  %361 : Float(768:1, 768:768) = aten::t(%360), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %361), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %359, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %364 : Tensor = prim::GetAttr[name="bias"](%357)
  %365 : Tensor = prim::GetAttr[name="weight"](%357)
  %366 : Float(768:1, 768:768) = aten::t(%365), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %366), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %364, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %369 : Tensor = prim::GetAttr[name="bias"](%356)
  %370 : Tensor = prim::GetAttr[name="weight"](%356)
  %371 : Float(768:1, 768:768) = aten::t(%370), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %371), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %369, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %374 : int = aten::size(%x.19, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %375 : int = aten::size(%x.19, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %376 : int[] = prim::ListConstruct(%374, %375, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %376), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %378 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %378), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %380 : int = aten::size(%x.21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %381 : int = aten::size(%x.21, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %382 : int[] = prim::ListConstruct(%380, %381, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %382), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %384 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %384), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %386 : int = aten::size(%x.23, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %387 : int = aten::size(%x.23, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %388 : int[] = prim::ListConstruct(%386, %387, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %388), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %390 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %390), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %392 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %392), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %399 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %400 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %399), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%400, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %402 : int = aten::size(%context_layer.8, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %403 : int = aten::size(%context_layer.8, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %404 : int[] = prim::ListConstruct(%402, %403, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %404), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %406 : __torch__.torch.nn.modules.normalization.___torch_mangle_5767.LayerNorm = prim::GetAttr[name="LayerNorm"](%354)
  %407 : __torch__.torch.nn.modules.linear.___torch_mangle_5766.Linear = prim::GetAttr[name="dense"](%354)
  %408 : Tensor = prim::GetAttr[name="bias"](%407)
  %409 : Tensor = prim::GetAttr[name="weight"](%407)
  %410 : Float(768:1, 768:768) = aten::t(%409), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %410), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %408, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %415 : Tensor = prim::GetAttr[name="bias"](%406)
  %416 : Tensor = prim::GetAttr[name="weight"](%406)
  %417 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %417, %416, %415, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %419 : __torch__.torch.nn.modules.linear.___torch_mangle_5771.Linear = prim::GetAttr[name="dense"](%352)
  %420 : Tensor = prim::GetAttr[name="bias"](%419)
  %421 : Tensor = prim::GetAttr[name="weight"](%419)
  %422 : Float(768:1, 3072:768) = aten::t(%421), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %422), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %420, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %426 : __torch__.torch.nn.modules.normalization.___torch_mangle_5774.LayerNorm = prim::GetAttr[name="LayerNorm"](%351)
  %427 : __torch__.torch.nn.modules.linear.___torch_mangle_5773.Linear = prim::GetAttr[name="dense"](%351)
  %428 : Tensor = prim::GetAttr[name="bias"](%427)
  %429 : Tensor = prim::GetAttr[name="weight"](%427)
  %430 : Float(3072:1, 768:3072) = aten::t(%429), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %430), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %428, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %435 : Tensor = prim::GetAttr[name="bias"](%426)
  %436 : Tensor = prim::GetAttr[name="weight"](%426)
  %437 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %437, %436, %435, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %439 : __torch__.transformers.modeling_bert.___torch_mangle_5793.BertOutput = prim::GetAttr[name="output"](%78)
  %440 : __torch__.transformers.modeling_bert.___torch_mangle_5789.BertIntermediate = prim::GetAttr[name="intermediate"](%78)
  %441 : __torch__.transformers.modeling_bert.___torch_mangle_5787.BertAttention = prim::GetAttr[name="attention"](%78)
  %442 : __torch__.transformers.modeling_bert.___torch_mangle_5786.BertSelfOutput = prim::GetAttr[name="output"](%441)
  %443 : __torch__.transformers.modeling_bert.___torch_mangle_5782.BertSelfAttention = prim::GetAttr[name="self"](%441)
  %444 : __torch__.torch.nn.modules.linear.___torch_mangle_5780.Linear = prim::GetAttr[name="value"](%443)
  %445 : __torch__.torch.nn.modules.linear.___torch_mangle_5779.Linear = prim::GetAttr[name="key"](%443)
  %446 : __torch__.torch.nn.modules.linear.___torch_mangle_5778.Linear = prim::GetAttr[name="query"](%443)
  %447 : Tensor = prim::GetAttr[name="bias"](%446)
  %448 : Tensor = prim::GetAttr[name="weight"](%446)
  %449 : Float(768:1, 768:768) = aten::t(%448), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %449), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %447, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %452 : Tensor = prim::GetAttr[name="bias"](%445)
  %453 : Tensor = prim::GetAttr[name="weight"](%445)
  %454 : Float(768:1, 768:768) = aten::t(%453), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %454), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %452, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %457 : Tensor = prim::GetAttr[name="bias"](%444)
  %458 : Tensor = prim::GetAttr[name="weight"](%444)
  %459 : Float(768:1, 768:768) = aten::t(%458), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %459), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %457, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %462 : int = aten::size(%x.25, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %463 : int = aten::size(%x.25, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %464 : int[] = prim::ListConstruct(%462, %463, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %464), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %466 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %466), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %468 : int = aten::size(%x.27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %469 : int = aten::size(%x.27, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %470 : int[] = prim::ListConstruct(%468, %469, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %470), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %472 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %472), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %474 : int = aten::size(%x.29, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %475 : int = aten::size(%x.29, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %476 : int[] = prim::ListConstruct(%474, %475, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %476), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %478 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %478), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %480 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %480), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %487 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %488 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %487), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%488, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %490 : int = aten::size(%context_layer.10, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %491 : int = aten::size(%context_layer.10, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %492 : int[] = prim::ListConstruct(%490, %491, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %492), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %494 : __torch__.torch.nn.modules.normalization.___torch_mangle_5784.LayerNorm = prim::GetAttr[name="LayerNorm"](%442)
  %495 : __torch__.torch.nn.modules.linear.___torch_mangle_5783.Linear = prim::GetAttr[name="dense"](%442)
  %496 : Tensor = prim::GetAttr[name="bias"](%495)
  %497 : Tensor = prim::GetAttr[name="weight"](%495)
  %498 : Float(768:1, 768:768) = aten::t(%497), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %498), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %496, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %503 : Tensor = prim::GetAttr[name="bias"](%494)
  %504 : Tensor = prim::GetAttr[name="weight"](%494)
  %505 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %505, %504, %503, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %507 : __torch__.torch.nn.modules.linear.___torch_mangle_5788.Linear = prim::GetAttr[name="dense"](%440)
  %508 : Tensor = prim::GetAttr[name="bias"](%507)
  %509 : Tensor = prim::GetAttr[name="weight"](%507)
  %510 : Float(768:1, 3072:768) = aten::t(%509), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %510), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %508, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %514 : __torch__.torch.nn.modules.normalization.___torch_mangle_5791.LayerNorm = prim::GetAttr[name="LayerNorm"](%439)
  %515 : __torch__.torch.nn.modules.linear.___torch_mangle_5790.Linear = prim::GetAttr[name="dense"](%439)
  %516 : Tensor = prim::GetAttr[name="bias"](%515)
  %517 : Tensor = prim::GetAttr[name="weight"](%515)
  %518 : Float(3072:1, 768:3072) = aten::t(%517), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %518), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %516, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %523 : Tensor = prim::GetAttr[name="bias"](%514)
  %524 : Tensor = prim::GetAttr[name="weight"](%514)
  %525 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %525, %524, %523, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %527 : __torch__.transformers.modeling_bert.___torch_mangle_5810.BertOutput = prim::GetAttr[name="output"](%76)
  %528 : __torch__.transformers.modeling_bert.___torch_mangle_5806.BertIntermediate = prim::GetAttr[name="intermediate"](%76)
  %529 : __torch__.transformers.modeling_bert.___torch_mangle_5804.BertAttention = prim::GetAttr[name="attention"](%76)
  %530 : __torch__.transformers.modeling_bert.___torch_mangle_5803.BertSelfOutput = prim::GetAttr[name="output"](%529)
  %531 : __torch__.transformers.modeling_bert.___torch_mangle_5799.BertSelfAttention = prim::GetAttr[name="self"](%529)
  %532 : __torch__.torch.nn.modules.linear.___torch_mangle_5797.Linear = prim::GetAttr[name="value"](%531)
  %533 : __torch__.torch.nn.modules.linear.___torch_mangle_5796.Linear = prim::GetAttr[name="key"](%531)
  %534 : __torch__.torch.nn.modules.linear.___torch_mangle_5795.Linear = prim::GetAttr[name="query"](%531)
  %535 : Tensor = prim::GetAttr[name="bias"](%534)
  %536 : Tensor = prim::GetAttr[name="weight"](%534)
  %537 : Float(768:1, 768:768) = aten::t(%536), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %537), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %535, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %540 : Tensor = prim::GetAttr[name="bias"](%533)
  %541 : Tensor = prim::GetAttr[name="weight"](%533)
  %542 : Float(768:1, 768:768) = aten::t(%541), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %542), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %540, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %545 : Tensor = prim::GetAttr[name="bias"](%532)
  %546 : Tensor = prim::GetAttr[name="weight"](%532)
  %547 : Float(768:1, 768:768) = aten::t(%546), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %547), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %545, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %550 : int = aten::size(%x.31, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %551 : int = aten::size(%x.31, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %552 : int[] = prim::ListConstruct(%550, %551, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %552), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %554 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %554), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %556 : int = aten::size(%x.33, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %557 : int = aten::size(%x.33, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %558 : int[] = prim::ListConstruct(%556, %557, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %558), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %560 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %560), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %562 : int = aten::size(%x.35, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %563 : int = aten::size(%x.35, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %564 : int[] = prim::ListConstruct(%562, %563, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %564), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %566 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %566), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %568 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %568), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %575 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %576 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %575), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%576, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %578 : int = aten::size(%context_layer.12, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %579 : int = aten::size(%context_layer.12, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %580 : int[] = prim::ListConstruct(%578, %579, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %580), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %582 : __torch__.torch.nn.modules.normalization.___torch_mangle_5801.LayerNorm = prim::GetAttr[name="LayerNorm"](%530)
  %583 : __torch__.torch.nn.modules.linear.___torch_mangle_5800.Linear = prim::GetAttr[name="dense"](%530)
  %584 : Tensor = prim::GetAttr[name="bias"](%583)
  %585 : Tensor = prim::GetAttr[name="weight"](%583)
  %586 : Float(768:1, 768:768) = aten::t(%585), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %586), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %584, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %591 : Tensor = prim::GetAttr[name="bias"](%582)
  %592 : Tensor = prim::GetAttr[name="weight"](%582)
  %593 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %593, %592, %591, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %595 : __torch__.torch.nn.modules.linear.___torch_mangle_5805.Linear = prim::GetAttr[name="dense"](%528)
  %596 : Tensor = prim::GetAttr[name="bias"](%595)
  %597 : Tensor = prim::GetAttr[name="weight"](%595)
  %598 : Float(768:1, 3072:768) = aten::t(%597), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %598), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %596, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %602 : __torch__.torch.nn.modules.normalization.___torch_mangle_5808.LayerNorm = prim::GetAttr[name="LayerNorm"](%527)
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_5807.Linear = prim::GetAttr[name="dense"](%527)
  %604 : Tensor = prim::GetAttr[name="bias"](%603)
  %605 : Tensor = prim::GetAttr[name="weight"](%603)
  %606 : Float(3072:1, 768:3072) = aten::t(%605), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %606), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %604, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %611 : Tensor = prim::GetAttr[name="bias"](%602)
  %612 : Tensor = prim::GetAttr[name="weight"](%602)
  %613 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %613, %612, %611, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %615 : __torch__.transformers.modeling_bert.___torch_mangle_5827.BertOutput = prim::GetAttr[name="output"](%74)
  %616 : __torch__.transformers.modeling_bert.___torch_mangle_5823.BertIntermediate = prim::GetAttr[name="intermediate"](%74)
  %617 : __torch__.transformers.modeling_bert.___torch_mangle_5821.BertAttention = prim::GetAttr[name="attention"](%74)
  %618 : __torch__.transformers.modeling_bert.___torch_mangle_5820.BertSelfOutput = prim::GetAttr[name="output"](%617)
  %619 : __torch__.transformers.modeling_bert.___torch_mangle_5816.BertSelfAttention = prim::GetAttr[name="self"](%617)
  %620 : __torch__.torch.nn.modules.linear.___torch_mangle_5814.Linear = prim::GetAttr[name="value"](%619)
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_5813.Linear = prim::GetAttr[name="key"](%619)
  %622 : __torch__.torch.nn.modules.linear.___torch_mangle_5812.Linear = prim::GetAttr[name="query"](%619)
  %623 : Tensor = prim::GetAttr[name="bias"](%622)
  %624 : Tensor = prim::GetAttr[name="weight"](%622)
  %625 : Float(768:1, 768:768) = aten::t(%624), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %625), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %623, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %628 : Tensor = prim::GetAttr[name="bias"](%621)
  %629 : Tensor = prim::GetAttr[name="weight"](%621)
  %630 : Float(768:1, 768:768) = aten::t(%629), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %630), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %628, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %633 : Tensor = prim::GetAttr[name="bias"](%620)
  %634 : Tensor = prim::GetAttr[name="weight"](%620)
  %635 : Float(768:1, 768:768) = aten::t(%634), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %635), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %633, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %638 : int = aten::size(%x.37, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %639 : int = aten::size(%x.37, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %640 : int[] = prim::ListConstruct(%638, %639, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %640), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %642 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %642), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %644 : int = aten::size(%x.39, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %645 : int = aten::size(%x.39, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %646 : int[] = prim::ListConstruct(%644, %645, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %646), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %648 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %648), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %650 : int = aten::size(%x.41, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %651 : int = aten::size(%x.41, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %652 : int[] = prim::ListConstruct(%650, %651, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %652), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %654 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %654), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %656 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %656), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %663 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %664 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %663), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%664, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %666 : int = aten::size(%context_layer.14, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %667 : int = aten::size(%context_layer.14, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %668 : int[] = prim::ListConstruct(%666, %667, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %668), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %670 : __torch__.torch.nn.modules.normalization.___torch_mangle_5818.LayerNorm = prim::GetAttr[name="LayerNorm"](%618)
  %671 : __torch__.torch.nn.modules.linear.___torch_mangle_5817.Linear = prim::GetAttr[name="dense"](%618)
  %672 : Tensor = prim::GetAttr[name="bias"](%671)
  %673 : Tensor = prim::GetAttr[name="weight"](%671)
  %674 : Float(768:1, 768:768) = aten::t(%673), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %674), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %672, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %679 : Tensor = prim::GetAttr[name="bias"](%670)
  %680 : Tensor = prim::GetAttr[name="weight"](%670)
  %681 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %681, %680, %679, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %683 : __torch__.torch.nn.modules.linear.___torch_mangle_5822.Linear = prim::GetAttr[name="dense"](%616)
  %684 : Tensor = prim::GetAttr[name="bias"](%683)
  %685 : Tensor = prim::GetAttr[name="weight"](%683)
  %686 : Float(768:1, 3072:768) = aten::t(%685), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %686), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %684, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %690 : __torch__.torch.nn.modules.normalization.___torch_mangle_5825.LayerNorm = prim::GetAttr[name="LayerNorm"](%615)
  %691 : __torch__.torch.nn.modules.linear.___torch_mangle_5824.Linear = prim::GetAttr[name="dense"](%615)
  %692 : Tensor = prim::GetAttr[name="bias"](%691)
  %693 : Tensor = prim::GetAttr[name="weight"](%691)
  %694 : Float(3072:1, 768:3072) = aten::t(%693), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %694), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %692, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %699 : Tensor = prim::GetAttr[name="bias"](%690)
  %700 : Tensor = prim::GetAttr[name="weight"](%690)
  %701 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %701, %700, %699, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %703 : __torch__.transformers.modeling_bert.___torch_mangle_5844.BertOutput = prim::GetAttr[name="output"](%72)
  %704 : __torch__.transformers.modeling_bert.___torch_mangle_5840.BertIntermediate = prim::GetAttr[name="intermediate"](%72)
  %705 : __torch__.transformers.modeling_bert.___torch_mangle_5838.BertAttention = prim::GetAttr[name="attention"](%72)
  %706 : __torch__.transformers.modeling_bert.___torch_mangle_5837.BertSelfOutput = prim::GetAttr[name="output"](%705)
  %707 : __torch__.transformers.modeling_bert.___torch_mangle_5833.BertSelfAttention = prim::GetAttr[name="self"](%705)
  %708 : __torch__.torch.nn.modules.linear.___torch_mangle_5831.Linear = prim::GetAttr[name="value"](%707)
  %709 : __torch__.torch.nn.modules.linear.___torch_mangle_5830.Linear = prim::GetAttr[name="key"](%707)
  %710 : __torch__.torch.nn.modules.linear.___torch_mangle_5829.Linear = prim::GetAttr[name="query"](%707)
  %711 : Tensor = prim::GetAttr[name="bias"](%710)
  %712 : Tensor = prim::GetAttr[name="weight"](%710)
  %713 : Float(768:1, 768:768) = aten::t(%712), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %713), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %711, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %716 : Tensor = prim::GetAttr[name="bias"](%709)
  %717 : Tensor = prim::GetAttr[name="weight"](%709)
  %718 : Float(768:1, 768:768) = aten::t(%717), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %718), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %716, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %721 : Tensor = prim::GetAttr[name="bias"](%708)
  %722 : Tensor = prim::GetAttr[name="weight"](%708)
  %723 : Float(768:1, 768:768) = aten::t(%722), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %723), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %721, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %726 : int = aten::size(%x.43, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %727 : int = aten::size(%x.43, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %728 : int[] = prim::ListConstruct(%726, %727, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %728), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %730 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %730), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %732 : int = aten::size(%x.45, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %733 : int = aten::size(%x.45, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %734 : int[] = prim::ListConstruct(%732, %733, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %734), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %736 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %736), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %738 : int = aten::size(%x.47, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %739 : int = aten::size(%x.47, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %740 : int[] = prim::ListConstruct(%738, %739, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %740), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %742 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %742), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %744 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %744), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %751 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %752 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %751), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%752, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %754 : int = aten::size(%context_layer.16, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %755 : int = aten::size(%context_layer.16, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %756 : int[] = prim::ListConstruct(%754, %755, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %756), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %758 : __torch__.torch.nn.modules.normalization.___torch_mangle_5835.LayerNorm = prim::GetAttr[name="LayerNorm"](%706)
  %759 : __torch__.torch.nn.modules.linear.___torch_mangle_5834.Linear = prim::GetAttr[name="dense"](%706)
  %760 : Tensor = prim::GetAttr[name="bias"](%759)
  %761 : Tensor = prim::GetAttr[name="weight"](%759)
  %762 : Float(768:1, 768:768) = aten::t(%761), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %762), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %760, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %767 : Tensor = prim::GetAttr[name="bias"](%758)
  %768 : Tensor = prim::GetAttr[name="weight"](%758)
  %769 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %769, %768, %767, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %771 : __torch__.torch.nn.modules.linear.___torch_mangle_5839.Linear = prim::GetAttr[name="dense"](%704)
  %772 : Tensor = prim::GetAttr[name="bias"](%771)
  %773 : Tensor = prim::GetAttr[name="weight"](%771)
  %774 : Float(768:1, 3072:768) = aten::t(%773), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %774), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %772, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %778 : __torch__.torch.nn.modules.normalization.___torch_mangle_5842.LayerNorm = prim::GetAttr[name="LayerNorm"](%703)
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_5841.Linear = prim::GetAttr[name="dense"](%703)
  %780 : Tensor = prim::GetAttr[name="bias"](%779)
  %781 : Tensor = prim::GetAttr[name="weight"](%779)
  %782 : Float(3072:1, 768:3072) = aten::t(%781), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %782), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %780, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %787 : Tensor = prim::GetAttr[name="bias"](%778)
  %788 : Tensor = prim::GetAttr[name="weight"](%778)
  %789 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %789, %788, %787, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %791 : __torch__.transformers.modeling_bert.___torch_mangle_5861.BertOutput = prim::GetAttr[name="output"](%70)
  %792 : __torch__.transformers.modeling_bert.___torch_mangle_5857.BertIntermediate = prim::GetAttr[name="intermediate"](%70)
  %793 : __torch__.transformers.modeling_bert.___torch_mangle_5855.BertAttention = prim::GetAttr[name="attention"](%70)
  %794 : __torch__.transformers.modeling_bert.___torch_mangle_5854.BertSelfOutput = prim::GetAttr[name="output"](%793)
  %795 : __torch__.transformers.modeling_bert.___torch_mangle_5850.BertSelfAttention = prim::GetAttr[name="self"](%793)
  %796 : __torch__.torch.nn.modules.linear.___torch_mangle_5848.Linear = prim::GetAttr[name="value"](%795)
  %797 : __torch__.torch.nn.modules.linear.___torch_mangle_5847.Linear = prim::GetAttr[name="key"](%795)
  %798 : __torch__.torch.nn.modules.linear.___torch_mangle_5846.Linear = prim::GetAttr[name="query"](%795)
  %799 : Tensor = prim::GetAttr[name="bias"](%798)
  %800 : Tensor = prim::GetAttr[name="weight"](%798)
  %801 : Float(768:1, 768:768) = aten::t(%800), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %801), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %799, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %804 : Tensor = prim::GetAttr[name="bias"](%797)
  %805 : Tensor = prim::GetAttr[name="weight"](%797)
  %806 : Float(768:1, 768:768) = aten::t(%805), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %806), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %804, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %809 : Tensor = prim::GetAttr[name="bias"](%796)
  %810 : Tensor = prim::GetAttr[name="weight"](%796)
  %811 : Float(768:1, 768:768) = aten::t(%810), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %811), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %809, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %814 : int = aten::size(%x.49, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %815 : int = aten::size(%x.49, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %816 : int[] = prim::ListConstruct(%814, %815, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %816), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %818 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %818), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %820 : int = aten::size(%x.51, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %821 : int = aten::size(%x.51, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %822 : int[] = prim::ListConstruct(%820, %821, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %822), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %824 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %824), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %826 : int = aten::size(%x.53, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %827 : int = aten::size(%x.53, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %828 : int[] = prim::ListConstruct(%826, %827, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %828), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %830 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %830), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %832 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %832), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %839 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %840 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %839), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%840, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %842 : int = aten::size(%context_layer.18, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %843 : int = aten::size(%context_layer.18, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %844 : int[] = prim::ListConstruct(%842, %843, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %844), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %846 : __torch__.torch.nn.modules.normalization.___torch_mangle_5852.LayerNorm = prim::GetAttr[name="LayerNorm"](%794)
  %847 : __torch__.torch.nn.modules.linear.___torch_mangle_5851.Linear = prim::GetAttr[name="dense"](%794)
  %848 : Tensor = prim::GetAttr[name="bias"](%847)
  %849 : Tensor = prim::GetAttr[name="weight"](%847)
  %850 : Float(768:1, 768:768) = aten::t(%849), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %850), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %848, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %855 : Tensor = prim::GetAttr[name="bias"](%846)
  %856 : Tensor = prim::GetAttr[name="weight"](%846)
  %857 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %857, %856, %855, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %859 : __torch__.torch.nn.modules.linear.___torch_mangle_5856.Linear = prim::GetAttr[name="dense"](%792)
  %860 : Tensor = prim::GetAttr[name="bias"](%859)
  %861 : Tensor = prim::GetAttr[name="weight"](%859)
  %862 : Float(768:1, 3072:768) = aten::t(%861), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %862), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %860, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %866 : __torch__.torch.nn.modules.normalization.___torch_mangle_5859.LayerNorm = prim::GetAttr[name="LayerNorm"](%791)
  %867 : __torch__.torch.nn.modules.linear.___torch_mangle_5858.Linear = prim::GetAttr[name="dense"](%791)
  %868 : Tensor = prim::GetAttr[name="bias"](%867)
  %869 : Tensor = prim::GetAttr[name="weight"](%867)
  %870 : Float(3072:1, 768:3072) = aten::t(%869), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %870), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %868, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %875 : Tensor = prim::GetAttr[name="bias"](%866)
  %876 : Tensor = prim::GetAttr[name="weight"](%866)
  %877 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %877, %876, %875, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %879 : __torch__.transformers.modeling_bert.___torch_mangle_5878.BertOutput = prim::GetAttr[name="output"](%68)
  %880 : __torch__.transformers.modeling_bert.___torch_mangle_5874.BertIntermediate = prim::GetAttr[name="intermediate"](%68)
  %881 : __torch__.transformers.modeling_bert.___torch_mangle_5872.BertAttention = prim::GetAttr[name="attention"](%68)
  %882 : __torch__.transformers.modeling_bert.___torch_mangle_5871.BertSelfOutput = prim::GetAttr[name="output"](%881)
  %883 : __torch__.transformers.modeling_bert.___torch_mangle_5867.BertSelfAttention = prim::GetAttr[name="self"](%881)
  %884 : __torch__.torch.nn.modules.linear.___torch_mangle_5865.Linear = prim::GetAttr[name="value"](%883)
  %885 : __torch__.torch.nn.modules.linear.___torch_mangle_5864.Linear = prim::GetAttr[name="key"](%883)
  %886 : __torch__.torch.nn.modules.linear.___torch_mangle_5863.Linear = prim::GetAttr[name="query"](%883)
  %887 : Tensor = prim::GetAttr[name="bias"](%886)
  %888 : Tensor = prim::GetAttr[name="weight"](%886)
  %889 : Float(768:1, 768:768) = aten::t(%888), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %889), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %887, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %892 : Tensor = prim::GetAttr[name="bias"](%885)
  %893 : Tensor = prim::GetAttr[name="weight"](%885)
  %894 : Float(768:1, 768:768) = aten::t(%893), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %894), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %892, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %897 : Tensor = prim::GetAttr[name="bias"](%884)
  %898 : Tensor = prim::GetAttr[name="weight"](%884)
  %899 : Float(768:1, 768:768) = aten::t(%898), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %899), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %897, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %902 : int = aten::size(%x.55, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %903 : int = aten::size(%x.55, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %904 : int[] = prim::ListConstruct(%902, %903, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %904), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %906 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %906), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %908 : int = aten::size(%x.57, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %909 : int = aten::size(%x.57, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %910 : int[] = prim::ListConstruct(%908, %909, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %910), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %912 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %912), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %914 : int = aten::size(%x.59, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %915 : int = aten::size(%x.59, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %916 : int[] = prim::ListConstruct(%914, %915, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %916), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %918 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %918), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %920 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %920), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %927 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %928 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %927), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%928, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %930 : int = aten::size(%context_layer.20, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %931 : int = aten::size(%context_layer.20, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %932 : int[] = prim::ListConstruct(%930, %931, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %932), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %934 : __torch__.torch.nn.modules.normalization.___torch_mangle_5869.LayerNorm = prim::GetAttr[name="LayerNorm"](%882)
  %935 : __torch__.torch.nn.modules.linear.___torch_mangle_5868.Linear = prim::GetAttr[name="dense"](%882)
  %936 : Tensor = prim::GetAttr[name="bias"](%935)
  %937 : Tensor = prim::GetAttr[name="weight"](%935)
  %938 : Float(768:1, 768:768) = aten::t(%937), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %938), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %936, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %943 : Tensor = prim::GetAttr[name="bias"](%934)
  %944 : Tensor = prim::GetAttr[name="weight"](%934)
  %945 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %945, %944, %943, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %947 : __torch__.torch.nn.modules.linear.___torch_mangle_5873.Linear = prim::GetAttr[name="dense"](%880)
  %948 : Tensor = prim::GetAttr[name="bias"](%947)
  %949 : Tensor = prim::GetAttr[name="weight"](%947)
  %950 : Float(768:1, 3072:768) = aten::t(%949), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %950), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %948, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %954 : __torch__.torch.nn.modules.normalization.___torch_mangle_5876.LayerNorm = prim::GetAttr[name="LayerNorm"](%879)
  %955 : __torch__.torch.nn.modules.linear.___torch_mangle_5875.Linear = prim::GetAttr[name="dense"](%879)
  %956 : Tensor = prim::GetAttr[name="bias"](%955)
  %957 : Tensor = prim::GetAttr[name="weight"](%955)
  %958 : Float(3072:1, 768:3072) = aten::t(%957), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %958), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %956, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %963 : Tensor = prim::GetAttr[name="bias"](%954)
  %964 : Tensor = prim::GetAttr[name="weight"](%954)
  %965 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %965, %964, %963, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %967 : __torch__.transformers.modeling_bert.___torch_mangle_5895.BertOutput = prim::GetAttr[name="output"](%66)
  %968 : __torch__.transformers.modeling_bert.___torch_mangle_5891.BertIntermediate = prim::GetAttr[name="intermediate"](%66)
  %969 : __torch__.transformers.modeling_bert.___torch_mangle_5889.BertAttention = prim::GetAttr[name="attention"](%66)
  %970 : __torch__.transformers.modeling_bert.___torch_mangle_5888.BertSelfOutput = prim::GetAttr[name="output"](%969)
  %971 : __torch__.transformers.modeling_bert.___torch_mangle_5884.BertSelfAttention = prim::GetAttr[name="self"](%969)
  %972 : __torch__.torch.nn.modules.linear.___torch_mangle_5882.Linear = prim::GetAttr[name="value"](%971)
  %973 : __torch__.torch.nn.modules.linear.___torch_mangle_5881.Linear = prim::GetAttr[name="key"](%971)
  %974 : __torch__.torch.nn.modules.linear.___torch_mangle_5880.Linear = prim::GetAttr[name="query"](%971)
  %975 : Tensor = prim::GetAttr[name="bias"](%974)
  %976 : Tensor = prim::GetAttr[name="weight"](%974)
  %977 : Float(768:1, 768:768) = aten::t(%976), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %977), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %975, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %980 : Tensor = prim::GetAttr[name="bias"](%973)
  %981 : Tensor = prim::GetAttr[name="weight"](%973)
  %982 : Float(768:1, 768:768) = aten::t(%981), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %982), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %980, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %985 : Tensor = prim::GetAttr[name="bias"](%972)
  %986 : Tensor = prim::GetAttr[name="weight"](%972)
  %987 : Float(768:1, 768:768) = aten::t(%986), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %987), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %985, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %990 : int = aten::size(%x.61, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %991 : int = aten::size(%x.61, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %992 : int[] = prim::ListConstruct(%990, %991, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %992), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %994 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %994), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %996 : int = aten::size(%x.63, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %997 : int = aten::size(%x.63, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %998 : int[] = prim::ListConstruct(%996, %997, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %998), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1000 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1000), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1002 : int = aten::size(%x.65, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1003 : int = aten::size(%x.65, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1004 : int[] = prim::ListConstruct(%1002, %1003, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1004), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1006 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1006), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1008 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1008), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1015 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %1016 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1015), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1016, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1018 : int = aten::size(%context_layer.22, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1019 : int = aten::size(%context_layer.22, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1020 : int[] = prim::ListConstruct(%1018, %1019, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1020), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1022 : __torch__.torch.nn.modules.normalization.___torch_mangle_5886.LayerNorm = prim::GetAttr[name="LayerNorm"](%970)
  %1023 : __torch__.torch.nn.modules.linear.___torch_mangle_5885.Linear = prim::GetAttr[name="dense"](%970)
  %1024 : Tensor = prim::GetAttr[name="bias"](%1023)
  %1025 : Tensor = prim::GetAttr[name="weight"](%1023)
  %1026 : Float(768:1, 768:768) = aten::t(%1025), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1026), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1024, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1031 : Tensor = prim::GetAttr[name="bias"](%1022)
  %1032 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1033 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1033, %1032, %1031, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1035 : __torch__.torch.nn.modules.linear.___torch_mangle_5890.Linear = prim::GetAttr[name="dense"](%968)
  %1036 : Tensor = prim::GetAttr[name="bias"](%1035)
  %1037 : Tensor = prim::GetAttr[name="weight"](%1035)
  %1038 : Float(768:1, 3072:768) = aten::t(%1037), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1038), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1036, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1042 : __torch__.torch.nn.modules.normalization.___torch_mangle_5893.LayerNorm = prim::GetAttr[name="LayerNorm"](%967)
  %1043 : __torch__.torch.nn.modules.linear.___torch_mangle_5892.Linear = prim::GetAttr[name="dense"](%967)
  %1044 : Tensor = prim::GetAttr[name="bias"](%1043)
  %1045 : Tensor = prim::GetAttr[name="weight"](%1043)
  %1046 : Float(3072:1, 768:3072) = aten::t(%1045), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1046), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1044, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1051 : Tensor = prim::GetAttr[name="bias"](%1042)
  %1052 : Tensor = prim::GetAttr[name="weight"](%1042)
  %1053 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1053, %1052, %1051, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1055 : __torch__.transformers.modeling_bert.___torch_mangle_5912.BertOutput = prim::GetAttr[name="output"](%64)
  %1056 : __torch__.transformers.modeling_bert.___torch_mangle_5908.BertIntermediate = prim::GetAttr[name="intermediate"](%64)
  %1057 : __torch__.transformers.modeling_bert.___torch_mangle_5906.BertAttention = prim::GetAttr[name="attention"](%64)
  %1058 : __torch__.transformers.modeling_bert.___torch_mangle_5905.BertSelfOutput = prim::GetAttr[name="output"](%1057)
  %1059 : __torch__.transformers.modeling_bert.___torch_mangle_5901.BertSelfAttention = prim::GetAttr[name="self"](%1057)
  %1060 : __torch__.torch.nn.modules.linear.___torch_mangle_5899.Linear = prim::GetAttr[name="value"](%1059)
  %1061 : __torch__.torch.nn.modules.linear.___torch_mangle_5898.Linear = prim::GetAttr[name="key"](%1059)
  %1062 : __torch__.torch.nn.modules.linear.___torch_mangle_5897.Linear = prim::GetAttr[name="query"](%1059)
  %1063 : Tensor = prim::GetAttr[name="bias"](%1062)
  %1064 : Tensor = prim::GetAttr[name="weight"](%1062)
  %1065 : Float(768:1, 768:768) = aten::t(%1064), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1065), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1063, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1068 : Tensor = prim::GetAttr[name="bias"](%1061)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1070 : Float(768:1, 768:768) = aten::t(%1069), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1070), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1068, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1073 : Tensor = prim::GetAttr[name="bias"](%1060)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1060)
  %1075 : Float(768:1, 768:768) = aten::t(%1074), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1075), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1073, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1078 : int = aten::size(%x.67, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1079 : int = aten::size(%x.67, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1080 : int[] = prim::ListConstruct(%1078, %1079, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1080), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1082 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1082), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1084 : int = aten::size(%x.69, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1085 : int = aten::size(%x.69, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1086 : int[] = prim::ListConstruct(%1084, %1085, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1086), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1088 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1088), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1090 : int = aten::size(%x.71, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1091 : int = aten::size(%x.71, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1092 : int[] = prim::ListConstruct(%1090, %1091, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1092), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1094 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1094), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1096 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1096), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1103 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %1104 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1103), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1104, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1106 : int = aten::size(%context_layer, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1107 : int = aten::size(%context_layer, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1108 : int[] = prim::ListConstruct(%1106, %1107, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1110 : __torch__.torch.nn.modules.normalization.___torch_mangle_5903.LayerNorm = prim::GetAttr[name="LayerNorm"](%1058)
  %1111 : __torch__.torch.nn.modules.linear.___torch_mangle_5902.Linear = prim::GetAttr[name="dense"](%1058)
  %1112 : Tensor = prim::GetAttr[name="bias"](%1111)
  %1113 : Tensor = prim::GetAttr[name="weight"](%1111)
  %1114 : Float(768:1, 768:768) = aten::t(%1113), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1114), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1112, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1119 : Tensor = prim::GetAttr[name="bias"](%1110)
  %1120 : Tensor = prim::GetAttr[name="weight"](%1110)
  %1121 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1121, %1120, %1119, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1123 : __torch__.torch.nn.modules.linear.___torch_mangle_5907.Linear = prim::GetAttr[name="dense"](%1056)
  %1124 : Tensor = prim::GetAttr[name="bias"](%1123)
  %1125 : Tensor = prim::GetAttr[name="weight"](%1123)
  %1126 : Float(768:1, 3072:768) = aten::t(%1125), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1126), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1124, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1130 : __torch__.torch.nn.modules.normalization.___torch_mangle_5910.LayerNorm = prim::GetAttr[name="LayerNorm"](%1055)
  %1131 : __torch__.torch.nn.modules.linear.___torch_mangle_5909.Linear = prim::GetAttr[name="dense"](%1055)
  %1132 : Tensor = prim::GetAttr[name="bias"](%1131)
  %1133 : Tensor = prim::GetAttr[name="weight"](%1131)
  %1134 : Float(3072:1, 768:3072) = aten::t(%1133), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1134), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1132, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %27), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1139 : Tensor = prim::GetAttr[name="bias"](%1130)
  %1140 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1141 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
  %input.125 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1141, %1140, %1139, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1143 : int = prim::Constant[value=768](), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.LayerNorm # torch/nn/functional.py:2048:0
  %1144 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.LayerNorm # torch/nn/functional.py:2048:0
  %1145 : bool = prim::Constant[value=1](), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.LayerNorm # torch/nn/functional.py:2048:0
  %1146 : int = prim::Constant[value=1](), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.dense # torch/nn/functional.py:1678:0
  %1147 : __torch__.transformers.modeling_bert.___torch_mangle_5921.BertLMPredictionHead = prim::GetAttr[name="predictions"](%3)
  %1148 : Tensor = prim::GetAttr[name="bias"](%1147)
  %1149 : __torch__.torch.nn.modules.linear.___torch_mangle_5920.Linear = prim::GetAttr[name="decoder"](%1147)
  %1150 : __torch__.transformers.modeling_bert.___torch_mangle_5919.BertPredictionHeadTransform = prim::GetAttr[name="transform"](%1147)
  %1151 : __torch__.torch.nn.modules.normalization.___torch_mangle_5918.LayerNorm = prim::GetAttr[name="LayerNorm"](%1150)
  %1152 : __torch__.torch.nn.modules.linear.___torch_mangle_5917.Linear = prim::GetAttr[name="dense"](%1150)
  %1153 : Tensor = prim::GetAttr[name="bias"](%1152)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1152)
  %1155 : Float(768:1, 768:768) = aten::t(%1154), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.125, %1155), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.dense # torch/nn/functional.py:1676:0
  %input.126 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.73, %1153, %1146), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.dense # torch/nn/functional.py:1678:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::gelu(%input.126), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform # torch/nn/functional.py:1369:0
  %1159 : Tensor = prim::GetAttr[name="bias"](%1151)
  %1160 : Tensor = prim::GetAttr[name="weight"](%1151)
  %1161 : int[] = prim::ListConstruct(%1143), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.LayerNorm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.127, %1161, %1160, %1159, %1144, %1145), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.transform/__module.cls.predictions.transform.LayerNorm # torch/nn/functional.py:2048:0
  %1163 : Tensor = prim::GetAttr[name="weight"](%1149)
  %1164 : Float(768:1, 30522:768) = aten::t(%1163), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.decoder # torch/nn/functional.py:1676:0
  %output : Float(17:396786, 13:30522, 30522:1) = aten::matmul(%input, %1164), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.decoder # torch/nn/functional.py:1676:0
  %1166 : Float(17:396786, 13:30522, 30522:1) = aten::add_(%output, %1148, %1146), scope: __module.cls/__module.cls.predictions/__module.cls.predictions.decoder # torch/nn/functional.py:1678:0
  %7 : (Float(17:396786, 13:30522, 30522:1)) = prim::TupleConstruct(%1166)
  return (%7)
