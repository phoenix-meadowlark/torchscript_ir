graph(%self.1 : __torch__.transformers.modeling_albert.AlbertForMaskedLM,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_albert.AlbertMLMHead = prim::GetAttr[name="predictions"](%self.1)
  %4 : __torch__.transformers.modeling_albert.AlbertModel = prim::GetAttr[name="albert"](%self.1)
  %8 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %9 : int = prim::Constant[value=4096](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %10 : Double() = prim::Constant[value={8}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %11 : int = prim::Constant[value=-2](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %12 : int = prim::Constant[value=3](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %13 : int = prim::Constant[value=64](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %14 : Double() = prim::Constant[value={0.5}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %15 : float = prim::Constant[value=3.](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %16 : Double() = prim::Constant[value={0.044715}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %17 : Double() = prim::Constant[value={0.797885}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %18 : Double() = prim::Constant[value={1}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %19 : int = prim::Constant[value=9223372036854775807](), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %20 : int = prim::Constant[value=-1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %21 : bool = prim::Constant[value=1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %22 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %23 : int = prim::Constant[value=128](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : float = prim::Constant[value=0.](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %25 : Double() = prim::Constant[value={-10000}](), scope: __module.albert # transformers/modeling_albert.py:678:0
  %26 : float = prim::Constant[value=1.](), scope: __module.albert # torch/tensor.py:396:0
  %27 : None = prim::Constant(), scope: __module.albert
  %28 : int = prim::Constant[value=6](), scope: __module.albert # transformers/modeling_albert.py:677:0
  %29 : int = prim::Constant[value=2](), scope: __module.albert # transformers/modeling_albert.py:676:0
  %30 : bool = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %31 : Device = prim::Constant[value="cpu"](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %32 : int = prim::Constant[value=4](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %33 : int = prim::Constant[value=1](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %34 : int = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %35 : __torch__.transformers.modeling_albert.AlbertTransformer = prim::GetAttr[name="encoder"](%4)
  %36 : __torch__.transformers.modeling_albert.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %37 : int = aten::size(%input_ids, %34), scope: __module.albert # transformers/modeling_albert.py:663:0
  %38 : int = aten::size(%input_ids, %33), scope: __module.albert # transformers/modeling_albert.py:663:0
  %39 : int[] = prim::ListConstruct(%37, %38), scope: __module.albert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%39, %32, %34, %31, %30), scope: __module.albert # transformers/modeling_albert.py:674:0
  %41 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %33), scope: __module.albert # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%41, %29), scope: __module.albert # transformers/modeling_albert.py:676:0
  %43 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %28, %30, %30, %27), scope: __module.albert # transformers/modeling_albert.py:677:0
  %44 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%43, %26, %33), scope: __module.albert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%44, %25), scope: __module.albert # transformers/modeling_albert.py:678:0
  %46 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%36)
  %47 : __torch__.torch.nn.modules.sparse.___torch_mangle_1.Embedding = prim::GetAttr[name="token_type_embeddings"](%36)
  %48 : __torch__.torch.nn.modules.sparse.___torch_mangle_0.Embedding = prim::GetAttr[name="position_embeddings"](%36)
  %49 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%36)
  %50 : Tensor = prim::GetAttr[name="position_ids"](%36)
  %51 : int = aten::size(%input_ids, %33), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:222:0
  %52 : Long(1:512, 512:1) = aten::slice(%50, %34, %34, %19, %33), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%52, %33, %34, %51, %33), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %54 : Tensor = prim::GetAttr[name="weight"](%49)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%54, %input_ids, %34, %30, %30), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %56 : Tensor = prim::GetAttr[name="weight"](%48)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%56, %input.1, %20, %30, %30), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %58 : Tensor = prim::GetAttr[name="weight"](%47)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%58, %input.2, %20, %30, %30), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %60 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %33), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%60, %token_type_embeddings, %33), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %62 : Tensor = prim::GetAttr[name="bias"](%46)
  %63 : Tensor = prim::GetAttr[name="weight"](%46)
  %64 : int[] = prim::ListConstruct(%23), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %64, %63, %62, %22, %21), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %24, %30), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_13.ModuleList = prim::GetAttr[name="albert_layer_groups"](%35)
  %68 : __torch__.transformers.modeling_albert.AlbertLayerGroup = prim::GetAttr[name="0"](%67)
  %69 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%35)
  %70 : Tensor = prim::GetAttr[name="bias"](%69)
  %71 : Tensor = prim::GetAttr[name="weight"](%69)
  %72 : Float(128:1, 4096:128) = aten::t(%71), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %72), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.1, %70, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %75 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %76 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%75)
  %77 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%76)
  %78 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%76)
  %79 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%76)
  %80 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%76)
  %81 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%80)
  %82 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%80)
  %83 : Tensor = prim::GetAttr[name="bias"](%82)
  %84 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%80)
  %85 : Tensor = prim::GetAttr[name="weight"](%84)
  %86 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%80)
  %87 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%80)
  %88 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%80)
  %89 : Tensor = prim::GetAttr[name="bias"](%88)
  %90 : Tensor = prim::GetAttr[name="weight"](%88)
  %91 : Float(4096:1, 4096:4096) = aten::t(%90), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %91), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.2, %89, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %94 : Tensor = prim::GetAttr[name="bias"](%87)
  %95 : Tensor = prim::GetAttr[name="weight"](%87)
  %96 : Float(4096:1, 4096:4096) = aten::t(%95), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %96), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.3, %94, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %99 : Tensor = prim::GetAttr[name="bias"](%86)
  %100 : Tensor = prim::GetAttr[name="weight"](%86)
  %101 : Float(4096:1, 4096:4096) = aten::t(%100), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %101), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.4, %99, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %104 : int = aten::size(%x.1, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %105 : int = aten::size(%x.1, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %106 : int[] = prim::ListConstruct(%104, %105, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %106), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %108 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %108), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %110 : int = aten::size(%x.3, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %111 : int = aten::size(%x.3, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %112 : int[] = prim::ListConstruct(%110, %111, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %112), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %114 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %114), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %116 : int = aten::size(%x.5, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %117 : int = aten::size(%x.5, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %118 : int[] = prim::ListConstruct(%116, %117, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %118), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %120 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %120), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %122 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %122), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %129 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %130 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %129), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %131 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%130, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %132 : Float(4096:1, 4096:4096) = aten::t(%85), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %133 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %134 : Float(64:64, 64:1, 4096:4096) = aten::view(%132, %133), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %135 : Float(64:64, 64:1, 4096:4096) = aten::to(%134, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%83, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %137 : Tensor[] = prim::ListConstruct(%131, %135), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %138 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %137), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(17:53248, 13:4096, 4096:1) = aten::add(%138, %b.1, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %142 : Tensor = prim::GetAttr[name="bias"](%81)
  %143 : Tensor = prim::GetAttr[name="weight"](%81)
  %144 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %144, %143, %142, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %146 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %147 : Float(17:53248, 13:4096, 4096:1), %148 : Float(4096:1) = prim::TupleUnpack(%146)
  %149 : Tensor = prim::GetAttr[name="bias"](%79)
  %150 : Tensor = prim::GetAttr[name="weight"](%79)
  %151 : Float(4096:1, 16384:4096) = aten::t(%150), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%147, %151), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.5, %149, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %154 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.7, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %155 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.7, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %156 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%155, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %157 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.7, %156, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %158 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%157, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %159 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%158), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %160 : Float(17:212992, 13:16384, 16384:1) = aten::add(%159, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%154, %160), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %162 : Tensor = prim::GetAttr[name="bias"](%78)
  %163 : Tensor = prim::GetAttr[name="weight"](%78)
  %164 : Float(16384:1, 4096:16384) = aten::t(%163), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %164), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.6, %162, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %147, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %168 : Tensor = prim::GetAttr[name="bias"](%77)
  %169 : Tensor = prim::GetAttr[name="weight"](%77)
  %170 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %170, %169, %168, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %172 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %148)
  %173 : Float(17:53248, 13:4096, 4096:1), %174 : Float(4096:1) = prim::TupleUnpack(%172)
  %175 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%173, %174)
  %176 : Float(17:53248, 13:4096, 4096:1), %177 : Float(4096:1) = prim::TupleUnpack(%175)
  %178 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %179 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%178)
  %180 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%179)
  %181 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%179)
  %182 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%179)
  %183 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%179)
  %184 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%183)
  %185 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%183)
  %186 : Tensor = prim::GetAttr[name="weight"](%185)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%183)
  %188 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%183)
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%183)
  %190 : Tensor = prim::GetAttr[name="bias"](%189)
  %191 : Tensor = prim::GetAttr[name="weight"](%189)
  %192 : Float(4096:1, 4096:4096) = aten::t(%191), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%176, %192), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.7, %190, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %195 : Tensor = prim::GetAttr[name="bias"](%188)
  %196 : Tensor = prim::GetAttr[name="weight"](%188)
  %197 : Float(4096:1, 4096:4096) = aten::t(%196), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%176, %197), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.8, %195, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %200 : Tensor = prim::GetAttr[name="bias"](%187)
  %201 : Tensor = prim::GetAttr[name="weight"](%187)
  %202 : Float(4096:1, 4096:4096) = aten::t(%201), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%176, %202), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.9, %200, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %205 : int = aten::size(%x.8, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %206 : int = aten::size(%x.8, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %207 : int[] = prim::ListConstruct(%205, %206, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %207), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %209 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %209), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %211 : int = aten::size(%x.10, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %212 : int = aten::size(%x.10, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %213 : int[] = prim::ListConstruct(%211, %212, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %213), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %215 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %215), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %217 : int = aten::size(%x.12, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %218 : int = aten::size(%x.12, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %219 : int[] = prim::ListConstruct(%217, %218, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %219), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %221 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %221), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %223 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %223), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %230 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %231 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %230), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %232 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%231, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %233 : Float(4096:1, 4096:4096) = aten::t(%186), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %234 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %235 : Float(64:64, 64:1, 4096:4096) = aten::view(%233, %234), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %236 : Float(64:64, 64:1, 4096:4096) = aten::to(%235, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%177, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %238 : Tensor[] = prim::ListConstruct(%232, %236), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %239 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %238), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(17:53248, 13:4096, 4096:1) = aten::add(%239, %b.2, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(17:53248, 13:4096, 4096:1) = aten::add(%176, %projected_context_layer.2, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %243 : Tensor = prim::GetAttr[name="bias"](%184)
  %244 : Tensor = prim::GetAttr[name="weight"](%184)
  %245 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %245, %244, %243, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %247 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %248 : Float(17:53248, 13:4096, 4096:1), %249 : Float(4096:1) = prim::TupleUnpack(%247)
  %250 : Tensor = prim::GetAttr[name="bias"](%182)
  %251 : Tensor = prim::GetAttr[name="weight"](%182)
  %252 : Float(4096:1, 16384:4096) = aten::t(%251), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%248, %252), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.10, %250, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %255 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.14, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %256 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.14, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %257 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%256, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %258 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.14, %257, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %259 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%258, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %260 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%259), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %261 : Float(17:212992, 13:16384, 16384:1) = aten::add(%260, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%255, %261), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %263 : Tensor = prim::GetAttr[name="bias"](%181)
  %264 : Tensor = prim::GetAttr[name="weight"](%181)
  %265 : Float(16384:1, 4096:16384) = aten::t(%264), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %265), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %263, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %248, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %269 : Tensor = prim::GetAttr[name="bias"](%180)
  %270 : Tensor = prim::GetAttr[name="weight"](%180)
  %271 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %271, %270, %269, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %273 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %249)
  %274 : Float(17:53248, 13:4096, 4096:1), %275 : Float(4096:1) = prim::TupleUnpack(%273)
  %276 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%274, %275)
  %277 : Float(17:53248, 13:4096, 4096:1), %278 : Float(4096:1) = prim::TupleUnpack(%276)
  %279 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %280 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%279)
  %281 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%280)
  %282 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%280)
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%280)
  %284 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%280)
  %285 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%284)
  %286 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%284)
  %287 : Tensor = prim::GetAttr[name="weight"](%286)
  %288 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%284)
  %289 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%284)
  %290 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%284)
  %291 : Tensor = prim::GetAttr[name="bias"](%290)
  %292 : Tensor = prim::GetAttr[name="weight"](%290)
  %293 : Float(4096:1, 4096:4096) = aten::t(%292), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%277, %293), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.12, %291, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %296 : Tensor = prim::GetAttr[name="bias"](%289)
  %297 : Tensor = prim::GetAttr[name="weight"](%289)
  %298 : Float(4096:1, 4096:4096) = aten::t(%297), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%277, %298), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.13, %296, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %301 : Tensor = prim::GetAttr[name="bias"](%288)
  %302 : Tensor = prim::GetAttr[name="weight"](%288)
  %303 : Float(4096:1, 4096:4096) = aten::t(%302), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%277, %303), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.14, %301, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %306 : int = aten::size(%x.15, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %307 : int = aten::size(%x.15, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %308 : int[] = prim::ListConstruct(%306, %307, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %308), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %310 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %310), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %312 : int = aten::size(%x.17, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %313 : int = aten::size(%x.17, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %314 : int[] = prim::ListConstruct(%312, %313, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %314), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %316 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %316), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %318 : int = aten::size(%x.19, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %319 : int = aten::size(%x.19, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %320 : int[] = prim::ListConstruct(%318, %319, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %320), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %322 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %322), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %324 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %324), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %331 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %332 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %331), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %333 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%332, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %334 : Float(4096:1, 4096:4096) = aten::t(%287), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %335 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %336 : Float(64:64, 64:1, 4096:4096) = aten::view(%334, %335), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %337 : Float(64:64, 64:1, 4096:4096) = aten::to(%336, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%278, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %339 : Tensor[] = prim::ListConstruct(%333, %337), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %340 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %339), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(17:53248, 13:4096, 4096:1) = aten::add(%340, %b.3, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:53248, 13:4096, 4096:1) = aten::add(%277, %projected_context_layer.3, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %344 : Tensor = prim::GetAttr[name="bias"](%285)
  %345 : Tensor = prim::GetAttr[name="weight"](%285)
  %346 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %346, %345, %344, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %348 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %349 : Float(17:53248, 13:4096, 4096:1), %350 : Float(4096:1) = prim::TupleUnpack(%348)
  %351 : Tensor = prim::GetAttr[name="bias"](%283)
  %352 : Tensor = prim::GetAttr[name="weight"](%283)
  %353 : Float(4096:1, 16384:4096) = aten::t(%352), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%349, %353), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.15, %351, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %356 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.21, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %357 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.21, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %358 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%357, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %359 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.21, %358, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %360 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%359, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %361 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%360), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %362 : Float(17:212992, 13:16384, 16384:1) = aten::add(%361, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%356, %362), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %364 : Tensor = prim::GetAttr[name="bias"](%282)
  %365 : Tensor = prim::GetAttr[name="weight"](%282)
  %366 : Float(16384:1, 4096:16384) = aten::t(%365), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %366), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.16, %364, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %349, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %370 : Tensor = prim::GetAttr[name="bias"](%281)
  %371 : Tensor = prim::GetAttr[name="weight"](%281)
  %372 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %372, %371, %370, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %374 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %350)
  %375 : Float(17:53248, 13:4096, 4096:1), %376 : Float(4096:1) = prim::TupleUnpack(%374)
  %377 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%375, %376)
  %378 : Float(17:53248, 13:4096, 4096:1), %379 : Float(4096:1) = prim::TupleUnpack(%377)
  %380 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %381 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%380)
  %382 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%381)
  %383 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%381)
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%381)
  %385 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%381)
  %386 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%385)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%385)
  %388 : Tensor = prim::GetAttr[name="weight"](%387)
  %389 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%385)
  %390 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%385)
  %391 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%385)
  %392 : Tensor = prim::GetAttr[name="bias"](%391)
  %393 : Tensor = prim::GetAttr[name="weight"](%391)
  %394 : Float(4096:1, 4096:4096) = aten::t(%393), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%378, %394), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %392, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %397 : Tensor = prim::GetAttr[name="bias"](%390)
  %398 : Tensor = prim::GetAttr[name="weight"](%390)
  %399 : Float(4096:1, 4096:4096) = aten::t(%398), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%378, %399), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.18, %397, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %402 : Tensor = prim::GetAttr[name="bias"](%389)
  %403 : Tensor = prim::GetAttr[name="weight"](%389)
  %404 : Float(4096:1, 4096:4096) = aten::t(%403), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%378, %404), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.19, %402, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %407 : int = aten::size(%x.22, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %408 : int = aten::size(%x.22, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %409 : int[] = prim::ListConstruct(%407, %408, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %409), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %411 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %411), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %413 : int = aten::size(%x.24, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %414 : int = aten::size(%x.24, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %415 : int[] = prim::ListConstruct(%413, %414, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %415), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %417 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %417), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %419 : int = aten::size(%x.26, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %420 : int = aten::size(%x.26, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %421 : int[] = prim::ListConstruct(%419, %420, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %421), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %423 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %423), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %425 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %425), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %432 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %433 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %432), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %434 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%433, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %435 : Float(4096:1, 4096:4096) = aten::t(%388), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %436 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %437 : Float(64:64, 64:1, 4096:4096) = aten::view(%435, %436), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %438 : Float(64:64, 64:1, 4096:4096) = aten::to(%437, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%379, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %440 : Tensor[] = prim::ListConstruct(%434, %438), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %441 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %440), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add(%441, %b.4, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::add(%378, %projected_context_layer.4, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %445 : Tensor = prim::GetAttr[name="bias"](%386)
  %446 : Tensor = prim::GetAttr[name="weight"](%386)
  %447 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %447, %446, %445, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %449 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %450 : Float(17:53248, 13:4096, 4096:1), %451 : Float(4096:1) = prim::TupleUnpack(%449)
  %452 : Tensor = prim::GetAttr[name="bias"](%384)
  %453 : Tensor = prim::GetAttr[name="weight"](%384)
  %454 : Float(4096:1, 16384:4096) = aten::t(%453), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%450, %454), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.20, %452, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %457 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.28, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %458 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.28, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %459 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%458, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %460 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.28, %459, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %461 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%460, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %462 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%461), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %463 : Float(17:212992, 13:16384, 16384:1) = aten::add(%462, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%457, %463), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %465 : Tensor = prim::GetAttr[name="bias"](%383)
  %466 : Tensor = prim::GetAttr[name="weight"](%383)
  %467 : Float(16384:1, 4096:16384) = aten::t(%466), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %467), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.21, %465, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %450, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %471 : Tensor = prim::GetAttr[name="bias"](%382)
  %472 : Tensor = prim::GetAttr[name="weight"](%382)
  %473 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %473, %472, %471, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %475 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %451)
  %476 : Float(17:53248, 13:4096, 4096:1), %477 : Float(4096:1) = prim::TupleUnpack(%475)
  %478 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%476, %477)
  %479 : Float(17:53248, 13:4096, 4096:1), %480 : Float(4096:1) = prim::TupleUnpack(%478)
  %481 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %482 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%481)
  %483 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%482)
  %484 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%482)
  %485 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%482)
  %486 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%482)
  %487 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%486)
  %488 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%486)
  %489 : Tensor = prim::GetAttr[name="weight"](%488)
  %490 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%486)
  %491 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%486)
  %492 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%486)
  %493 : Tensor = prim::GetAttr[name="bias"](%492)
  %494 : Tensor = prim::GetAttr[name="weight"](%492)
  %495 : Float(4096:1, 4096:4096) = aten::t(%494), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%479, %495), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.22, %493, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %498 : Tensor = prim::GetAttr[name="bias"](%491)
  %499 : Tensor = prim::GetAttr[name="weight"](%491)
  %500 : Float(4096:1, 4096:4096) = aten::t(%499), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%479, %500), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %498, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %503 : Tensor = prim::GetAttr[name="bias"](%490)
  %504 : Tensor = prim::GetAttr[name="weight"](%490)
  %505 : Float(4096:1, 4096:4096) = aten::t(%504), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%479, %505), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.24, %503, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %508 : int = aten::size(%x.29, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %509 : int = aten::size(%x.29, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %510 : int[] = prim::ListConstruct(%508, %509, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %510), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %512 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %512), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %514 : int = aten::size(%x.31, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %515 : int = aten::size(%x.31, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %516 : int[] = prim::ListConstruct(%514, %515, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %516), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %518 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %518), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %520 : int = aten::size(%x.33, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %521 : int = aten::size(%x.33, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %522 : int[] = prim::ListConstruct(%520, %521, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %522), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %524 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %524), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %526 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %526), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %533 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %534 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %533), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %535 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%534, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %536 : Float(4096:1, 4096:4096) = aten::t(%489), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %537 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %538 : Float(64:64, 64:1, 4096:4096) = aten::view(%536, %537), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %539 : Float(64:64, 64:1, 4096:4096) = aten::to(%538, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%480, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %541 : Tensor[] = prim::ListConstruct(%535, %539), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %542 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %541), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(17:53248, 13:4096, 4096:1) = aten::add(%542, %b.5, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:53248, 13:4096, 4096:1) = aten::add(%479, %projected_context_layer.5, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %546 : Tensor = prim::GetAttr[name="bias"](%487)
  %547 : Tensor = prim::GetAttr[name="weight"](%487)
  %548 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %548, %547, %546, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %550 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %551 : Float(17:53248, 13:4096, 4096:1), %552 : Float(4096:1) = prim::TupleUnpack(%550)
  %553 : Tensor = prim::GetAttr[name="bias"](%485)
  %554 : Tensor = prim::GetAttr[name="weight"](%485)
  %555 : Float(4096:1, 16384:4096) = aten::t(%554), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%551, %555), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.25, %553, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %558 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %559 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.35, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %560 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%559, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %561 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.35, %560, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %562 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%561, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %563 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%562), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %564 : Float(17:212992, 13:16384, 16384:1) = aten::add(%563, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%558, %564), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %566 : Tensor = prim::GetAttr[name="bias"](%484)
  %567 : Tensor = prim::GetAttr[name="weight"](%484)
  %568 : Float(16384:1, 4096:16384) = aten::t(%567), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %568), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.26, %566, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %551, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %572 : Tensor = prim::GetAttr[name="bias"](%483)
  %573 : Tensor = prim::GetAttr[name="weight"](%483)
  %574 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %574, %573, %572, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %576 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %552)
  %577 : Float(17:53248, 13:4096, 4096:1), %578 : Float(4096:1) = prim::TupleUnpack(%576)
  %579 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%577, %578)
  %580 : Float(17:53248, 13:4096, 4096:1), %581 : Float(4096:1) = prim::TupleUnpack(%579)
  %582 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %583 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%582)
  %584 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%583)
  %585 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%583)
  %586 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%583)
  %587 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%583)
  %588 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%587)
  %589 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%587)
  %590 : Tensor = prim::GetAttr[name="weight"](%589)
  %591 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%587)
  %592 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%587)
  %593 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%587)
  %594 : Tensor = prim::GetAttr[name="bias"](%593)
  %595 : Tensor = prim::GetAttr[name="weight"](%593)
  %596 : Float(4096:1, 4096:4096) = aten::t(%595), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%580, %596), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.27, %594, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %599 : Tensor = prim::GetAttr[name="bias"](%592)
  %600 : Tensor = prim::GetAttr[name="weight"](%592)
  %601 : Float(4096:1, 4096:4096) = aten::t(%600), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%580, %601), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.28, %599, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %604 : Tensor = prim::GetAttr[name="bias"](%591)
  %605 : Tensor = prim::GetAttr[name="weight"](%591)
  %606 : Float(4096:1, 4096:4096) = aten::t(%605), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%580, %606), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %604, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %609 : int = aten::size(%x.36, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %610 : int = aten::size(%x.36, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %611 : int[] = prim::ListConstruct(%609, %610, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %611), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %613 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %613), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %615 : int = aten::size(%x.38, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %616 : int = aten::size(%x.38, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %617 : int[] = prim::ListConstruct(%615, %616, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %617), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %619 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %619), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %621 : int = aten::size(%x.40, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %622 : int = aten::size(%x.40, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %623 : int[] = prim::ListConstruct(%621, %622, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %623), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %625 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %625), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %627 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %627), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %634 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %635 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %634), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %636 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%635, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %637 : Float(4096:1, 4096:4096) = aten::t(%590), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %638 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %639 : Float(64:64, 64:1, 4096:4096) = aten::view(%637, %638), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %640 : Float(64:64, 64:1, 4096:4096) = aten::to(%639, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%581, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %642 : Tensor[] = prim::ListConstruct(%636, %640), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %643 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %642), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(17:53248, 13:4096, 4096:1) = aten::add(%643, %b.6, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:53248, 13:4096, 4096:1) = aten::add(%580, %projected_context_layer.6, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %647 : Tensor = prim::GetAttr[name="bias"](%588)
  %648 : Tensor = prim::GetAttr[name="weight"](%588)
  %649 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %649, %648, %647, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %651 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %652 : Float(17:53248, 13:4096, 4096:1), %653 : Float(4096:1) = prim::TupleUnpack(%651)
  %654 : Tensor = prim::GetAttr[name="bias"](%586)
  %655 : Tensor = prim::GetAttr[name="weight"](%586)
  %656 : Float(4096:1, 16384:4096) = aten::t(%655), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%652, %656), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.30, %654, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %659 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.42, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %660 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.42, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %661 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%660, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %662 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.42, %661, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %663 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%662, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %664 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%663), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %665 : Float(17:212992, 13:16384, 16384:1) = aten::add(%664, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%659, %665), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %667 : Tensor = prim::GetAttr[name="bias"](%585)
  %668 : Tensor = prim::GetAttr[name="weight"](%585)
  %669 : Float(16384:1, 4096:16384) = aten::t(%668), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %669), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.31, %667, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %652, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %673 : Tensor = prim::GetAttr[name="bias"](%584)
  %674 : Tensor = prim::GetAttr[name="weight"](%584)
  %675 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %675, %674, %673, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %677 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %653)
  %678 : Float(17:53248, 13:4096, 4096:1), %679 : Float(4096:1) = prim::TupleUnpack(%677)
  %680 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%678, %679)
  %681 : Float(17:53248, 13:4096, 4096:1), %682 : Float(4096:1) = prim::TupleUnpack(%680)
  %683 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %684 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%683)
  %685 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%684)
  %686 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%684)
  %687 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%684)
  %688 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%684)
  %689 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%688)
  %690 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%688)
  %691 : Tensor = prim::GetAttr[name="weight"](%690)
  %692 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%688)
  %693 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%688)
  %694 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%688)
  %695 : Tensor = prim::GetAttr[name="bias"](%694)
  %696 : Tensor = prim::GetAttr[name="weight"](%694)
  %697 : Float(4096:1, 4096:4096) = aten::t(%696), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%681, %697), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.32, %695, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %700 : Tensor = prim::GetAttr[name="bias"](%693)
  %701 : Tensor = prim::GetAttr[name="weight"](%693)
  %702 : Float(4096:1, 4096:4096) = aten::t(%701), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%681, %702), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.33, %700, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %705 : Tensor = prim::GetAttr[name="bias"](%692)
  %706 : Tensor = prim::GetAttr[name="weight"](%692)
  %707 : Float(4096:1, 4096:4096) = aten::t(%706), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%681, %707), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.34, %705, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %710 : int = aten::size(%x.43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %711 : int = aten::size(%x.43, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %712 : int[] = prim::ListConstruct(%710, %711, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %712), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %714 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %714), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %716 : int = aten::size(%x.45, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %717 : int = aten::size(%x.45, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %718 : int[] = prim::ListConstruct(%716, %717, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %718), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %720 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %720), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %722 : int = aten::size(%x.47, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %723 : int = aten::size(%x.47, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %724 : int[] = prim::ListConstruct(%722, %723, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %724), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %726 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %726), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %728 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %728), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %735 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %736 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %735), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %737 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%736, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %738 : Float(4096:1, 4096:4096) = aten::t(%691), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %739 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %740 : Float(64:64, 64:1, 4096:4096) = aten::view(%738, %739), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %741 : Float(64:64, 64:1, 4096:4096) = aten::to(%740, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%682, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %743 : Tensor[] = prim::ListConstruct(%737, %741), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %744 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %743), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::add(%744, %b.7, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(17:53248, 13:4096, 4096:1) = aten::add(%681, %projected_context_layer.7, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %748 : Tensor = prim::GetAttr[name="bias"](%689)
  %749 : Tensor = prim::GetAttr[name="weight"](%689)
  %750 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %750, %749, %748, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %752 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %753 : Float(17:53248, 13:4096, 4096:1), %754 : Float(4096:1) = prim::TupleUnpack(%752)
  %755 : Tensor = prim::GetAttr[name="bias"](%687)
  %756 : Tensor = prim::GetAttr[name="weight"](%687)
  %757 : Float(4096:1, 16384:4096) = aten::t(%756), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%753, %757), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.35, %755, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %760 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.49, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %761 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.49, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %762 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%761, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %763 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.49, %762, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %764 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%763, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %765 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%764), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %766 : Float(17:212992, 13:16384, 16384:1) = aten::add(%765, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%760, %766), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %768 : Tensor = prim::GetAttr[name="bias"](%686)
  %769 : Tensor = prim::GetAttr[name="weight"](%686)
  %770 : Float(16384:1, 4096:16384) = aten::t(%769), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %770), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.36, %768, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %753, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %774 : Tensor = prim::GetAttr[name="bias"](%685)
  %775 : Tensor = prim::GetAttr[name="weight"](%685)
  %776 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %776, %775, %774, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %778 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %754)
  %779 : Float(17:53248, 13:4096, 4096:1), %780 : Float(4096:1) = prim::TupleUnpack(%778)
  %781 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%779, %780)
  %782 : Float(17:53248, 13:4096, 4096:1), %783 : Float(4096:1) = prim::TupleUnpack(%781)
  %784 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %785 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%784)
  %786 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%785)
  %787 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%785)
  %788 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%785)
  %789 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%785)
  %790 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%789)
  %791 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%789)
  %792 : Tensor = prim::GetAttr[name="weight"](%791)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%789)
  %794 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%789)
  %795 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%789)
  %796 : Tensor = prim::GetAttr[name="bias"](%795)
  %797 : Tensor = prim::GetAttr[name="weight"](%795)
  %798 : Float(4096:1, 4096:4096) = aten::t(%797), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%782, %798), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.37, %796, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %801 : Tensor = prim::GetAttr[name="bias"](%794)
  %802 : Tensor = prim::GetAttr[name="weight"](%794)
  %803 : Float(4096:1, 4096:4096) = aten::t(%802), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%782, %803), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.38, %801, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %806 : Tensor = prim::GetAttr[name="bias"](%793)
  %807 : Tensor = prim::GetAttr[name="weight"](%793)
  %808 : Float(4096:1, 4096:4096) = aten::t(%807), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%782, %808), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.39, %806, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %811 : int = aten::size(%x.50, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %812 : int = aten::size(%x.50, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %813 : int[] = prim::ListConstruct(%811, %812, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %813), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %815 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %815), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %817 : int = aten::size(%x.52, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %818 : int = aten::size(%x.52, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %819 : int[] = prim::ListConstruct(%817, %818, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %819), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %821 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %821), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %823 : int = aten::size(%x.54, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %824 : int = aten::size(%x.54, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %825 : int[] = prim::ListConstruct(%823, %824, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %825), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %827 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %827), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %829 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %829), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %836 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %837 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %836), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %838 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%837, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %839 : Float(4096:1, 4096:4096) = aten::t(%792), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %840 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %841 : Float(64:64, 64:1, 4096:4096) = aten::view(%839, %840), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %842 : Float(64:64, 64:1, 4096:4096) = aten::to(%841, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%783, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %844 : Tensor[] = prim::ListConstruct(%838, %842), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %845 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %844), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(17:53248, 13:4096, 4096:1) = aten::add(%845, %b.8, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:53248, 13:4096, 4096:1) = aten::add(%782, %projected_context_layer.8, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %849 : Tensor = prim::GetAttr[name="bias"](%790)
  %850 : Tensor = prim::GetAttr[name="weight"](%790)
  %851 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %851, %850, %849, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %853 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %854 : Float(17:53248, 13:4096, 4096:1), %855 : Float(4096:1) = prim::TupleUnpack(%853)
  %856 : Tensor = prim::GetAttr[name="bias"](%788)
  %857 : Tensor = prim::GetAttr[name="weight"](%788)
  %858 : Float(4096:1, 16384:4096) = aten::t(%857), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%854, %858), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.40, %856, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %861 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.56, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %862 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.56, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %863 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%862, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %864 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.56, %863, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %865 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%864, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %866 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%865), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %867 : Float(17:212992, 13:16384, 16384:1) = aten::add(%866, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%861, %867), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %869 : Tensor = prim::GetAttr[name="bias"](%787)
  %870 : Tensor = prim::GetAttr[name="weight"](%787)
  %871 : Float(16384:1, 4096:16384) = aten::t(%870), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %871), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %869, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %854, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %875 : Tensor = prim::GetAttr[name="bias"](%786)
  %876 : Tensor = prim::GetAttr[name="weight"](%786)
  %877 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %877, %876, %875, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %879 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %855)
  %880 : Float(17:53248, 13:4096, 4096:1), %881 : Float(4096:1) = prim::TupleUnpack(%879)
  %882 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%880, %881)
  %883 : Float(17:53248, 13:4096, 4096:1), %884 : Float(4096:1) = prim::TupleUnpack(%882)
  %885 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %886 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%885)
  %887 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%886)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%886)
  %889 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%886)
  %890 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%886)
  %891 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%890)
  %892 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%890)
  %893 : Tensor = prim::GetAttr[name="weight"](%892)
  %894 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%890)
  %895 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%890)
  %896 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%890)
  %897 : Tensor = prim::GetAttr[name="bias"](%896)
  %898 : Tensor = prim::GetAttr[name="weight"](%896)
  %899 : Float(4096:1, 4096:4096) = aten::t(%898), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%883, %899), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.42, %897, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %902 : Tensor = prim::GetAttr[name="bias"](%895)
  %903 : Tensor = prim::GetAttr[name="weight"](%895)
  %904 : Float(4096:1, 4096:4096) = aten::t(%903), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%883, %904), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.43, %902, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %907 : Tensor = prim::GetAttr[name="bias"](%894)
  %908 : Tensor = prim::GetAttr[name="weight"](%894)
  %909 : Float(4096:1, 4096:4096) = aten::t(%908), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%883, %909), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.44, %907, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %912 : int = aten::size(%x.57, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %913 : int = aten::size(%x.57, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %914 : int[] = prim::ListConstruct(%912, %913, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %914), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %916 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %916), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %918 : int = aten::size(%x.59, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %919 : int = aten::size(%x.59, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %920 : int[] = prim::ListConstruct(%918, %919, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %920), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %922 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %922), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %924 : int = aten::size(%x.61, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %925 : int = aten::size(%x.61, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %926 : int[] = prim::ListConstruct(%924, %925, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %926), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %928 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %928), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %930 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %930), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %937 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %938 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %937), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %939 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%938, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %940 : Float(4096:1, 4096:4096) = aten::t(%893), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %941 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %942 : Float(64:64, 64:1, 4096:4096) = aten::view(%940, %941), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %943 : Float(64:64, 64:1, 4096:4096) = aten::to(%942, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%884, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %945 : Tensor[] = prim::ListConstruct(%939, %943), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %946 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %945), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(17:53248, 13:4096, 4096:1) = aten::add(%946, %b.9, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:53248, 13:4096, 4096:1) = aten::add(%883, %projected_context_layer.9, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %950 : Tensor = prim::GetAttr[name="bias"](%891)
  %951 : Tensor = prim::GetAttr[name="weight"](%891)
  %952 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %952, %951, %950, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %954 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %955 : Float(17:53248, 13:4096, 4096:1), %956 : Float(4096:1) = prim::TupleUnpack(%954)
  %957 : Tensor = prim::GetAttr[name="bias"](%889)
  %958 : Tensor = prim::GetAttr[name="weight"](%889)
  %959 : Float(4096:1, 16384:4096) = aten::t(%958), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%955, %959), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.45, %957, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %962 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.63, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %963 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.63, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %964 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%963, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %965 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.63, %964, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %966 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%965, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %967 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%966), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %968 : Float(17:212992, 13:16384, 16384:1) = aten::add(%967, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%962, %968), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %970 : Tensor = prim::GetAttr[name="bias"](%888)
  %971 : Tensor = prim::GetAttr[name="weight"](%888)
  %972 : Float(16384:1, 4096:16384) = aten::t(%971), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %972), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.46, %970, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %955, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %976 : Tensor = prim::GetAttr[name="bias"](%887)
  %977 : Tensor = prim::GetAttr[name="weight"](%887)
  %978 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %978, %977, %976, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %980 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %956)
  %981 : Float(17:53248, 13:4096, 4096:1), %982 : Float(4096:1) = prim::TupleUnpack(%980)
  %983 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%981, %982)
  %984 : Float(17:53248, 13:4096, 4096:1), %985 : Float(4096:1) = prim::TupleUnpack(%983)
  %986 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %987 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%986)
  %988 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%987)
  %989 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%987)
  %990 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%987)
  %991 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%987)
  %992 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%991)
  %993 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%991)
  %994 : Tensor = prim::GetAttr[name="weight"](%993)
  %995 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%991)
  %996 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%991)
  %997 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%991)
  %998 : Tensor = prim::GetAttr[name="bias"](%997)
  %999 : Tensor = prim::GetAttr[name="weight"](%997)
  %1000 : Float(4096:1, 4096:4096) = aten::t(%999), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%984, %1000), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %998, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1003 : Tensor = prim::GetAttr[name="bias"](%996)
  %1004 : Tensor = prim::GetAttr[name="weight"](%996)
  %1005 : Float(4096:1, 4096:4096) = aten::t(%1004), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%984, %1005), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1003, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1008 : Tensor = prim::GetAttr[name="bias"](%995)
  %1009 : Tensor = prim::GetAttr[name="weight"](%995)
  %1010 : Float(4096:1, 4096:4096) = aten::t(%1009), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%984, %1010), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1008, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1013 : int = aten::size(%x.64, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1014 : int = aten::size(%x.64, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1015 : int[] = prim::ListConstruct(%1013, %1014, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1015), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1017 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1017), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1019 : int = aten::size(%x.66, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1020 : int = aten::size(%x.66, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1021 : int[] = prim::ListConstruct(%1019, %1020, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1021), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1023 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1023), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1025 : int = aten::size(%x.68, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1026 : int = aten::size(%x.68, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1027 : int[] = prim::ListConstruct(%1025, %1026, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1027), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1029 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1029), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1031 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1031), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1038 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1039 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1038), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1040 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1039, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1041 : Float(4096:1, 4096:4096) = aten::t(%994), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1042 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1043 : Float(64:64, 64:1, 4096:4096) = aten::view(%1041, %1042), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1044 : Float(64:64, 64:1, 4096:4096) = aten::to(%1043, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%985, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1046 : Tensor[] = prim::ListConstruct(%1040, %1044), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1047 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %1046), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1047, %b.10, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:53248, 13:4096, 4096:1) = aten::add(%984, %projected_context_layer.10, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1051 : Tensor = prim::GetAttr[name="bias"](%992)
  %1052 : Tensor = prim::GetAttr[name="weight"](%992)
  %1053 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1053, %1052, %1051, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1055 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1056 : Float(17:53248, 13:4096, 4096:1), %1057 : Float(4096:1) = prim::TupleUnpack(%1055)
  %1058 : Tensor = prim::GetAttr[name="bias"](%990)
  %1059 : Tensor = prim::GetAttr[name="weight"](%990)
  %1060 : Float(4096:1, 16384:4096) = aten::t(%1059), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1056, %1060), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1058, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1063 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.70, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1064 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.70, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1065 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1064, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1066 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.70, %1065, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1067 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1066, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1068 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1067), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1069 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1068, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1063, %1069), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1071 : Tensor = prim::GetAttr[name="bias"](%989)
  %1072 : Tensor = prim::GetAttr[name="weight"](%989)
  %1073 : Float(16384:1, 4096:16384) = aten::t(%1072), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1073), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1071, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1056, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1077 : Tensor = prim::GetAttr[name="bias"](%988)
  %1078 : Tensor = prim::GetAttr[name="weight"](%988)
  %1079 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1079, %1078, %1077, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1081 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1057)
  %1082 : Float(17:53248, 13:4096, 4096:1), %1083 : Float(4096:1) = prim::TupleUnpack(%1081)
  %1084 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1082, %1083)
  %1085 : Float(17:53248, 13:4096, 4096:1), %1086 : Float(4096:1) = prim::TupleUnpack(%1084)
  %1087 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %1088 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%1087)
  %1089 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1088)
  %1090 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%1088)
  %1091 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%1088)
  %1092 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%1088)
  %1093 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%1092)
  %1094 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%1092)
  %1095 : Tensor = prim::GetAttr[name="weight"](%1094)
  %1096 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%1092)
  %1097 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%1092)
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%1092)
  %1099 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1100 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1101 : Float(4096:1, 4096:4096) = aten::t(%1100), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1085, %1101), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1099, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1104 : Tensor = prim::GetAttr[name="bias"](%1097)
  %1105 : Tensor = prim::GetAttr[name="weight"](%1097)
  %1106 : Float(4096:1, 4096:4096) = aten::t(%1105), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1085, %1106), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1104, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1109 : Tensor = prim::GetAttr[name="bias"](%1096)
  %1110 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1111 : Float(4096:1, 4096:4096) = aten::t(%1110), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1085, %1111), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1109, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1114 : int = aten::size(%x.71, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1115 : int = aten::size(%x.71, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1116 : int[] = prim::ListConstruct(%1114, %1115, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1116), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1118 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1118), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1120 : int = aten::size(%x.73, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1121 : int = aten::size(%x.73, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1122 : int[] = prim::ListConstruct(%1120, %1121, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1122), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1124 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1124), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1126 : int = aten::size(%x.75, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1127 : int = aten::size(%x.75, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1128 : int[] = prim::ListConstruct(%1126, %1127, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1128), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1130 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1130), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1132 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1132), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1139 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1140 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1139), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1141 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1140, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1142 : Float(4096:1, 4096:4096) = aten::t(%1095), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1143 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1144 : Float(64:64, 64:1, 4096:4096) = aten::view(%1142, %1143), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1145 : Float(64:64, 64:1, 4096:4096) = aten::to(%1144, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1086, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1147 : Tensor[] = prim::ListConstruct(%1141, %1145), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1148 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %1147), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1148, %b.11, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1085, %projected_context_layer.11, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1152 : Tensor = prim::GetAttr[name="bias"](%1093)
  %1153 : Tensor = prim::GetAttr[name="weight"](%1093)
  %1154 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1154, %1153, %1152, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1156 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1157 : Float(17:53248, 13:4096, 4096:1), %1158 : Float(4096:1) = prim::TupleUnpack(%1156)
  %1159 : Tensor = prim::GetAttr[name="bias"](%1091)
  %1160 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1161 : Float(4096:1, 16384:4096) = aten::t(%1160), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1157, %1161), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1159, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1164 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.77, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1165 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.77, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1166 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1165, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1167 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.77, %1166, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1168 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1167, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1169 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1168), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1170 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1169, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1164, %1170), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1172 : Tensor = prim::GetAttr[name="bias"](%1090)
  %1173 : Tensor = prim::GetAttr[name="weight"](%1090)
  %1174 : Float(16384:1, 4096:16384) = aten::t(%1173), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1174), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1172, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1157, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1178 : Tensor = prim::GetAttr[name="bias"](%1089)
  %1179 : Tensor = prim::GetAttr[name="weight"](%1089)
  %1180 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1180, %1179, %1178, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1182 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1158)
  %1183 : Float(17:53248, 13:4096, 4096:1), %1184 : Float(4096:1) = prim::TupleUnpack(%1182)
  %1185 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1183, %1184)
  %1186 : Float(17:53248, 13:4096, 4096:1), %1187 : Float(4096:1) = prim::TupleUnpack(%1185)
  %1188 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="albert_layers"](%68)
  %1189 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name="0"](%1188)
  %1190 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1189)
  %1191 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name="ffn_output"](%1189)
  %1192 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="ffn"](%1189)
  %1193 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name="attention"](%1189)
  %1194 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name="LayerNorm"](%1193)
  %1195 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="dense"](%1193)
  %1196 : Tensor = prim::GetAttr[name="weight"](%1195)
  %1197 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="value"](%1193)
  %1198 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="key"](%1193)
  %1199 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="query"](%1193)
  %1200 : Tensor = prim::GetAttr[name="bias"](%1199)
  %1201 : Tensor = prim::GetAttr[name="weight"](%1199)
  %1202 : Float(4096:1, 4096:4096) = aten::t(%1201), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1186, %1202), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1200, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1205 : Tensor = prim::GetAttr[name="bias"](%1198)
  %1206 : Tensor = prim::GetAttr[name="weight"](%1198)
  %1207 : Float(4096:1, 4096:4096) = aten::t(%1206), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1186, %1207), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1205, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1210 : Tensor = prim::GetAttr[name="bias"](%1197)
  %1211 : Tensor = prim::GetAttr[name="weight"](%1197)
  %1212 : Float(4096:1, 4096:4096) = aten::t(%1211), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1186, %1212), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1210, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1215 : int = aten::size(%x.78, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1216 : int = aten::size(%x.78, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1217 : int[] = prim::ListConstruct(%1215, %1216, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1217), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1219 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1219), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1221 : int = aten::size(%x.80, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1222 : int = aten::size(%x.80, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1223 : int[] = prim::ListConstruct(%1221, %1222, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1223), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1225 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1225), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1227 : int = aten::size(%x.82, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1228 : int = aten::size(%x.82, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1229 : int[] = prim::ListConstruct(%1227, %1228, %13, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1229), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1231 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1231), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1233 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %20, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1233), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %20, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1240 : int[] = prim::ListConstruct(%34, %29, %33, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1241 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1240), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1242 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1241, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1243 : Float(4096:1, 4096:4096) = aten::t(%1196), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1244 : int[] = prim::ListConstruct(%13, %13, %9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1245 : Float(64:64, 64:1, 4096:4096) = aten::view(%1243, %1244), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1246 : Float(64:64, 64:1, 4096:4096) = aten::to(%1245, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1187, %28, %30, %30, %27), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1248 : Tensor[] = prim::ListConstruct(%1242, %1246), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1249 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%8, %1248), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1249, %b, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %24, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1186, %projected_context_layer, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1253 : Tensor = prim::GetAttr[name="bias"](%1194)
  %1254 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1255 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1255, %1254, %1253, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1257 : Tensor = prim::GetAttr[name="bias"](%1192)
  %1258 : Tensor = prim::GetAttr[name="weight"](%1192)
  %1259 : Float(4096:1, 16384:4096) = aten::t(%1258), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1259), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.84 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1257, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1262 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.84, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1263 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.84, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1264 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1263, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1265 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.84, %1264, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1266 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1265, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1267 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1266), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1268 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1267, %18, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1262, %1268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1270 : Tensor = prim::GetAttr[name="bias"](%1191)
  %1271 : Tensor = prim::GetAttr[name="weight"](%1191)
  %1272 : Float(16384:1, 4096:16384) = aten::t(%1271), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.61 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1272), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.61, %1270, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1276 : Tensor = prim::GetAttr[name="bias"](%1190)
  %1277 : Tensor = prim::GetAttr[name="weight"](%1190)
  %1278 : int[] = prim::ListConstruct(%9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.90 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1278, %1277, %1276, %22, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1280 : int = prim::Constant[value=128](), scope: __module.predictions/__module.predictions.LayerNorm # torch/nn/functional.py:2048:0
  %1281 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.predictions/__module.predictions.LayerNorm # torch/nn/functional.py:2048:0
  %1282 : bool = prim::Constant[value=1](), scope: __module.predictions/__module.predictions.LayerNorm # torch/nn/functional.py:2048:0
  %1283 : Double() = prim::Constant[value={1}](), scope: __module.predictions # transformers/activations.py:30:0
  %1284 : Double() = prim::Constant[value={0.797885}](), scope: __module.predictions # transformers/activations.py:30:0
  %1285 : Double() = prim::Constant[value={0.044715}](), scope: __module.predictions # transformers/activations.py:30:0
  %1286 : float = prim::Constant[value=3.](), scope: __module.predictions # transformers/activations.py:30:0
  %1287 : Double() = prim::Constant[value={0.5}](), scope: __module.predictions # transformers/activations.py:30:0
  %1288 : int = prim::Constant[value=1](), scope: __module.predictions/__module.predictions.dense # torch/nn/functional.py:1678:0
  %1289 : Tensor = prim::GetAttr[name="bias"](%3)
  %1290 : __torch__.torch.nn.modules.linear.___torch_mangle_16.Linear = prim::GetAttr[name="decoder"](%3)
  %1291 : __torch__.torch.nn.modules.normalization.___torch_mangle_14.LayerNorm = prim::GetAttr[name="LayerNorm"](%3)
  %1292 : __torch__.torch.nn.modules.linear.___torch_mangle_15.Linear = prim::GetAttr[name="dense"](%3)
  %1293 : Tensor = prim::GetAttr[name="bias"](%1292)
  %1294 : Tensor = prim::GetAttr[name="weight"](%1292)
  %1295 : Float(4096:1, 128:4096) = aten::t(%1294), scope: __module.predictions/__module.predictions.dense # torch/nn/functional.py:1676:0
  %output.62 : Float(17:1664, 13:128, 128:1) = aten::matmul(%input.90, %1295), scope: __module.predictions/__module.predictions.dense # torch/nn/functional.py:1676:0
  %x : Float(17:1664, 13:128, 128:1) = aten::add_(%output.62, %1293, %1288), scope: __module.predictions/__module.predictions.dense # torch/nn/functional.py:1678:0
  %1298 : Float(17:1664, 13:128, 128:1) = aten::mul(%x, %1287), scope: __module.predictions # transformers/activations.py:30:0
  %1299 : Float(17:1664, 13:128, 128:1) = aten::pow(%x, %1286), scope: __module.predictions # transformers/activations.py:30:0
  %1300 : Float(17:1664, 13:128, 128:1) = aten::mul(%1299, %1285), scope: __module.predictions # transformers/activations.py:30:0
  %1301 : Float(17:1664, 13:128, 128:1) = aten::add(%x, %1300, %1288), scope: __module.predictions # transformers/activations.py:30:0
  %1302 : Float(17:1664, 13:128, 128:1) = aten::mul(%1301, %1284), scope: __module.predictions # transformers/activations.py:30:0
  %1303 : Float(17:1664, 13:128, 128:1) = aten::tanh(%1302), scope: __module.predictions # transformers/activations.py:30:0
  %1304 : Float(17:1664, 13:128, 128:1) = aten::add(%1303, %1283, %1288), scope: __module.predictions # transformers/activations.py:30:0
  %input.91 : Float(17:1664, 13:128, 128:1) = aten::mul(%1298, %1304), scope: __module.predictions # transformers/activations.py:30:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%1291)
  %1307 : Tensor = prim::GetAttr[name="weight"](%1291)
  %1308 : int[] = prim::ListConstruct(%1280), scope: __module.predictions/__module.predictions.LayerNorm
  %input : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.91, %1308, %1307, %1306, %1281, %1282), scope: __module.predictions/__module.predictions.LayerNorm # torch/nn/functional.py:2048:0
  %1310 : Tensor = prim::GetAttr[name="weight"](%1290)
  %1311 : Float(128:1, 30000:128) = aten::t(%1310), scope: __module.predictions/__module.predictions.decoder # torch/nn/functional.py:1676:0
  %output : Float(17:390000, 13:30000, 30000:1) = aten::matmul(%input, %1311), scope: __module.predictions/__module.predictions.decoder # torch/nn/functional.py:1676:0
  %1313 : Float(17:390000, 13:30000, 30000:1) = aten::add_(%output, %1289, %1288), scope: __module.predictions/__module.predictions.decoder # torch/nn/functional.py:1678:0
  %7 : (Float(17:390000, 13:30000, 30000:1)) = prim::TupleConstruct(%1313)
  return (%7)
