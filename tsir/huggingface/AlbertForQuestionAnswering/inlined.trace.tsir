graph(%self.1 : __torch__.transformers.modeling_albert.AlbertForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_198.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_albert.___torch_mangle_197.AlbertModel = prim::GetAttr[name="albert"](%self.1)
  %17 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %18 : int = prim::Constant[value=4096](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %19 : Double() = prim::Constant[value={8}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %20 : int = prim::Constant[value=-2](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %21 : int = prim::Constant[value=3](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %22 : int = prim::Constant[value=64](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %23 : Double() = prim::Constant[value={0.5}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %24 : float = prim::Constant[value=3.](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %25 : Double() = prim::Constant[value={0.044715}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %26 : Double() = prim::Constant[value={0.797885}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %27 : Double() = prim::Constant[value={1}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %28 : int = prim::Constant[value=9223372036854775807](), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %29 : int = prim::Constant[value=-1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %30 : bool = prim::Constant[value=1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %31 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %32 : int = prim::Constant[value=128](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %33 : float = prim::Constant[value=0.](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %34 : Double() = prim::Constant[value={-10000}](), scope: __module.albert # transformers/modeling_albert.py:678:0
  %35 : float = prim::Constant[value=1.](), scope: __module.albert # torch/tensor.py:396:0
  %36 : None = prim::Constant(), scope: __module.albert
  %37 : int = prim::Constant[value=6](), scope: __module.albert # transformers/modeling_albert.py:677:0
  %38 : int = prim::Constant[value=2](), scope: __module.albert # transformers/modeling_albert.py:676:0
  %39 : bool = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %40 : Device = prim::Constant[value="cpu"](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %41 : int = prim::Constant[value=4](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %42 : int = prim::Constant[value=1](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %43 : int = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %44 : __torch__.transformers.modeling_albert.___torch_mangle_196.AlbertTransformer = prim::GetAttr[name="encoder"](%4)
  %45 : __torch__.transformers.modeling_albert.___torch_mangle_178.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %46 : int = aten::size(%input_ids, %43), scope: __module.albert # transformers/modeling_albert.py:663:0
  %47 : int = aten::size(%input_ids, %42), scope: __module.albert # transformers/modeling_albert.py:663:0
  %48 : int[] = prim::ListConstruct(%46, %47), scope: __module.albert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%48, %41, %43, %40, %39), scope: __module.albert # transformers/modeling_albert.py:674:0
  %50 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %42), scope: __module.albert # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%50, %38), scope: __module.albert # transformers/modeling_albert.py:676:0
  %52 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %37, %39, %39, %36), scope: __module.albert # transformers/modeling_albert.py:677:0
  %53 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%52, %35, %42), scope: __module.albert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%53, %34), scope: __module.albert # transformers/modeling_albert.py:678:0
  %55 : __torch__.torch.nn.modules.normalization.___torch_mangle_176.LayerNorm = prim::GetAttr[name="LayerNorm"](%45)
  %56 : __torch__.torch.nn.modules.sparse.___torch_mangle_175.Embedding = prim::GetAttr[name="token_type_embeddings"](%45)
  %57 : __torch__.torch.nn.modules.sparse.___torch_mangle_174.Embedding = prim::GetAttr[name="position_embeddings"](%45)
  %58 : __torch__.torch.nn.modules.sparse.___torch_mangle_173.Embedding = prim::GetAttr[name="word_embeddings"](%45)
  %59 : Tensor = prim::GetAttr[name="position_ids"](%45)
  %60 : int = aten::size(%input_ids, %42), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:222:0
  %61 : Long(1:512, 512:1) = aten::slice(%59, %43, %43, %28, %42), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%61, %42, %43, %60, %42), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %63 : Tensor = prim::GetAttr[name="weight"](%58)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%63, %input_ids, %43, %39, %39), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %65 : Tensor = prim::GetAttr[name="weight"](%57)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%65, %input.1, %29, %39, %39), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %67 : Tensor = prim::GetAttr[name="weight"](%56)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%67, %input.2, %29, %39, %39), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %69 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %42), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%69, %token_type_embeddings, %42), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %71 : Tensor = prim::GetAttr[name="bias"](%55)
  %72 : Tensor = prim::GetAttr[name="weight"](%55)
  %73 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %73, %72, %71, %31, %30), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %33, %39), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_195.ModuleList = prim::GetAttr[name="albert_layer_groups"](%44)
  %77 : __torch__.transformers.modeling_albert.___torch_mangle_194.AlbertLayerGroup = prim::GetAttr[name="0"](%76)
  %78 : __torch__.torch.nn.modules.linear.___torch_mangle_179.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%44)
  %79 : Tensor = prim::GetAttr[name="bias"](%78)
  %80 : Tensor = prim::GetAttr[name="weight"](%78)
  %81 : Float(128:1, 4096:128) = aten::t(%80), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %81), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.1, %79, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %85 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%84)
  %86 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%85)
  %87 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%85)
  %88 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%85)
  %89 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%85)
  %90 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%89)
  %91 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%89)
  %92 : Tensor = prim::GetAttr[name="bias"](%91)
  %93 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%89)
  %94 : Tensor = prim::GetAttr[name="weight"](%93)
  %95 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%89)
  %96 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%89)
  %97 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%89)
  %98 : Tensor = prim::GetAttr[name="bias"](%97)
  %99 : Tensor = prim::GetAttr[name="weight"](%97)
  %100 : Float(4096:1, 4096:4096) = aten::t(%99), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %100), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.2, %98, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %103 : Tensor = prim::GetAttr[name="bias"](%96)
  %104 : Tensor = prim::GetAttr[name="weight"](%96)
  %105 : Float(4096:1, 4096:4096) = aten::t(%104), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %105), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.3, %103, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %108 : Tensor = prim::GetAttr[name="bias"](%95)
  %109 : Tensor = prim::GetAttr[name="weight"](%95)
  %110 : Float(4096:1, 4096:4096) = aten::t(%109), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %110), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.4, %108, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %113 : int = aten::size(%x.1, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %114 : int = aten::size(%x.1, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %115 : int[] = prim::ListConstruct(%113, %114, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %115), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %117 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %117), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %119 : int = aten::size(%x.3, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %120 : int = aten::size(%x.3, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %121 : int[] = prim::ListConstruct(%119, %120, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %121), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %123 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %123), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %125 : int = aten::size(%x.5, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %126 : int = aten::size(%x.5, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %127 : int[] = prim::ListConstruct(%125, %126, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %127), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %129 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %129), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %131 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %131), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %138 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %139 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %138), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %140 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%139, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %141 : Float(4096:1, 4096:4096) = aten::t(%94), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %142 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %143 : Float(64:64, 64:1, 4096:4096) = aten::view(%141, %142), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %144 : Float(64:64, 64:1, 4096:4096) = aten::to(%143, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%92, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %146 : Tensor[] = prim::ListConstruct(%140, %144), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %147 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %146), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(17:53248, 13:4096, 4096:1) = aten::add(%147, %b.1, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %151 : Tensor = prim::GetAttr[name="bias"](%90)
  %152 : Tensor = prim::GetAttr[name="weight"](%90)
  %153 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %153, %152, %151, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %155 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %156 : Float(17:53248, 13:4096, 4096:1), %157 : Float(4096:1) = prim::TupleUnpack(%155)
  %158 : Tensor = prim::GetAttr[name="bias"](%88)
  %159 : Tensor = prim::GetAttr[name="weight"](%88)
  %160 : Float(4096:1, 16384:4096) = aten::t(%159), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%156, %160), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.5, %158, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %163 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.7, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %164 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.7, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %165 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%164, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %166 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.7, %165, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %167 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%166, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %168 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%167), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %169 : Float(17:212992, 13:16384, 16384:1) = aten::add(%168, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%163, %169), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %171 : Tensor = prim::GetAttr[name="bias"](%87)
  %172 : Tensor = prim::GetAttr[name="weight"](%87)
  %173 : Float(16384:1, 4096:16384) = aten::t(%172), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %173), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.6, %171, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %156, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %177 : Tensor = prim::GetAttr[name="bias"](%86)
  %178 : Tensor = prim::GetAttr[name="weight"](%86)
  %179 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %179, %178, %177, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %181 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %157)
  %182 : Float(17:53248, 13:4096, 4096:1), %183 : Float(4096:1) = prim::TupleUnpack(%181)
  %184 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%182, %183)
  %185 : Float(17:53248, 13:4096, 4096:1), %186 : Float(4096:1) = prim::TupleUnpack(%184)
  %187 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %188 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%187)
  %189 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%188)
  %190 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%188)
  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%188)
  %192 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%188)
  %193 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%192)
  %194 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%192)
  %195 : Tensor = prim::GetAttr[name="weight"](%194)
  %196 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%192)
  %197 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%192)
  %198 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%192)
  %199 : Tensor = prim::GetAttr[name="bias"](%198)
  %200 : Tensor = prim::GetAttr[name="weight"](%198)
  %201 : Float(4096:1, 4096:4096) = aten::t(%200), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%185, %201), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.7, %199, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %204 : Tensor = prim::GetAttr[name="bias"](%197)
  %205 : Tensor = prim::GetAttr[name="weight"](%197)
  %206 : Float(4096:1, 4096:4096) = aten::t(%205), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%185, %206), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.8, %204, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %209 : Tensor = prim::GetAttr[name="bias"](%196)
  %210 : Tensor = prim::GetAttr[name="weight"](%196)
  %211 : Float(4096:1, 4096:4096) = aten::t(%210), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%185, %211), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.9, %209, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %214 : int = aten::size(%x.8, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %215 : int = aten::size(%x.8, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %216 : int[] = prim::ListConstruct(%214, %215, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %216), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %218 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %218), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %220 : int = aten::size(%x.10, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %221 : int = aten::size(%x.10, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %222 : int[] = prim::ListConstruct(%220, %221, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %222), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %224 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %224), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %226 : int = aten::size(%x.12, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %227 : int = aten::size(%x.12, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %228 : int[] = prim::ListConstruct(%226, %227, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %228), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %230 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %230), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %232 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %232), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %239 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %240 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %239), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %241 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%240, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %242 : Float(4096:1, 4096:4096) = aten::t(%195), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %243 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %244 : Float(64:64, 64:1, 4096:4096) = aten::view(%242, %243), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %245 : Float(64:64, 64:1, 4096:4096) = aten::to(%244, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%186, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %247 : Tensor[] = prim::ListConstruct(%241, %245), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %248 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %247), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(17:53248, 13:4096, 4096:1) = aten::add(%248, %b.2, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(17:53248, 13:4096, 4096:1) = aten::add(%185, %projected_context_layer.2, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %252 : Tensor = prim::GetAttr[name="bias"](%193)
  %253 : Tensor = prim::GetAttr[name="weight"](%193)
  %254 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %254, %253, %252, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %256 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %257 : Float(17:53248, 13:4096, 4096:1), %258 : Float(4096:1) = prim::TupleUnpack(%256)
  %259 : Tensor = prim::GetAttr[name="bias"](%191)
  %260 : Tensor = prim::GetAttr[name="weight"](%191)
  %261 : Float(4096:1, 16384:4096) = aten::t(%260), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%257, %261), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.10, %259, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %264 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.14, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %265 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.14, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %266 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%265, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %267 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.14, %266, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %268 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%267, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %269 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %270 : Float(17:212992, 13:16384, 16384:1) = aten::add(%269, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%264, %270), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %272 : Tensor = prim::GetAttr[name="bias"](%190)
  %273 : Tensor = prim::GetAttr[name="weight"](%190)
  %274 : Float(16384:1, 4096:16384) = aten::t(%273), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %274), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %272, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %257, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %278 : Tensor = prim::GetAttr[name="bias"](%189)
  %279 : Tensor = prim::GetAttr[name="weight"](%189)
  %280 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %280, %279, %278, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %282 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %258)
  %283 : Float(17:53248, 13:4096, 4096:1), %284 : Float(4096:1) = prim::TupleUnpack(%282)
  %285 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%283, %284)
  %286 : Float(17:53248, 13:4096, 4096:1), %287 : Float(4096:1) = prim::TupleUnpack(%285)
  %288 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %289 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%288)
  %290 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%289)
  %291 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%289)
  %292 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%289)
  %293 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%289)
  %294 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%293)
  %295 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%293)
  %296 : Tensor = prim::GetAttr[name="weight"](%295)
  %297 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%293)
  %298 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%293)
  %299 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%293)
  %300 : Tensor = prim::GetAttr[name="bias"](%299)
  %301 : Tensor = prim::GetAttr[name="weight"](%299)
  %302 : Float(4096:1, 4096:4096) = aten::t(%301), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%286, %302), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.12, %300, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %305 : Tensor = prim::GetAttr[name="bias"](%298)
  %306 : Tensor = prim::GetAttr[name="weight"](%298)
  %307 : Float(4096:1, 4096:4096) = aten::t(%306), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%286, %307), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.13, %305, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %310 : Tensor = prim::GetAttr[name="bias"](%297)
  %311 : Tensor = prim::GetAttr[name="weight"](%297)
  %312 : Float(4096:1, 4096:4096) = aten::t(%311), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%286, %312), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.14, %310, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %315 : int = aten::size(%x.15, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %316 : int = aten::size(%x.15, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %317 : int[] = prim::ListConstruct(%315, %316, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %317), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %319 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %319), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %321 : int = aten::size(%x.17, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %322 : int = aten::size(%x.17, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %323 : int[] = prim::ListConstruct(%321, %322, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %323), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %325 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %325), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %327 : int = aten::size(%x.19, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %328 : int = aten::size(%x.19, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %329 : int[] = prim::ListConstruct(%327, %328, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %329), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %331 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %331), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %333 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %333), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %340 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %341 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %340), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %342 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%341, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %343 : Float(4096:1, 4096:4096) = aten::t(%296), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %344 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %345 : Float(64:64, 64:1, 4096:4096) = aten::view(%343, %344), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %346 : Float(64:64, 64:1, 4096:4096) = aten::to(%345, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%287, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %348 : Tensor[] = prim::ListConstruct(%342, %346), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %349 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %348), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(17:53248, 13:4096, 4096:1) = aten::add(%349, %b.3, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:53248, 13:4096, 4096:1) = aten::add(%286, %projected_context_layer.3, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %353 : Tensor = prim::GetAttr[name="bias"](%294)
  %354 : Tensor = prim::GetAttr[name="weight"](%294)
  %355 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %355, %354, %353, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %357 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %358 : Float(17:53248, 13:4096, 4096:1), %359 : Float(4096:1) = prim::TupleUnpack(%357)
  %360 : Tensor = prim::GetAttr[name="bias"](%292)
  %361 : Tensor = prim::GetAttr[name="weight"](%292)
  %362 : Float(4096:1, 16384:4096) = aten::t(%361), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%358, %362), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.15, %360, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %365 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.21, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %366 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.21, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %367 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%366, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %368 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.21, %367, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %369 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%368, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %370 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%369), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %371 : Float(17:212992, 13:16384, 16384:1) = aten::add(%370, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%365, %371), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %373 : Tensor = prim::GetAttr[name="bias"](%291)
  %374 : Tensor = prim::GetAttr[name="weight"](%291)
  %375 : Float(16384:1, 4096:16384) = aten::t(%374), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %375), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.16, %373, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %358, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %379 : Tensor = prim::GetAttr[name="bias"](%290)
  %380 : Tensor = prim::GetAttr[name="weight"](%290)
  %381 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %381, %380, %379, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %383 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %359)
  %384 : Float(17:53248, 13:4096, 4096:1), %385 : Float(4096:1) = prim::TupleUnpack(%383)
  %386 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%384, %385)
  %387 : Float(17:53248, 13:4096, 4096:1), %388 : Float(4096:1) = prim::TupleUnpack(%386)
  %389 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %390 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%389)
  %391 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%390)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%390)
  %393 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%390)
  %394 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%390)
  %395 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%394)
  %396 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%394)
  %397 : Tensor = prim::GetAttr[name="weight"](%396)
  %398 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%394)
  %399 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%394)
  %400 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%394)
  %401 : Tensor = prim::GetAttr[name="bias"](%400)
  %402 : Tensor = prim::GetAttr[name="weight"](%400)
  %403 : Float(4096:1, 4096:4096) = aten::t(%402), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%387, %403), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %401, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %406 : Tensor = prim::GetAttr[name="bias"](%399)
  %407 : Tensor = prim::GetAttr[name="weight"](%399)
  %408 : Float(4096:1, 4096:4096) = aten::t(%407), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%387, %408), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.18, %406, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %411 : Tensor = prim::GetAttr[name="bias"](%398)
  %412 : Tensor = prim::GetAttr[name="weight"](%398)
  %413 : Float(4096:1, 4096:4096) = aten::t(%412), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%387, %413), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.19, %411, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %416 : int = aten::size(%x.22, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %417 : int = aten::size(%x.22, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %418 : int[] = prim::ListConstruct(%416, %417, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %418), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %420 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %420), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %422 : int = aten::size(%x.24, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %423 : int = aten::size(%x.24, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %424 : int[] = prim::ListConstruct(%422, %423, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %424), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %426 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %426), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %428 : int = aten::size(%x.26, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %429 : int = aten::size(%x.26, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %430 : int[] = prim::ListConstruct(%428, %429, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %430), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %432 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %432), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %434 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %434), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %441 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %442 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %441), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %443 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%442, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %444 : Float(4096:1, 4096:4096) = aten::t(%397), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %445 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %446 : Float(64:64, 64:1, 4096:4096) = aten::view(%444, %445), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %447 : Float(64:64, 64:1, 4096:4096) = aten::to(%446, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%388, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %449 : Tensor[] = prim::ListConstruct(%443, %447), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %450 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %449), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add(%450, %b.4, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::add(%387, %projected_context_layer.4, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %454 : Tensor = prim::GetAttr[name="bias"](%395)
  %455 : Tensor = prim::GetAttr[name="weight"](%395)
  %456 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %456, %455, %454, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %458 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %459 : Float(17:53248, 13:4096, 4096:1), %460 : Float(4096:1) = prim::TupleUnpack(%458)
  %461 : Tensor = prim::GetAttr[name="bias"](%393)
  %462 : Tensor = prim::GetAttr[name="weight"](%393)
  %463 : Float(4096:1, 16384:4096) = aten::t(%462), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%459, %463), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.20, %461, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %466 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.28, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %467 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.28, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %468 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%467, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %469 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.28, %468, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %470 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%469, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %471 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%470), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %472 : Float(17:212992, 13:16384, 16384:1) = aten::add(%471, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%466, %472), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %474 : Tensor = prim::GetAttr[name="bias"](%392)
  %475 : Tensor = prim::GetAttr[name="weight"](%392)
  %476 : Float(16384:1, 4096:16384) = aten::t(%475), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %476), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.21, %474, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %459, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %480 : Tensor = prim::GetAttr[name="bias"](%391)
  %481 : Tensor = prim::GetAttr[name="weight"](%391)
  %482 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %482, %481, %480, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %484 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %460)
  %485 : Float(17:53248, 13:4096, 4096:1), %486 : Float(4096:1) = prim::TupleUnpack(%484)
  %487 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%485, %486)
  %488 : Float(17:53248, 13:4096, 4096:1), %489 : Float(4096:1) = prim::TupleUnpack(%487)
  %490 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %491 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%490)
  %492 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%491)
  %493 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%491)
  %494 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%491)
  %495 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%491)
  %496 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%495)
  %497 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%495)
  %498 : Tensor = prim::GetAttr[name="weight"](%497)
  %499 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%495)
  %500 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%495)
  %501 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%495)
  %502 : Tensor = prim::GetAttr[name="bias"](%501)
  %503 : Tensor = prim::GetAttr[name="weight"](%501)
  %504 : Float(4096:1, 4096:4096) = aten::t(%503), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%488, %504), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.22, %502, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %507 : Tensor = prim::GetAttr[name="bias"](%500)
  %508 : Tensor = prim::GetAttr[name="weight"](%500)
  %509 : Float(4096:1, 4096:4096) = aten::t(%508), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%488, %509), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %507, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %512 : Tensor = prim::GetAttr[name="bias"](%499)
  %513 : Tensor = prim::GetAttr[name="weight"](%499)
  %514 : Float(4096:1, 4096:4096) = aten::t(%513), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%488, %514), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.24, %512, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %517 : int = aten::size(%x.29, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %518 : int = aten::size(%x.29, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %519 : int[] = prim::ListConstruct(%517, %518, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %519), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %521 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %521), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %523 : int = aten::size(%x.31, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %524 : int = aten::size(%x.31, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %525 : int[] = prim::ListConstruct(%523, %524, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %525), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %527 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %527), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %529 : int = aten::size(%x.33, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %530 : int = aten::size(%x.33, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %531 : int[] = prim::ListConstruct(%529, %530, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %531), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %533 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %533), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %535 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %535), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %542 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %543 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %542), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %544 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%543, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %545 : Float(4096:1, 4096:4096) = aten::t(%498), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %546 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %547 : Float(64:64, 64:1, 4096:4096) = aten::view(%545, %546), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %548 : Float(64:64, 64:1, 4096:4096) = aten::to(%547, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%489, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %550 : Tensor[] = prim::ListConstruct(%544, %548), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %551 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %550), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(17:53248, 13:4096, 4096:1) = aten::add(%551, %b.5, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:53248, 13:4096, 4096:1) = aten::add(%488, %projected_context_layer.5, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %555 : Tensor = prim::GetAttr[name="bias"](%496)
  %556 : Tensor = prim::GetAttr[name="weight"](%496)
  %557 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %557, %556, %555, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %559 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %560 : Float(17:53248, 13:4096, 4096:1), %561 : Float(4096:1) = prim::TupleUnpack(%559)
  %562 : Tensor = prim::GetAttr[name="bias"](%494)
  %563 : Tensor = prim::GetAttr[name="weight"](%494)
  %564 : Float(4096:1, 16384:4096) = aten::t(%563), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%560, %564), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.25, %562, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %567 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.35, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %568 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.35, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %569 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%568, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %570 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.35, %569, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %571 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%570, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %572 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%571), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %573 : Float(17:212992, 13:16384, 16384:1) = aten::add(%572, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%567, %573), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %575 : Tensor = prim::GetAttr[name="bias"](%493)
  %576 : Tensor = prim::GetAttr[name="weight"](%493)
  %577 : Float(16384:1, 4096:16384) = aten::t(%576), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %577), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.26, %575, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %560, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %581 : Tensor = prim::GetAttr[name="bias"](%492)
  %582 : Tensor = prim::GetAttr[name="weight"](%492)
  %583 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %583, %582, %581, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %585 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %561)
  %586 : Float(17:53248, 13:4096, 4096:1), %587 : Float(4096:1) = prim::TupleUnpack(%585)
  %588 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%586, %587)
  %589 : Float(17:53248, 13:4096, 4096:1), %590 : Float(4096:1) = prim::TupleUnpack(%588)
  %591 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %592 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%591)
  %593 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%592)
  %594 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%592)
  %595 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%592)
  %596 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%592)
  %597 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%596)
  %598 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%596)
  %599 : Tensor = prim::GetAttr[name="weight"](%598)
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%596)
  %601 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%596)
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%596)
  %603 : Tensor = prim::GetAttr[name="bias"](%602)
  %604 : Tensor = prim::GetAttr[name="weight"](%602)
  %605 : Float(4096:1, 4096:4096) = aten::t(%604), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%589, %605), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.27, %603, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %608 : Tensor = prim::GetAttr[name="bias"](%601)
  %609 : Tensor = prim::GetAttr[name="weight"](%601)
  %610 : Float(4096:1, 4096:4096) = aten::t(%609), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%589, %610), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.28, %608, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %613 : Tensor = prim::GetAttr[name="bias"](%600)
  %614 : Tensor = prim::GetAttr[name="weight"](%600)
  %615 : Float(4096:1, 4096:4096) = aten::t(%614), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%589, %615), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %613, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %618 : int = aten::size(%x.36, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %619 : int = aten::size(%x.36, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %620 : int[] = prim::ListConstruct(%618, %619, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %620), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %622 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %622), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %624 : int = aten::size(%x.38, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %625 : int = aten::size(%x.38, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %626 : int[] = prim::ListConstruct(%624, %625, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %626), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %628 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %628), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %630 : int = aten::size(%x.40, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %631 : int = aten::size(%x.40, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %632 : int[] = prim::ListConstruct(%630, %631, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %632), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %634 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %634), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %636 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %636), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %643 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %644 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %643), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %645 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%644, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %646 : Float(4096:1, 4096:4096) = aten::t(%599), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %647 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %648 : Float(64:64, 64:1, 4096:4096) = aten::view(%646, %647), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %649 : Float(64:64, 64:1, 4096:4096) = aten::to(%648, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%590, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %651 : Tensor[] = prim::ListConstruct(%645, %649), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %652 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %651), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(17:53248, 13:4096, 4096:1) = aten::add(%652, %b.6, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:53248, 13:4096, 4096:1) = aten::add(%589, %projected_context_layer.6, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %656 : Tensor = prim::GetAttr[name="bias"](%597)
  %657 : Tensor = prim::GetAttr[name="weight"](%597)
  %658 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %658, %657, %656, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %660 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %661 : Float(17:53248, 13:4096, 4096:1), %662 : Float(4096:1) = prim::TupleUnpack(%660)
  %663 : Tensor = prim::GetAttr[name="bias"](%595)
  %664 : Tensor = prim::GetAttr[name="weight"](%595)
  %665 : Float(4096:1, 16384:4096) = aten::t(%664), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%661, %665), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.30, %663, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %668 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.42, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %669 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.42, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %670 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%669, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %671 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.42, %670, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %672 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%671, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %673 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%672), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %674 : Float(17:212992, 13:16384, 16384:1) = aten::add(%673, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%668, %674), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %676 : Tensor = prim::GetAttr[name="bias"](%594)
  %677 : Tensor = prim::GetAttr[name="weight"](%594)
  %678 : Float(16384:1, 4096:16384) = aten::t(%677), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %678), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.31, %676, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %661, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %682 : Tensor = prim::GetAttr[name="bias"](%593)
  %683 : Tensor = prim::GetAttr[name="weight"](%593)
  %684 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %684, %683, %682, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %686 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %662)
  %687 : Float(17:53248, 13:4096, 4096:1), %688 : Float(4096:1) = prim::TupleUnpack(%686)
  %689 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%687, %688)
  %690 : Float(17:53248, 13:4096, 4096:1), %691 : Float(4096:1) = prim::TupleUnpack(%689)
  %692 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %693 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%692)
  %694 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%693)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%693)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%693)
  %697 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%693)
  %698 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%697)
  %699 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%697)
  %700 : Tensor = prim::GetAttr[name="weight"](%699)
  %701 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%697)
  %702 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%697)
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%697)
  %704 : Tensor = prim::GetAttr[name="bias"](%703)
  %705 : Tensor = prim::GetAttr[name="weight"](%703)
  %706 : Float(4096:1, 4096:4096) = aten::t(%705), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%690, %706), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.32, %704, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %709 : Tensor = prim::GetAttr[name="bias"](%702)
  %710 : Tensor = prim::GetAttr[name="weight"](%702)
  %711 : Float(4096:1, 4096:4096) = aten::t(%710), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%690, %711), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.33, %709, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %714 : Tensor = prim::GetAttr[name="bias"](%701)
  %715 : Tensor = prim::GetAttr[name="weight"](%701)
  %716 : Float(4096:1, 4096:4096) = aten::t(%715), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%690, %716), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.34, %714, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %719 : int = aten::size(%x.43, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %720 : int = aten::size(%x.43, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %721 : int[] = prim::ListConstruct(%719, %720, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %721), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %723 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %723), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %725 : int = aten::size(%x.45, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %726 : int = aten::size(%x.45, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %727 : int[] = prim::ListConstruct(%725, %726, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %727), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %729 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %729), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %731 : int = aten::size(%x.47, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %732 : int = aten::size(%x.47, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %733 : int[] = prim::ListConstruct(%731, %732, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %733), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %735 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %735), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %737 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %737), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %744 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %745 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %744), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %746 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%745, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %747 : Float(4096:1, 4096:4096) = aten::t(%700), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %748 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %749 : Float(64:64, 64:1, 4096:4096) = aten::view(%747, %748), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %750 : Float(64:64, 64:1, 4096:4096) = aten::to(%749, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%691, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %752 : Tensor[] = prim::ListConstruct(%746, %750), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %753 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %752), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::add(%753, %b.7, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(17:53248, 13:4096, 4096:1) = aten::add(%690, %projected_context_layer.7, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %757 : Tensor = prim::GetAttr[name="bias"](%698)
  %758 : Tensor = prim::GetAttr[name="weight"](%698)
  %759 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %759, %758, %757, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %761 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %762 : Float(17:53248, 13:4096, 4096:1), %763 : Float(4096:1) = prim::TupleUnpack(%761)
  %764 : Tensor = prim::GetAttr[name="bias"](%696)
  %765 : Tensor = prim::GetAttr[name="weight"](%696)
  %766 : Float(4096:1, 16384:4096) = aten::t(%765), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%762, %766), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.35, %764, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %769 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.49, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %770 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.49, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %771 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%770, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %772 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.49, %771, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %773 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%772, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %774 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%773), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %775 : Float(17:212992, 13:16384, 16384:1) = aten::add(%774, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%769, %775), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %777 : Tensor = prim::GetAttr[name="bias"](%695)
  %778 : Tensor = prim::GetAttr[name="weight"](%695)
  %779 : Float(16384:1, 4096:16384) = aten::t(%778), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %779), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.36, %777, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %762, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %783 : Tensor = prim::GetAttr[name="bias"](%694)
  %784 : Tensor = prim::GetAttr[name="weight"](%694)
  %785 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %785, %784, %783, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %787 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %763)
  %788 : Float(17:53248, 13:4096, 4096:1), %789 : Float(4096:1) = prim::TupleUnpack(%787)
  %790 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%788, %789)
  %791 : Float(17:53248, 13:4096, 4096:1), %792 : Float(4096:1) = prim::TupleUnpack(%790)
  %793 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %794 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%793)
  %795 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%794)
  %796 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%794)
  %797 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%794)
  %798 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%794)
  %799 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%798)
  %800 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%798)
  %801 : Tensor = prim::GetAttr[name="weight"](%800)
  %802 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%798)
  %803 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%798)
  %804 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%798)
  %805 : Tensor = prim::GetAttr[name="bias"](%804)
  %806 : Tensor = prim::GetAttr[name="weight"](%804)
  %807 : Float(4096:1, 4096:4096) = aten::t(%806), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%791, %807), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.37, %805, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %810 : Tensor = prim::GetAttr[name="bias"](%803)
  %811 : Tensor = prim::GetAttr[name="weight"](%803)
  %812 : Float(4096:1, 4096:4096) = aten::t(%811), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%791, %812), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.38, %810, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %815 : Tensor = prim::GetAttr[name="bias"](%802)
  %816 : Tensor = prim::GetAttr[name="weight"](%802)
  %817 : Float(4096:1, 4096:4096) = aten::t(%816), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%791, %817), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.39, %815, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %820 : int = aten::size(%x.50, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %821 : int = aten::size(%x.50, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %822 : int[] = prim::ListConstruct(%820, %821, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %822), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %824 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %824), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %826 : int = aten::size(%x.52, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %827 : int = aten::size(%x.52, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %828 : int[] = prim::ListConstruct(%826, %827, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %828), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %830 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %830), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %832 : int = aten::size(%x.54, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %833 : int = aten::size(%x.54, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %834 : int[] = prim::ListConstruct(%832, %833, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %834), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %836 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %836), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %838 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %838), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %845 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %846 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %845), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %847 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%846, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %848 : Float(4096:1, 4096:4096) = aten::t(%801), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %849 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %850 : Float(64:64, 64:1, 4096:4096) = aten::view(%848, %849), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %851 : Float(64:64, 64:1, 4096:4096) = aten::to(%850, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%792, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %853 : Tensor[] = prim::ListConstruct(%847, %851), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %854 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %853), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(17:53248, 13:4096, 4096:1) = aten::add(%854, %b.8, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:53248, 13:4096, 4096:1) = aten::add(%791, %projected_context_layer.8, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %858 : Tensor = prim::GetAttr[name="bias"](%799)
  %859 : Tensor = prim::GetAttr[name="weight"](%799)
  %860 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %860, %859, %858, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %862 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %863 : Float(17:53248, 13:4096, 4096:1), %864 : Float(4096:1) = prim::TupleUnpack(%862)
  %865 : Tensor = prim::GetAttr[name="bias"](%797)
  %866 : Tensor = prim::GetAttr[name="weight"](%797)
  %867 : Float(4096:1, 16384:4096) = aten::t(%866), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%863, %867), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.40, %865, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %870 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.56, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %871 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.56, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %872 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%871, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %873 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.56, %872, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %874 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%873, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %875 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%874), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %876 : Float(17:212992, 13:16384, 16384:1) = aten::add(%875, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%870, %876), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %878 : Tensor = prim::GetAttr[name="bias"](%796)
  %879 : Tensor = prim::GetAttr[name="weight"](%796)
  %880 : Float(16384:1, 4096:16384) = aten::t(%879), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %880), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %878, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %863, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %884 : Tensor = prim::GetAttr[name="bias"](%795)
  %885 : Tensor = prim::GetAttr[name="weight"](%795)
  %886 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %886, %885, %884, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %888 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %864)
  %889 : Float(17:53248, 13:4096, 4096:1), %890 : Float(4096:1) = prim::TupleUnpack(%888)
  %891 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%889, %890)
  %892 : Float(17:53248, 13:4096, 4096:1), %893 : Float(4096:1) = prim::TupleUnpack(%891)
  %894 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %895 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%894)
  %896 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%895)
  %897 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%895)
  %898 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%895)
  %899 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%895)
  %900 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%899)
  %901 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%899)
  %902 : Tensor = prim::GetAttr[name="weight"](%901)
  %903 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%899)
  %904 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%899)
  %905 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%899)
  %906 : Tensor = prim::GetAttr[name="bias"](%905)
  %907 : Tensor = prim::GetAttr[name="weight"](%905)
  %908 : Float(4096:1, 4096:4096) = aten::t(%907), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%892, %908), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.42, %906, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %911 : Tensor = prim::GetAttr[name="bias"](%904)
  %912 : Tensor = prim::GetAttr[name="weight"](%904)
  %913 : Float(4096:1, 4096:4096) = aten::t(%912), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%892, %913), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.43, %911, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %916 : Tensor = prim::GetAttr[name="bias"](%903)
  %917 : Tensor = prim::GetAttr[name="weight"](%903)
  %918 : Float(4096:1, 4096:4096) = aten::t(%917), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%892, %918), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.44, %916, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %921 : int = aten::size(%x.57, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %922 : int = aten::size(%x.57, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %923 : int[] = prim::ListConstruct(%921, %922, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %923), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %925 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %925), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %927 : int = aten::size(%x.59, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %928 : int = aten::size(%x.59, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %929 : int[] = prim::ListConstruct(%927, %928, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %929), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %931 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %931), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %933 : int = aten::size(%x.61, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %934 : int = aten::size(%x.61, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %935 : int[] = prim::ListConstruct(%933, %934, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %935), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %937 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %937), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %939 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %939), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %946 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %947 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %946), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %948 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%947, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %949 : Float(4096:1, 4096:4096) = aten::t(%902), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %950 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %951 : Float(64:64, 64:1, 4096:4096) = aten::view(%949, %950), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %952 : Float(64:64, 64:1, 4096:4096) = aten::to(%951, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%893, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %954 : Tensor[] = prim::ListConstruct(%948, %952), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %955 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %954), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(17:53248, 13:4096, 4096:1) = aten::add(%955, %b.9, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:53248, 13:4096, 4096:1) = aten::add(%892, %projected_context_layer.9, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %959 : Tensor = prim::GetAttr[name="bias"](%900)
  %960 : Tensor = prim::GetAttr[name="weight"](%900)
  %961 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %961, %960, %959, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %963 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %964 : Float(17:53248, 13:4096, 4096:1), %965 : Float(4096:1) = prim::TupleUnpack(%963)
  %966 : Tensor = prim::GetAttr[name="bias"](%898)
  %967 : Tensor = prim::GetAttr[name="weight"](%898)
  %968 : Float(4096:1, 16384:4096) = aten::t(%967), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%964, %968), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.45, %966, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %971 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.63, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %972 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.63, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %973 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%972, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %974 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.63, %973, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %975 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%974, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %976 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%975), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %977 : Float(17:212992, 13:16384, 16384:1) = aten::add(%976, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%971, %977), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %979 : Tensor = prim::GetAttr[name="bias"](%897)
  %980 : Tensor = prim::GetAttr[name="weight"](%897)
  %981 : Float(16384:1, 4096:16384) = aten::t(%980), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %981), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.46, %979, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %964, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %985 : Tensor = prim::GetAttr[name="bias"](%896)
  %986 : Tensor = prim::GetAttr[name="weight"](%896)
  %987 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %987, %986, %985, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %989 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %965)
  %990 : Float(17:53248, 13:4096, 4096:1), %991 : Float(4096:1) = prim::TupleUnpack(%989)
  %992 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%990, %991)
  %993 : Float(17:53248, 13:4096, 4096:1), %994 : Float(4096:1) = prim::TupleUnpack(%992)
  %995 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %996 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%995)
  %997 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%996)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%996)
  %999 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%996)
  %1000 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%996)
  %1001 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%1000)
  %1002 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%1000)
  %1003 : Tensor = prim::GetAttr[name="weight"](%1002)
  %1004 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%1000)
  %1005 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%1000)
  %1006 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%1000)
  %1007 : Tensor = prim::GetAttr[name="bias"](%1006)
  %1008 : Tensor = prim::GetAttr[name="weight"](%1006)
  %1009 : Float(4096:1, 4096:4096) = aten::t(%1008), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%993, %1009), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %1007, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1012 : Tensor = prim::GetAttr[name="bias"](%1005)
  %1013 : Tensor = prim::GetAttr[name="weight"](%1005)
  %1014 : Float(4096:1, 4096:4096) = aten::t(%1013), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%993, %1014), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1012, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1017 : Tensor = prim::GetAttr[name="bias"](%1004)
  %1018 : Tensor = prim::GetAttr[name="weight"](%1004)
  %1019 : Float(4096:1, 4096:4096) = aten::t(%1018), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%993, %1019), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1017, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1022 : int = aten::size(%x.64, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1023 : int = aten::size(%x.64, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1024 : int[] = prim::ListConstruct(%1022, %1023, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1024), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1026 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1026), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1028 : int = aten::size(%x.66, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1029 : int = aten::size(%x.66, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1030 : int[] = prim::ListConstruct(%1028, %1029, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1030), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1032 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1032), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1034 : int = aten::size(%x.68, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1035 : int = aten::size(%x.68, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1036 : int[] = prim::ListConstruct(%1034, %1035, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1036), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1038 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1038), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1040 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1040), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1047 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1048 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1047), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1049 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1048, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1050 : Float(4096:1, 4096:4096) = aten::t(%1003), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1051 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1052 : Float(64:64, 64:1, 4096:4096) = aten::view(%1050, %1051), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1053 : Float(64:64, 64:1, 4096:4096) = aten::to(%1052, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%994, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1055 : Tensor[] = prim::ListConstruct(%1049, %1053), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1056 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %1055), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1056, %b.10, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:53248, 13:4096, 4096:1) = aten::add(%993, %projected_context_layer.10, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1060 : Tensor = prim::GetAttr[name="bias"](%1001)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1001)
  %1062 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1062, %1061, %1060, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1064 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1065 : Float(17:53248, 13:4096, 4096:1), %1066 : Float(4096:1) = prim::TupleUnpack(%1064)
  %1067 : Tensor = prim::GetAttr[name="bias"](%999)
  %1068 : Tensor = prim::GetAttr[name="weight"](%999)
  %1069 : Float(4096:1, 16384:4096) = aten::t(%1068), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1065, %1069), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1067, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1072 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.70, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1073 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.70, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1074 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1073, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1075 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.70, %1074, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1076 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1075, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1077 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1076), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1078 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1077, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1072, %1078), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%998)
  %1081 : Tensor = prim::GetAttr[name="weight"](%998)
  %1082 : Float(16384:1, 4096:16384) = aten::t(%1081), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1082), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1080, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1065, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1086 : Tensor = prim::GetAttr[name="bias"](%997)
  %1087 : Tensor = prim::GetAttr[name="weight"](%997)
  %1088 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1088, %1087, %1086, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1090 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1066)
  %1091 : Float(17:53248, 13:4096, 4096:1), %1092 : Float(4096:1) = prim::TupleUnpack(%1090)
  %1093 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1091, %1092)
  %1094 : Float(17:53248, 13:4096, 4096:1), %1095 : Float(4096:1) = prim::TupleUnpack(%1093)
  %1096 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %1097 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%1096)
  %1098 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1097)
  %1099 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%1097)
  %1100 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%1097)
  %1101 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%1097)
  %1102 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%1101)
  %1103 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%1101)
  %1104 : Tensor = prim::GetAttr[name="weight"](%1103)
  %1105 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%1101)
  %1106 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%1101)
  %1107 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%1101)
  %1108 : Tensor = prim::GetAttr[name="bias"](%1107)
  %1109 : Tensor = prim::GetAttr[name="weight"](%1107)
  %1110 : Float(4096:1, 4096:4096) = aten::t(%1109), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1094, %1110), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1108, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1113 : Tensor = prim::GetAttr[name="bias"](%1106)
  %1114 : Tensor = prim::GetAttr[name="weight"](%1106)
  %1115 : Float(4096:1, 4096:4096) = aten::t(%1114), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1094, %1115), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1113, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1118 : Tensor = prim::GetAttr[name="bias"](%1105)
  %1119 : Tensor = prim::GetAttr[name="weight"](%1105)
  %1120 : Float(4096:1, 4096:4096) = aten::t(%1119), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1094, %1120), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1118, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1123 : int = aten::size(%x.71, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1124 : int = aten::size(%x.71, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1125 : int[] = prim::ListConstruct(%1123, %1124, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1125), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1127 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1127), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1129 : int = aten::size(%x.73, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1130 : int = aten::size(%x.73, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1131 : int[] = prim::ListConstruct(%1129, %1130, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1131), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1133 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1133), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1135 : int = aten::size(%x.75, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1136 : int = aten::size(%x.75, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1137 : int[] = prim::ListConstruct(%1135, %1136, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1137), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1139 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1139), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1141 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1141), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1148 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1149 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1148), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1150 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1149, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1151 : Float(4096:1, 4096:4096) = aten::t(%1104), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1152 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1153 : Float(64:64, 64:1, 4096:4096) = aten::view(%1151, %1152), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1154 : Float(64:64, 64:1, 4096:4096) = aten::to(%1153, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1095, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1156 : Tensor[] = prim::ListConstruct(%1150, %1154), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1157 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %1156), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1157, %b.11, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1094, %projected_context_layer.11, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1161 : Tensor = prim::GetAttr[name="bias"](%1102)
  %1162 : Tensor = prim::GetAttr[name="weight"](%1102)
  %1163 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1163, %1162, %1161, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1165 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1166 : Float(17:53248, 13:4096, 4096:1), %1167 : Float(4096:1) = prim::TupleUnpack(%1165)
  %1168 : Tensor = prim::GetAttr[name="bias"](%1100)
  %1169 : Tensor = prim::GetAttr[name="weight"](%1100)
  %1170 : Float(4096:1, 16384:4096) = aten::t(%1169), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1166, %1170), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1168, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1173 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.77, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1174 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.77, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1175 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1174, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1176 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.77, %1175, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1177 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1176, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1178 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1177), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1179 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1178, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1173, %1179), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1181 : Tensor = prim::GetAttr[name="bias"](%1099)
  %1182 : Tensor = prim::GetAttr[name="weight"](%1099)
  %1183 : Float(16384:1, 4096:16384) = aten::t(%1182), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1183), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1181, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1166, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1187 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1189 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1189, %1188, %1187, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1191 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1167)
  %1192 : Float(17:53248, 13:4096, 4096:1), %1193 : Float(4096:1) = prim::TupleUnpack(%1191)
  %1194 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1192, %1193)
  %1195 : Float(17:53248, 13:4096, 4096:1), %1196 : Float(4096:1) = prim::TupleUnpack(%1194)
  %1197 : __torch__.torch.nn.modules.container.___torch_mangle_193.ModuleList = prim::GetAttr[name="albert_layers"](%77)
  %1198 : __torch__.transformers.modeling_albert.___torch_mangle_192.AlbertLayer = prim::GetAttr[name="0"](%1197)
  %1199 : __torch__.torch.nn.modules.normalization.___torch_mangle_180.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1198)
  %1200 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="ffn_output"](%1198)
  %1201 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="ffn"](%1198)
  %1202 : __torch__.transformers.modeling_albert.___torch_mangle_188.AlbertAttention = prim::GetAttr[name="attention"](%1198)
  %1203 : __torch__.torch.nn.modules.normalization.___torch_mangle_187.LayerNorm = prim::GetAttr[name="LayerNorm"](%1202)
  %1204 : __torch__.torch.nn.modules.linear.___torch_mangle_186.Linear = prim::GetAttr[name="dense"](%1202)
  %1205 : Tensor = prim::GetAttr[name="weight"](%1204)
  %1206 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="value"](%1202)
  %1207 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="key"](%1202)
  %1208 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="query"](%1202)
  %1209 : Tensor = prim::GetAttr[name="bias"](%1208)
  %1210 : Tensor = prim::GetAttr[name="weight"](%1208)
  %1211 : Float(4096:1, 4096:4096) = aten::t(%1210), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1195, %1211), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1209, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1214 : Tensor = prim::GetAttr[name="bias"](%1207)
  %1215 : Tensor = prim::GetAttr[name="weight"](%1207)
  %1216 : Float(4096:1, 4096:4096) = aten::t(%1215), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1195, %1216), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1214, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1219 : Tensor = prim::GetAttr[name="bias"](%1206)
  %1220 : Tensor = prim::GetAttr[name="weight"](%1206)
  %1221 : Float(4096:1, 4096:4096) = aten::t(%1220), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1195, %1221), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1219, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1224 : int = aten::size(%x.78, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1225 : int = aten::size(%x.78, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1226 : int[] = prim::ListConstruct(%1224, %1225, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1226), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1228 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1228), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1230 : int = aten::size(%x.80, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1231 : int = aten::size(%x.80, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1232 : int[] = prim::ListConstruct(%1230, %1231, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1232), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1234 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1234), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1236 : int = aten::size(%x.82, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1237 : int = aten::size(%x.82, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1238 : int[] = prim::ListConstruct(%1236, %1237, %22, %22), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1238), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1240 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1240), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1242 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %29, %20), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1242), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1249 : int[] = prim::ListConstruct(%43, %38, %42, %21), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1250 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1249), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1251 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1250, %43), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1252 : Float(4096:1, 4096:4096) = aten::t(%1205), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1253 : int[] = prim::ListConstruct(%22, %22, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1254 : Float(64:64, 64:1, 4096:4096) = aten::view(%1252, %1253), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1255 : Float(64:64, 64:1, 4096:4096) = aten::to(%1254, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1196, %37, %39, %39, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1257 : Tensor[] = prim::ListConstruct(%1251, %1255), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1258 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%17, %1257), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1258, %b, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %33, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1195, %projected_context_layer, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1262 : Tensor = prim::GetAttr[name="bias"](%1203)
  %1263 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1264 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1264, %1263, %1262, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1266 : Tensor = prim::GetAttr[name="bias"](%1201)
  %1267 : Tensor = prim::GetAttr[name="weight"](%1201)
  %1268 : Float(4096:1, 16384:4096) = aten::t(%1267), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1266, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1271 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1272 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x, %24), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1273 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1272, %25), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1274 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x, %1273, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1275 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1274, %26), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1276 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1275), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1277 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1276, %27, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1271, %1277), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1279 : Tensor = prim::GetAttr[name="bias"](%1200)
  %1280 : Tensor = prim::GetAttr[name="weight"](%1200)
  %1281 : Float(16384:1, 4096:16384) = aten::t(%1280), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.61 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1281), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.61, %1279, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %42), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1285 : Tensor = prim::GetAttr[name="bias"](%1199)
  %1286 : Tensor = prim::GetAttr[name="weight"](%1199)
  %1287 : int[] = prim::ListConstruct(%18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1287, %1286, %1285, %31, %30), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1289 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %1290 : Tensor = prim::GetAttr[name="bias"](%3)
  %1291 : Tensor = prim::GetAttr[name="weight"](%3)
  %1292 : Float(4096:1, 2:4096) = aten::t(%1291), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1292), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %1294 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1290, %1289), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%1294, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1192:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_albert.py:1192:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1193:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_albert.py:1193:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
