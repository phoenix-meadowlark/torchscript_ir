graph(%self.1 : __torch__.transformers.modeling_longformer.LongformerForMaskedLM,
      %input_ids.1 : Long(17:13, 13:1),
      %input.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_longformer.LongformerLMHead = prim::GetAttr[name="lm_head"](%self.1)
  %4 : __torch__.transformers.modeling_longformer.LongformerModel = prim::GetAttr[name="longformer"](%self.1)
  %8 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %9 : int = prim::Constant[value=16384](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %10 : int = prim::Constant[value=65536](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %11 : float = prim::Constant[value=0.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %12 : int = prim::Constant[value=17](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %13 : float = prim::Constant[value=-10000.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %14 : int = prim::Constant[value=-256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %15 : float = prim::Constant[value=-inf](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %16 : int = prim::Constant[value=-255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %17 : int = prim::Constant[value=255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %18 : int = prim::Constant[value=-257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %19 : int = prim::Constant[value=204](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %20 : int = prim::Constant[value=257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %21 : int = prim::Constant[value=513](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %22 : int = prim::Constant[value=256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %23 : int = prim::Constant[value=-2](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %24 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %25 : int = prim::Constant[value=13056](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %26 : int = prim::Constant[value=3342336](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %27 : Long() = prim::Constant[value={2}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %28 : Long() = prim::Constant[value={512}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %29 : Long() = prim::Constant[value={256}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %30 : int = prim::Constant[value=64](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %31 : int = prim::Constant[value=12](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %32 : Double() = prim::Constant[value={8}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %33 : Long() = prim::Constant[value={1}](), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %34 : int = prim::Constant[value=-1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %35 : bool = prim::Constant[value=1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %36 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %37 : int = prim::Constant[value=768](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %38 : float = prim::Constant[value=0.10000000000000001](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %39 : Double() = prim::Constant[value={-10000}](), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %40 : float = prim::Constant[value=1.](), scope: __module.longformer # torch/tensor.py:396:0
  %41 : None = prim::Constant(), scope: __module.longformer
  %42 : int = prim::Constant[value=6](), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %43 : int = prim::Constant[value=3](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %44 : int = prim::Constant[value=2](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %45 : int = prim::Constant[value=9223372036854775807](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %46 : int = prim::Constant[value=512](), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %47 : bool = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %48 : Device = prim::Constant[value="cpu"](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %49 : int = prim::Constant[value=4](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %50 : int = prim::Constant[value=1](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %51 : int = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %52 : __torch__.transformers.modeling_longformer.LongformerEncoder = prim::GetAttr[name="encoder"](%4)
  %53 : __torch__.transformers.modeling_longformer.LongformerEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %54 : int = aten::size(%input_ids.1, %51), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %55 : int = aten::size(%input_ids.1, %50), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %56 : int[] = prim::ListConstruct(%54, %55), scope: __module.longformer
  %input.2 : Long(17:13, 13:1) = aten::zeros(%56, %49, %51, %48, %47), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %58 : int = aten::size(%input_ids.1, %50), scope: __module.longformer # transformers/modeling_longformer.py:1136:0
  %seq_len.1 : Long() = prim::NumToTensor(%58), scope: __module.longformer
  %60 : Long() = aten::remainder(%seq_len.1, %46), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %61 : Long() = aten::rsub(%60, %46, %50), scope: __module.longformer # torch/tensor.py:396:0
  %padding_len : Long() = aten::remainder(%61, %46), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %63 : int = aten::Int(%padding_len), scope: __module.longformer
  %64 : int = aten::Int(%padding_len), scope: __module.longformer
  %65 : int = aten::Int(%padding_len), scope: __module.longformer
  %66 : int[] = prim::ListConstruct(%51, %65), scope: __module.longformer
  %input_ids : Long(17:512, 512:1) = aten::constant_pad_nd(%input_ids.1, %66, %50), scope: __module.longformer # torch/nn/functional.py:3552:0
  %68 : int[] = prim::ListConstruct(%51, %64), scope: __module.longformer
  %attention_mask.1 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.1, %68, %51), scope: __module.longformer # torch/nn/functional.py:3552:0
  %70 : int[] = prim::ListConstruct(%51, %63), scope: __module.longformer
  %input.4 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.2, %70, %51), scope: __module.longformer # torch/nn/functional.py:3552:0
  %72 : Long(17:512, 512:1) = aten::slice(%attention_mask.1, %51, %51, %45, %50), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %73 : Long(17:512, 1:512, 512:1) = aten::unsqueeze(%72, %50), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %74 : Long(17:512, 1:512, 1:512, 512:1) = aten::unsqueeze(%73, %44), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:512, 1:512, 1:512, 512:1) = aten::slice(%74, %43, %51, %45, %50), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %76 : Float(17:512, 1:512, 1:512, 512:1) = aten::to(%extended_attention_mask, %42, %47, %47, %41), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %77 : Float(17:512, 1:512, 1:512, 512:1) = aten::rsub(%76, %40, %50), scope: __module.longformer # torch/tensor.py:396:0
  %attention_mask : Float(17:512, 1:512, 1:512, 512:1) = aten::mul(%77, %39), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %79 : __torch__.torch.nn.modules.normalization.___torch_mangle_6134.LayerNorm = prim::GetAttr[name="LayerNorm"](%53)
  %80 : __torch__.torch.nn.modules.sparse.___torch_mangle_6133.Embedding = prim::GetAttr[name="token_type_embeddings"](%53)
  %81 : __torch__.torch.nn.modules.sparse.___torch_mangle_6132.Embedding = prim::GetAttr[name="position_embeddings"](%53)
  %82 : __torch__.torch.nn.modules.sparse.___torch_mangle_6131.Embedding = prim::GetAttr[name="word_embeddings"](%53)
  %83 : Bool(17:512, 512:1) = aten::ne(%input_ids, %50), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %mask : Int(17:512, 512:1) = aten::to(%83, %43, %47, %47, %41), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %85 : Long(17:512, 512:1) = aten::cumsum(%mask, %50, %41), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %86 : Int(17:512, 512:1) = aten::type_as(%85, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %incremental_indices : Int(17:512, 512:1) = aten::mul(%86, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %88 : Long(17:512, 512:1) = aten::to(%incremental_indices, %49, %47, %47, %41), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %89 : Long(17:512, 512:1) = aten::add(%88, %33, %50), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %input.3 : Long(17:512, 512:1) = aten::to(%89, %49, %51, %48, %47, %47, %47, %41), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:147:0
  %91 : Tensor = prim::GetAttr[name="weight"](%82)
  %inputs_embeds : Float(17:393216, 512:768, 768:1) = aten::embedding(%91, %input_ids, %50, %47, %47), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %93 : Tensor = prim::GetAttr[name="weight"](%81)
  %position_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%93, %input.3, %50, %47, %47), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %95 : Tensor = prim::GetAttr[name="weight"](%80)
  %token_type_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%95, %input.4, %34, %47, %47), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %97 : Float(17:393216, 512:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %50), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %input.5 : Float(17:393216, 512:768, 768:1) = aten::add(%97, %token_type_embeddings, %50), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %99 : Tensor = prim::GetAttr[name="bias"](%79)
  %100 : Tensor = prim::GetAttr[name="weight"](%79)
  %101 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm
  %input.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.5, %101, %100, %99, %36, %35), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %hidden_states.1 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.6, %38, %47), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %105 : __torch__.transformers.modeling_longformer.___torch_mangle_6357.LongformerLayer = prim::GetAttr[name="11"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %107 : __torch__.transformers.modeling_longformer.___torch_mangle_6338.LongformerLayer = prim::GetAttr[name="10"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %109 : __torch__.transformers.modeling_longformer.___torch_mangle_6319.LongformerLayer = prim::GetAttr[name="9"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %111 : __torch__.transformers.modeling_longformer.___torch_mangle_6300.LongformerLayer = prim::GetAttr[name="8"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %113 : __torch__.transformers.modeling_longformer.___torch_mangle_6281.LongformerLayer = prim::GetAttr[name="7"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %115 : __torch__.transformers.modeling_longformer.___torch_mangle_6262.LongformerLayer = prim::GetAttr[name="6"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %117 : __torch__.transformers.modeling_longformer.___torch_mangle_6243.LongformerLayer = prim::GetAttr[name="5"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %119 : __torch__.transformers.modeling_longformer.___torch_mangle_6224.LongformerLayer = prim::GetAttr[name="4"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %121 : __torch__.transformers.modeling_longformer.___torch_mangle_6205.LongformerLayer = prim::GetAttr[name="3"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %123 : __torch__.transformers.modeling_longformer.___torch_mangle_6186.LongformerLayer = prim::GetAttr[name="2"](%122)
  %124 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %125 : __torch__.transformers.modeling_longformer.___torch_mangle_6167.LongformerLayer = prim::GetAttr[name="1"](%124)
  %126 : __torch__.torch.nn.modules.container.___torch_mangle_6358.ModuleList = prim::GetAttr[name="layer"](%52)
  %127 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="0"](%126)
  %128 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%127)
  %129 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%127)
  %130 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%127)
  %131 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%130)
  %132 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%130)
  %133 : __torch__.torch.nn.modules.linear.___torch_mangle_6138.Linear = prim::GetAttr[name="value"](%132)
  %134 : __torch__.torch.nn.modules.linear.___torch_mangle_6137.Linear = prim::GetAttr[name="key"](%132)
  %135 : __torch__.torch.nn.modules.linear.___torch_mangle_6136.Linear = prim::GetAttr[name="query"](%132)
  %136 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %137 : Float(17:512, 512:1) = aten::squeeze(%136, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.1 : Bool(17:512, 512:1) = aten::lt(%137, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %input.7 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.1, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
  %140 : Tensor = prim::GetAttr[name="bias"](%135)
  %141 : Tensor = prim::GetAttr[name="weight"](%135)
  %142 : Float(768:1, 768:768) = aten::t(%141), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %142), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.1, %140, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %145 : Tensor = prim::GetAttr[name="bias"](%134)
  %146 : Tensor = prim::GetAttr[name="weight"](%134)
  %147 : Float(768:1, 768:768) = aten::t(%146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.2, %145, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %150 : Tensor = prim::GetAttr[name="bias"](%133)
  %151 : Tensor = prim::GetAttr[name="weight"](%133)
  %152 : Float(768:1, 768:768) = aten::t(%151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.3, %150, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %155 : int = aten::size(%input.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %156 : int = aten::size(%input.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %157 : int = aten::size(%input.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.1, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %159 : int[] = prim::ListConstruct(%155, %156, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %160 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.2, %159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %query.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%160, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %162 : int[] = prim::ListConstruct(%155, %156, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %163 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.1, %162), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %key.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%163, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %165 : int = aten::size(%query.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.2 : Long() = prim::NumToTensor(%165), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %167 : int = aten::size(%query.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.3 : Long() = prim::NumToTensor(%167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %169 : int = aten::size(%query.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.1 : Long() = prim::NumToTensor(%169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %171 : int = aten::size(%query.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %172 : Long() = aten::floor_divide(%seq_len.3, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.1 : Long() = aten::sub(%172, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %174 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.1, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %175 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %176 : int = aten::Int(%175), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %177 : int[] = prim::ListConstruct(%176, %167, %171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.2 : Float(204:64, 512:13056, 64:1) = aten::reshape(%174, %177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %179 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.1, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %180 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %181 : int = aten::Int(%180), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %182 : int[] = prim::ListConstruct(%181, %167, %171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.4 : Float(204:64, 512:13056, 64:1) = aten::reshape(%179, %182), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %184 : int = aten::size(%hidden_states.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %185 : int = aten::size(%hidden_states.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %186 : Long() = prim::NumToTensor(%185), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %187 : Long() = aten::floor_divide(%186, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %188 : int = aten::Int(%187), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %189 : int = aten::size(%hidden_states.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %190 : int[] = prim::ListConstruct(%184, %188, %46, %189), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.3 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.2, %190), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %192 : int = aten::size(%hidden_states.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %193 : int = aten::size(%hidden_states.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %194 : Long() = prim::NumToTensor(%193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %195 : int = aten::size(%hidden_states.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %196 : int = aten::size(%hidden_states.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %197 : Long() = aten::mul(%194, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %198 : Long() = aten::sub(%197, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %199 : int = aten::Int(%198), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %200 : int[] = prim::ListConstruct(%192, %199, %195, %196), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %201 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %202 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.3, %200, %201, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %203 : int = aten::size(%hidden_states.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %204 : int = aten::size(%hidden_states.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %205 : Long() = prim::NumToTensor(%204), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %206 : Long() = aten::floor_divide(%205, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %207 : int = aten::Int(%206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %208 : int = aten::size(%hidden_states.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %209 : int[] = prim::ListConstruct(%203, %207, %46, %208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.5 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.4, %209), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %211 : int = aten::size(%hidden_states.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %212 : int = aten::size(%hidden_states.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %213 : Long() = prim::NumToTensor(%212), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %214 : int = aten::size(%hidden_states.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %215 : int = aten::size(%hidden_states.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %216 : Long() = aten::mul(%213, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %217 : Long() = aten::sub(%216, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %218 : int = aten::Int(%217), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %219 : int[] = prim::ListConstruct(%211, %218, %214, %215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %220 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.5, %219, %220, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %222 : Tensor[] = prim::ListConstruct(%202, %221), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.8 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %222), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %224 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.1 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.8, %224, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %226 : int = aten::size(%hidden_states_padded.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %227 : int = aten::size(%hidden_states_padded.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %228 : int = aten::size(%hidden_states_padded.1, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %229 : int = aten::size(%hidden_states_padded.1, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %230 : int[] = prim::ListConstruct(%226, %227, %228, %229), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.1 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.1, %230), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %232 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %233 : int = aten::Int(%232), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %234 : Long() = aten::add(%chunks_count.1, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %235 : int = aten::Int(%234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %236 : int[] = prim::ListConstruct(%233, %235, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.1, %236, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %238 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %239 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%238, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %240 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%239, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %241 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%240, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %242 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %243 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%242, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %244 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%243, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %245 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%244, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %246 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %247 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%241, %246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %248 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%245, %247, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %249 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %250 : Float(204:262656, 512:513, 513:1) = aten::select(%249, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %251 : Float(204:262656, 256:513, 513:1) = aten::slice(%250, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %252 : Float(204:262656, 256:513, 257:1) = aten::slice(%251, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %253 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %254 : Float(204:262656, 256:513, 513:1) = aten::select(%253, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %255 : Float(204:262656, 256:513, 513:1) = aten::slice(%254, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %256 : Float(204:262656, 256:513, 257:1) = aten::slice(%255, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %257 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %258 : Float(204:262656, 256:513, 257:1) = aten::view(%252, %257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %259 : Float(204:262656, 256:513, 257:1) = aten::copy_(%256, %258, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %260 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %261 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%260, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %262 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%261, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %263 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%262, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %264 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %265 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%264, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %266 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%265, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %267 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%266, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %268 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %269 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%263, %268), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %270 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%267, %269, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %271 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %272 : Float(204:262656, 512:513, 513:1) = aten::select(%271, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %273 : Float(204:262656, 255:513, 513:1) = aten::slice(%272, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %274 : Float(204:262656, 255:513, 255:1) = aten::slice(%273, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %275 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %276 : Float(204:262656, 256:513, 513:1) = aten::select(%275, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %277 : Float(204:262656, 255:513, 513:1) = aten::slice(%276, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %278 : Float(204:262656, 255:513, 255:1) = aten::slice(%277, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %279 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %280 : Float(204:262656, 255:513, 255:1) = aten::view(%274, %279), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %281 : Float(204:262656, 255:513, 255:1) = aten::copy_(%278, %280, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %282 : int[] = prim::ListConstruct(%165, %169, %167, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %283 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.1, %282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%283, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %285 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %286 : Float(256:257, 257:1) = aten::ones(%285, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %287 : Float(256:257, 257:1) = aten::tril(%286, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %288 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.1 : Float(256:257, 257:1) = aten::flip(%287, %288), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %290 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %291 : Float(1:65792, 256:257, 257:1) = aten::slice(%290, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %292 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%291, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%292, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %294 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.1, %294), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %296 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %297 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%296, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %298 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%297, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%298, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %300 : int = aten::size(%beginning_input.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %301 : int = aten::size(%beginning_input.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %302 : int = aten::size(%beginning_input.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %303 : int = aten::size(%beginning_input.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %304 : int[] = prim::ListConstruct(%300, %301, %302, %303), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %305 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.1, %304, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %306 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%305, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %307 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.1, %306, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %308 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %309 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%308, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %310 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%309, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%310, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %312 : int = aten::size(%ending_input.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %313 : int = aten::size(%ending_input.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %314 : int = aten::size(%ending_input.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %315 : int = aten::size(%ending_input.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %316 : int[] = prim::ListConstruct(%312, %313, %314, %315), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %317 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.1, %316, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %318 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%317, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %319 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.1, %318, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %320 : Bool(17:512, 512:1) = aten::ne(%137, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %321 : Bool(17:512, 512:1) = aten::slice(%320, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %322 : Bool(17:512, 512:1) = aten::slice(%321, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %323 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%322, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.1 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%323, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %325 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.1, %query.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.1 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%325, %remove_from_windowed_attention_mask.1, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %327 : int = aten::size(%float_mask.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %328 : int = aten::size(%float_mask.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %329 : int = aten::size(%float_mask.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %330 : int = aten::size(%float_mask.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %331 : int[] = prim::ListConstruct(%327, %328, %329, %330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %query.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%331, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %333 : int = aten::size(%query.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.3 : Long() = prim::NumToTensor(%333), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %335 : int = aten::size(%query.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.4 : Long() = prim::NumToTensor(%335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %337 : int = aten::size(%query.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.2 : Long() = prim::NumToTensor(%337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %339 : int = aten::size(%query.2, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %340 : Long() = aten::floor_divide(%seq_len.4, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.2 : Long() = aten::sub(%340, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %342 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.2, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %343 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %344 : int = aten::Int(%343), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %345 : int[] = prim::ListConstruct(%344, %335, %339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.6 : Float(17:512, 512:1, 1:1) = aten::reshape(%342, %345), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %347 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.1, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %348 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %349 : int = aten::Int(%348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %350 : int[] = prim::ListConstruct(%349, %335, %339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.8 : Float(17:512, 512:1, 1:1) = aten::reshape(%347, %350), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %352 : int = aten::size(%hidden_states.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %353 : int = aten::size(%hidden_states.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %354 : Long() = prim::NumToTensor(%353), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %355 : Long() = aten::floor_divide(%354, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %356 : int = aten::Int(%355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %357 : int = aten::size(%hidden_states.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %358 : int[] = prim::ListConstruct(%352, %356, %46, %357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.7 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.6, %358), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %360 : int = aten::size(%hidden_states.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %361 : int = aten::size(%hidden_states.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %362 : Long() = prim::NumToTensor(%361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %363 : int = aten::size(%hidden_states.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %364 : int = aten::size(%hidden_states.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %365 : Long() = aten::mul(%362, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %366 : Long() = aten::sub(%365, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %367 : int = aten::Int(%366), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %368 : int[] = prim::ListConstruct(%360, %367, %363, %364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %369 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %370 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.7, %368, %369, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %371 : int = aten::size(%hidden_states.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %372 : int = aten::size(%hidden_states.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %373 : Long() = prim::NumToTensor(%372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %374 : Long() = aten::floor_divide(%373, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %375 : int = aten::Int(%374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %376 : int = aten::size(%hidden_states.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %377 : int[] = prim::ListConstruct(%371, %375, %46, %376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.9 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.8, %377), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %379 : int = aten::size(%hidden_states.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %380 : int = aten::size(%hidden_states.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %381 : Long() = prim::NumToTensor(%380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %382 : int = aten::size(%hidden_states.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %383 : int = aten::size(%hidden_states.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %384 : Long() = aten::mul(%381, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %385 : Long() = aten::sub(%384, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %386 : int = aten::Int(%385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %387 : int[] = prim::ListConstruct(%379, %386, %382, %383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %388 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %389 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.9, %387, %388, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %390 : Tensor[] = prim::ListConstruct(%370, %389), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.9 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %392 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.2 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.9, %392, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %394 : int = aten::size(%hidden_states_padded.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %395 : int = aten::size(%hidden_states_padded.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %396 : int = aten::size(%hidden_states_padded.2, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %397 : int = aten::size(%hidden_states_padded.2, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %398 : int[] = prim::ListConstruct(%394, %395, %396, %397), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.2 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.2, %398), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %400 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %401 : int = aten::Int(%400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %402 : Long() = aten::add(%chunks_count.2, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %403 : int = aten::Int(%402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %404 : int[] = prim::ListConstruct(%401, %403, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.2 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.2, %404, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %406 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %407 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%406, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %408 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%407, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %409 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%408, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %410 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %411 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%410, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %412 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%411, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %413 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%412, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %414 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %415 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%409, %414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %416 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%413, %415, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %417 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %418 : Float(17:262656, 512:513, 513:1) = aten::select(%417, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %419 : Float(17:262656, 256:513, 513:1) = aten::slice(%418, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %420 : Float(17:262656, 256:513, 257:1) = aten::slice(%419, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %421 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %422 : Float(17:262656, 256:513, 513:1) = aten::select(%421, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %423 : Float(17:262656, 256:513, 513:1) = aten::slice(%422, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %424 : Float(17:262656, 256:513, 257:1) = aten::slice(%423, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %425 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %426 : Float(17:262656, 256:513, 257:1) = aten::view(%420, %425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %427 : Float(17:262656, 256:513, 257:1) = aten::copy_(%424, %426, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %428 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %429 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%428, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %430 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%429, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %431 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%430, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %432 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %433 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%432, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %434 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%433, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %435 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%434, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %436 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %437 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%431, %436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %438 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%435, %437, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %439 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %440 : Float(17:262656, 512:513, 513:1) = aten::select(%439, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %441 : Float(17:262656, 255:513, 513:1) = aten::slice(%440, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %442 : Float(17:262656, 255:513, 255:1) = aten::slice(%441, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %443 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %444 : Float(17:262656, 256:513, 513:1) = aten::select(%443, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %445 : Float(17:262656, 255:513, 513:1) = aten::slice(%444, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %446 : Float(17:262656, 255:513, 255:1) = aten::slice(%445, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %447 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %448 : Float(17:262656, 255:513, 255:1) = aten::view(%442, %447), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %449 : Float(17:262656, 255:513, 255:1) = aten::copy_(%446, %448, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %450 : int[] = prim::ListConstruct(%333, %337, %335, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %451 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.2, %450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.2 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%451, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %453 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %454 : Float(256:257, 257:1) = aten::ones(%453, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %455 : Float(256:257, 257:1) = aten::tril(%454, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %456 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.2 : Float(256:257, 257:1) = aten::flip(%455, %456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %458 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %459 : Float(1:65792, 256:257, 257:1) = aten::slice(%458, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %460 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%459, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%460, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %462 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.2, %462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %464 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %465 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%464, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %466 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%465, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%466, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %468 : int = aten::size(%beginning_input.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %469 : int = aten::size(%beginning_input.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %470 : int = aten::size(%beginning_input.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %471 : int = aten::size(%beginning_input.2, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %472 : int[] = prim::ListConstruct(%468, %469, %470, %471), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %473 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.2, %472, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %474 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%473, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %475 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.2, %474, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %476 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %477 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%476, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %478 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%477, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%478, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %480 : int = aten::size(%ending_input.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %481 : int = aten::size(%ending_input.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %482 : int = aten::size(%ending_input.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %483 : int = aten::size(%ending_input.2, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %484 : int[] = prim::ListConstruct(%480, %481, %482, %483), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %485 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.2, %484, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %486 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%485, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %487 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.2, %486, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.1, %input_tensor.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.1 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.1, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
  %490 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.1, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %491 : Bool(17:512, 512:1) = aten::slice(%490, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %492 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%491, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %493 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%492, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %input.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.1, %493, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.10, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
  %496 : int[] = prim::ListConstruct(%155, %156, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %497 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.1, %496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %value.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%497, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %499 : int = aten::size(%value.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.4 : Long() = prim::NumToTensor(%499), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %501 : int = aten::size(%value.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.5 : Long() = prim::NumToTensor(%501), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %503 : int = aten::size(%value.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.3 : Long() = prim::NumToTensor(%503), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %505 : int = aten::size(%value.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %506 : Long() = aten::floor_divide(%seq_len.5, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.3 : Long() = aten::sub(%506, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
  %508 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.2, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %509 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:546:0
  %510 : int = aten::Int(%509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %511 : Long() = aten::floor_divide(%seq_len.5, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %512 : int = aten::Int(%511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %513 : int[] = prim::ListConstruct(%510, %512, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%508, %513), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %515 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.1, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %516 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %517 : int = aten::Int(%516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %518 : int[] = prim::ListConstruct(%517, %501, %505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.11 : Float(204:64, 512:13056, 64:1) = aten::reshape(%515, %518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %520 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %padded_value.1 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.11, %520, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %522 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %523 : int = aten::Int(%522), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %524 : Long() = aten::add(%chunks_count.3, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %525 : int = aten::Int(%524), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %526 : int[] = prim::ListConstruct(%523, %525, %37, %505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %527 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %528 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.1, %526, %527, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %529 : int = aten::size(%chunked_hidden_states.1, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %530 : int = aten::size(%chunked_hidden_states.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %531 : int = aten::size(%chunked_hidden_states.1, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.1 : Long() = prim::NumToTensor(%531), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %533 : int = aten::size(%chunked_hidden_states.1, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.1 : Long() = prim::NumToTensor(%533), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %535 : Long() = aten::add(%window_overlap.1, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
  %536 : int = aten::Int(%535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %537 : int[] = prim::ListConstruct(%51, %536), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.2 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.1, %537, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %539 : int[] = prim::ListConstruct(%529, %530, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.3 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.2, %539), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:424:0
  %541 : Long() = aten::neg(%window_overlap.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:428:0
  %542 : int = aten::Int(%541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %543 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %544 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%543, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.4 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%544, %44, %51, %542, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %546 : Long() = aten::add(%window_overlap.1, %hidden_dim.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:431:0
  %547 : int = aten::Int(%546), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %548 : int[] = prim::ListConstruct(%529, %530, %531, %547), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.5 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.4, %548), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:430:0
  %550 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %551 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%550, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %552 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%551, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %553 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%552, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %554 : Tensor[] = prim::ListConstruct(%553, %528), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %context.1 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %554), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %556 : int[] = prim::ListConstruct(%499, %503, %501, %505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %557 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.1, %556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.1 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%557, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %559 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.1, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %560 : int[] = prim::ListConstruct(%155, %156, %157), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %561 : Float(512:13056, 17:768, 768:1) = aten::reshape(%559, %560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.2 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%561, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %input.12 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.2, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
  %564 : __torch__.torch.nn.modules.normalization.___torch_mangle_6143.LayerNorm = prim::GetAttr[name="LayerNorm"](%131)
  %565 : __torch__.torch.nn.modules.linear.___torch_mangle_6142.Linear = prim::GetAttr[name="dense"](%131)
  %566 : Tensor = prim::GetAttr[name="bias"](%565)
  %567 : Tensor = prim::GetAttr[name="weight"](%565)
  %568 : Float(768:1, 768:768) = aten::t(%567), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.12, %568), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.4, %566, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.13, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.10, %hidden_states.1, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output # transformers/modeling_longformer.py:758:0
  %573 : Tensor = prim::GetAttr[name="bias"](%564)
  %574 : Tensor = prim::GetAttr[name="weight"](%564)
  %575 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.3 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.14, %575, %574, %573, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %577 : __torch__.torch.nn.modules.linear.___torch_mangle_6145.Linear = prim::GetAttr[name="dense"](%129)
  %578 : Tensor = prim::GetAttr[name="bias"](%577)
  %579 : Tensor = prim::GetAttr[name="weight"](%577)
  %580 : Float(768:1, 3072:768) = aten::t(%579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.3, %580), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.15 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.5, %578, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.16 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %584 : __torch__.torch.nn.modules.normalization.___torch_mangle_6147.LayerNorm = prim::GetAttr[name="LayerNorm"](%128)
  %585 : __torch__.torch.nn.modules.linear.___torch_mangle_6146.Linear = prim::GetAttr[name="dense"](%128)
  %586 : Tensor = prim::GetAttr[name="bias"](%585)
  %587 : Tensor = prim::GetAttr[name="weight"](%585)
  %588 : Float(3072:1, 768:3072) = aten::t(%587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.16, %588), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.17 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.6, %586, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.17, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.11, %input_tensor.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output # transformers/modeling_longformer.py:830:0
  %593 : Tensor = prim::GetAttr[name="bias"](%584)
  %594 : Tensor = prim::GetAttr[name="weight"](%584)
  %595 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm
  %hidden_states.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.18, %595, %594, %593, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %597 : __torch__.transformers.modeling_longformer.___torch_mangle_6166.LongformerOutput = prim::GetAttr[name="output"](%125)
  %598 : __torch__.transformers.modeling_longformer.___torch_mangle_6162.LongformerIntermediate = prim::GetAttr[name="intermediate"](%125)
  %599 : __torch__.transformers.modeling_longformer.___torch_mangle_6160.LongformerAttention = prim::GetAttr[name="attention"](%125)
  %600 : __torch__.transformers.modeling_longformer.___torch_mangle_6159.LongformerSelfOutput = prim::GetAttr[name="output"](%599)
  %601 : __torch__.transformers.modeling_longformer.___torch_mangle_6155.LongformerSelfAttention = prim::GetAttr[name="self"](%599)
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_6151.Linear = prim::GetAttr[name="value"](%601)
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_6150.Linear = prim::GetAttr[name="key"](%601)
  %604 : __torch__.torch.nn.modules.linear.___torch_mangle_6149.Linear = prim::GetAttr[name="query"](%601)
  %605 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %606 : Float(17:512, 512:1) = aten::squeeze(%605, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.2 : Bool(17:512, 512:1) = aten::lt(%606, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %input.19 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.12, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
  %609 : Tensor = prim::GetAttr[name="bias"](%604)
  %610 : Tensor = prim::GetAttr[name="weight"](%604)
  %611 : Float(768:1, 768:768) = aten::t(%610), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %611), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.7, %609, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %614 : Tensor = prim::GetAttr[name="bias"](%603)
  %615 : Tensor = prim::GetAttr[name="weight"](%603)
  %616 : Float(768:1, 768:768) = aten::t(%615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %616), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.8, %614, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %619 : Tensor = prim::GetAttr[name="bias"](%602)
  %620 : Tensor = prim::GetAttr[name="weight"](%602)
  %621 : Float(768:1, 768:768) = aten::t(%620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.9, %619, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %624 : int = aten::size(%input.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %625 : int = aten::size(%input.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %626 : int = aten::size(%input.19, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.3, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:261:0
  %628 : int[] = prim::ListConstruct(%624, %625, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %629 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.4, %628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %query.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%629, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %631 : int[] = prim::ListConstruct(%624, %625, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %632 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.2, %631), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %key.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%632, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %634 : int = aten::size(%query.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.6 : Long() = prim::NumToTensor(%634), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %636 : int = aten::size(%query.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.7 : Long() = prim::NumToTensor(%636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %638 : int = aten::size(%query.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.4 : Long() = prim::NumToTensor(%638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %640 : int = aten::size(%query.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %641 : Long() = aten::floor_divide(%seq_len.7, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.4 : Long() = aten::sub(%641, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %643 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.3, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %644 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %645 : int = aten::Int(%644), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %646 : int[] = prim::ListConstruct(%645, %636, %640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.13 : Float(204:64, 512:13056, 64:1) = aten::reshape(%643, %646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %648 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.2, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %649 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %650 : int = aten::Int(%649), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %651 : int[] = prim::ListConstruct(%650, %636, %640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.15 : Float(204:64, 512:13056, 64:1) = aten::reshape(%648, %651), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %653 : int = aten::size(%hidden_states.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %654 : int = aten::size(%hidden_states.13, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %655 : Long() = prim::NumToTensor(%654), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %656 : Long() = aten::floor_divide(%655, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %657 : int = aten::Int(%656), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %658 : int = aten::size(%hidden_states.13, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %659 : int[] = prim::ListConstruct(%653, %657, %46, %658), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.14 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.13, %659), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %661 : int = aten::size(%hidden_states.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %662 : int = aten::size(%hidden_states.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %663 : Long() = prim::NumToTensor(%662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %664 : int = aten::size(%hidden_states.14, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %665 : int = aten::size(%hidden_states.14, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %666 : Long() = aten::mul(%663, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %667 : Long() = aten::sub(%666, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %668 : int = aten::Int(%667), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %669 : int[] = prim::ListConstruct(%661, %668, %664, %665), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %670 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %671 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.14, %669, %670, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %672 : int = aten::size(%hidden_states.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %673 : int = aten::size(%hidden_states.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %674 : Long() = prim::NumToTensor(%673), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %675 : Long() = aten::floor_divide(%674, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %676 : int = aten::Int(%675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %677 : int = aten::size(%hidden_states.15, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %678 : int[] = prim::ListConstruct(%672, %676, %46, %677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.16 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.15, %678), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %680 : int = aten::size(%hidden_states.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %681 : int = aten::size(%hidden_states.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %682 : Long() = prim::NumToTensor(%681), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %683 : int = aten::size(%hidden_states.16, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %684 : int = aten::size(%hidden_states.16, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %685 : Long() = aten::mul(%682, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %686 : Long() = aten::sub(%685, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %687 : int = aten::Int(%686), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %688 : int[] = prim::ListConstruct(%680, %687, %683, %684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %689 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %690 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.16, %688, %689, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %691 : Tensor[] = prim::ListConstruct(%671, %690), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.20 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %691), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %693 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.3 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.20, %693, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %695 : int = aten::size(%hidden_states_padded.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %696 : int = aten::size(%hidden_states_padded.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %697 : int = aten::size(%hidden_states_padded.3, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %698 : int = aten::size(%hidden_states_padded.3, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %699 : int[] = prim::ListConstruct(%695, %696, %697, %698), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.3 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.3, %699), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %701 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %702 : int = aten::Int(%701), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %703 : Long() = aten::add(%chunks_count.4, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %704 : int = aten::Int(%703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %705 : int[] = prim::ListConstruct(%702, %704, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.3 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.3, %705, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %707 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %708 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%707, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %709 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%708, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %710 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%709, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %711 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %712 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%711, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %713 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%712, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %714 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%713, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %715 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %716 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%710, %715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %717 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%714, %716, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %718 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %719 : Float(204:262656, 512:513, 513:1) = aten::select(%718, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %720 : Float(204:262656, 256:513, 513:1) = aten::slice(%719, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %721 : Float(204:262656, 256:513, 257:1) = aten::slice(%720, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %722 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %723 : Float(204:262656, 256:513, 513:1) = aten::select(%722, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %724 : Float(204:262656, 256:513, 513:1) = aten::slice(%723, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %725 : Float(204:262656, 256:513, 257:1) = aten::slice(%724, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %726 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %727 : Float(204:262656, 256:513, 257:1) = aten::view(%721, %726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %728 : Float(204:262656, 256:513, 257:1) = aten::copy_(%725, %727, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %729 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %730 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%729, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %731 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%730, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %732 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%731, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %733 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %734 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%733, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %735 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%734, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %736 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%735, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %737 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %738 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%732, %737), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %739 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%736, %738, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %740 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %741 : Float(204:262656, 512:513, 513:1) = aten::select(%740, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %742 : Float(204:262656, 255:513, 513:1) = aten::slice(%741, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %743 : Float(204:262656, 255:513, 255:1) = aten::slice(%742, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %744 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %745 : Float(204:262656, 256:513, 513:1) = aten::select(%744, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %746 : Float(204:262656, 255:513, 513:1) = aten::slice(%745, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %747 : Float(204:262656, 255:513, 255:1) = aten::slice(%746, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %748 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %749 : Float(204:262656, 255:513, 255:1) = aten::view(%743, %748), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %750 : Float(204:262656, 255:513, 255:1) = aten::copy_(%747, %749, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %751 : int[] = prim::ListConstruct(%634, %638, %636, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %752 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.3, %751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%752, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %754 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %755 : Float(256:257, 257:1) = aten::ones(%754, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %756 : Float(256:257, 257:1) = aten::tril(%755, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %757 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.3 : Float(256:257, 257:1) = aten::flip(%756, %757), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %759 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %760 : Float(1:65792, 256:257, 257:1) = aten::slice(%759, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %761 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%760, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%761, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %763 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.3, %763), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %765 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %766 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%765, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %767 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%766, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%767, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %769 : int = aten::size(%beginning_input.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %770 : int = aten::size(%beginning_input.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %771 : int = aten::size(%beginning_input.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %772 : int = aten::size(%beginning_input.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %773 : int[] = prim::ListConstruct(%769, %770, %771, %772), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %774 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.3, %773, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %775 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%774, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %776 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.3, %775, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %777 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %778 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%777, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %779 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%778, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%779, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %781 : int = aten::size(%ending_input.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %782 : int = aten::size(%ending_input.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %783 : int = aten::size(%ending_input.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %784 : int = aten::size(%ending_input.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %786 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.3, %785, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %787 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%786, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %788 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.3, %787, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %789 : Bool(17:512, 512:1) = aten::ne(%606, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %790 : Bool(17:512, 512:1) = aten::slice(%789, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %791 : Bool(17:512, 512:1) = aten::slice(%790, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %792 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%791, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.2 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%792, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %794 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.2, %query.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%794, %remove_from_windowed_attention_mask.2, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %796 : int = aten::size(%float_mask.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %797 : int = aten::size(%float_mask.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %798 : int = aten::size(%float_mask.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %799 : int = aten::size(%float_mask.2, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %800 : int[] = prim::ListConstruct(%796, %797, %798, %799), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %query.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%800, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %802 : int = aten::size(%query.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.7 : Long() = prim::NumToTensor(%802), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %804 : int = aten::size(%query.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.8 : Long() = prim::NumToTensor(%804), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %806 : int = aten::size(%query.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.5 : Long() = prim::NumToTensor(%806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %808 : int = aten::size(%query.4, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %809 : Long() = aten::floor_divide(%seq_len.8, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.5 : Long() = aten::sub(%809, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %811 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.4, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %812 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %813 : int = aten::Int(%812), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %814 : int[] = prim::ListConstruct(%813, %804, %808), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.17 : Float(17:512, 512:1, 1:1) = aten::reshape(%811, %814), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %816 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.2, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %817 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %818 : int = aten::Int(%817), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %819 : int[] = prim::ListConstruct(%818, %804, %808), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.19 : Float(17:512, 512:1, 1:1) = aten::reshape(%816, %819), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %821 : int = aten::size(%hidden_states.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %822 : int = aten::size(%hidden_states.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %823 : Long() = prim::NumToTensor(%822), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %824 : Long() = aten::floor_divide(%823, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %825 : int = aten::Int(%824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %826 : int = aten::size(%hidden_states.17, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %827 : int[] = prim::ListConstruct(%821, %825, %46, %826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.18 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.17, %827), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %829 : int = aten::size(%hidden_states.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %830 : int = aten::size(%hidden_states.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %831 : Long() = prim::NumToTensor(%830), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %832 : int = aten::size(%hidden_states.18, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %833 : int = aten::size(%hidden_states.18, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %834 : Long() = aten::mul(%831, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %835 : Long() = aten::sub(%834, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %836 : int = aten::Int(%835), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %837 : int[] = prim::ListConstruct(%829, %836, %832, %833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %838 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %839 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.18, %837, %838, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %840 : int = aten::size(%hidden_states.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %841 : int = aten::size(%hidden_states.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %842 : Long() = prim::NumToTensor(%841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %843 : Long() = aten::floor_divide(%842, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %844 : int = aten::Int(%843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %845 : int = aten::size(%hidden_states.19, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %846 : int[] = prim::ListConstruct(%840, %844, %46, %845), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.20 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.19, %846), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %848 : int = aten::size(%hidden_states.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %849 : int = aten::size(%hidden_states.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %850 : Long() = prim::NumToTensor(%849), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %851 : int = aten::size(%hidden_states.20, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %852 : int = aten::size(%hidden_states.20, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %853 : Long() = aten::mul(%850, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %854 : Long() = aten::sub(%853, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %855 : int = aten::Int(%854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %856 : int[] = prim::ListConstruct(%848, %855, %851, %852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %857 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %858 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.20, %856, %857, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %859 : Tensor[] = prim::ListConstruct(%839, %858), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.21 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %859), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %861 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.4 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.21, %861, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %863 : int = aten::size(%hidden_states_padded.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %864 : int = aten::size(%hidden_states_padded.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %865 : int = aten::size(%hidden_states_padded.4, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %866 : int = aten::size(%hidden_states_padded.4, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %867 : int[] = prim::ListConstruct(%863, %864, %865, %866), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.4 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.4, %867), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %869 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %870 : int = aten::Int(%869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %871 : Long() = aten::add(%chunks_count.5, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %872 : int = aten::Int(%871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %873 : int[] = prim::ListConstruct(%870, %872, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.4 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.4, %873, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %875 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %876 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%875, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %877 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%876, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %878 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%877, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %879 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %880 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%879, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %881 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%880, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %882 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%881, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %883 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %884 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%878, %883), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %885 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%882, %884, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %886 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %887 : Float(17:262656, 512:513, 513:1) = aten::select(%886, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %888 : Float(17:262656, 256:513, 513:1) = aten::slice(%887, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %889 : Float(17:262656, 256:513, 257:1) = aten::slice(%888, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %890 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %891 : Float(17:262656, 256:513, 513:1) = aten::select(%890, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %892 : Float(17:262656, 256:513, 513:1) = aten::slice(%891, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %893 : Float(17:262656, 256:513, 257:1) = aten::slice(%892, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %894 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %895 : Float(17:262656, 256:513, 257:1) = aten::view(%889, %894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %896 : Float(17:262656, 256:513, 257:1) = aten::copy_(%893, %895, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %897 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %898 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%897, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %899 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%898, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %900 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%899, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %901 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %902 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%901, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %903 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%902, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %904 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%903, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %905 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %906 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%900, %905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %907 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%904, %906, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %908 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %909 : Float(17:262656, 512:513, 513:1) = aten::select(%908, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %910 : Float(17:262656, 255:513, 513:1) = aten::slice(%909, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %911 : Float(17:262656, 255:513, 255:1) = aten::slice(%910, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %912 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %913 : Float(17:262656, 256:513, 513:1) = aten::select(%912, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %914 : Float(17:262656, 255:513, 513:1) = aten::slice(%913, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %915 : Float(17:262656, 255:513, 255:1) = aten::slice(%914, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %916 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %917 : Float(17:262656, 255:513, 255:1) = aten::view(%911, %916), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %918 : Float(17:262656, 255:513, 255:1) = aten::copy_(%915, %917, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %919 : int[] = prim::ListConstruct(%802, %806, %804, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %920 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.4, %919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.5 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%920, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %922 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %923 : Float(256:257, 257:1) = aten::ones(%922, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %924 : Float(256:257, 257:1) = aten::tril(%923, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %925 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.4 : Float(256:257, 257:1) = aten::flip(%924, %925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %927 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %928 : Float(1:65792, 256:257, 257:1) = aten::slice(%927, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %929 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%928, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%929, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %931 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.4, %931), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %933 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %934 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%933, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %935 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%934, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%935, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %937 : int = aten::size(%beginning_input.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %938 : int = aten::size(%beginning_input.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %939 : int = aten::size(%beginning_input.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %940 : int = aten::size(%beginning_input.4, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %941 : int[] = prim::ListConstruct(%937, %938, %939, %940), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %942 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.4, %941, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %943 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%942, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %944 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.4, %943, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %945 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %946 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%945, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %947 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%946, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%947, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %949 : int = aten::size(%ending_input.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %950 : int = aten::size(%ending_input.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %951 : int = aten::size(%ending_input.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %952 : int = aten::size(%ending_input.4, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %953 : int[] = prim::ListConstruct(%949, %950, %951, %952), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %954 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.4, %953, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %955 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%954, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %956 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.4, %955, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.2 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.4, %input_tensor.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.2, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
  %959 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.2, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %960 : Bool(17:512, 512:1) = aten::slice(%959, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %961 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%960, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %962 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%961, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %input.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.2, %962, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.22, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
  %965 : int[] = prim::ListConstruct(%624, %625, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %966 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.2, %965), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %value.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%966, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %968 : int = aten::size(%value.2, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.8 : Long() = prim::NumToTensor(%968), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %970 : int = aten::size(%value.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.9 : Long() = prim::NumToTensor(%970), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %972 : int = aten::size(%value.2, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.6 : Long() = prim::NumToTensor(%972), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %974 : int = aten::size(%value.2, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %975 : Long() = aten::floor_divide(%seq_len.9, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.6 : Long() = aten::sub(%975, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
  %977 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.4, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %978 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:546:0
  %979 : int = aten::Int(%978), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %980 : Long() = aten::floor_divide(%seq_len.9, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %981 : int = aten::Int(%980), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %982 : int[] = prim::ListConstruct(%979, %981, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.6 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%977, %982), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %984 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.2, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %985 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %986 : int = aten::Int(%985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %987 : int[] = prim::ListConstruct(%986, %970, %974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.23 : Float(204:64, 512:13056, 64:1) = aten::reshape(%984, %987), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %989 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %padded_value.2 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.23, %989, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %991 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %992 : int = aten::Int(%991), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %993 : Long() = aten::add(%chunks_count.6, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %994 : int = aten::Int(%993), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %995 : int[] = prim::ListConstruct(%992, %994, %37, %974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %996 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %997 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.2, %995, %996, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
  %998 : int = aten::size(%chunked_hidden_states.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %999 : int = aten::size(%chunked_hidden_states.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %1000 : int = aten::size(%chunked_hidden_states.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.2 : Long() = prim::NumToTensor(%1000), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1002 : int = aten::size(%chunked_hidden_states.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.2 : Long() = prim::NumToTensor(%1002), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1004 : Long() = aten::add(%window_overlap.2, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
  %1005 : int = aten::Int(%1004), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1006 : int[] = prim::ListConstruct(%51, %1005), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.7 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.6, %1006, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %1008 : int[] = prim::ListConstruct(%998, %999, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.8 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.7, %1008), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:424:0
  %1010 : Long() = aten::neg(%window_overlap.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:428:0
  %1011 : int = aten::Int(%1010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1012 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1013 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1012, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.9 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1013, %44, %51, %1011, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1015 : Long() = aten::add(%window_overlap.2, %hidden_dim.2, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:431:0
  %1016 : int = aten::Int(%1015), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1017 : int[] = prim::ListConstruct(%998, %999, %1000, %1016), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.10 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.9, %1017), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:430:0
  %1019 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1020 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1019, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1021 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1020, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1022 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1021, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1023 : Tensor[] = prim::ListConstruct(%1022, %997), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %context.2 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %1023), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %1025 : int[] = prim::ListConstruct(%968, %972, %970, %974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1026 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.2, %1025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.3 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1026, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %1028 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.3, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %1029 : int[] = prim::ListConstruct(%624, %625, %626), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1030 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1028, %1029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.4 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1030, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %input.24 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.4, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
  %1033 : __torch__.torch.nn.modules.normalization.___torch_mangle_6157.LayerNorm = prim::GetAttr[name="LayerNorm"](%600)
  %1034 : __torch__.torch.nn.modules.linear.___torch_mangle_6156.Linear = prim::GetAttr[name="dense"](%600)
  %1035 : Tensor = prim::GetAttr[name="bias"](%1034)
  %1036 : Tensor = prim::GetAttr[name="weight"](%1034)
  %1037 : Float(768:1, 768:768) = aten::t(%1036), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.24, %1037), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.25 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.10, %1035, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.25, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.26 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.21, %hidden_states.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output # transformers/modeling_longformer.py:758:0
  %1042 : Tensor = prim::GetAttr[name="bias"](%1033)
  %1043 : Tensor = prim::GetAttr[name="weight"](%1033)
  %1044 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.26, %1044, %1043, %1042, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1046 : __torch__.torch.nn.modules.linear.___torch_mangle_6161.Linear = prim::GetAttr[name="dense"](%598)
  %1047 : Tensor = prim::GetAttr[name="bias"](%1046)
  %1048 : Tensor = prim::GetAttr[name="weight"](%1046)
  %1049 : Float(768:1, 3072:768) = aten::t(%1048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.6, %1049), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.27 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.11, %1047, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.28 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %1053 : __torch__.torch.nn.modules.normalization.___torch_mangle_6164.LayerNorm = prim::GetAttr[name="LayerNorm"](%597)
  %1054 : __torch__.torch.nn.modules.linear.___torch_mangle_6163.Linear = prim::GetAttr[name="dense"](%597)
  %1055 : Tensor = prim::GetAttr[name="bias"](%1054)
  %1056 : Tensor = prim::GetAttr[name="weight"](%1054)
  %1057 : Float(3072:1, 768:3072) = aten::t(%1056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.28, %1057), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.12, %1055, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.29, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output # transformers/modeling_longformer.py:830:0
  %1062 : Tensor = prim::GetAttr[name="bias"](%1053)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1053)
  %1064 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm
  %hidden_states.23 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.30, %1064, %1063, %1062, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %1066 : __torch__.transformers.modeling_longformer.___torch_mangle_6185.LongformerOutput = prim::GetAttr[name="output"](%123)
  %1067 : __torch__.transformers.modeling_longformer.___torch_mangle_6181.LongformerIntermediate = prim::GetAttr[name="intermediate"](%123)
  %1068 : __torch__.transformers.modeling_longformer.___torch_mangle_6179.LongformerAttention = prim::GetAttr[name="attention"](%123)
  %1069 : __torch__.transformers.modeling_longformer.___torch_mangle_6178.LongformerSelfOutput = prim::GetAttr[name="output"](%1068)
  %1070 : __torch__.transformers.modeling_longformer.___torch_mangle_6174.LongformerSelfAttention = prim::GetAttr[name="self"](%1068)
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_6170.Linear = prim::GetAttr[name="value"](%1070)
  %1072 : __torch__.torch.nn.modules.linear.___torch_mangle_6169.Linear = prim::GetAttr[name="key"](%1070)
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_6168.Linear = prim::GetAttr[name="query"](%1070)
  %1074 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %1075 : Float(17:512, 512:1) = aten::squeeze(%1074, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.3 : Bool(17:512, 512:1) = aten::lt(%1075, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %input.31 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.23, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
  %1078 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1080 : Float(768:1, 768:768) = aten::t(%1079), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1080), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.13, %1078, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %1083 : Tensor = prim::GetAttr[name="bias"](%1072)
  %1084 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1085 : Float(768:1, 768:768) = aten::t(%1084), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1085), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.14, %1083, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1089 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1090 : Float(768:1, 768:768) = aten::t(%1089), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1090), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.15, %1088, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %1093 : int = aten::size(%input.31, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1094 : int = aten::size(%input.31, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1095 : int = aten::size(%input.31, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.5, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:261:0
  %1097 : int[] = prim::ListConstruct(%1093, %1094, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1098 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.6, %1097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %query.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1098, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %1100 : int[] = prim::ListConstruct(%1093, %1094, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1101 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.3, %1100), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %key.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1101, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %1103 : int = aten::size(%query.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.10 : Long() = prim::NumToTensor(%1103), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1105 : int = aten::size(%query.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.11 : Long() = prim::NumToTensor(%1105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1107 : int = aten::size(%query.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.7 : Long() = prim::NumToTensor(%1107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1109 : int = aten::size(%query.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1110 : Long() = aten::floor_divide(%seq_len.11, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.7 : Long() = aten::sub(%1110, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1112 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.5, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1113 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1114 : int = aten::Int(%1113), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1115 : int[] = prim::ListConstruct(%1114, %1105, %1109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.24 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1112, %1115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1117 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.3, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1118 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1119 : int = aten::Int(%1118), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1120 : int[] = prim::ListConstruct(%1119, %1105, %1109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.26 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1117, %1120), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1122 : int = aten::size(%hidden_states.24, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1123 : int = aten::size(%hidden_states.24, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1124 : Long() = prim::NumToTensor(%1123), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1125 : Long() = aten::floor_divide(%1124, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1126 : int = aten::Int(%1125), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1127 : int = aten::size(%hidden_states.24, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1128 : int[] = prim::ListConstruct(%1122, %1126, %46, %1127), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.25 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.24, %1128), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1130 : int = aten::size(%hidden_states.25, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1131 : int = aten::size(%hidden_states.25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1132 : Long() = prim::NumToTensor(%1131), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1133 : int = aten::size(%hidden_states.25, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1134 : int = aten::size(%hidden_states.25, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1135 : Long() = aten::mul(%1132, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1136 : Long() = aten::sub(%1135, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1137 : int = aten::Int(%1136), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1138 : int[] = prim::ListConstruct(%1130, %1137, %1133, %1134), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1139 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1140 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.25, %1138, %1139, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1141 : int = aten::size(%hidden_states.26, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1142 : int = aten::size(%hidden_states.26, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1143 : Long() = prim::NumToTensor(%1142), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1144 : Long() = aten::floor_divide(%1143, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1145 : int = aten::Int(%1144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1146 : int = aten::size(%hidden_states.26, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1147 : int[] = prim::ListConstruct(%1141, %1145, %46, %1146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.27 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.26, %1147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1149 : int = aten::size(%hidden_states.27, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1150 : int = aten::size(%hidden_states.27, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1151 : Long() = prim::NumToTensor(%1150), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1152 : int = aten::size(%hidden_states.27, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1153 : int = aten::size(%hidden_states.27, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1154 : Long() = aten::mul(%1151, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1155 : Long() = aten::sub(%1154, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1156 : int = aten::Int(%1155), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1157 : int[] = prim::ListConstruct(%1149, %1156, %1152, %1153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1158 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1159 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.27, %1157, %1158, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1160 : Tensor[] = prim::ListConstruct(%1140, %1159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.32 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %1160), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1162 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.5 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.32, %1162, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1164 : int = aten::size(%hidden_states_padded.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1165 : int = aten::size(%hidden_states_padded.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1166 : int = aten::size(%hidden_states_padded.5, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1167 : int = aten::size(%hidden_states_padded.5, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1168 : int[] = prim::ListConstruct(%1164, %1165, %1166, %1167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.5 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.5, %1168), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1170 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1171 : int = aten::Int(%1170), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1172 : Long() = aten::add(%chunks_count.7, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1173 : int = aten::Int(%1172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1174 : int[] = prim::ListConstruct(%1171, %1173, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.5 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.5, %1174, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1176 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1177 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1176, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1178 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1177, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1179 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1178, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1180 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1181 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1180, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1182 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1181, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1183 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1182, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1184 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1185 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1179, %1184), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1186 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1183, %1185, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1187 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1188 : Float(204:262656, 512:513, 513:1) = aten::select(%1187, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1189 : Float(204:262656, 256:513, 513:1) = aten::slice(%1188, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1190 : Float(204:262656, 256:513, 257:1) = aten::slice(%1189, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1191 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1192 : Float(204:262656, 256:513, 513:1) = aten::select(%1191, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1193 : Float(204:262656, 256:513, 513:1) = aten::slice(%1192, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1194 : Float(204:262656, 256:513, 257:1) = aten::slice(%1193, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1195 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1196 : Float(204:262656, 256:513, 257:1) = aten::view(%1190, %1195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1197 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1194, %1196, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1198 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1199 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1198, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1200 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1199, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1201 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1200, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1202 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1203 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1202, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1204 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1203, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1205 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1204, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1206 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1207 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1201, %1206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1208 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1205, %1207, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1209 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1210 : Float(204:262656, 512:513, 513:1) = aten::select(%1209, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1211 : Float(204:262656, 255:513, 513:1) = aten::slice(%1210, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1212 : Float(204:262656, 255:513, 255:1) = aten::slice(%1211, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1213 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1214 : Float(204:262656, 256:513, 513:1) = aten::select(%1213, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1215 : Float(204:262656, 255:513, 513:1) = aten::slice(%1214, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1216 : Float(204:262656, 255:513, 255:1) = aten::slice(%1215, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1217 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1218 : Float(204:262656, 255:513, 255:1) = aten::view(%1212, %1217), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1219 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1216, %1218, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1220 : int[] = prim::ListConstruct(%1103, %1107, %1105, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1221 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.5, %1220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1221, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1223 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1224 : Float(256:257, 257:1) = aten::ones(%1223, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1225 : Float(256:257, 257:1) = aten::tril(%1224, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1226 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.5 : Float(256:257, 257:1) = aten::flip(%1225, %1226), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1228 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1229 : Float(1:65792, 256:257, 257:1) = aten::slice(%1228, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1230 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1229, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1230, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1232 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.5, %1232), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1234 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1235 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1234, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1236 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1235, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1236, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1238 : int = aten::size(%beginning_input.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1239 : int = aten::size(%beginning_input.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1240 : int = aten::size(%beginning_input.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1241 : int = aten::size(%beginning_input.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1242 : int[] = prim::ListConstruct(%1238, %1239, %1240, %1241), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1243 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.5, %1242, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1244 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1243, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1245 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.5, %1244, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1246 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1247 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1246, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1248 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1247, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1248, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1250 : int = aten::size(%ending_input.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1251 : int = aten::size(%ending_input.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1252 : int = aten::size(%ending_input.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1253 : int = aten::size(%ending_input.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1254 : int[] = prim::ListConstruct(%1250, %1251, %1252, %1253), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1255 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.5, %1254, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1256 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1255, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1257 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.5, %1256, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %1258 : Bool(17:512, 512:1) = aten::ne(%1075, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1259 : Bool(17:512, 512:1) = aten::slice(%1258, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1260 : Bool(17:512, 512:1) = aten::slice(%1259, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1261 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1260, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.3 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1261, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1263 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.3, %query.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.3 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1263, %remove_from_windowed_attention_mask.3, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %1265 : int = aten::size(%float_mask.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1266 : int = aten::size(%float_mask.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1267 : int = aten::size(%float_mask.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1268 : int = aten::size(%float_mask.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1269 : int[] = prim::ListConstruct(%1265, %1266, %1267, %1268), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %query.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1269, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1271 : int = aten::size(%query.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.11 : Long() = prim::NumToTensor(%1271), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1273 : int = aten::size(%query.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.12 : Long() = prim::NumToTensor(%1273), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1275 : int = aten::size(%query.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.8 : Long() = prim::NumToTensor(%1275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1277 : int = aten::size(%query.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1278 : Long() = aten::floor_divide(%seq_len.12, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.8 : Long() = aten::sub(%1278, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1280 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.6, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1281 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1282 : int = aten::Int(%1281), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1283 : int[] = prim::ListConstruct(%1282, %1273, %1277), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.28 : Float(17:512, 512:1, 1:1) = aten::reshape(%1280, %1283), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1285 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.3, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1286 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1287 : int = aten::Int(%1286), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1288 : int[] = prim::ListConstruct(%1287, %1273, %1277), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.30 : Float(17:512, 512:1, 1:1) = aten::reshape(%1285, %1288), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1290 : int = aten::size(%hidden_states.28, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1291 : int = aten::size(%hidden_states.28, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1292 : Long() = prim::NumToTensor(%1291), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1293 : Long() = aten::floor_divide(%1292, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1294 : int = aten::Int(%1293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1295 : int = aten::size(%hidden_states.28, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1296 : int[] = prim::ListConstruct(%1290, %1294, %46, %1295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.29 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.28, %1296), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1298 : int = aten::size(%hidden_states.29, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1299 : int = aten::size(%hidden_states.29, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1300 : Long() = prim::NumToTensor(%1299), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1301 : int = aten::size(%hidden_states.29, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1302 : int = aten::size(%hidden_states.29, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1303 : Long() = aten::mul(%1300, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1304 : Long() = aten::sub(%1303, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1305 : int = aten::Int(%1304), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1306 : int[] = prim::ListConstruct(%1298, %1305, %1301, %1302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1307 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1308 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.29, %1306, %1307, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1309 : int = aten::size(%hidden_states.30, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1310 : int = aten::size(%hidden_states.30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1311 : Long() = prim::NumToTensor(%1310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1312 : Long() = aten::floor_divide(%1311, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1313 : int = aten::Int(%1312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1314 : int = aten::size(%hidden_states.30, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1315 : int[] = prim::ListConstruct(%1309, %1313, %46, %1314), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.31 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.30, %1315), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1317 : int = aten::size(%hidden_states.31, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1318 : int = aten::size(%hidden_states.31, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1319 : Long() = prim::NumToTensor(%1318), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1320 : int = aten::size(%hidden_states.31, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1321 : int = aten::size(%hidden_states.31, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1322 : Long() = aten::mul(%1319, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1323 : Long() = aten::sub(%1322, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1324 : int = aten::Int(%1323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1325 : int[] = prim::ListConstruct(%1317, %1324, %1320, %1321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1326 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1327 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.31, %1325, %1326, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1328 : Tensor[] = prim::ListConstruct(%1308, %1327), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.33 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %1328), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1330 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.6 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.33, %1330, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1332 : int = aten::size(%hidden_states_padded.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1333 : int = aten::size(%hidden_states_padded.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1334 : int = aten::size(%hidden_states_padded.6, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1335 : int = aten::size(%hidden_states_padded.6, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1336 : int[] = prim::ListConstruct(%1332, %1333, %1334, %1335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.6 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.6, %1336), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1338 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1339 : int = aten::Int(%1338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1340 : Long() = aten::add(%chunks_count.8, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1341 : int = aten::Int(%1340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1342 : int[] = prim::ListConstruct(%1339, %1341, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.6 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.6, %1342, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1344 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1345 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1344, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1346 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1345, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1347 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1346, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1348 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1349 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1348, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1350 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1349, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1351 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1350, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1352 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1353 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1347, %1352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1354 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1351, %1353, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1355 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1356 : Float(17:262656, 512:513, 513:1) = aten::select(%1355, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1357 : Float(17:262656, 256:513, 513:1) = aten::slice(%1356, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1358 : Float(17:262656, 256:513, 257:1) = aten::slice(%1357, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1359 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1360 : Float(17:262656, 256:513, 513:1) = aten::select(%1359, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1361 : Float(17:262656, 256:513, 513:1) = aten::slice(%1360, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1362 : Float(17:262656, 256:513, 257:1) = aten::slice(%1361, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1363 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1364 : Float(17:262656, 256:513, 257:1) = aten::view(%1358, %1363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1365 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1362, %1364, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1366 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1367 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1366, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1368 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1367, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1369 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1368, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1370 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1371 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1370, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1372 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1371, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1373 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1372, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1374 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1375 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1369, %1374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1376 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1373, %1375, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1377 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1378 : Float(17:262656, 512:513, 513:1) = aten::select(%1377, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1379 : Float(17:262656, 255:513, 513:1) = aten::slice(%1378, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1380 : Float(17:262656, 255:513, 255:1) = aten::slice(%1379, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1381 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1382 : Float(17:262656, 256:513, 513:1) = aten::select(%1381, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1383 : Float(17:262656, 255:513, 513:1) = aten::slice(%1382, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1384 : Float(17:262656, 255:513, 255:1) = aten::slice(%1383, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1385 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1386 : Float(17:262656, 255:513, 255:1) = aten::view(%1380, %1385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1387 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1384, %1386, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1388 : int[] = prim::ListConstruct(%1271, %1275, %1273, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1389 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.6, %1388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.8 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1389, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1391 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1392 : Float(256:257, 257:1) = aten::ones(%1391, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1393 : Float(256:257, 257:1) = aten::tril(%1392, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1394 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.6 : Float(256:257, 257:1) = aten::flip(%1393, %1394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1396 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1397 : Float(1:65792, 256:257, 257:1) = aten::slice(%1396, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1398 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1397, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1398, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1400 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.6, %1400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1402 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1403 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1402, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1404 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1403, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1404, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1406 : int = aten::size(%beginning_input.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1407 : int = aten::size(%beginning_input.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1408 : int = aten::size(%beginning_input.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1409 : int = aten::size(%beginning_input.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1410 : int[] = prim::ListConstruct(%1406, %1407, %1408, %1409), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1411 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.6, %1410, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1412 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1411, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1413 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.6, %1412, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1414 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1415 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1414, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1416 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1415, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1416, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1418 : int = aten::size(%ending_input.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1419 : int = aten::size(%ending_input.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1420 : int = aten::size(%ending_input.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1421 : int = aten::size(%ending_input.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1422 : int[] = prim::ListConstruct(%1418, %1419, %1420, %1421), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1423 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.6, %1422, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1424 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1423, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1425 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.6, %1424, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.3 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.7, %input_tensor.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.3 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.3, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
  %1428 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.3, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1429 : Bool(17:512, 512:1) = aten::slice(%1428, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1430 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1429, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1431 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1430, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %input.34 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.3, %1431, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.34, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
  %1434 : int[] = prim::ListConstruct(%1093, %1094, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1435 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.3, %1434), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %value.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1435, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %1437 : int = aten::size(%value.3, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.12 : Long() = prim::NumToTensor(%1437), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1439 : int = aten::size(%value.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.13 : Long() = prim::NumToTensor(%1439), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1441 : int = aten::size(%value.3, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.9 : Long() = prim::NumToTensor(%1441), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1443 : int = aten::size(%value.3, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %1444 : Long() = aten::floor_divide(%seq_len.13, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.9 : Long() = aten::sub(%1444, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
  %1446 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.6, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1447 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:546:0
  %1448 : int = aten::Int(%1447), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1449 : Long() = aten::floor_divide(%seq_len.13, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1450 : int = aten::Int(%1449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1451 : int[] = prim::ListConstruct(%1448, %1450, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1446, %1451), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1453 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.3, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1454 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1455 : int = aten::Int(%1454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1456 : int[] = prim::ListConstruct(%1455, %1439, %1443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1453, %1456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1458 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %padded_value.3 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.35, %1458, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1460 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1461 : int = aten::Int(%1460), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1462 : Long() = aten::add(%chunks_count.9, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1463 : int = aten::Int(%1462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1464 : int[] = prim::ListConstruct(%1461, %1463, %37, %1443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1465 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1466 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.3, %1464, %1465, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
  %1467 : int = aten::size(%chunked_hidden_states.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1468 : int = aten::size(%chunked_hidden_states.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1469 : int = aten::size(%chunked_hidden_states.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.3 : Long() = prim::NumToTensor(%1469), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1471 : int = aten::size(%chunked_hidden_states.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.3 : Long() = prim::NumToTensor(%1471), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1473 : Long() = aten::add(%window_overlap.3, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
  %1474 : int = aten::Int(%1473), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1475 : int[] = prim::ListConstruct(%51, %1474), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.12 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.11, %1475, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1477 : int[] = prim::ListConstruct(%1467, %1468, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.13 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.12, %1477), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:424:0
  %1479 : Long() = aten::neg(%window_overlap.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:428:0
  %1480 : int = aten::Int(%1479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1481 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1482 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1481, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.14 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1482, %44, %51, %1480, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1484 : Long() = aten::add(%window_overlap.3, %hidden_dim.3, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:431:0
  %1485 : int = aten::Int(%1484), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1486 : int[] = prim::ListConstruct(%1467, %1468, %1469, %1485), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.15 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.14, %1486), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:430:0
  %1488 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1489 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1488, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1490 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1489, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1491 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1490, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1492 : Tensor[] = prim::ListConstruct(%1491, %1466), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %context.3 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %1492), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1494 : int[] = prim::ListConstruct(%1437, %1441, %1439, %1443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1495 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.3, %1494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.5 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1495, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %1497 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.5, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %1498 : int[] = prim::ListConstruct(%1093, %1094, %1095), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1499 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1497, %1498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.6 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1499, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %input.36 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.6, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
  %1502 : __torch__.torch.nn.modules.normalization.___torch_mangle_6176.LayerNorm = prim::GetAttr[name="LayerNorm"](%1069)
  %1503 : __torch__.torch.nn.modules.linear.___torch_mangle_6175.Linear = prim::GetAttr[name="dense"](%1069)
  %1504 : Tensor = prim::GetAttr[name="bias"](%1503)
  %1505 : Tensor = prim::GetAttr[name="weight"](%1503)
  %1506 : Float(768:1, 768:768) = aten::t(%1505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.36, %1506), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.37 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.16, %1504, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.32 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.37, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.32, %hidden_states.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output # transformers/modeling_longformer.py:758:0
  %1511 : Tensor = prim::GetAttr[name="bias"](%1502)
  %1512 : Tensor = prim::GetAttr[name="weight"](%1502)
  %1513 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.9 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.38, %1513, %1512, %1511, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1515 : __torch__.torch.nn.modules.linear.___torch_mangle_6180.Linear = prim::GetAttr[name="dense"](%1067)
  %1516 : Tensor = prim::GetAttr[name="bias"](%1515)
  %1517 : Tensor = prim::GetAttr[name="weight"](%1515)
  %1518 : Float(768:1, 3072:768) = aten::t(%1517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.9, %1518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.17, %1516, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.40 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %1522 : __torch__.torch.nn.modules.normalization.___torch_mangle_6183.LayerNorm = prim::GetAttr[name="LayerNorm"](%1066)
  %1523 : __torch__.torch.nn.modules.linear.___torch_mangle_6182.Linear = prim::GetAttr[name="dense"](%1066)
  %1524 : Tensor = prim::GetAttr[name="bias"](%1523)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1523)
  %1526 : Float(3072:1, 768:3072) = aten::t(%1525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.40, %1526), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.18, %1524, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.33 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.41, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.33, %input_tensor.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output # transformers/modeling_longformer.py:830:0
  %1531 : Tensor = prim::GetAttr[name="bias"](%1522)
  %1532 : Tensor = prim::GetAttr[name="weight"](%1522)
  %1533 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm
  %hidden_states.34 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.42, %1533, %1532, %1531, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %1535 : __torch__.transformers.modeling_longformer.___torch_mangle_6204.LongformerOutput = prim::GetAttr[name="output"](%121)
  %1536 : __torch__.transformers.modeling_longformer.___torch_mangle_6200.LongformerIntermediate = prim::GetAttr[name="intermediate"](%121)
  %1537 : __torch__.transformers.modeling_longformer.___torch_mangle_6198.LongformerAttention = prim::GetAttr[name="attention"](%121)
  %1538 : __torch__.transformers.modeling_longformer.___torch_mangle_6197.LongformerSelfOutput = prim::GetAttr[name="output"](%1537)
  %1539 : __torch__.transformers.modeling_longformer.___torch_mangle_6193.LongformerSelfAttention = prim::GetAttr[name="self"](%1537)
  %1540 : __torch__.torch.nn.modules.linear.___torch_mangle_6189.Linear = prim::GetAttr[name="value"](%1539)
  %1541 : __torch__.torch.nn.modules.linear.___torch_mangle_6188.Linear = prim::GetAttr[name="key"](%1539)
  %1542 : __torch__.torch.nn.modules.linear.___torch_mangle_6187.Linear = prim::GetAttr[name="query"](%1539)
  %1543 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %1544 : Float(17:512, 512:1) = aten::squeeze(%1543, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.4 : Bool(17:512, 512:1) = aten::lt(%1544, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %input.43 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.34, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
  %1547 : Tensor = prim::GetAttr[name="bias"](%1542)
  %1548 : Tensor = prim::GetAttr[name="weight"](%1542)
  %1549 : Float(768:1, 768:768) = aten::t(%1548), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.19, %1547, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %1552 : Tensor = prim::GetAttr[name="bias"](%1541)
  %1553 : Tensor = prim::GetAttr[name="weight"](%1541)
  %1554 : Float(768:1, 768:768) = aten::t(%1553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1554), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.20, %1552, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %1557 : Tensor = prim::GetAttr[name="bias"](%1540)
  %1558 : Tensor = prim::GetAttr[name="weight"](%1540)
  %1559 : Float(768:1, 768:768) = aten::t(%1558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1559), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.21, %1557, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %1562 : int = aten::size(%input.43, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1563 : int = aten::size(%input.43, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1564 : int = aten::size(%input.43, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.7, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:261:0
  %1566 : int[] = prim::ListConstruct(%1562, %1563, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1567 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.8, %1566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %query.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1567, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %1569 : int[] = prim::ListConstruct(%1562, %1563, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1570 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.4, %1569), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %key.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1570, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %1572 : int = aten::size(%query.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.14 : Long() = prim::NumToTensor(%1572), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1574 : int = aten::size(%query.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.15 : Long() = prim::NumToTensor(%1574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1576 : int = aten::size(%query.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.10 : Long() = prim::NumToTensor(%1576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1578 : int = aten::size(%query.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1579 : Long() = aten::floor_divide(%seq_len.15, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.10 : Long() = aten::sub(%1579, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1581 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.7, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1582 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1583 : int = aten::Int(%1582), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1584 : int[] = prim::ListConstruct(%1583, %1574, %1578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1581, %1584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1586 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.4, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1587 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1588 : int = aten::Int(%1587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1589 : int[] = prim::ListConstruct(%1588, %1574, %1578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.37 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1586, %1589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1591 : int = aten::size(%hidden_states.35, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1592 : int = aten::size(%hidden_states.35, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1593 : Long() = prim::NumToTensor(%1592), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1594 : Long() = aten::floor_divide(%1593, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1595 : int = aten::Int(%1594), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1596 : int = aten::size(%hidden_states.35, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1597 : int[] = prim::ListConstruct(%1591, %1595, %46, %1596), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.36 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.35, %1597), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1599 : int = aten::size(%hidden_states.36, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1600 : int = aten::size(%hidden_states.36, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1601 : Long() = prim::NumToTensor(%1600), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1602 : int = aten::size(%hidden_states.36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1603 : int = aten::size(%hidden_states.36, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1604 : Long() = aten::mul(%1601, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1605 : Long() = aten::sub(%1604, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1606 : int = aten::Int(%1605), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1607 : int[] = prim::ListConstruct(%1599, %1606, %1602, %1603), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1608 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1609 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.36, %1607, %1608, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1610 : int = aten::size(%hidden_states.37, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1611 : int = aten::size(%hidden_states.37, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1612 : Long() = prim::NumToTensor(%1611), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1613 : Long() = aten::floor_divide(%1612, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1614 : int = aten::Int(%1613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1615 : int = aten::size(%hidden_states.37, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1616 : int[] = prim::ListConstruct(%1610, %1614, %46, %1615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.38 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.37, %1616), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1618 : int = aten::size(%hidden_states.38, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1619 : int = aten::size(%hidden_states.38, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1620 : Long() = prim::NumToTensor(%1619), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1621 : int = aten::size(%hidden_states.38, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1622 : int = aten::size(%hidden_states.38, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1623 : Long() = aten::mul(%1620, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1624 : Long() = aten::sub(%1623, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1625 : int = aten::Int(%1624), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1626 : int[] = prim::ListConstruct(%1618, %1625, %1621, %1622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1627 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1628 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.38, %1626, %1627, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1629 : Tensor[] = prim::ListConstruct(%1609, %1628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.44 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %1629), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1631 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.7 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.44, %1631, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1633 : int = aten::size(%hidden_states_padded.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1634 : int = aten::size(%hidden_states_padded.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1635 : int = aten::size(%hidden_states_padded.7, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1636 : int = aten::size(%hidden_states_padded.7, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1637 : int[] = prim::ListConstruct(%1633, %1634, %1635, %1636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.7 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.7, %1637), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1639 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1640 : int = aten::Int(%1639), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1641 : Long() = aten::add(%chunks_count.10, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1642 : int = aten::Int(%1641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1643 : int[] = prim::ListConstruct(%1640, %1642, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.7 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.7, %1643, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1645 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1646 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1645, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1647 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1646, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1648 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1647, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1649 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1650 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1649, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1651 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1650, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1652 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1651, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1653 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1654 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1648, %1653), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1655 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1652, %1654, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1656 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1657 : Float(204:262656, 512:513, 513:1) = aten::select(%1656, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1658 : Float(204:262656, 256:513, 513:1) = aten::slice(%1657, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1659 : Float(204:262656, 256:513, 257:1) = aten::slice(%1658, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1660 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1661 : Float(204:262656, 256:513, 513:1) = aten::select(%1660, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1662 : Float(204:262656, 256:513, 513:1) = aten::slice(%1661, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1663 : Float(204:262656, 256:513, 257:1) = aten::slice(%1662, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1664 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1665 : Float(204:262656, 256:513, 257:1) = aten::view(%1659, %1664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1666 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1663, %1665, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1667 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1668 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1667, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1669 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1668, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1670 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1669, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1671 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1672 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1671, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1673 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1672, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1674 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1673, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1675 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1676 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1670, %1675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1677 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1674, %1676, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1678 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1679 : Float(204:262656, 512:513, 513:1) = aten::select(%1678, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1680 : Float(204:262656, 255:513, 513:1) = aten::slice(%1679, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1681 : Float(204:262656, 255:513, 255:1) = aten::slice(%1680, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1682 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1683 : Float(204:262656, 256:513, 513:1) = aten::select(%1682, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1684 : Float(204:262656, 255:513, 513:1) = aten::slice(%1683, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1685 : Float(204:262656, 255:513, 255:1) = aten::slice(%1684, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1686 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1687 : Float(204:262656, 255:513, 255:1) = aten::view(%1681, %1686), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1688 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1685, %1687, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1689 : int[] = prim::ListConstruct(%1572, %1576, %1574, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1690 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.7, %1689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1690, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1692 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1693 : Float(256:257, 257:1) = aten::ones(%1692, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1694 : Float(256:257, 257:1) = aten::tril(%1693, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1695 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.7 : Float(256:257, 257:1) = aten::flip(%1694, %1695), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1697 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1698 : Float(1:65792, 256:257, 257:1) = aten::slice(%1697, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1699 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1698, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1699, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1701 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.7, %1701), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1703 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1704 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1703, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1705 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1704, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1705, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1707 : int = aten::size(%beginning_input.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1708 : int = aten::size(%beginning_input.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1709 : int = aten::size(%beginning_input.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1710 : int = aten::size(%beginning_input.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1711 : int[] = prim::ListConstruct(%1707, %1708, %1709, %1710), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1712 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.7, %1711, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1713 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1712, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1714 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.7, %1713, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1715 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1716 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1715, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1717 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1716, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1717, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1719 : int = aten::size(%ending_input.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1720 : int = aten::size(%ending_input.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1721 : int = aten::size(%ending_input.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1722 : int = aten::size(%ending_input.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1723 : int[] = prim::ListConstruct(%1719, %1720, %1721, %1722), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1724 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.7, %1723, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1725 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1724, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1726 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.7, %1725, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %1727 : Bool(17:512, 512:1) = aten::ne(%1544, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1728 : Bool(17:512, 512:1) = aten::slice(%1727, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1729 : Bool(17:512, 512:1) = aten::slice(%1728, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1730 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1729, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.4 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1730, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1732 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.4, %query.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1732, %remove_from_windowed_attention_mask.4, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %1734 : int = aten::size(%float_mask.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1735 : int = aten::size(%float_mask.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1736 : int = aten::size(%float_mask.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1737 : int = aten::size(%float_mask.4, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1738 : int[] = prim::ListConstruct(%1734, %1735, %1736, %1737), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %query.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1738, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1740 : int = aten::size(%query.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.15 : Long() = prim::NumToTensor(%1740), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1742 : int = aten::size(%query.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.16 : Long() = prim::NumToTensor(%1742), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1744 : int = aten::size(%query.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.11 : Long() = prim::NumToTensor(%1744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1746 : int = aten::size(%query.8, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1747 : Long() = aten::floor_divide(%seq_len.16, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.11 : Long() = aten::sub(%1747, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1749 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.8, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1750 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1751 : int = aten::Int(%1750), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1752 : int[] = prim::ListConstruct(%1751, %1742, %1746), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.39 : Float(17:512, 512:1, 1:1) = aten::reshape(%1749, %1752), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1754 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.4, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1755 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1756 : int = aten::Int(%1755), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1757 : int[] = prim::ListConstruct(%1756, %1742, %1746), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.41 : Float(17:512, 512:1, 1:1) = aten::reshape(%1754, %1757), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1759 : int = aten::size(%hidden_states.39, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1760 : int = aten::size(%hidden_states.39, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1761 : Long() = prim::NumToTensor(%1760), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1762 : Long() = aten::floor_divide(%1761, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1763 : int = aten::Int(%1762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1764 : int = aten::size(%hidden_states.39, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1765 : int[] = prim::ListConstruct(%1759, %1763, %46, %1764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.40 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.39, %1765), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1767 : int = aten::size(%hidden_states.40, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1768 : int = aten::size(%hidden_states.40, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1769 : Long() = prim::NumToTensor(%1768), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1770 : int = aten::size(%hidden_states.40, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1771 : int = aten::size(%hidden_states.40, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1772 : Long() = aten::mul(%1769, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1773 : Long() = aten::sub(%1772, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1774 : int = aten::Int(%1773), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1775 : int[] = prim::ListConstruct(%1767, %1774, %1770, %1771), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1776 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.40, %1775, %1776, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1778 : int = aten::size(%hidden_states.41, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1779 : int = aten::size(%hidden_states.41, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1780 : Long() = prim::NumToTensor(%1779), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1781 : Long() = aten::floor_divide(%1780, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1782 : int = aten::Int(%1781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1783 : int = aten::size(%hidden_states.41, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1784 : int[] = prim::ListConstruct(%1778, %1782, %46, %1783), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.42 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.41, %1784), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1786 : int = aten::size(%hidden_states.42, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1787 : int = aten::size(%hidden_states.42, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1788 : Long() = prim::NumToTensor(%1787), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1789 : int = aten::size(%hidden_states.42, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1790 : int = aten::size(%hidden_states.42, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1791 : Long() = aten::mul(%1788, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1792 : Long() = aten::sub(%1791, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1793 : int = aten::Int(%1792), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1794 : int[] = prim::ListConstruct(%1786, %1793, %1789, %1790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1795 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1796 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.42, %1794, %1795, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1797 : Tensor[] = prim::ListConstruct(%1777, %1796), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.45 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %1797), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1799 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.8 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.45, %1799, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1801 : int = aten::size(%hidden_states_padded.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1802 : int = aten::size(%hidden_states_padded.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1803 : int = aten::size(%hidden_states_padded.8, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1804 : int = aten::size(%hidden_states_padded.8, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1805 : int[] = prim::ListConstruct(%1801, %1802, %1803, %1804), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.8 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.8, %1805), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1807 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1808 : int = aten::Int(%1807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1809 : Long() = aten::add(%chunks_count.11, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1810 : int = aten::Int(%1809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1811 : int[] = prim::ListConstruct(%1808, %1810, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.8 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.8, %1811, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1813 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1814 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1813, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1815 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1814, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1816 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1815, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1817 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1818 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1817, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1819 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1818, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1820 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1819, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1821 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1822 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1816, %1821), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1823 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1820, %1822, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1824 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1825 : Float(17:262656, 512:513, 513:1) = aten::select(%1824, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1826 : Float(17:262656, 256:513, 513:1) = aten::slice(%1825, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1827 : Float(17:262656, 256:513, 257:1) = aten::slice(%1826, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1828 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1829 : Float(17:262656, 256:513, 513:1) = aten::select(%1828, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1830 : Float(17:262656, 256:513, 513:1) = aten::slice(%1829, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1831 : Float(17:262656, 256:513, 257:1) = aten::slice(%1830, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1832 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1833 : Float(17:262656, 256:513, 257:1) = aten::view(%1827, %1832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1834 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1831, %1833, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1836 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1835, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1837 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1836, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1838 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1837, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1839 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1840 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1839, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1841 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1840, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1842 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1841, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1843 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1844 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1838, %1843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1845 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1842, %1844, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1846 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1847 : Float(17:262656, 512:513, 513:1) = aten::select(%1846, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1848 : Float(17:262656, 255:513, 513:1) = aten::slice(%1847, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1849 : Float(17:262656, 255:513, 255:1) = aten::slice(%1848, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1851 : Float(17:262656, 256:513, 513:1) = aten::select(%1850, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1852 : Float(17:262656, 255:513, 513:1) = aten::slice(%1851, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1853 : Float(17:262656, 255:513, 255:1) = aten::slice(%1852, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1854 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1855 : Float(17:262656, 255:513, 255:1) = aten::view(%1849, %1854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1856 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1853, %1855, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1857 : int[] = prim::ListConstruct(%1740, %1744, %1742, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1858 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.8, %1857), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.11 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1858, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1860 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1861 : Float(256:257, 257:1) = aten::ones(%1860, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1862 : Float(256:257, 257:1) = aten::tril(%1861, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1863 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.8 : Float(256:257, 257:1) = aten::flip(%1862, %1863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1865 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1866 : Float(1:65792, 256:257, 257:1) = aten::slice(%1865, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1867 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1866, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1867, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1869 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.8, %1869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1871 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1872 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1871, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1873 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1872, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1873, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1875 : int = aten::size(%beginning_input.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1876 : int = aten::size(%beginning_input.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1877 : int = aten::size(%beginning_input.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1878 : int = aten::size(%beginning_input.8, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1879 : int[] = prim::ListConstruct(%1875, %1876, %1877, %1878), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1880 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.8, %1879, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1881 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1880, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1882 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.8, %1881, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1883 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1884 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1883, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1885 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1884, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1885, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1887 : int = aten::size(%ending_input.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1888 : int = aten::size(%ending_input.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1889 : int = aten::size(%ending_input.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1890 : int = aten::size(%ending_input.8, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1891 : int[] = prim::ListConstruct(%1887, %1888, %1889, %1890), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1892 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.8, %1891, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1893 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1892, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1894 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.8, %1893, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.10, %input_tensor.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.4, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
  %1897 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.4, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1898 : Bool(17:512, 512:1) = aten::slice(%1897, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1899 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1898, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1900 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1899, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %input.46 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.4, %1900, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.46, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
  %1903 : int[] = prim::ListConstruct(%1562, %1563, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1904 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.4, %1903), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %value.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1904, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %1906 : int = aten::size(%value.4, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.16 : Long() = prim::NumToTensor(%1906), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1908 : int = aten::size(%value.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.17 : Long() = prim::NumToTensor(%1908), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1910 : int = aten::size(%value.4, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.12 : Long() = prim::NumToTensor(%1910), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1912 : int = aten::size(%value.4, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %1913 : Long() = aten::floor_divide(%seq_len.17, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.12 : Long() = aten::sub(%1913, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
  %1915 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.8, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1916 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:546:0
  %1917 : int = aten::Int(%1916), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1918 : Long() = aten::floor_divide(%seq_len.17, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1919 : int = aten::Int(%1918), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1920 : int[] = prim::ListConstruct(%1917, %1919, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.16 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1915, %1920), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1922 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.4, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1923 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1924 : int = aten::Int(%1923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1925 : int[] = prim::ListConstruct(%1924, %1908, %1912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.47 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1922, %1925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1927 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %padded_value.4 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.47, %1927, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1929 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1930 : int = aten::Int(%1929), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1931 : Long() = aten::add(%chunks_count.12, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1932 : int = aten::Int(%1931), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1933 : int[] = prim::ListConstruct(%1930, %1932, %37, %1912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1934 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1935 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.4, %1933, %1934, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
  %1936 : int = aten::size(%chunked_hidden_states.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1937 : int = aten::size(%chunked_hidden_states.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1938 : int = aten::size(%chunked_hidden_states.16, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.4 : Long() = prim::NumToTensor(%1938), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1940 : int = aten::size(%chunked_hidden_states.16, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.4 : Long() = prim::NumToTensor(%1940), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1942 : Long() = aten::add(%window_overlap.4, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
  %1943 : int = aten::Int(%1942), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1944 : int[] = prim::ListConstruct(%51, %1943), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.17 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.16, %1944, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1946 : int[] = prim::ListConstruct(%1936, %1937, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.18 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.17, %1946), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:424:0
  %1948 : Long() = aten::neg(%window_overlap.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:428:0
  %1949 : int = aten::Int(%1948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1950 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1951 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1950, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.19 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1951, %44, %51, %1949, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1953 : Long() = aten::add(%window_overlap.4, %hidden_dim.4, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:431:0
  %1954 : int = aten::Int(%1953), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1955 : int[] = prim::ListConstruct(%1936, %1937, %1938, %1954), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.20 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.19, %1955), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:430:0
  %1957 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1958 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1957, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1959 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1958, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1960 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1959, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1961 : Tensor[] = prim::ListConstruct(%1960, %1935), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %context.4 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %1961), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1963 : int[] = prim::ListConstruct(%1906, %1910, %1908, %1912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1964 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.4, %1963), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.7 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1964, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %1966 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.7, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %1967 : int[] = prim::ListConstruct(%1562, %1563, %1564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1968 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1966, %1967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.8 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1968, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %input.48 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.8, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
  %1971 : __torch__.torch.nn.modules.normalization.___torch_mangle_6195.LayerNorm = prim::GetAttr[name="LayerNorm"](%1538)
  %1972 : __torch__.torch.nn.modules.linear.___torch_mangle_6194.Linear = prim::GetAttr[name="dense"](%1538)
  %1973 : Tensor = prim::GetAttr[name="bias"](%1972)
  %1974 : Tensor = prim::GetAttr[name="weight"](%1972)
  %1975 : Float(768:1, 768:768) = aten::t(%1974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.48, %1975), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.22, %1973, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.43 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.49, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.43, %hidden_states.34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output # transformers/modeling_longformer.py:758:0
  %1980 : Tensor = prim::GetAttr[name="bias"](%1971)
  %1981 : Tensor = prim::GetAttr[name="weight"](%1971)
  %1982 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.50, %1982, %1981, %1980, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1984 : __torch__.torch.nn.modules.linear.___torch_mangle_6199.Linear = prim::GetAttr[name="dense"](%1536)
  %1985 : Tensor = prim::GetAttr[name="bias"](%1984)
  %1986 : Tensor = prim::GetAttr[name="weight"](%1984)
  %1987 : Float(768:1, 3072:768) = aten::t(%1986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.12, %1987), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.23, %1985, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %1991 : __torch__.torch.nn.modules.normalization.___torch_mangle_6202.LayerNorm = prim::GetAttr[name="LayerNorm"](%1535)
  %1992 : __torch__.torch.nn.modules.linear.___torch_mangle_6201.Linear = prim::GetAttr[name="dense"](%1535)
  %1993 : Tensor = prim::GetAttr[name="bias"](%1992)
  %1994 : Tensor = prim::GetAttr[name="weight"](%1992)
  %1995 : Float(3072:1, 768:3072) = aten::t(%1994), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.52, %1995), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.24, %1993, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.44 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.53, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.44, %input_tensor.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output # transformers/modeling_longformer.py:830:0
  %2000 : Tensor = prim::GetAttr[name="bias"](%1991)
  %2001 : Tensor = prim::GetAttr[name="weight"](%1991)
  %2002 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm
  %hidden_states.45 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.54, %2002, %2001, %2000, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %2004 : __torch__.transformers.modeling_longformer.___torch_mangle_6223.LongformerOutput = prim::GetAttr[name="output"](%119)
  %2005 : __torch__.transformers.modeling_longformer.___torch_mangle_6219.LongformerIntermediate = prim::GetAttr[name="intermediate"](%119)
  %2006 : __torch__.transformers.modeling_longformer.___torch_mangle_6217.LongformerAttention = prim::GetAttr[name="attention"](%119)
  %2007 : __torch__.transformers.modeling_longformer.___torch_mangle_6216.LongformerSelfOutput = prim::GetAttr[name="output"](%2006)
  %2008 : __torch__.transformers.modeling_longformer.___torch_mangle_6212.LongformerSelfAttention = prim::GetAttr[name="self"](%2006)
  %2009 : __torch__.torch.nn.modules.linear.___torch_mangle_6208.Linear = prim::GetAttr[name="value"](%2008)
  %2010 : __torch__.torch.nn.modules.linear.___torch_mangle_6207.Linear = prim::GetAttr[name="key"](%2008)
  %2011 : __torch__.torch.nn.modules.linear.___torch_mangle_6206.Linear = prim::GetAttr[name="query"](%2008)
  %2012 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %2013 : Float(17:512, 512:1) = aten::squeeze(%2012, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.5 : Bool(17:512, 512:1) = aten::lt(%2013, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %input.55 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.45, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
  %2016 : Tensor = prim::GetAttr[name="bias"](%2011)
  %2017 : Tensor = prim::GetAttr[name="weight"](%2011)
  %2018 : Float(768:1, 768:768) = aten::t(%2017), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2018), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.25, %2016, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %2021 : Tensor = prim::GetAttr[name="bias"](%2010)
  %2022 : Tensor = prim::GetAttr[name="weight"](%2010)
  %2023 : Float(768:1, 768:768) = aten::t(%2022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2023), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.26, %2021, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %2026 : Tensor = prim::GetAttr[name="bias"](%2009)
  %2027 : Tensor = prim::GetAttr[name="weight"](%2009)
  %2028 : Float(768:1, 768:768) = aten::t(%2027), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2028), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.27, %2026, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %2031 : int = aten::size(%input.55, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2032 : int = aten::size(%input.55, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2033 : int = aten::size(%input.55, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.9, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:261:0
  %2035 : int[] = prim::ListConstruct(%2031, %2032, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2036 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.10, %2035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %query.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2036, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %2038 : int[] = prim::ListConstruct(%2031, %2032, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2039 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.5, %2038), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %key.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2039, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %2041 : int = aten::size(%query.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.18 : Long() = prim::NumToTensor(%2041), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2043 : int = aten::size(%query.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.19 : Long() = prim::NumToTensor(%2043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2045 : int = aten::size(%query.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.13 : Long() = prim::NumToTensor(%2045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2047 : int = aten::size(%query.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2048 : Long() = aten::floor_divide(%seq_len.19, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.13 : Long() = aten::sub(%2048, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2050 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.9, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2051 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2052 : int = aten::Int(%2051), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2053 : int[] = prim::ListConstruct(%2052, %2043, %2047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.46 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2050, %2053), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2055 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.5, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2056 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2057 : int = aten::Int(%2056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2058 : int[] = prim::ListConstruct(%2057, %2043, %2047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.48 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2055, %2058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2060 : int = aten::size(%hidden_states.46, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2061 : int = aten::size(%hidden_states.46, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2062 : Long() = prim::NumToTensor(%2061), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2063 : Long() = aten::floor_divide(%2062, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2064 : int = aten::Int(%2063), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2065 : int = aten::size(%hidden_states.46, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2066 : int[] = prim::ListConstruct(%2060, %2064, %46, %2065), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.47 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.46, %2066), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2068 : int = aten::size(%hidden_states.47, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2069 : int = aten::size(%hidden_states.47, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2070 : Long() = prim::NumToTensor(%2069), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2071 : int = aten::size(%hidden_states.47, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2072 : int = aten::size(%hidden_states.47, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2073 : Long() = aten::mul(%2070, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2074 : Long() = aten::sub(%2073, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2075 : int = aten::Int(%2074), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2076 : int[] = prim::ListConstruct(%2068, %2075, %2071, %2072), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2077 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2078 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.47, %2076, %2077, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2079 : int = aten::size(%hidden_states.48, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2080 : int = aten::size(%hidden_states.48, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2081 : Long() = prim::NumToTensor(%2080), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2082 : Long() = aten::floor_divide(%2081, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2083 : int = aten::Int(%2082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2084 : int = aten::size(%hidden_states.48, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2085 : int[] = prim::ListConstruct(%2079, %2083, %46, %2084), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.49 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.48, %2085), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2087 : int = aten::size(%hidden_states.49, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2088 : int = aten::size(%hidden_states.49, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2089 : Long() = prim::NumToTensor(%2088), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2090 : int = aten::size(%hidden_states.49, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2091 : int = aten::size(%hidden_states.49, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2092 : Long() = aten::mul(%2089, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2093 : Long() = aten::sub(%2092, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2094 : int = aten::Int(%2093), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2095 : int[] = prim::ListConstruct(%2087, %2094, %2090, %2091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2096 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2097 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.49, %2095, %2096, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2098 : Tensor[] = prim::ListConstruct(%2078, %2097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.56 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %2098), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2100 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.9 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.56, %2100, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2102 : int = aten::size(%hidden_states_padded.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2103 : int = aten::size(%hidden_states_padded.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2104 : int = aten::size(%hidden_states_padded.9, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2105 : int = aten::size(%hidden_states_padded.9, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2106 : int[] = prim::ListConstruct(%2102, %2103, %2104, %2105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.9 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.9, %2106), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2108 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2109 : int = aten::Int(%2108), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2110 : Long() = aten::add(%chunks_count.13, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2111 : int = aten::Int(%2110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2112 : int[] = prim::ListConstruct(%2109, %2111, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.9 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.9, %2112, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2114 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2115 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2114, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2116 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2115, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2117 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2116, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2118 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2119 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2118, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2120 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2119, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2121 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2120, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2122 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2123 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2117, %2122), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2124 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2121, %2123, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2125 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2126 : Float(204:262656, 512:513, 513:1) = aten::select(%2125, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2127 : Float(204:262656, 256:513, 513:1) = aten::slice(%2126, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2128 : Float(204:262656, 256:513, 257:1) = aten::slice(%2127, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2129 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2130 : Float(204:262656, 256:513, 513:1) = aten::select(%2129, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2131 : Float(204:262656, 256:513, 513:1) = aten::slice(%2130, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2132 : Float(204:262656, 256:513, 257:1) = aten::slice(%2131, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2133 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2134 : Float(204:262656, 256:513, 257:1) = aten::view(%2128, %2133), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2135 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2132, %2134, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2136 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2137 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2136, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2138 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2137, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2139 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2138, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2140 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2141 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2140, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2142 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2141, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2143 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2142, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2144 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2145 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2139, %2144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2146 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2143, %2145, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2147 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2148 : Float(204:262656, 512:513, 513:1) = aten::select(%2147, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2149 : Float(204:262656, 255:513, 513:1) = aten::slice(%2148, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2150 : Float(204:262656, 255:513, 255:1) = aten::slice(%2149, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2151 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2152 : Float(204:262656, 256:513, 513:1) = aten::select(%2151, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2153 : Float(204:262656, 255:513, 513:1) = aten::slice(%2152, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2154 : Float(204:262656, 255:513, 255:1) = aten::slice(%2153, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2155 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2156 : Float(204:262656, 255:513, 255:1) = aten::view(%2150, %2155), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2157 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2154, %2156, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2158 : int[] = prim::ListConstruct(%2041, %2045, %2043, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2159 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.9, %2158), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.13 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2159, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2161 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2162 : Float(256:257, 257:1) = aten::ones(%2161, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2163 : Float(256:257, 257:1) = aten::tril(%2162, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2164 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.9 : Float(256:257, 257:1) = aten::flip(%2163, %2164), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2166 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2167 : Float(1:65792, 256:257, 257:1) = aten::slice(%2166, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2168 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2167, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2168, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2170 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.9, %2170), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2172 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2173 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2172, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2174 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2173, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2174, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2176 : int = aten::size(%beginning_input.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2177 : int = aten::size(%beginning_input.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2178 : int = aten::size(%beginning_input.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2179 : int = aten::size(%beginning_input.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2180 : int[] = prim::ListConstruct(%2176, %2177, %2178, %2179), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2181 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.9, %2180, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2182 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2181, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2183 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.9, %2182, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2184 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2185 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2184, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2186 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2185, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2186, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2188 : int = aten::size(%ending_input.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2189 : int = aten::size(%ending_input.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2190 : int = aten::size(%ending_input.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2191 : int = aten::size(%ending_input.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2192 : int[] = prim::ListConstruct(%2188, %2189, %2190, %2191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2193 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.9, %2192, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2194 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2193, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2195 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.9, %2194, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %2196 : Bool(17:512, 512:1) = aten::ne(%2013, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2197 : Bool(17:512, 512:1) = aten::slice(%2196, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2198 : Bool(17:512, 512:1) = aten::slice(%2197, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2199 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2198, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.5 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2199, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2201 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.5, %query.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.5 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2201, %remove_from_windowed_attention_mask.5, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %2203 : int = aten::size(%float_mask.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2204 : int = aten::size(%float_mask.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2205 : int = aten::size(%float_mask.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2206 : int = aten::size(%float_mask.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2207 : int[] = prim::ListConstruct(%2203, %2204, %2205, %2206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %query.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2207, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2209 : int = aten::size(%query.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.19 : Long() = prim::NumToTensor(%2209), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2211 : int = aten::size(%query.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.20 : Long() = prim::NumToTensor(%2211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2213 : int = aten::size(%query.10, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.14 : Long() = prim::NumToTensor(%2213), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2215 : int = aten::size(%query.10, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2216 : Long() = aten::floor_divide(%seq_len.20, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.14 : Long() = aten::sub(%2216, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2218 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.10, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2219 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2220 : int = aten::Int(%2219), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2221 : int[] = prim::ListConstruct(%2220, %2211, %2215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.50 : Float(17:512, 512:1, 1:1) = aten::reshape(%2218, %2221), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2223 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.5, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2224 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2225 : int = aten::Int(%2224), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2226 : int[] = prim::ListConstruct(%2225, %2211, %2215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.52 : Float(17:512, 512:1, 1:1) = aten::reshape(%2223, %2226), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2228 : int = aten::size(%hidden_states.50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2229 : int = aten::size(%hidden_states.50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2230 : Long() = prim::NumToTensor(%2229), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2231 : Long() = aten::floor_divide(%2230, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2232 : int = aten::Int(%2231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2233 : int = aten::size(%hidden_states.50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2234 : int[] = prim::ListConstruct(%2228, %2232, %46, %2233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.51 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.50, %2234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2236 : int = aten::size(%hidden_states.51, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2237 : int = aten::size(%hidden_states.51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2238 : Long() = prim::NumToTensor(%2237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2239 : int = aten::size(%hidden_states.51, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2240 : int = aten::size(%hidden_states.51, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2241 : Long() = aten::mul(%2238, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2242 : Long() = aten::sub(%2241, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2243 : int = aten::Int(%2242), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2244 : int[] = prim::ListConstruct(%2236, %2243, %2239, %2240), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2245 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2246 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.51, %2244, %2245, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2247 : int = aten::size(%hidden_states.52, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2248 : int = aten::size(%hidden_states.52, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2249 : Long() = prim::NumToTensor(%2248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2250 : Long() = aten::floor_divide(%2249, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2251 : int = aten::Int(%2250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2252 : int = aten::size(%hidden_states.52, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2253 : int[] = prim::ListConstruct(%2247, %2251, %46, %2252), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.53 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.52, %2253), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2255 : int = aten::size(%hidden_states.53, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2256 : int = aten::size(%hidden_states.53, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2257 : Long() = prim::NumToTensor(%2256), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2258 : int = aten::size(%hidden_states.53, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2259 : int = aten::size(%hidden_states.53, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2260 : Long() = aten::mul(%2257, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2261 : Long() = aten::sub(%2260, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2262 : int = aten::Int(%2261), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2263 : int[] = prim::ListConstruct(%2255, %2262, %2258, %2259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2264 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2265 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.53, %2263, %2264, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2266 : Tensor[] = prim::ListConstruct(%2246, %2265), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.57 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %2266), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2268 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.10 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.57, %2268, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2270 : int = aten::size(%hidden_states_padded.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2271 : int = aten::size(%hidden_states_padded.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2272 : int = aten::size(%hidden_states_padded.10, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2273 : int = aten::size(%hidden_states_padded.10, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2274 : int[] = prim::ListConstruct(%2270, %2271, %2272, %2273), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.10 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.10, %2274), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2276 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2277 : int = aten::Int(%2276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2278 : Long() = aten::add(%chunks_count.14, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2279 : int = aten::Int(%2278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2280 : int[] = prim::ListConstruct(%2277, %2279, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.10 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.10, %2280, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2282 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2283 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2282, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2284 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2283, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2285 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2284, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2286 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2287 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2286, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2288 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2287, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2289 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2288, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2290 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2291 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2285, %2290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2292 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2289, %2291, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2293 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2294 : Float(17:262656, 512:513, 513:1) = aten::select(%2293, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2295 : Float(17:262656, 256:513, 513:1) = aten::slice(%2294, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2296 : Float(17:262656, 256:513, 257:1) = aten::slice(%2295, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2297 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2298 : Float(17:262656, 256:513, 513:1) = aten::select(%2297, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2299 : Float(17:262656, 256:513, 513:1) = aten::slice(%2298, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2300 : Float(17:262656, 256:513, 257:1) = aten::slice(%2299, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2301 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2302 : Float(17:262656, 256:513, 257:1) = aten::view(%2296, %2301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2303 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2300, %2302, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2304 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2305 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2304, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2306 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2305, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2307 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2306, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2308 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2309 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2308, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2310 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2309, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2311 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2310, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2312 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2313 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2307, %2312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2314 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2311, %2313, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2315 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2316 : Float(17:262656, 512:513, 513:1) = aten::select(%2315, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2317 : Float(17:262656, 255:513, 513:1) = aten::slice(%2316, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2318 : Float(17:262656, 255:513, 255:1) = aten::slice(%2317, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2319 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2320 : Float(17:262656, 256:513, 513:1) = aten::select(%2319, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2321 : Float(17:262656, 255:513, 513:1) = aten::slice(%2320, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2322 : Float(17:262656, 255:513, 255:1) = aten::slice(%2321, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2323 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2324 : Float(17:262656, 255:513, 255:1) = aten::view(%2318, %2323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2325 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2322, %2324, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2326 : int[] = prim::ListConstruct(%2209, %2213, %2211, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2327 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.10, %2326), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.14 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2327, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2329 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2330 : Float(256:257, 257:1) = aten::ones(%2329, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2331 : Float(256:257, 257:1) = aten::tril(%2330, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2332 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.10 : Float(256:257, 257:1) = aten::flip(%2331, %2332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2334 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2335 : Float(1:65792, 256:257, 257:1) = aten::slice(%2334, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2336 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2335, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2336, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2338 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.10, %2338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2340 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2341 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2340, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2342 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2341, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2342, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2344 : int = aten::size(%beginning_input.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2345 : int = aten::size(%beginning_input.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2346 : int = aten::size(%beginning_input.10, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2347 : int = aten::size(%beginning_input.10, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2348 : int[] = prim::ListConstruct(%2344, %2345, %2346, %2347), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2349 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.10, %2348, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2350 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2349, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2351 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.10, %2350, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2352 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2353 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2352, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2354 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2353, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2354, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2356 : int = aten::size(%ending_input.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2357 : int = aten::size(%ending_input.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2358 : int = aten::size(%ending_input.10, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2359 : int = aten::size(%ending_input.10, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2360 : int[] = prim::ListConstruct(%2356, %2357, %2358, %2359), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2361 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.10, %2360, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2362 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2361, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2363 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.10, %2362, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.5 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.13, %input_tensor.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.5 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.5, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
  %2366 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.5, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2367 : Bool(17:512, 512:1) = aten::slice(%2366, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2368 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2367, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2369 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2368, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %input.58 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.5, %2369, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.58, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
  %2372 : int[] = prim::ListConstruct(%2031, %2032, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2373 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.5, %2372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %value.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2373, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %2375 : int = aten::size(%value.5, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.20 : Long() = prim::NumToTensor(%2375), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2377 : int = aten::size(%value.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.21 : Long() = prim::NumToTensor(%2377), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2379 : int = aten::size(%value.5, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.15 : Long() = prim::NumToTensor(%2379), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2381 : int = aten::size(%value.5, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %2382 : Long() = aten::floor_divide(%seq_len.21, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.15 : Long() = aten::sub(%2382, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
  %2384 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.10, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2385 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:546:0
  %2386 : int = aten::Int(%2385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2387 : Long() = aten::floor_divide(%seq_len.21, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2388 : int = aten::Int(%2387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2389 : int[] = prim::ListConstruct(%2386, %2388, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2384, %2389), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2391 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.5, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2392 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2393 : int = aten::Int(%2392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2394 : int[] = prim::ListConstruct(%2393, %2377, %2381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2391, %2394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2396 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %padded_value.5 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.59, %2396, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2398 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2399 : int = aten::Int(%2398), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2400 : Long() = aten::add(%chunks_count.15, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2401 : int = aten::Int(%2400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2402 : int[] = prim::ListConstruct(%2399, %2401, %37, %2381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2403 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2404 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.5, %2402, %2403, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
  %2405 : int = aten::size(%chunked_hidden_states.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2406 : int = aten::size(%chunked_hidden_states.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2407 : int = aten::size(%chunked_hidden_states.21, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.5 : Long() = prim::NumToTensor(%2407), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2409 : int = aten::size(%chunked_hidden_states.21, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.5 : Long() = prim::NumToTensor(%2409), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2411 : Long() = aten::add(%window_overlap.5, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
  %2412 : int = aten::Int(%2411), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2413 : int[] = prim::ListConstruct(%51, %2412), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.22 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.21, %2413, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2415 : int[] = prim::ListConstruct(%2405, %2406, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.23 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.22, %2415), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:424:0
  %2417 : Long() = aten::neg(%window_overlap.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:428:0
  %2418 : int = aten::Int(%2417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2419 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2420 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2419, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.24 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2420, %44, %51, %2418, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2422 : Long() = aten::add(%window_overlap.5, %hidden_dim.5, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:431:0
  %2423 : int = aten::Int(%2422), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2424 : int[] = prim::ListConstruct(%2405, %2406, %2407, %2423), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.25 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.24, %2424), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:430:0
  %2426 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.25, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2427 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2426, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2428 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2427, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2429 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2428, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2430 : Tensor[] = prim::ListConstruct(%2429, %2404), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %context.5 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %2430), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2432 : int[] = prim::ListConstruct(%2375, %2379, %2377, %2381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2433 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.5, %2432), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.9 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2433, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %2435 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.9, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %2436 : int[] = prim::ListConstruct(%2031, %2032, %2033), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2437 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2435, %2436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.10 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2437, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %input.60 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.10, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
  %2440 : __torch__.torch.nn.modules.normalization.___torch_mangle_6214.LayerNorm = prim::GetAttr[name="LayerNorm"](%2007)
  %2441 : __torch__.torch.nn.modules.linear.___torch_mangle_6213.Linear = prim::GetAttr[name="dense"](%2007)
  %2442 : Tensor = prim::GetAttr[name="bias"](%2441)
  %2443 : Tensor = prim::GetAttr[name="weight"](%2441)
  %2444 : Float(768:1, 768:768) = aten::t(%2443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.60, %2444), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.28, %2442, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.54 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.61, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.62 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.54, %hidden_states.45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output # transformers/modeling_longformer.py:758:0
  %2449 : Tensor = prim::GetAttr[name="bias"](%2440)
  %2450 : Tensor = prim::GetAttr[name="weight"](%2440)
  %2451 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.15 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.62, %2451, %2450, %2449, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2453 : __torch__.torch.nn.modules.linear.___torch_mangle_6218.Linear = prim::GetAttr[name="dense"](%2005)
  %2454 : Tensor = prim::GetAttr[name="bias"](%2453)
  %2455 : Tensor = prim::GetAttr[name="weight"](%2453)
  %2456 : Float(768:1, 3072:768) = aten::t(%2455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.15, %2456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.29, %2454, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.64 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %2460 : __torch__.torch.nn.modules.normalization.___torch_mangle_6221.LayerNorm = prim::GetAttr[name="LayerNorm"](%2004)
  %2461 : __torch__.torch.nn.modules.linear.___torch_mangle_6220.Linear = prim::GetAttr[name="dense"](%2004)
  %2462 : Tensor = prim::GetAttr[name="bias"](%2461)
  %2463 : Tensor = prim::GetAttr[name="weight"](%2461)
  %2464 : Float(3072:1, 768:3072) = aten::t(%2463), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.64, %2464), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.65 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.30, %2462, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.55 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.65, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.55, %input_tensor.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output # transformers/modeling_longformer.py:830:0
  %2469 : Tensor = prim::GetAttr[name="bias"](%2460)
  %2470 : Tensor = prim::GetAttr[name="weight"](%2460)
  %2471 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm
  %hidden_states.56 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.66, %2471, %2470, %2469, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %2473 : __torch__.transformers.modeling_longformer.___torch_mangle_6242.LongformerOutput = prim::GetAttr[name="output"](%117)
  %2474 : __torch__.transformers.modeling_longformer.___torch_mangle_6238.LongformerIntermediate = prim::GetAttr[name="intermediate"](%117)
  %2475 : __torch__.transformers.modeling_longformer.___torch_mangle_6236.LongformerAttention = prim::GetAttr[name="attention"](%117)
  %2476 : __torch__.transformers.modeling_longformer.___torch_mangle_6235.LongformerSelfOutput = prim::GetAttr[name="output"](%2475)
  %2477 : __torch__.transformers.modeling_longformer.___torch_mangle_6231.LongformerSelfAttention = prim::GetAttr[name="self"](%2475)
  %2478 : __torch__.torch.nn.modules.linear.___torch_mangle_6227.Linear = prim::GetAttr[name="value"](%2477)
  %2479 : __torch__.torch.nn.modules.linear.___torch_mangle_6226.Linear = prim::GetAttr[name="key"](%2477)
  %2480 : __torch__.torch.nn.modules.linear.___torch_mangle_6225.Linear = prim::GetAttr[name="query"](%2477)
  %2481 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %2482 : Float(17:512, 512:1) = aten::squeeze(%2481, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.6 : Bool(17:512, 512:1) = aten::lt(%2482, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %input.67 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.56, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
  %2485 : Tensor = prim::GetAttr[name="bias"](%2480)
  %2486 : Tensor = prim::GetAttr[name="weight"](%2480)
  %2487 : Float(768:1, 768:768) = aten::t(%2486), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.31, %2485, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %2490 : Tensor = prim::GetAttr[name="bias"](%2479)
  %2491 : Tensor = prim::GetAttr[name="weight"](%2479)
  %2492 : Float(768:1, 768:768) = aten::t(%2491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2492), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.32, %2490, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %2495 : Tensor = prim::GetAttr[name="bias"](%2478)
  %2496 : Tensor = prim::GetAttr[name="weight"](%2478)
  %2497 : Float(768:1, 768:768) = aten::t(%2496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2497), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.33, %2495, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %2500 : int = aten::size(%input.67, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2501 : int = aten::size(%input.67, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2502 : int = aten::size(%input.67, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.12 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.11, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:261:0
  %2504 : int[] = prim::ListConstruct(%2500, %2501, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2505 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.12, %2504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %query.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2505, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %2507 : int[] = prim::ListConstruct(%2500, %2501, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2508 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.6, %2507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %key.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2508, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %2510 : int = aten::size(%query.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.22 : Long() = prim::NumToTensor(%2510), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2512 : int = aten::size(%query.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.23 : Long() = prim::NumToTensor(%2512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2514 : int = aten::size(%query.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.16 : Long() = prim::NumToTensor(%2514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2516 : int = aten::size(%query.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2517 : Long() = aten::floor_divide(%seq_len.23, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.16 : Long() = aten::sub(%2517, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2519 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.11, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2520 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2521 : int = aten::Int(%2520), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2522 : int[] = prim::ListConstruct(%2521, %2512, %2516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.57 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2519, %2522), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2524 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.6, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2525 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2526 : int = aten::Int(%2525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2527 : int[] = prim::ListConstruct(%2526, %2512, %2516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2524, %2527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2529 : int = aten::size(%hidden_states.57, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2530 : int = aten::size(%hidden_states.57, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2531 : Long() = prim::NumToTensor(%2530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2532 : Long() = aten::floor_divide(%2531, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2533 : int = aten::Int(%2532), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2534 : int = aten::size(%hidden_states.57, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2535 : int[] = prim::ListConstruct(%2529, %2533, %46, %2534), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.58 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.57, %2535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2537 : int = aten::size(%hidden_states.58, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2538 : int = aten::size(%hidden_states.58, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2539 : Long() = prim::NumToTensor(%2538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2540 : int = aten::size(%hidden_states.58, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2541 : int = aten::size(%hidden_states.58, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2542 : Long() = aten::mul(%2539, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2543 : Long() = aten::sub(%2542, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2544 : int = aten::Int(%2543), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2545 : int[] = prim::ListConstruct(%2537, %2544, %2540, %2541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2546 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2547 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.58, %2545, %2546, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2548 : int = aten::size(%hidden_states.59, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2549 : int = aten::size(%hidden_states.59, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2550 : Long() = prim::NumToTensor(%2549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2551 : Long() = aten::floor_divide(%2550, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2552 : int = aten::Int(%2551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2553 : int = aten::size(%hidden_states.59, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2554 : int[] = prim::ListConstruct(%2548, %2552, %46, %2553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.60 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.59, %2554), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2556 : int = aten::size(%hidden_states.60, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2557 : int = aten::size(%hidden_states.60, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2558 : Long() = prim::NumToTensor(%2557), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2559 : int = aten::size(%hidden_states.60, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2560 : int = aten::size(%hidden_states.60, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2561 : Long() = aten::mul(%2558, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2562 : Long() = aten::sub(%2561, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2563 : int = aten::Int(%2562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2564 : int[] = prim::ListConstruct(%2556, %2563, %2559, %2560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2565 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2566 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.60, %2564, %2565, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2567 : Tensor[] = prim::ListConstruct(%2547, %2566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.68 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %2567), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2569 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.11 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.68, %2569, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2571 : int = aten::size(%hidden_states_padded.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2572 : int = aten::size(%hidden_states_padded.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2573 : int = aten::size(%hidden_states_padded.11, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2574 : int = aten::size(%hidden_states_padded.11, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2575 : int[] = prim::ListConstruct(%2571, %2572, %2573, %2574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.11 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.11, %2575), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2577 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2578 : int = aten::Int(%2577), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2579 : Long() = aten::add(%chunks_count.16, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2580 : int = aten::Int(%2579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2581 : int[] = prim::ListConstruct(%2578, %2580, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.11, %2581, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2583 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2584 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2583, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2585 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2584, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2586 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2585, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2587 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2588 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2587, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2589 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2588, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2590 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2589, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2591 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2592 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2586, %2591), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2593 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2590, %2592, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2594 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2595 : Float(204:262656, 512:513, 513:1) = aten::select(%2594, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2596 : Float(204:262656, 256:513, 513:1) = aten::slice(%2595, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2597 : Float(204:262656, 256:513, 257:1) = aten::slice(%2596, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2598 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2599 : Float(204:262656, 256:513, 513:1) = aten::select(%2598, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2600 : Float(204:262656, 256:513, 513:1) = aten::slice(%2599, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2601 : Float(204:262656, 256:513, 257:1) = aten::slice(%2600, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2602 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2603 : Float(204:262656, 256:513, 257:1) = aten::view(%2597, %2602), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2604 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2601, %2603, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2605 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2606 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2605, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2607 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2606, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2608 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2607, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2609 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2610 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2609, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2611 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2610, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2612 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2611, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2613 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2614 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2608, %2613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2615 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2612, %2614, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2616 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2617 : Float(204:262656, 512:513, 513:1) = aten::select(%2616, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2618 : Float(204:262656, 255:513, 513:1) = aten::slice(%2617, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2619 : Float(204:262656, 255:513, 255:1) = aten::slice(%2618, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2620 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2621 : Float(204:262656, 256:513, 513:1) = aten::select(%2620, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2622 : Float(204:262656, 255:513, 513:1) = aten::slice(%2621, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2623 : Float(204:262656, 255:513, 255:1) = aten::slice(%2622, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2624 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2625 : Float(204:262656, 255:513, 255:1) = aten::view(%2619, %2624), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2626 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2623, %2625, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2627 : int[] = prim::ListConstruct(%2510, %2514, %2512, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2628 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.11, %2627), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.16 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2628, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2630 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2631 : Float(256:257, 257:1) = aten::ones(%2630, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2632 : Float(256:257, 257:1) = aten::tril(%2631, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2633 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.11 : Float(256:257, 257:1) = aten::flip(%2632, %2633), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2635 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2636 : Float(1:65792, 256:257, 257:1) = aten::slice(%2635, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2637 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2636, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2637, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2639 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.11, %2639), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2641 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2642 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2641, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2643 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2642, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2643, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2645 : int = aten::size(%beginning_input.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2646 : int = aten::size(%beginning_input.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2647 : int = aten::size(%beginning_input.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2648 : int = aten::size(%beginning_input.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2649 : int[] = prim::ListConstruct(%2645, %2646, %2647, %2648), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2650 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.11, %2649, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2651 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2650, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2652 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.11, %2651, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2653 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2654 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2653, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2655 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2654, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2655, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2657 : int = aten::size(%ending_input.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2658 : int = aten::size(%ending_input.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2659 : int = aten::size(%ending_input.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2660 : int = aten::size(%ending_input.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2661 : int[] = prim::ListConstruct(%2657, %2658, %2659, %2660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2662 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.11, %2661, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2663 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2662, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2664 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.11, %2663, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %2665 : Bool(17:512, 512:1) = aten::ne(%2482, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2666 : Bool(17:512, 512:1) = aten::slice(%2665, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2667 : Bool(17:512, 512:1) = aten::slice(%2666, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2668 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2667, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.6 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2668, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2670 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.6, %query.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2670, %remove_from_windowed_attention_mask.6, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %2672 : int = aten::size(%float_mask.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2673 : int = aten::size(%float_mask.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2674 : int = aten::size(%float_mask.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2675 : int = aten::size(%float_mask.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2676 : int[] = prim::ListConstruct(%2672, %2673, %2674, %2675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %query.12 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2676, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2678 : int = aten::size(%query.12, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.23 : Long() = prim::NumToTensor(%2678), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2680 : int = aten::size(%query.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.24 : Long() = prim::NumToTensor(%2680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2682 : int = aten::size(%query.12, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.17 : Long() = prim::NumToTensor(%2682), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2684 : int = aten::size(%query.12, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2685 : Long() = aten::floor_divide(%seq_len.24, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.17 : Long() = aten::sub(%2685, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2687 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.12, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2688 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2689 : int = aten::Int(%2688), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2690 : int[] = prim::ListConstruct(%2689, %2680, %2684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.61 : Float(17:512, 512:1, 1:1) = aten::reshape(%2687, %2690), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2692 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.6, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2693 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2694 : int = aten::Int(%2693), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2695 : int[] = prim::ListConstruct(%2694, %2680, %2684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.63 : Float(17:512, 512:1, 1:1) = aten::reshape(%2692, %2695), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2697 : int = aten::size(%hidden_states.61, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2698 : int = aten::size(%hidden_states.61, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2699 : Long() = prim::NumToTensor(%2698), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2700 : Long() = aten::floor_divide(%2699, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2701 : int = aten::Int(%2700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2702 : int = aten::size(%hidden_states.61, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2703 : int[] = prim::ListConstruct(%2697, %2701, %46, %2702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.62 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.61, %2703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2705 : int = aten::size(%hidden_states.62, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2706 : int = aten::size(%hidden_states.62, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2707 : Long() = prim::NumToTensor(%2706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2708 : int = aten::size(%hidden_states.62, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2709 : int = aten::size(%hidden_states.62, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2710 : Long() = aten::mul(%2707, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2711 : Long() = aten::sub(%2710, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2712 : int = aten::Int(%2711), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2713 : int[] = prim::ListConstruct(%2705, %2712, %2708, %2709), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2714 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2715 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.62, %2713, %2714, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2716 : int = aten::size(%hidden_states.63, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2717 : int = aten::size(%hidden_states.63, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2718 : Long() = prim::NumToTensor(%2717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2719 : Long() = aten::floor_divide(%2718, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2720 : int = aten::Int(%2719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2721 : int = aten::size(%hidden_states.63, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2722 : int[] = prim::ListConstruct(%2716, %2720, %46, %2721), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.64 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.63, %2722), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2724 : int = aten::size(%hidden_states.64, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2725 : int = aten::size(%hidden_states.64, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2726 : Long() = prim::NumToTensor(%2725), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2727 : int = aten::size(%hidden_states.64, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2728 : int = aten::size(%hidden_states.64, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2729 : Long() = aten::mul(%2726, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2730 : Long() = aten::sub(%2729, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2731 : int = aten::Int(%2730), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2732 : int[] = prim::ListConstruct(%2724, %2731, %2727, %2728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2733 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2734 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.64, %2732, %2733, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2735 : Tensor[] = prim::ListConstruct(%2715, %2734), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.69 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %2735), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2737 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.12 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.69, %2737, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2739 : int = aten::size(%hidden_states_padded.12, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2740 : int = aten::size(%hidden_states_padded.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2741 : int = aten::size(%hidden_states_padded.12, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2742 : int = aten::size(%hidden_states_padded.12, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2743 : int[] = prim::ListConstruct(%2739, %2740, %2741, %2742), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.12 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.12, %2743), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2745 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2746 : int = aten::Int(%2745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2747 : Long() = aten::add(%chunks_count.17, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2748 : int = aten::Int(%2747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2749 : int[] = prim::ListConstruct(%2746, %2748, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.12 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.12, %2749, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2751 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2752 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2751, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2753 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2752, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2754 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2753, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2755 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2756 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2755, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2757 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2756, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2758 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2757, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2759 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2760 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2754, %2759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2761 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2758, %2760, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2762 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2763 : Float(17:262656, 512:513, 513:1) = aten::select(%2762, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2764 : Float(17:262656, 256:513, 513:1) = aten::slice(%2763, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2765 : Float(17:262656, 256:513, 257:1) = aten::slice(%2764, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2766 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2767 : Float(17:262656, 256:513, 513:1) = aten::select(%2766, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2768 : Float(17:262656, 256:513, 513:1) = aten::slice(%2767, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2769 : Float(17:262656, 256:513, 257:1) = aten::slice(%2768, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2770 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2771 : Float(17:262656, 256:513, 257:1) = aten::view(%2765, %2770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2772 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2769, %2771, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2773 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2774 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2773, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2775 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2774, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2776 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2775, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2777 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2778 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2777, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2779 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2778, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2780 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2779, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2781 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2782 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2776, %2781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2783 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2780, %2782, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2784 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2785 : Float(17:262656, 512:513, 513:1) = aten::select(%2784, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2786 : Float(17:262656, 255:513, 513:1) = aten::slice(%2785, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2787 : Float(17:262656, 255:513, 255:1) = aten::slice(%2786, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2788 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2789 : Float(17:262656, 256:513, 513:1) = aten::select(%2788, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2790 : Float(17:262656, 255:513, 513:1) = aten::slice(%2789, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2791 : Float(17:262656, 255:513, 255:1) = aten::slice(%2790, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2792 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2793 : Float(17:262656, 255:513, 255:1) = aten::view(%2787, %2792), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2794 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2791, %2793, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2795 : int[] = prim::ListConstruct(%2678, %2682, %2680, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2796 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.12, %2795), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.17 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2796, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2798 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2799 : Float(256:257, 257:1) = aten::ones(%2798, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2800 : Float(256:257, 257:1) = aten::tril(%2799, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2801 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.12 : Float(256:257, 257:1) = aten::flip(%2800, %2801), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2803 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.12, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2804 : Float(1:65792, 256:257, 257:1) = aten::slice(%2803, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2805 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2804, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2805, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2807 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.12, %2807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2809 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2810 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2809, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2811 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2810, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2811, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2813 : int = aten::size(%beginning_input.12, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2814 : int = aten::size(%beginning_input.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2815 : int = aten::size(%beginning_input.12, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2816 : int = aten::size(%beginning_input.12, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2817 : int[] = prim::ListConstruct(%2813, %2814, %2815, %2816), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2818 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.12, %2817, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2819 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2818, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2820 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.12, %2819, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2821 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2822 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2821, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2823 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2822, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2823, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2825 : int = aten::size(%ending_input.12, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2826 : int = aten::size(%ending_input.12, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2827 : int = aten::size(%ending_input.12, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2828 : int = aten::size(%ending_input.12, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2829 : int[] = prim::ListConstruct(%2825, %2826, %2827, %2828), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2830 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.12, %2829, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2831 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2830, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2832 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.12, %2831, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.6 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.16, %input_tensor.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.6, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
  %2835 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.6, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2836 : Bool(17:512, 512:1) = aten::slice(%2835, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2837 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2836, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2838 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2837, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %input.70 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.6, %2838, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.12 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.70, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
  %2841 : int[] = prim::ListConstruct(%2500, %2501, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2842 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.6, %2841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %value.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2842, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %2844 : int = aten::size(%value.6, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.24 : Long() = prim::NumToTensor(%2844), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2846 : int = aten::size(%value.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.25 : Long() = prim::NumToTensor(%2846), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2848 : int = aten::size(%value.6, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.18 : Long() = prim::NumToTensor(%2848), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2850 : int = aten::size(%value.6, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %2851 : Long() = aten::floor_divide(%seq_len.25, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.18 : Long() = aten::sub(%2851, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
  %2853 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.12, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2854 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:546:0
  %2855 : int = aten::Int(%2854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2856 : Long() = aten::floor_divide(%seq_len.25, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2857 : int = aten::Int(%2856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2858 : int[] = prim::ListConstruct(%2855, %2857, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.26 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2853, %2858), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2860 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.6, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2861 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2862 : int = aten::Int(%2861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2863 : int[] = prim::ListConstruct(%2862, %2846, %2850), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.71 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2860, %2863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2865 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %padded_value.6 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.71, %2865, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2867 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2868 : int = aten::Int(%2867), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2869 : Long() = aten::add(%chunks_count.18, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2870 : int = aten::Int(%2869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2871 : int[] = prim::ListConstruct(%2868, %2870, %37, %2850), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2872 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2873 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.6, %2871, %2872, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
  %2874 : int = aten::size(%chunked_hidden_states.26, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2875 : int = aten::size(%chunked_hidden_states.26, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2876 : int = aten::size(%chunked_hidden_states.26, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.6 : Long() = prim::NumToTensor(%2876), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2878 : int = aten::size(%chunked_hidden_states.26, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.6 : Long() = prim::NumToTensor(%2878), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2880 : Long() = aten::add(%window_overlap.6, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
  %2881 : int = aten::Int(%2880), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2882 : int[] = prim::ListConstruct(%51, %2881), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.27 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.26, %2882, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2884 : int[] = prim::ListConstruct(%2874, %2875, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.28 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.27, %2884), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:424:0
  %2886 : Long() = aten::neg(%window_overlap.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:428:0
  %2887 : int = aten::Int(%2886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2888 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.28, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2889 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2888, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.29 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2889, %44, %51, %2887, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2891 : Long() = aten::add(%window_overlap.6, %hidden_dim.6, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:431:0
  %2892 : int = aten::Int(%2891), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2893 : int[] = prim::ListConstruct(%2874, %2875, %2876, %2892), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.30 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.29, %2893), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:430:0
  %2895 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.30, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2896 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2895, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2897 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2896, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2898 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2897, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2899 : Tensor[] = prim::ListConstruct(%2898, %2873), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %context.6 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %2899), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2901 : int[] = prim::ListConstruct(%2844, %2848, %2846, %2850), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2902 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.6, %2901), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.11 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2902, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %2904 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.11, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %2905 : int[] = prim::ListConstruct(%2500, %2501, %2502), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2906 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2904, %2905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.12 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2906, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %input.72 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.12, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
  %2909 : __torch__.torch.nn.modules.normalization.___torch_mangle_6233.LayerNorm = prim::GetAttr[name="LayerNorm"](%2476)
  %2910 : __torch__.torch.nn.modules.linear.___torch_mangle_6232.Linear = prim::GetAttr[name="dense"](%2476)
  %2911 : Tensor = prim::GetAttr[name="bias"](%2910)
  %2912 : Tensor = prim::GetAttr[name="weight"](%2910)
  %2913 : Float(768:1, 768:768) = aten::t(%2912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.72, %2913), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.34, %2911, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.65 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.73, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.65, %hidden_states.56, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output # transformers/modeling_longformer.py:758:0
  %2918 : Tensor = prim::GetAttr[name="bias"](%2909)
  %2919 : Tensor = prim::GetAttr[name="weight"](%2909)
  %2920 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.18 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.74, %2920, %2919, %2918, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2922 : __torch__.torch.nn.modules.linear.___torch_mangle_6237.Linear = prim::GetAttr[name="dense"](%2474)
  %2923 : Tensor = prim::GetAttr[name="bias"](%2922)
  %2924 : Tensor = prim::GetAttr[name="weight"](%2922)
  %2925 : Float(768:1, 3072:768) = aten::t(%2924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.18, %2925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.75 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.35, %2923, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.76 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.75), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %2929 : __torch__.torch.nn.modules.normalization.___torch_mangle_6240.LayerNorm = prim::GetAttr[name="LayerNorm"](%2473)
  %2930 : __torch__.torch.nn.modules.linear.___torch_mangle_6239.Linear = prim::GetAttr[name="dense"](%2473)
  %2931 : Tensor = prim::GetAttr[name="bias"](%2930)
  %2932 : Tensor = prim::GetAttr[name="weight"](%2930)
  %2933 : Float(3072:1, 768:3072) = aten::t(%2932), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.76, %2933), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.77 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.36, %2931, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.66 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.77, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.66, %input_tensor.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output # transformers/modeling_longformer.py:830:0
  %2938 : Tensor = prim::GetAttr[name="bias"](%2929)
  %2939 : Tensor = prim::GetAttr[name="weight"](%2929)
  %2940 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm
  %hidden_states.67 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.78, %2940, %2939, %2938, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %2942 : __torch__.transformers.modeling_longformer.___torch_mangle_6261.LongformerOutput = prim::GetAttr[name="output"](%115)
  %2943 : __torch__.transformers.modeling_longformer.___torch_mangle_6257.LongformerIntermediate = prim::GetAttr[name="intermediate"](%115)
  %2944 : __torch__.transformers.modeling_longformer.___torch_mangle_6255.LongformerAttention = prim::GetAttr[name="attention"](%115)
  %2945 : __torch__.transformers.modeling_longformer.___torch_mangle_6254.LongformerSelfOutput = prim::GetAttr[name="output"](%2944)
  %2946 : __torch__.transformers.modeling_longformer.___torch_mangle_6250.LongformerSelfAttention = prim::GetAttr[name="self"](%2944)
  %2947 : __torch__.torch.nn.modules.linear.___torch_mangle_6246.Linear = prim::GetAttr[name="value"](%2946)
  %2948 : __torch__.torch.nn.modules.linear.___torch_mangle_6245.Linear = prim::GetAttr[name="key"](%2946)
  %2949 : __torch__.torch.nn.modules.linear.___torch_mangle_6244.Linear = prim::GetAttr[name="query"](%2946)
  %2950 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %2951 : Float(17:512, 512:1) = aten::squeeze(%2950, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.7 : Bool(17:512, 512:1) = aten::lt(%2951, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %input.79 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.67, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
  %2954 : Tensor = prim::GetAttr[name="bias"](%2949)
  %2955 : Tensor = prim::GetAttr[name="weight"](%2949)
  %2956 : Float(768:1, 768:768) = aten::t(%2955), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2956), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.13 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.37, %2954, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %2959 : Tensor = prim::GetAttr[name="bias"](%2948)
  %2960 : Tensor = prim::GetAttr[name="weight"](%2948)
  %2961 : Float(768:1, 768:768) = aten::t(%2960), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2961), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.38, %2959, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %2964 : Tensor = prim::GetAttr[name="bias"](%2947)
  %2965 : Tensor = prim::GetAttr[name="weight"](%2947)
  %2966 : Float(768:1, 768:768) = aten::t(%2965), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2966), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.39, %2964, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %2969 : int = aten::size(%input.79, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2970 : int = aten::size(%input.79, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2971 : int = aten::size(%input.79, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.14 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.13, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:261:0
  %2973 : int[] = prim::ListConstruct(%2969, %2970, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2974 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.14, %2973), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %query.13 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2974, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %2976 : int[] = prim::ListConstruct(%2969, %2970, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2977 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.7, %2976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %key.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2977, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %2979 : int = aten::size(%query.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.26 : Long() = prim::NumToTensor(%2979), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2981 : int = aten::size(%query.13, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.27 : Long() = prim::NumToTensor(%2981), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2983 : int = aten::size(%query.13, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.19 : Long() = prim::NumToTensor(%2983), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2985 : int = aten::size(%query.13, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %2986 : Long() = aten::floor_divide(%seq_len.27, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.19 : Long() = aten::sub(%2986, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %2988 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.13, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2989 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2990 : int = aten::Int(%2989), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2991 : int[] = prim::ListConstruct(%2990, %2981, %2985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.68 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2988, %2991), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2993 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.7, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %2994 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %2995 : int = aten::Int(%2994), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2996 : int[] = prim::ListConstruct(%2995, %2981, %2985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.70 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2993, %2996), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %2998 : int = aten::size(%hidden_states.68, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %2999 : int = aten::size(%hidden_states.68, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3000 : Long() = prim::NumToTensor(%2999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3001 : Long() = aten::floor_divide(%3000, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3002 : int = aten::Int(%3001), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3003 : int = aten::size(%hidden_states.68, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3004 : int[] = prim::ListConstruct(%2998, %3002, %46, %3003), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.69 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.68, %3004), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3006 : int = aten::size(%hidden_states.69, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3007 : int = aten::size(%hidden_states.69, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3008 : Long() = prim::NumToTensor(%3007), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3009 : int = aten::size(%hidden_states.69, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3010 : int = aten::size(%hidden_states.69, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3011 : Long() = aten::mul(%3008, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3012 : Long() = aten::sub(%3011, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3013 : int = aten::Int(%3012), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3014 : int[] = prim::ListConstruct(%3006, %3013, %3009, %3010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3015 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3016 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.69, %3014, %3015, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3017 : int = aten::size(%hidden_states.70, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3018 : int = aten::size(%hidden_states.70, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3019 : Long() = prim::NumToTensor(%3018), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3020 : Long() = aten::floor_divide(%3019, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3021 : int = aten::Int(%3020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3022 : int = aten::size(%hidden_states.70, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3023 : int[] = prim::ListConstruct(%3017, %3021, %46, %3022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.71 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.70, %3023), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3025 : int = aten::size(%hidden_states.71, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3026 : int = aten::size(%hidden_states.71, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3027 : Long() = prim::NumToTensor(%3026), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3028 : int = aten::size(%hidden_states.71, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3029 : int = aten::size(%hidden_states.71, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3030 : Long() = aten::mul(%3027, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3031 : Long() = aten::sub(%3030, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3032 : int = aten::Int(%3031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3033 : int[] = prim::ListConstruct(%3025, %3032, %3028, %3029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3034 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3035 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.71, %3033, %3034, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3036 : Tensor[] = prim::ListConstruct(%3016, %3035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.80 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %3036), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3038 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.13 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.80, %3038, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3040 : int = aten::size(%hidden_states_padded.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3041 : int = aten::size(%hidden_states_padded.13, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3042 : int = aten::size(%hidden_states_padded.13, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3043 : int = aten::size(%hidden_states_padded.13, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3044 : int[] = prim::ListConstruct(%3040, %3041, %3042, %3043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.13 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.13, %3044), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3046 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3047 : int = aten::Int(%3046), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3048 : Long() = aten::add(%chunks_count.19, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3049 : int = aten::Int(%3048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3050 : int[] = prim::ListConstruct(%3047, %3049, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.13 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.13, %3050, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3052 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3053 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3052, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3054 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3053, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3055 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3054, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3056 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3057 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3056, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3058 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3057, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3059 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3058, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3060 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3061 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3055, %3060), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3062 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3059, %3061, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3063 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3064 : Float(204:262656, 512:513, 513:1) = aten::select(%3063, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3065 : Float(204:262656, 256:513, 513:1) = aten::slice(%3064, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3066 : Float(204:262656, 256:513, 257:1) = aten::slice(%3065, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3067 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3068 : Float(204:262656, 256:513, 513:1) = aten::select(%3067, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3069 : Float(204:262656, 256:513, 513:1) = aten::slice(%3068, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3070 : Float(204:262656, 256:513, 257:1) = aten::slice(%3069, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3071 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3072 : Float(204:262656, 256:513, 257:1) = aten::view(%3066, %3071), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3073 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3070, %3072, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3074 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3075 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3074, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3076 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3075, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3077 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3076, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3078 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3079 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3078, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3080 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3079, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3081 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3080, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3082 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3083 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3077, %3082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3084 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3081, %3083, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3085 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3086 : Float(204:262656, 512:513, 513:1) = aten::select(%3085, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3087 : Float(204:262656, 255:513, 513:1) = aten::slice(%3086, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3088 : Float(204:262656, 255:513, 255:1) = aten::slice(%3087, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3089 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3090 : Float(204:262656, 256:513, 513:1) = aten::select(%3089, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3091 : Float(204:262656, 255:513, 513:1) = aten::slice(%3090, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3092 : Float(204:262656, 255:513, 255:1) = aten::slice(%3091, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3093 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3094 : Float(204:262656, 255:513, 255:1) = aten::view(%3088, %3093), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3095 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3092, %3094, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3096 : int[] = prim::ListConstruct(%2979, %2983, %2981, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3097 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.13, %3096), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.19 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3097, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3099 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3100 : Float(256:257, 257:1) = aten::ones(%3099, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3101 : Float(256:257, 257:1) = aten::tril(%3100, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3102 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.13 : Float(256:257, 257:1) = aten::flip(%3101, %3102), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3104 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3105 : Float(1:65792, 256:257, 257:1) = aten::slice(%3104, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3106 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3105, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3106, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3108 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.13, %3108), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3110 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3111 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3110, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3112 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3111, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3112, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3114 : int = aten::size(%beginning_input.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3115 : int = aten::size(%beginning_input.13, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3116 : int = aten::size(%beginning_input.13, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3117 : int = aten::size(%beginning_input.13, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3118 : int[] = prim::ListConstruct(%3114, %3115, %3116, %3117), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3119 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.13, %3118, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3120 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3119, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3121 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.13, %3120, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3122 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3123 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3122, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3124 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3123, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3124, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3126 : int = aten::size(%ending_input.13, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3127 : int = aten::size(%ending_input.13, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3128 : int = aten::size(%ending_input.13, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3129 : int = aten::size(%ending_input.13, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3130 : int[] = prim::ListConstruct(%3126, %3127, %3128, %3129), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3131 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.13, %3130, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3132 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3131, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3133 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.13, %3132, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %3134 : Bool(17:512, 512:1) = aten::ne(%2951, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3135 : Bool(17:512, 512:1) = aten::slice(%3134, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3136 : Bool(17:512, 512:1) = aten::slice(%3135, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3137 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3136, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.7 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3137, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3139 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.7, %query.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.7 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3139, %remove_from_windowed_attention_mask.7, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %3141 : int = aten::size(%float_mask.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3142 : int = aten::size(%float_mask.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3143 : int = aten::size(%float_mask.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3144 : int = aten::size(%float_mask.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3145 : int[] = prim::ListConstruct(%3141, %3142, %3143, %3144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %query.14 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3145, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3147 : int = aten::size(%query.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.27 : Long() = prim::NumToTensor(%3147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3149 : int = aten::size(%query.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.28 : Long() = prim::NumToTensor(%3149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3151 : int = aten::size(%query.14, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.20 : Long() = prim::NumToTensor(%3151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3153 : int = aten::size(%query.14, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %3154 : Long() = aten::floor_divide(%seq_len.28, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.20 : Long() = aten::sub(%3154, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %3156 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.14, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3157 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3158 : int = aten::Int(%3157), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3159 : int[] = prim::ListConstruct(%3158, %3149, %3153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.72 : Float(17:512, 512:1, 1:1) = aten::reshape(%3156, %3159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3161 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.7, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3162 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3163 : int = aten::Int(%3162), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3164 : int[] = prim::ListConstruct(%3163, %3149, %3153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.74 : Float(17:512, 512:1, 1:1) = aten::reshape(%3161, %3164), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3166 : int = aten::size(%hidden_states.72, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3167 : int = aten::size(%hidden_states.72, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3168 : Long() = prim::NumToTensor(%3167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3169 : Long() = aten::floor_divide(%3168, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3170 : int = aten::Int(%3169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3171 : int = aten::size(%hidden_states.72, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3172 : int[] = prim::ListConstruct(%3166, %3170, %46, %3171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.73 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.72, %3172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3174 : int = aten::size(%hidden_states.73, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3175 : int = aten::size(%hidden_states.73, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3176 : Long() = prim::NumToTensor(%3175), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3177 : int = aten::size(%hidden_states.73, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3178 : int = aten::size(%hidden_states.73, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3179 : Long() = aten::mul(%3176, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3180 : Long() = aten::sub(%3179, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3181 : int = aten::Int(%3180), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3182 : int[] = prim::ListConstruct(%3174, %3181, %3177, %3178), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3183 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3184 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.73, %3182, %3183, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3185 : int = aten::size(%hidden_states.74, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3186 : int = aten::size(%hidden_states.74, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3187 : Long() = prim::NumToTensor(%3186), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3188 : Long() = aten::floor_divide(%3187, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3189 : int = aten::Int(%3188), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3190 : int = aten::size(%hidden_states.74, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3191 : int[] = prim::ListConstruct(%3185, %3189, %46, %3190), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.75 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.74, %3191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3193 : int = aten::size(%hidden_states.75, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3194 : int = aten::size(%hidden_states.75, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3195 : Long() = prim::NumToTensor(%3194), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3196 : int = aten::size(%hidden_states.75, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3197 : int = aten::size(%hidden_states.75, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3198 : Long() = aten::mul(%3195, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3199 : Long() = aten::sub(%3198, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3200 : int = aten::Int(%3199), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3201 : int[] = prim::ListConstruct(%3193, %3200, %3196, %3197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3202 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3203 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.75, %3201, %3202, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3204 : Tensor[] = prim::ListConstruct(%3184, %3203), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.81 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %3204), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3206 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.14 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.81, %3206, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3208 : int = aten::size(%hidden_states_padded.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3209 : int = aten::size(%hidden_states_padded.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3210 : int = aten::size(%hidden_states_padded.14, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3211 : int = aten::size(%hidden_states_padded.14, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3212 : int[] = prim::ListConstruct(%3208, %3209, %3210, %3211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.14 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.14, %3212), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3214 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3215 : int = aten::Int(%3214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3216 : Long() = aten::add(%chunks_count.20, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3217 : int = aten::Int(%3216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3218 : int[] = prim::ListConstruct(%3215, %3217, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.14 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.14, %3218, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3220 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3221 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3220, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3222 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3221, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3223 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3222, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3224 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3225 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3224, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3226 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3225, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3227 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3226, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3228 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3229 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3223, %3228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3230 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3227, %3229, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3231 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3232 : Float(17:262656, 512:513, 513:1) = aten::select(%3231, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3233 : Float(17:262656, 256:513, 513:1) = aten::slice(%3232, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3234 : Float(17:262656, 256:513, 257:1) = aten::slice(%3233, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3235 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3236 : Float(17:262656, 256:513, 513:1) = aten::select(%3235, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3237 : Float(17:262656, 256:513, 513:1) = aten::slice(%3236, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3238 : Float(17:262656, 256:513, 257:1) = aten::slice(%3237, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3239 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3240 : Float(17:262656, 256:513, 257:1) = aten::view(%3234, %3239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3241 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3238, %3240, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3242 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3243 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3242, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3244 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3243, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3245 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3244, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3246 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3247 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3246, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3248 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3247, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3249 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3248, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3250 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3251 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3245, %3250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3252 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3249, %3251, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3253 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3254 : Float(17:262656, 512:513, 513:1) = aten::select(%3253, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3255 : Float(17:262656, 255:513, 513:1) = aten::slice(%3254, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3256 : Float(17:262656, 255:513, 255:1) = aten::slice(%3255, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3257 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3258 : Float(17:262656, 256:513, 513:1) = aten::select(%3257, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3259 : Float(17:262656, 255:513, 513:1) = aten::slice(%3258, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3260 : Float(17:262656, 255:513, 255:1) = aten::slice(%3259, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3261 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3262 : Float(17:262656, 255:513, 255:1) = aten::view(%3256, %3261), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3263 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3260, %3262, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3264 : int[] = prim::ListConstruct(%3147, %3151, %3149, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3265 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.14, %3264), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.20 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3265, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3267 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3268 : Float(256:257, 257:1) = aten::ones(%3267, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3269 : Float(256:257, 257:1) = aten::tril(%3268, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3270 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.14 : Float(256:257, 257:1) = aten::flip(%3269, %3270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3272 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3273 : Float(1:65792, 256:257, 257:1) = aten::slice(%3272, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3274 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3273, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3274, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3276 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.14, %3276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3278 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3279 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3278, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3280 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3279, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3280, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3282 : int = aten::size(%beginning_input.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3283 : int = aten::size(%beginning_input.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3284 : int = aten::size(%beginning_input.14, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3285 : int = aten::size(%beginning_input.14, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3286 : int[] = prim::ListConstruct(%3282, %3283, %3284, %3285), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3287 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.14, %3286, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3288 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3287, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3289 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.14, %3288, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3290 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3291 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3290, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3292 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3291, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3292, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3294 : int = aten::size(%ending_input.14, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3295 : int = aten::size(%ending_input.14, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3296 : int = aten::size(%ending_input.14, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3297 : int = aten::size(%ending_input.14, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3298 : int[] = prim::ListConstruct(%3294, %3295, %3296, %3297), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3299 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.14, %3298, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3300 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3299, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3301 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.14, %3300, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.19, %input_tensor.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.7 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.7, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
  %3304 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.7, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3305 : Bool(17:512, 512:1) = aten::slice(%3304, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3306 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3305, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3307 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3306, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %input.82 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.7, %3307, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.14 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.82, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
  %3310 : int[] = prim::ListConstruct(%2969, %2970, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3311 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.7, %3310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %value.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3311, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %3313 : int = aten::size(%value.7, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.28 : Long() = prim::NumToTensor(%3313), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3315 : int = aten::size(%value.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.29 : Long() = prim::NumToTensor(%3315), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3317 : int = aten::size(%value.7, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.21 : Long() = prim::NumToTensor(%3317), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3319 : int = aten::size(%value.7, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %3320 : Long() = aten::floor_divide(%seq_len.29, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.21 : Long() = aten::sub(%3320, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
  %3322 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.14, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3323 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:546:0
  %3324 : int = aten::Int(%3323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3325 : Long() = aten::floor_divide(%seq_len.29, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3326 : int = aten::Int(%3325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3327 : int[] = prim::ListConstruct(%3324, %3326, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.31 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3322, %3327), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3329 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.7, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3330 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3331 : int = aten::Int(%3330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3332 : int[] = prim::ListConstruct(%3331, %3315, %3319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.83 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3329, %3332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3334 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %padded_value.7 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.83, %3334, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3336 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3337 : int = aten::Int(%3336), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3338 : Long() = aten::add(%chunks_count.21, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3339 : int = aten::Int(%3338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3340 : int[] = prim::ListConstruct(%3337, %3339, %37, %3319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3341 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3342 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.7, %3340, %3341, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
  %3343 : int = aten::size(%chunked_hidden_states.31, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3344 : int = aten::size(%chunked_hidden_states.31, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3345 : int = aten::size(%chunked_hidden_states.31, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.7 : Long() = prim::NumToTensor(%3345), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3347 : int = aten::size(%chunked_hidden_states.31, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.7 : Long() = prim::NumToTensor(%3347), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3349 : Long() = aten::add(%window_overlap.7, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
  %3350 : int = aten::Int(%3349), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3351 : int[] = prim::ListConstruct(%51, %3350), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.32 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.31, %3351, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3353 : int[] = prim::ListConstruct(%3343, %3344, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.33 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.32, %3353), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:424:0
  %3355 : Long() = aten::neg(%window_overlap.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:428:0
  %3356 : int = aten::Int(%3355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3357 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.33, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3358 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3357, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.34 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3358, %44, %51, %3356, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3360 : Long() = aten::add(%window_overlap.7, %hidden_dim.7, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:431:0
  %3361 : int = aten::Int(%3360), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3362 : int[] = prim::ListConstruct(%3343, %3344, %3345, %3361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.35 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.34, %3362), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:430:0
  %3364 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.35, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3365 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3364, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3366 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3365, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3367 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3366, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3368 : Tensor[] = prim::ListConstruct(%3367, %3342), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %context.7 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %3368), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3370 : int[] = prim::ListConstruct(%3313, %3317, %3315, %3319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3371 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.7, %3370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.13 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3371, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %3373 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.13, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %3374 : int[] = prim::ListConstruct(%2969, %2970, %2971), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3375 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3373, %3374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.14 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3375, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %input.84 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.14, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
  %3378 : __torch__.torch.nn.modules.normalization.___torch_mangle_6252.LayerNorm = prim::GetAttr[name="LayerNorm"](%2945)
  %3379 : __torch__.torch.nn.modules.linear.___torch_mangle_6251.Linear = prim::GetAttr[name="dense"](%2945)
  %3380 : Tensor = prim::GetAttr[name="bias"](%3379)
  %3381 : Tensor = prim::GetAttr[name="weight"](%3379)
  %3382 : Float(768:1, 768:768) = aten::t(%3381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.84, %3382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.85 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.40, %3380, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.76 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.85, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.86 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.76, %hidden_states.67, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output # transformers/modeling_longformer.py:758:0
  %3387 : Tensor = prim::GetAttr[name="bias"](%3378)
  %3388 : Tensor = prim::GetAttr[name="weight"](%3378)
  %3389 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.21 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.86, %3389, %3388, %3387, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3391 : __torch__.torch.nn.modules.linear.___torch_mangle_6256.Linear = prim::GetAttr[name="dense"](%2943)
  %3392 : Tensor = prim::GetAttr[name="bias"](%3391)
  %3393 : Tensor = prim::GetAttr[name="weight"](%3391)
  %3394 : Float(768:1, 3072:768) = aten::t(%3393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.21, %3394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.87 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.41, %3392, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.88 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.87), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %3398 : __torch__.torch.nn.modules.normalization.___torch_mangle_6259.LayerNorm = prim::GetAttr[name="LayerNorm"](%2942)
  %3399 : __torch__.torch.nn.modules.linear.___torch_mangle_6258.Linear = prim::GetAttr[name="dense"](%2942)
  %3400 : Tensor = prim::GetAttr[name="bias"](%3399)
  %3401 : Tensor = prim::GetAttr[name="weight"](%3399)
  %3402 : Float(3072:1, 768:3072) = aten::t(%3401), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.88, %3402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.42, %3400, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.77 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.89, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.77, %input_tensor.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output # transformers/modeling_longformer.py:830:0
  %3407 : Tensor = prim::GetAttr[name="bias"](%3398)
  %3408 : Tensor = prim::GetAttr[name="weight"](%3398)
  %3409 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm
  %hidden_states.78 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.90, %3409, %3408, %3407, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %3411 : __torch__.transformers.modeling_longformer.___torch_mangle_6280.LongformerOutput = prim::GetAttr[name="output"](%113)
  %3412 : __torch__.transformers.modeling_longformer.___torch_mangle_6276.LongformerIntermediate = prim::GetAttr[name="intermediate"](%113)
  %3413 : __torch__.transformers.modeling_longformer.___torch_mangle_6274.LongformerAttention = prim::GetAttr[name="attention"](%113)
  %3414 : __torch__.transformers.modeling_longformer.___torch_mangle_6273.LongformerSelfOutput = prim::GetAttr[name="output"](%3413)
  %3415 : __torch__.transformers.modeling_longformer.___torch_mangle_6269.LongformerSelfAttention = prim::GetAttr[name="self"](%3413)
  %3416 : __torch__.torch.nn.modules.linear.___torch_mangle_6265.Linear = prim::GetAttr[name="value"](%3415)
  %3417 : __torch__.torch.nn.modules.linear.___torch_mangle_6264.Linear = prim::GetAttr[name="key"](%3415)
  %3418 : __torch__.torch.nn.modules.linear.___torch_mangle_6263.Linear = prim::GetAttr[name="query"](%3415)
  %3419 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %3420 : Float(17:512, 512:1) = aten::squeeze(%3419, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.8 : Bool(17:512, 512:1) = aten::lt(%3420, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %input.91 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.78, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
  %3423 : Tensor = prim::GetAttr[name="bias"](%3418)
  %3424 : Tensor = prim::GetAttr[name="weight"](%3418)
  %3425 : Float(768:1, 768:768) = aten::t(%3424), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.15 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.43, %3423, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %3428 : Tensor = prim::GetAttr[name="bias"](%3417)
  %3429 : Tensor = prim::GetAttr[name="weight"](%3417)
  %3430 : Float(768:1, 768:768) = aten::t(%3429), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3430), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.44, %3428, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %3433 : Tensor = prim::GetAttr[name="bias"](%3416)
  %3434 : Tensor = prim::GetAttr[name="weight"](%3416)
  %3435 : Float(768:1, 768:768) = aten::t(%3434), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3435), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.45, %3433, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %3438 : int = aten::size(%input.91, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3439 : int = aten::size(%input.91, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3440 : int = aten::size(%input.91, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.16 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.15, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:261:0
  %3442 : int[] = prim::ListConstruct(%3438, %3439, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3443 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.16, %3442), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %query.15 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3443, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %3445 : int[] = prim::ListConstruct(%3438, %3439, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3446 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.8, %3445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %key.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3446, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %3448 : int = aten::size(%query.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.30 : Long() = prim::NumToTensor(%3448), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3450 : int = aten::size(%query.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.31 : Long() = prim::NumToTensor(%3450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3452 : int = aten::size(%query.15, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.22 : Long() = prim::NumToTensor(%3452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3454 : int = aten::size(%query.15, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3455 : Long() = aten::floor_divide(%seq_len.31, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.22 : Long() = aten::sub(%3455, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3457 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.15, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3458 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3459 : int = aten::Int(%3458), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3460 : int[] = prim::ListConstruct(%3459, %3450, %3454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.79 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3457, %3460), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3462 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.8, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3463 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3464 : int = aten::Int(%3463), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3465 : int[] = prim::ListConstruct(%3464, %3450, %3454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.81 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3462, %3465), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3467 : int = aten::size(%hidden_states.79, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3468 : int = aten::size(%hidden_states.79, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3469 : Long() = prim::NumToTensor(%3468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3470 : Long() = aten::floor_divide(%3469, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3471 : int = aten::Int(%3470), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3472 : int = aten::size(%hidden_states.79, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3473 : int[] = prim::ListConstruct(%3467, %3471, %46, %3472), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.80 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.79, %3473), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3475 : int = aten::size(%hidden_states.80, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3476 : int = aten::size(%hidden_states.80, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3477 : Long() = prim::NumToTensor(%3476), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3478 : int = aten::size(%hidden_states.80, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3479 : int = aten::size(%hidden_states.80, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3480 : Long() = aten::mul(%3477, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3481 : Long() = aten::sub(%3480, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3482 : int = aten::Int(%3481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3483 : int[] = prim::ListConstruct(%3475, %3482, %3478, %3479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3484 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3485 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.80, %3483, %3484, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3486 : int = aten::size(%hidden_states.81, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3487 : int = aten::size(%hidden_states.81, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3488 : Long() = prim::NumToTensor(%3487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3489 : Long() = aten::floor_divide(%3488, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3490 : int = aten::Int(%3489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3491 : int = aten::size(%hidden_states.81, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3492 : int[] = prim::ListConstruct(%3486, %3490, %46, %3491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.82 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.81, %3492), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3494 : int = aten::size(%hidden_states.82, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3495 : int = aten::size(%hidden_states.82, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3496 : Long() = prim::NumToTensor(%3495), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3497 : int = aten::size(%hidden_states.82, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3498 : int = aten::size(%hidden_states.82, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3499 : Long() = aten::mul(%3496, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3500 : Long() = aten::sub(%3499, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3501 : int = aten::Int(%3500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3502 : int[] = prim::ListConstruct(%3494, %3501, %3497, %3498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3503 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3504 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.82, %3502, %3503, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3505 : Tensor[] = prim::ListConstruct(%3485, %3504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.92 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %3505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3507 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.15 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.92, %3507, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3509 : int = aten::size(%hidden_states_padded.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3510 : int = aten::size(%hidden_states_padded.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3511 : int = aten::size(%hidden_states_padded.15, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3512 : int = aten::size(%hidden_states_padded.15, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3513 : int[] = prim::ListConstruct(%3509, %3510, %3511, %3512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.15 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.15, %3513), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3515 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3516 : int = aten::Int(%3515), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3517 : Long() = aten::add(%chunks_count.22, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3518 : int = aten::Int(%3517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3519 : int[] = prim::ListConstruct(%3516, %3518, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.15 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.15, %3519, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3521 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3522 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3521, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3523 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3522, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3524 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3523, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3525 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3526 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3525, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3527 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3526, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3528 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3527, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3529 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3530 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3524, %3529), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3531 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3528, %3530, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3532 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3533 : Float(204:262656, 512:513, 513:1) = aten::select(%3532, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3534 : Float(204:262656, 256:513, 513:1) = aten::slice(%3533, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3535 : Float(204:262656, 256:513, 257:1) = aten::slice(%3534, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3536 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3537 : Float(204:262656, 256:513, 513:1) = aten::select(%3536, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3538 : Float(204:262656, 256:513, 513:1) = aten::slice(%3537, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3539 : Float(204:262656, 256:513, 257:1) = aten::slice(%3538, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3540 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3541 : Float(204:262656, 256:513, 257:1) = aten::view(%3535, %3540), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3542 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3539, %3541, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3543 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3544 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3543, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3545 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3544, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3546 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3545, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3547 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3548 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3547, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3549 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3548, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3550 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3549, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3551 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3552 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3546, %3551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3553 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3550, %3552, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3554 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3555 : Float(204:262656, 512:513, 513:1) = aten::select(%3554, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3556 : Float(204:262656, 255:513, 513:1) = aten::slice(%3555, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3557 : Float(204:262656, 255:513, 255:1) = aten::slice(%3556, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3558 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3559 : Float(204:262656, 256:513, 513:1) = aten::select(%3558, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3560 : Float(204:262656, 255:513, 513:1) = aten::slice(%3559, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3561 : Float(204:262656, 255:513, 255:1) = aten::slice(%3560, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3562 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3563 : Float(204:262656, 255:513, 255:1) = aten::view(%3557, %3562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3564 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3561, %3563, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3565 : int[] = prim::ListConstruct(%3448, %3452, %3450, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3566 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.15, %3565), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.22 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3566, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3568 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3569 : Float(256:257, 257:1) = aten::ones(%3568, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3570 : Float(256:257, 257:1) = aten::tril(%3569, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3571 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.15 : Float(256:257, 257:1) = aten::flip(%3570, %3571), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3573 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3574 : Float(1:65792, 256:257, 257:1) = aten::slice(%3573, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3575 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3574, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3575, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3577 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.15, %3577), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3579 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3580 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3579, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3581 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3580, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3581, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3583 : int = aten::size(%beginning_input.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3584 : int = aten::size(%beginning_input.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3585 : int = aten::size(%beginning_input.15, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3586 : int = aten::size(%beginning_input.15, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3587 : int[] = prim::ListConstruct(%3583, %3584, %3585, %3586), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3588 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.15, %3587, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3589 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3588, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3590 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.15, %3589, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3591 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3592 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3591, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3593 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3592, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3593, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3595 : int = aten::size(%ending_input.15, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3596 : int = aten::size(%ending_input.15, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3597 : int = aten::size(%ending_input.15, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3598 : int = aten::size(%ending_input.15, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3599 : int[] = prim::ListConstruct(%3595, %3596, %3597, %3598), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3600 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.15, %3599, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3601 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3600, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3602 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.15, %3601, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %3603 : Bool(17:512, 512:1) = aten::ne(%3420, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3604 : Bool(17:512, 512:1) = aten::slice(%3603, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3605 : Bool(17:512, 512:1) = aten::slice(%3604, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3606 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3605, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.8 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3606, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3608 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.8, %query.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3608, %remove_from_windowed_attention_mask.8, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %3610 : int = aten::size(%float_mask.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3611 : int = aten::size(%float_mask.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3612 : int = aten::size(%float_mask.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3613 : int = aten::size(%float_mask.8, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3614 : int[] = prim::ListConstruct(%3610, %3611, %3612, %3613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %query.16 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3614, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3616 : int = aten::size(%query.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.31 : Long() = prim::NumToTensor(%3616), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3618 : int = aten::size(%query.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.32 : Long() = prim::NumToTensor(%3618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3620 : int = aten::size(%query.16, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.23 : Long() = prim::NumToTensor(%3620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3622 : int = aten::size(%query.16, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3623 : Long() = aten::floor_divide(%seq_len.32, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.23 : Long() = aten::sub(%3623, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3625 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.16, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3626 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3627 : int = aten::Int(%3626), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3628 : int[] = prim::ListConstruct(%3627, %3618, %3622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.83 : Float(17:512, 512:1, 1:1) = aten::reshape(%3625, %3628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3630 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.8, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3631 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3632 : int = aten::Int(%3631), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3633 : int[] = prim::ListConstruct(%3632, %3618, %3622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.85 : Float(17:512, 512:1, 1:1) = aten::reshape(%3630, %3633), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3635 : int = aten::size(%hidden_states.83, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3636 : int = aten::size(%hidden_states.83, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3637 : Long() = prim::NumToTensor(%3636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3638 : Long() = aten::floor_divide(%3637, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3639 : int = aten::Int(%3638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3640 : int = aten::size(%hidden_states.83, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3641 : int[] = prim::ListConstruct(%3635, %3639, %46, %3640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.84 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.83, %3641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3643 : int = aten::size(%hidden_states.84, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3644 : int = aten::size(%hidden_states.84, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3645 : Long() = prim::NumToTensor(%3644), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3646 : int = aten::size(%hidden_states.84, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3647 : int = aten::size(%hidden_states.84, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3648 : Long() = aten::mul(%3645, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3649 : Long() = aten::sub(%3648, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3650 : int = aten::Int(%3649), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3651 : int[] = prim::ListConstruct(%3643, %3650, %3646, %3647), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3652 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3653 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.84, %3651, %3652, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3654 : int = aten::size(%hidden_states.85, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3655 : int = aten::size(%hidden_states.85, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3656 : Long() = prim::NumToTensor(%3655), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3657 : Long() = aten::floor_divide(%3656, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3658 : int = aten::Int(%3657), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3659 : int = aten::size(%hidden_states.85, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3660 : int[] = prim::ListConstruct(%3654, %3658, %46, %3659), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.86 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.85, %3660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3662 : int = aten::size(%hidden_states.86, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3663 : int = aten::size(%hidden_states.86, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3664 : Long() = prim::NumToTensor(%3663), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3665 : int = aten::size(%hidden_states.86, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3666 : int = aten::size(%hidden_states.86, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3667 : Long() = aten::mul(%3664, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3668 : Long() = aten::sub(%3667, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3669 : int = aten::Int(%3668), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3670 : int[] = prim::ListConstruct(%3662, %3669, %3665, %3666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3671 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3672 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.86, %3670, %3671, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3673 : Tensor[] = prim::ListConstruct(%3653, %3672), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.93 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %3673), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3675 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.16 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.93, %3675, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3677 : int = aten::size(%hidden_states_padded.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3678 : int = aten::size(%hidden_states_padded.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3679 : int = aten::size(%hidden_states_padded.16, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3680 : int = aten::size(%hidden_states_padded.16, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3681 : int[] = prim::ListConstruct(%3677, %3678, %3679, %3680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.16 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.16, %3681), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3683 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3684 : int = aten::Int(%3683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3685 : Long() = aten::add(%chunks_count.23, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3686 : int = aten::Int(%3685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3687 : int[] = prim::ListConstruct(%3684, %3686, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.16 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.16, %3687, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3689 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3690 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3689, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3691 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3690, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3692 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3691, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3693 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3694 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3693, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3695 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3694, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3696 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3695, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3697 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3698 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3692, %3697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3699 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3696, %3698, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3700 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3701 : Float(17:262656, 512:513, 513:1) = aten::select(%3700, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3702 : Float(17:262656, 256:513, 513:1) = aten::slice(%3701, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3703 : Float(17:262656, 256:513, 257:1) = aten::slice(%3702, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3704 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3705 : Float(17:262656, 256:513, 513:1) = aten::select(%3704, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3706 : Float(17:262656, 256:513, 513:1) = aten::slice(%3705, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3707 : Float(17:262656, 256:513, 257:1) = aten::slice(%3706, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3708 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3709 : Float(17:262656, 256:513, 257:1) = aten::view(%3703, %3708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3710 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3707, %3709, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3711 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3712 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3711, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3713 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3712, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3714 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3713, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3715 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3716 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3715, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3717 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3716, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3718 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3717, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3719 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3720 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3714, %3719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3721 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3718, %3720, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3722 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3723 : Float(17:262656, 512:513, 513:1) = aten::select(%3722, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3724 : Float(17:262656, 255:513, 513:1) = aten::slice(%3723, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3725 : Float(17:262656, 255:513, 255:1) = aten::slice(%3724, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3726 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3727 : Float(17:262656, 256:513, 513:1) = aten::select(%3726, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3728 : Float(17:262656, 255:513, 513:1) = aten::slice(%3727, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3729 : Float(17:262656, 255:513, 255:1) = aten::slice(%3728, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3730 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3731 : Float(17:262656, 255:513, 255:1) = aten::view(%3725, %3730), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3732 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3729, %3731, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3733 : int[] = prim::ListConstruct(%3616, %3620, %3618, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3734 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.16, %3733), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.23 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3734, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3736 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3737 : Float(256:257, 257:1) = aten::ones(%3736, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3738 : Float(256:257, 257:1) = aten::tril(%3737, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3739 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.16 : Float(256:257, 257:1) = aten::flip(%3738, %3739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3741 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3742 : Float(1:65792, 256:257, 257:1) = aten::slice(%3741, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3743 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3742, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3743, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3745 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.16, %3745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3747 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3748 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3747, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3749 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3748, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3749, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3751 : int = aten::size(%beginning_input.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3752 : int = aten::size(%beginning_input.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3753 : int = aten::size(%beginning_input.16, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3754 : int = aten::size(%beginning_input.16, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3755 : int[] = prim::ListConstruct(%3751, %3752, %3753, %3754), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3756 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.16, %3755, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3757 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3756, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3758 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.16, %3757, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3759 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3760 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3759, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3761 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3760, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3761, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3763 : int = aten::size(%ending_input.16, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3764 : int = aten::size(%ending_input.16, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3765 : int = aten::size(%ending_input.16, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3766 : int = aten::size(%ending_input.16, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3767 : int[] = prim::ListConstruct(%3763, %3764, %3765, %3766), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3768 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.16, %3767, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3769 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3768, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3770 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.16, %3769, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.8 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.22, %input_tensor.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.8, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
  %3773 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.8, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3774 : Bool(17:512, 512:1) = aten::slice(%3773, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3775 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3774, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3776 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3775, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %input.94 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.8, %3776, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.16 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.94, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
  %3779 : int[] = prim::ListConstruct(%3438, %3439, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3780 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.8, %3779), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %value.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3780, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %3782 : int = aten::size(%value.8, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.32 : Long() = prim::NumToTensor(%3782), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3784 : int = aten::size(%value.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.33 : Long() = prim::NumToTensor(%3784), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3786 : int = aten::size(%value.8, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.24 : Long() = prim::NumToTensor(%3786), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3788 : int = aten::size(%value.8, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %3789 : Long() = aten::floor_divide(%seq_len.33, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.24 : Long() = aten::sub(%3789, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
  %3791 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.16, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3792 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:546:0
  %3793 : int = aten::Int(%3792), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3794 : Long() = aten::floor_divide(%seq_len.33, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3795 : int = aten::Int(%3794), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3796 : int[] = prim::ListConstruct(%3793, %3795, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.36 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3791, %3796), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3798 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.8, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3799 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3800 : int = aten::Int(%3799), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3801 : int[] = prim::ListConstruct(%3800, %3784, %3788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.95 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3798, %3801), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3803 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %padded_value.8 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.95, %3803, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3805 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3806 : int = aten::Int(%3805), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3807 : Long() = aten::add(%chunks_count.24, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3808 : int = aten::Int(%3807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3809 : int[] = prim::ListConstruct(%3806, %3808, %37, %3788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3810 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3811 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.8, %3809, %3810, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
  %3812 : int = aten::size(%chunked_hidden_states.36, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3813 : int = aten::size(%chunked_hidden_states.36, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3814 : int = aten::size(%chunked_hidden_states.36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.8 : Long() = prim::NumToTensor(%3814), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3816 : int = aten::size(%chunked_hidden_states.36, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.8 : Long() = prim::NumToTensor(%3816), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3818 : Long() = aten::add(%window_overlap.8, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
  %3819 : int = aten::Int(%3818), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3820 : int[] = prim::ListConstruct(%51, %3819), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.37 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.36, %3820, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3822 : int[] = prim::ListConstruct(%3812, %3813, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.38 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.37, %3822), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:424:0
  %3824 : Long() = aten::neg(%window_overlap.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:428:0
  %3825 : int = aten::Int(%3824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3826 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.38, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3827 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3826, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.39 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3827, %44, %51, %3825, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3829 : Long() = aten::add(%window_overlap.8, %hidden_dim.8, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:431:0
  %3830 : int = aten::Int(%3829), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3831 : int[] = prim::ListConstruct(%3812, %3813, %3814, %3830), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.40 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.39, %3831), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:430:0
  %3833 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.40, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3834 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3833, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3835 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3834, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3836 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3835, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3837 : Tensor[] = prim::ListConstruct(%3836, %3811), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %context.8 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %3837), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3839 : int[] = prim::ListConstruct(%3782, %3786, %3784, %3788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3840 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.8, %3839), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.15 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3840, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %3842 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.15, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %3843 : int[] = prim::ListConstruct(%3438, %3439, %3440), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3844 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3842, %3843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.16 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3844, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %input.96 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.16, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
  %3847 : __torch__.torch.nn.modules.normalization.___torch_mangle_6271.LayerNorm = prim::GetAttr[name="LayerNorm"](%3414)
  %3848 : __torch__.torch.nn.modules.linear.___torch_mangle_6270.Linear = prim::GetAttr[name="dense"](%3414)
  %3849 : Tensor = prim::GetAttr[name="bias"](%3848)
  %3850 : Tensor = prim::GetAttr[name="weight"](%3848)
  %3851 : Float(768:1, 768:768) = aten::t(%3850), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.96, %3851), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.97 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.46, %3849, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.87 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.97, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.98 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.87, %hidden_states.78, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output # transformers/modeling_longformer.py:758:0
  %3856 : Tensor = prim::GetAttr[name="bias"](%3847)
  %3857 : Tensor = prim::GetAttr[name="weight"](%3847)
  %3858 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.24 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.98, %3858, %3857, %3856, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3860 : __torch__.torch.nn.modules.linear.___torch_mangle_6275.Linear = prim::GetAttr[name="dense"](%3412)
  %3861 : Tensor = prim::GetAttr[name="bias"](%3860)
  %3862 : Tensor = prim::GetAttr[name="weight"](%3860)
  %3863 : Float(768:1, 3072:768) = aten::t(%3862), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.24, %3863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.47, %3861, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.100 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.99), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %3867 : __torch__.torch.nn.modules.normalization.___torch_mangle_6278.LayerNorm = prim::GetAttr[name="LayerNorm"](%3411)
  %3868 : __torch__.torch.nn.modules.linear.___torch_mangle_6277.Linear = prim::GetAttr[name="dense"](%3411)
  %3869 : Tensor = prim::GetAttr[name="bias"](%3868)
  %3870 : Tensor = prim::GetAttr[name="weight"](%3868)
  %3871 : Float(3072:1, 768:3072) = aten::t(%3870), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.100, %3871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.48, %3869, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.88 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.101, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.102 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.88, %input_tensor.24, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output # transformers/modeling_longformer.py:830:0
  %3876 : Tensor = prim::GetAttr[name="bias"](%3867)
  %3877 : Tensor = prim::GetAttr[name="weight"](%3867)
  %3878 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm
  %hidden_states.89 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.102, %3878, %3877, %3876, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %3880 : __torch__.transformers.modeling_longformer.___torch_mangle_6299.LongformerOutput = prim::GetAttr[name="output"](%111)
  %3881 : __torch__.transformers.modeling_longformer.___torch_mangle_6295.LongformerIntermediate = prim::GetAttr[name="intermediate"](%111)
  %3882 : __torch__.transformers.modeling_longformer.___torch_mangle_6293.LongformerAttention = prim::GetAttr[name="attention"](%111)
  %3883 : __torch__.transformers.modeling_longformer.___torch_mangle_6292.LongformerSelfOutput = prim::GetAttr[name="output"](%3882)
  %3884 : __torch__.transformers.modeling_longformer.___torch_mangle_6288.LongformerSelfAttention = prim::GetAttr[name="self"](%3882)
  %3885 : __torch__.torch.nn.modules.linear.___torch_mangle_6284.Linear = prim::GetAttr[name="value"](%3884)
  %3886 : __torch__.torch.nn.modules.linear.___torch_mangle_6283.Linear = prim::GetAttr[name="key"](%3884)
  %3887 : __torch__.torch.nn.modules.linear.___torch_mangle_6282.Linear = prim::GetAttr[name="query"](%3884)
  %3888 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %3889 : Float(17:512, 512:1) = aten::squeeze(%3888, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.9 : Bool(17:512, 512:1) = aten::lt(%3889, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %input.103 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.89, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
  %3892 : Tensor = prim::GetAttr[name="bias"](%3887)
  %3893 : Tensor = prim::GetAttr[name="weight"](%3887)
  %3894 : Float(768:1, 768:768) = aten::t(%3893), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.17 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.49, %3892, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %3897 : Tensor = prim::GetAttr[name="bias"](%3886)
  %3898 : Tensor = prim::GetAttr[name="weight"](%3886)
  %3899 : Float(768:1, 768:768) = aten::t(%3898), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3899), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.50, %3897, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %3902 : Tensor = prim::GetAttr[name="bias"](%3885)
  %3903 : Tensor = prim::GetAttr[name="weight"](%3885)
  %3904 : Float(768:1, 768:768) = aten::t(%3903), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3904), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.51, %3902, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %3907 : int = aten::size(%input.103, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3908 : int = aten::size(%input.103, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3909 : int = aten::size(%input.103, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.18 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.17, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:261:0
  %3911 : int[] = prim::ListConstruct(%3907, %3908, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3912 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.18, %3911), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %query.17 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3912, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %3914 : int[] = prim::ListConstruct(%3907, %3908, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3915 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.9, %3914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %key.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3915, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %3917 : int = aten::size(%query.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.34 : Long() = prim::NumToTensor(%3917), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3919 : int = aten::size(%query.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.35 : Long() = prim::NumToTensor(%3919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3921 : int = aten::size(%query.17, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.25 : Long() = prim::NumToTensor(%3921), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3923 : int = aten::size(%query.17, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %3924 : Long() = aten::floor_divide(%seq_len.35, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.25 : Long() = aten::sub(%3924, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %3926 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.17, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3927 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3928 : int = aten::Int(%3927), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3929 : int[] = prim::ListConstruct(%3928, %3919, %3923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.90 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3926, %3929), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3931 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.9, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3932 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3933 : int = aten::Int(%3932), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3934 : int[] = prim::ListConstruct(%3933, %3919, %3923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.92 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3931, %3934), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3936 : int = aten::size(%hidden_states.90, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3937 : int = aten::size(%hidden_states.90, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3938 : Long() = prim::NumToTensor(%3937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3939 : Long() = aten::floor_divide(%3938, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3940 : int = aten::Int(%3939), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3941 : int = aten::size(%hidden_states.90, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3942 : int[] = prim::ListConstruct(%3936, %3940, %46, %3941), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.91 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.90, %3942), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3944 : int = aten::size(%hidden_states.91, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3945 : int = aten::size(%hidden_states.91, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3946 : Long() = prim::NumToTensor(%3945), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3947 : int = aten::size(%hidden_states.91, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3948 : int = aten::size(%hidden_states.91, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3949 : Long() = aten::mul(%3946, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3950 : Long() = aten::sub(%3949, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3951 : int = aten::Int(%3950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3952 : int[] = prim::ListConstruct(%3944, %3951, %3947, %3948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3953 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3954 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.91, %3952, %3953, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3955 : int = aten::size(%hidden_states.92, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3956 : int = aten::size(%hidden_states.92, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3957 : Long() = prim::NumToTensor(%3956), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3958 : Long() = aten::floor_divide(%3957, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3959 : int = aten::Int(%3958), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3960 : int = aten::size(%hidden_states.92, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3961 : int[] = prim::ListConstruct(%3955, %3959, %46, %3960), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.93 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.92, %3961), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3963 : int = aten::size(%hidden_states.93, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3964 : int = aten::size(%hidden_states.93, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3965 : Long() = prim::NumToTensor(%3964), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3966 : int = aten::size(%hidden_states.93, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3967 : int = aten::size(%hidden_states.93, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3968 : Long() = aten::mul(%3965, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3969 : Long() = aten::sub(%3968, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3970 : int = aten::Int(%3969), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3971 : int[] = prim::ListConstruct(%3963, %3970, %3966, %3967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3972 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3973 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.93, %3971, %3972, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3974 : Tensor[] = prim::ListConstruct(%3954, %3973), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.104 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %3974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %3976 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.17 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.104, %3976, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %3978 : int = aten::size(%hidden_states_padded.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3979 : int = aten::size(%hidden_states_padded.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3980 : int = aten::size(%hidden_states_padded.17, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3981 : int = aten::size(%hidden_states_padded.17, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3982 : int[] = prim::ListConstruct(%3978, %3979, %3980, %3981), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.17 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.17, %3982), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %3984 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %3985 : int = aten::Int(%3984), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3986 : Long() = aten::add(%chunks_count.25, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %3987 : int = aten::Int(%3986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3988 : int[] = prim::ListConstruct(%3985, %3987, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.17 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.17, %3988, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %3990 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3991 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3990, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3992 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3991, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3993 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3992, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3994 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3995 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3994, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3996 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3995, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3997 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3996, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3998 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3999 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3993, %3998), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4000 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3997, %3999, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4001 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4002 : Float(204:262656, 512:513, 513:1) = aten::select(%4001, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4003 : Float(204:262656, 256:513, 513:1) = aten::slice(%4002, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4004 : Float(204:262656, 256:513, 257:1) = aten::slice(%4003, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4005 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4006 : Float(204:262656, 256:513, 513:1) = aten::select(%4005, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4007 : Float(204:262656, 256:513, 513:1) = aten::slice(%4006, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4008 : Float(204:262656, 256:513, 257:1) = aten::slice(%4007, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4009 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4010 : Float(204:262656, 256:513, 257:1) = aten::view(%4004, %4009), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4011 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4008, %4010, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4012 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4013 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4012, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4014 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4013, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4015 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4014, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4016 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4017 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4016, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4018 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4017, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4019 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4018, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4020 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4021 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4015, %4020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4022 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4019, %4021, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4023 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4024 : Float(204:262656, 512:513, 513:1) = aten::select(%4023, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4025 : Float(204:262656, 255:513, 513:1) = aten::slice(%4024, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4026 : Float(204:262656, 255:513, 255:1) = aten::slice(%4025, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4027 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4028 : Float(204:262656, 256:513, 513:1) = aten::select(%4027, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4029 : Float(204:262656, 255:513, 513:1) = aten::slice(%4028, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4030 : Float(204:262656, 255:513, 255:1) = aten::slice(%4029, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4031 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4032 : Float(204:262656, 255:513, 255:1) = aten::view(%4026, %4031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4033 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4030, %4032, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4034 : int[] = prim::ListConstruct(%3917, %3921, %3919, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4035 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.17, %4034), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.25 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4035, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4037 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4038 : Float(256:257, 257:1) = aten::ones(%4037, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4039 : Float(256:257, 257:1) = aten::tril(%4038, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4040 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.17 : Float(256:257, 257:1) = aten::flip(%4039, %4040), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4042 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4043 : Float(1:65792, 256:257, 257:1) = aten::slice(%4042, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4044 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4043, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4044, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4046 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.17, %4046), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4048 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4049 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4048, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4050 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4049, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4050, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4052 : int = aten::size(%beginning_input.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4053 : int = aten::size(%beginning_input.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4054 : int = aten::size(%beginning_input.17, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4055 : int = aten::size(%beginning_input.17, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4056 : int[] = prim::ListConstruct(%4052, %4053, %4054, %4055), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4057 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.17, %4056, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4058 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4057, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4059 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.17, %4058, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4060 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4061 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4060, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4062 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4061, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4062, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4064 : int = aten::size(%ending_input.17, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4065 : int = aten::size(%ending_input.17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4066 : int = aten::size(%ending_input.17, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4067 : int = aten::size(%ending_input.17, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4068 : int[] = prim::ListConstruct(%4064, %4065, %4066, %4067), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4069 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.17, %4068, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4070 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4069, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4071 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.17, %4070, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %4072 : Bool(17:512, 512:1) = aten::ne(%3889, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4073 : Bool(17:512, 512:1) = aten::slice(%4072, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4074 : Bool(17:512, 512:1) = aten::slice(%4073, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4075 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4074, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.9 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4075, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4077 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.9, %query.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.9 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4077, %remove_from_windowed_attention_mask.9, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %4079 : int = aten::size(%float_mask.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4080 : int = aten::size(%float_mask.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4081 : int = aten::size(%float_mask.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4082 : int = aten::size(%float_mask.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4083 : int[] = prim::ListConstruct(%4079, %4080, %4081, %4082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %query.18 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4083, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4085 : int = aten::size(%query.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.35 : Long() = prim::NumToTensor(%4085), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4087 : int = aten::size(%query.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.36 : Long() = prim::NumToTensor(%4087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4089 : int = aten::size(%query.18, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.26 : Long() = prim::NumToTensor(%4089), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4091 : int = aten::size(%query.18, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %4092 : Long() = aten::floor_divide(%seq_len.36, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.26 : Long() = aten::sub(%4092, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %4094 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.18, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4095 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4096 : int = aten::Int(%4095), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4097 : int[] = prim::ListConstruct(%4096, %4087, %4091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.94 : Float(17:512, 512:1, 1:1) = aten::reshape(%4094, %4097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4099 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.9, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4100 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4101 : int = aten::Int(%4100), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4102 : int[] = prim::ListConstruct(%4101, %4087, %4091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.96 : Float(17:512, 512:1, 1:1) = aten::reshape(%4099, %4102), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4104 : int = aten::size(%hidden_states.94, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4105 : int = aten::size(%hidden_states.94, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4106 : Long() = prim::NumToTensor(%4105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4107 : Long() = aten::floor_divide(%4106, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4108 : int = aten::Int(%4107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4109 : int = aten::size(%hidden_states.94, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4110 : int[] = prim::ListConstruct(%4104, %4108, %46, %4109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.95 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.94, %4110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4112 : int = aten::size(%hidden_states.95, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4113 : int = aten::size(%hidden_states.95, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4114 : Long() = prim::NumToTensor(%4113), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4115 : int = aten::size(%hidden_states.95, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4116 : int = aten::size(%hidden_states.95, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4117 : Long() = aten::mul(%4114, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4118 : Long() = aten::sub(%4117, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4119 : int = aten::Int(%4118), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4120 : int[] = prim::ListConstruct(%4112, %4119, %4115, %4116), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4121 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4122 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.95, %4120, %4121, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4123 : int = aten::size(%hidden_states.96, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4124 : int = aten::size(%hidden_states.96, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4125 : Long() = prim::NumToTensor(%4124), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4126 : Long() = aten::floor_divide(%4125, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4127 : int = aten::Int(%4126), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4128 : int = aten::size(%hidden_states.96, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4129 : int[] = prim::ListConstruct(%4123, %4127, %46, %4128), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.97 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.96, %4129), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4131 : int = aten::size(%hidden_states.97, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4132 : int = aten::size(%hidden_states.97, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4133 : Long() = prim::NumToTensor(%4132), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4134 : int = aten::size(%hidden_states.97, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4135 : int = aten::size(%hidden_states.97, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4136 : Long() = aten::mul(%4133, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4137 : Long() = aten::sub(%4136, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4138 : int = aten::Int(%4137), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4139 : int[] = prim::ListConstruct(%4131, %4138, %4134, %4135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4140 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4141 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.97, %4139, %4140, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4142 : Tensor[] = prim::ListConstruct(%4122, %4141), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.105 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %4142), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4144 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.18 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.105, %4144, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4146 : int = aten::size(%hidden_states_padded.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4147 : int = aten::size(%hidden_states_padded.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4148 : int = aten::size(%hidden_states_padded.18, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4149 : int = aten::size(%hidden_states_padded.18, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4150 : int[] = prim::ListConstruct(%4146, %4147, %4148, %4149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.18 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.18, %4150), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %4152 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4153 : int = aten::Int(%4152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4154 : Long() = aten::add(%chunks_count.26, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4155 : int = aten::Int(%4154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4156 : int[] = prim::ListConstruct(%4153, %4155, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.18 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.18, %4156, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %4158 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4159 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4158, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4160 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4159, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4161 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4160, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4162 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4163 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4162, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4164 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4163, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4165 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4164, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4166 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4167 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4161, %4166), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4168 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4165, %4167, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4169 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4170 : Float(17:262656, 512:513, 513:1) = aten::select(%4169, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4171 : Float(17:262656, 256:513, 513:1) = aten::slice(%4170, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4172 : Float(17:262656, 256:513, 257:1) = aten::slice(%4171, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4173 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4174 : Float(17:262656, 256:513, 513:1) = aten::select(%4173, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4175 : Float(17:262656, 256:513, 513:1) = aten::slice(%4174, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4176 : Float(17:262656, 256:513, 257:1) = aten::slice(%4175, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4177 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4178 : Float(17:262656, 256:513, 257:1) = aten::view(%4172, %4177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4179 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4176, %4178, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4180 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4181 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4180, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4182 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4181, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4183 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4182, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4184 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4185 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4184, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4186 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4185, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4187 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4186, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4188 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4189 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4183, %4188), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4190 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4187, %4189, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4191 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4192 : Float(17:262656, 512:513, 513:1) = aten::select(%4191, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4193 : Float(17:262656, 255:513, 513:1) = aten::slice(%4192, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4194 : Float(17:262656, 255:513, 255:1) = aten::slice(%4193, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4195 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4196 : Float(17:262656, 256:513, 513:1) = aten::select(%4195, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4197 : Float(17:262656, 255:513, 513:1) = aten::slice(%4196, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4198 : Float(17:262656, 255:513, 255:1) = aten::slice(%4197, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4199 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4200 : Float(17:262656, 255:513, 255:1) = aten::view(%4194, %4199), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4201 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4198, %4200, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4202 : int[] = prim::ListConstruct(%4085, %4089, %4087, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4203 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.18, %4202), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.26 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4203, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4205 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4206 : Float(256:257, 257:1) = aten::ones(%4205, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4207 : Float(256:257, 257:1) = aten::tril(%4206, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4208 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.18 : Float(256:257, 257:1) = aten::flip(%4207, %4208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4210 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4211 : Float(1:65792, 256:257, 257:1) = aten::slice(%4210, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4212 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4211, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4212, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4214 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.18, %4214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4216 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4217 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4216, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4218 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4217, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4218, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4220 : int = aten::size(%beginning_input.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4221 : int = aten::size(%beginning_input.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4222 : int = aten::size(%beginning_input.18, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4223 : int = aten::size(%beginning_input.18, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4224 : int[] = prim::ListConstruct(%4220, %4221, %4222, %4223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4225 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.18, %4224, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4226 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4225, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4227 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.18, %4226, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4228 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4229 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4228, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4230 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4229, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4230, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4232 : int = aten::size(%ending_input.18, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4233 : int = aten::size(%ending_input.18, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4234 : int = aten::size(%ending_input.18, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4235 : int = aten::size(%ending_input.18, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4236 : int[] = prim::ListConstruct(%4232, %4233, %4234, %4235), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4237 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.18, %4236, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4238 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4237, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4239 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.18, %4238, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.9 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.25, %input_tensor.26, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.9 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.9, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
  %4242 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.9, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4243 : Bool(17:512, 512:1) = aten::slice(%4242, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4244 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4243, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4245 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4244, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %input.106 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.9, %4245, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.18 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.106, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
  %4248 : int[] = prim::ListConstruct(%3907, %3908, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4249 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.9, %4248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %value.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4249, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %4251 : int = aten::size(%value.9, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.36 : Long() = prim::NumToTensor(%4251), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4253 : int = aten::size(%value.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.37 : Long() = prim::NumToTensor(%4253), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4255 : int = aten::size(%value.9, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.27 : Long() = prim::NumToTensor(%4255), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4257 : int = aten::size(%value.9, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %4258 : Long() = aten::floor_divide(%seq_len.37, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.27 : Long() = aten::sub(%4258, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
  %4260 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.18, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4261 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:546:0
  %4262 : int = aten::Int(%4261), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4263 : Long() = aten::floor_divide(%seq_len.37, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4264 : int = aten::Int(%4263), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4265 : int[] = prim::ListConstruct(%4262, %4264, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.41 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4260, %4265), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4267 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.9, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4268 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4269 : int = aten::Int(%4268), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4270 : int[] = prim::ListConstruct(%4269, %4253, %4257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.107 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4267, %4270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4272 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %padded_value.9 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.107, %4272, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4274 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4275 : int = aten::Int(%4274), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4276 : Long() = aten::add(%chunks_count.27, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4277 : int = aten::Int(%4276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4278 : int[] = prim::ListConstruct(%4275, %4277, %37, %4257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4279 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4280 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.9, %4278, %4279, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
  %4281 : int = aten::size(%chunked_hidden_states.41, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4282 : int = aten::size(%chunked_hidden_states.41, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4283 : int = aten::size(%chunked_hidden_states.41, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.9 : Long() = prim::NumToTensor(%4283), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4285 : int = aten::size(%chunked_hidden_states.41, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.9 : Long() = prim::NumToTensor(%4285), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4287 : Long() = aten::add(%window_overlap.9, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
  %4288 : int = aten::Int(%4287), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4289 : int[] = prim::ListConstruct(%51, %4288), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.42 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.41, %4289, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4291 : int[] = prim::ListConstruct(%4281, %4282, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.43 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.42, %4291), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:424:0
  %4293 : Long() = aten::neg(%window_overlap.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:428:0
  %4294 : int = aten::Int(%4293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4295 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.43, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4296 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4295, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.44 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4296, %44, %51, %4294, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4298 : Long() = aten::add(%window_overlap.9, %hidden_dim.9, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:431:0
  %4299 : int = aten::Int(%4298), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4300 : int[] = prim::ListConstruct(%4281, %4282, %4283, %4299), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.45 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.44, %4300), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:430:0
  %4302 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.45, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4303 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4302, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4304 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4303, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4305 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4304, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4306 : Tensor[] = prim::ListConstruct(%4305, %4280), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %context.9 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %4306), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4308 : int[] = prim::ListConstruct(%4251, %4255, %4253, %4257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4309 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.9, %4308), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.17 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4309, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %4311 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.17, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %4312 : int[] = prim::ListConstruct(%3907, %3908, %3909), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4313 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4311, %4312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.18 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4313, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %input.108 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.18, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
  %4316 : __torch__.torch.nn.modules.normalization.___torch_mangle_6290.LayerNorm = prim::GetAttr[name="LayerNorm"](%3883)
  %4317 : __torch__.torch.nn.modules.linear.___torch_mangle_6289.Linear = prim::GetAttr[name="dense"](%3883)
  %4318 : Tensor = prim::GetAttr[name="bias"](%4317)
  %4319 : Tensor = prim::GetAttr[name="weight"](%4317)
  %4320 : Float(768:1, 768:768) = aten::t(%4319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.108, %4320), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.52, %4318, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.98 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.109, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.98, %hidden_states.89, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output # transformers/modeling_longformer.py:758:0
  %4325 : Tensor = prim::GetAttr[name="bias"](%4316)
  %4326 : Tensor = prim::GetAttr[name="weight"](%4316)
  %4327 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.27 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.110, %4327, %4326, %4325, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4329 : __torch__.torch.nn.modules.linear.___torch_mangle_6294.Linear = prim::GetAttr[name="dense"](%3881)
  %4330 : Tensor = prim::GetAttr[name="bias"](%4329)
  %4331 : Tensor = prim::GetAttr[name="weight"](%4329)
  %4332 : Float(768:1, 3072:768) = aten::t(%4331), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.27, %4332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.53, %4330, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %4336 : __torch__.torch.nn.modules.normalization.___torch_mangle_6297.LayerNorm = prim::GetAttr[name="LayerNorm"](%3880)
  %4337 : __torch__.torch.nn.modules.linear.___torch_mangle_6296.Linear = prim::GetAttr[name="dense"](%3880)
  %4338 : Tensor = prim::GetAttr[name="bias"](%4337)
  %4339 : Tensor = prim::GetAttr[name="weight"](%4337)
  %4340 : Float(3072:1, 768:3072) = aten::t(%4339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.112, %4340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.54, %4338, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.99 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.113, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.99, %input_tensor.27, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output # transformers/modeling_longformer.py:830:0
  %4345 : Tensor = prim::GetAttr[name="bias"](%4336)
  %4346 : Tensor = prim::GetAttr[name="weight"](%4336)
  %4347 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm
  %hidden_states.100 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.114, %4347, %4346, %4345, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %4349 : __torch__.transformers.modeling_longformer.___torch_mangle_6318.LongformerOutput = prim::GetAttr[name="output"](%109)
  %4350 : __torch__.transformers.modeling_longformer.___torch_mangle_6314.LongformerIntermediate = prim::GetAttr[name="intermediate"](%109)
  %4351 : __torch__.transformers.modeling_longformer.___torch_mangle_6312.LongformerAttention = prim::GetAttr[name="attention"](%109)
  %4352 : __torch__.transformers.modeling_longformer.___torch_mangle_6311.LongformerSelfOutput = prim::GetAttr[name="output"](%4351)
  %4353 : __torch__.transformers.modeling_longformer.___torch_mangle_6307.LongformerSelfAttention = prim::GetAttr[name="self"](%4351)
  %4354 : __torch__.torch.nn.modules.linear.___torch_mangle_6303.Linear = prim::GetAttr[name="value"](%4353)
  %4355 : __torch__.torch.nn.modules.linear.___torch_mangle_6302.Linear = prim::GetAttr[name="key"](%4353)
  %4356 : __torch__.torch.nn.modules.linear.___torch_mangle_6301.Linear = prim::GetAttr[name="query"](%4353)
  %4357 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %4358 : Float(17:512, 512:1) = aten::squeeze(%4357, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.10 : Bool(17:512, 512:1) = aten::lt(%4358, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %input.115 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.100, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
  %4361 : Tensor = prim::GetAttr[name="bias"](%4356)
  %4362 : Tensor = prim::GetAttr[name="weight"](%4356)
  %4363 : Float(768:1, 768:768) = aten::t(%4362), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.19 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.55, %4361, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %4366 : Tensor = prim::GetAttr[name="bias"](%4355)
  %4367 : Tensor = prim::GetAttr[name="weight"](%4355)
  %4368 : Float(768:1, 768:768) = aten::t(%4367), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4368), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.56, %4366, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %4371 : Tensor = prim::GetAttr[name="bias"](%4354)
  %4372 : Tensor = prim::GetAttr[name="weight"](%4354)
  %4373 : Float(768:1, 768:768) = aten::t(%4372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4373), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.57, %4371, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %4376 : int = aten::size(%input.115, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4377 : int = aten::size(%input.115, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4378 : int = aten::size(%input.115, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.20 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.19, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:261:0
  %4380 : int[] = prim::ListConstruct(%4376, %4377, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4381 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.20, %4380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %query.19 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4381, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %4383 : int[] = prim::ListConstruct(%4376, %4377, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4384 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.10, %4383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %key.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4384, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %4386 : int = aten::size(%query.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.38 : Long() = prim::NumToTensor(%4386), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4388 : int = aten::size(%query.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.39 : Long() = prim::NumToTensor(%4388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4390 : int = aten::size(%query.19, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.28 : Long() = prim::NumToTensor(%4390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4392 : int = aten::size(%query.19, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4393 : Long() = aten::floor_divide(%seq_len.39, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.28 : Long() = aten::sub(%4393, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4395 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.19, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4396 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4397 : int = aten::Int(%4396), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4398 : int[] = prim::ListConstruct(%4397, %4388, %4392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.101 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4395, %4398), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4400 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.10, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4401 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4402 : int = aten::Int(%4401), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4403 : int[] = prim::ListConstruct(%4402, %4388, %4392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.103 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4400, %4403), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4405 : int = aten::size(%hidden_states.101, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4406 : int = aten::size(%hidden_states.101, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4407 : Long() = prim::NumToTensor(%4406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4408 : Long() = aten::floor_divide(%4407, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4409 : int = aten::Int(%4408), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4410 : int = aten::size(%hidden_states.101, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4411 : int[] = prim::ListConstruct(%4405, %4409, %46, %4410), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.102 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.101, %4411), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4413 : int = aten::size(%hidden_states.102, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4414 : int = aten::size(%hidden_states.102, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4415 : Long() = prim::NumToTensor(%4414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4416 : int = aten::size(%hidden_states.102, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4417 : int = aten::size(%hidden_states.102, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4418 : Long() = aten::mul(%4415, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4419 : Long() = aten::sub(%4418, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4420 : int = aten::Int(%4419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4421 : int[] = prim::ListConstruct(%4413, %4420, %4416, %4417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4422 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4423 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.102, %4421, %4422, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4424 : int = aten::size(%hidden_states.103, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4425 : int = aten::size(%hidden_states.103, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4426 : Long() = prim::NumToTensor(%4425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4427 : Long() = aten::floor_divide(%4426, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4428 : int = aten::Int(%4427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4429 : int = aten::size(%hidden_states.103, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4430 : int[] = prim::ListConstruct(%4424, %4428, %46, %4429), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.104 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.103, %4430), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4432 : int = aten::size(%hidden_states.104, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4433 : int = aten::size(%hidden_states.104, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4434 : Long() = prim::NumToTensor(%4433), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4435 : int = aten::size(%hidden_states.104, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4436 : int = aten::size(%hidden_states.104, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4437 : Long() = aten::mul(%4434, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4438 : Long() = aten::sub(%4437, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4439 : int = aten::Int(%4438), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4440 : int[] = prim::ListConstruct(%4432, %4439, %4435, %4436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4441 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4442 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.104, %4440, %4441, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4443 : Tensor[] = prim::ListConstruct(%4423, %4442), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.116 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %4443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4445 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.19 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.116, %4445, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4447 : int = aten::size(%hidden_states_padded.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4448 : int = aten::size(%hidden_states_padded.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4449 : int = aten::size(%hidden_states_padded.19, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4450 : int = aten::size(%hidden_states_padded.19, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4451 : int[] = prim::ListConstruct(%4447, %4448, %4449, %4450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.19 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.19, %4451), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4453 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4454 : int = aten::Int(%4453), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4455 : Long() = aten::add(%chunks_count.28, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4456 : int = aten::Int(%4455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4457 : int[] = prim::ListConstruct(%4454, %4456, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.19 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.19, %4457, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4459 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4460 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4459, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4461 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4460, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4462 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4461, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4463 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4464 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4463, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4465 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4464, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4466 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4465, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4467 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4468 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4462, %4467), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4469 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4466, %4468, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4470 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4471 : Float(204:262656, 512:513, 513:1) = aten::select(%4470, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4472 : Float(204:262656, 256:513, 513:1) = aten::slice(%4471, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4473 : Float(204:262656, 256:513, 257:1) = aten::slice(%4472, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4474 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4475 : Float(204:262656, 256:513, 513:1) = aten::select(%4474, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4476 : Float(204:262656, 256:513, 513:1) = aten::slice(%4475, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4477 : Float(204:262656, 256:513, 257:1) = aten::slice(%4476, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4478 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4479 : Float(204:262656, 256:513, 257:1) = aten::view(%4473, %4478), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4480 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4477, %4479, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4481 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4482 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4481, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4483 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4482, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4484 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4483, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4485 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4486 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4485, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4487 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4486, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4488 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4487, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4489 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4490 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4484, %4489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4491 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4488, %4490, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4492 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4493 : Float(204:262656, 512:513, 513:1) = aten::select(%4492, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4494 : Float(204:262656, 255:513, 513:1) = aten::slice(%4493, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4495 : Float(204:262656, 255:513, 255:1) = aten::slice(%4494, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4496 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4497 : Float(204:262656, 256:513, 513:1) = aten::select(%4496, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4498 : Float(204:262656, 255:513, 513:1) = aten::slice(%4497, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4499 : Float(204:262656, 255:513, 255:1) = aten::slice(%4498, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4500 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4501 : Float(204:262656, 255:513, 255:1) = aten::view(%4495, %4500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4502 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4499, %4501, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4503 : int[] = prim::ListConstruct(%4386, %4390, %4388, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4504 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.19, %4503), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.28 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4504, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4506 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4507 : Float(256:257, 257:1) = aten::ones(%4506, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4508 : Float(256:257, 257:1) = aten::tril(%4507, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4509 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.19 : Float(256:257, 257:1) = aten::flip(%4508, %4509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4511 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4512 : Float(1:65792, 256:257, 257:1) = aten::slice(%4511, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4513 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4512, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4513, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4515 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.19, %4515), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4517 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4518 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4517, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4519 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4518, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4519, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4521 : int = aten::size(%beginning_input.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4522 : int = aten::size(%beginning_input.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4523 : int = aten::size(%beginning_input.19, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4524 : int = aten::size(%beginning_input.19, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4525 : int[] = prim::ListConstruct(%4521, %4522, %4523, %4524), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4526 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.19, %4525, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4527 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4526, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4528 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.19, %4527, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4529 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4530 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4529, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4531 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4530, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4531, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4533 : int = aten::size(%ending_input.19, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4534 : int = aten::size(%ending_input.19, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4535 : int = aten::size(%ending_input.19, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4536 : int = aten::size(%ending_input.19, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4537 : int[] = prim::ListConstruct(%4533, %4534, %4535, %4536), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4538 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.19, %4537, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4539 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4538, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4540 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.19, %4539, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %4541 : Bool(17:512, 512:1) = aten::ne(%4358, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4542 : Bool(17:512, 512:1) = aten::slice(%4541, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4543 : Bool(17:512, 512:1) = aten::slice(%4542, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4544 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4543, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.10 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4544, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4546 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.10, %query.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4546, %remove_from_windowed_attention_mask.10, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %4548 : int = aten::size(%float_mask.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4549 : int = aten::size(%float_mask.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4550 : int = aten::size(%float_mask.10, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4551 : int = aten::size(%float_mask.10, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4552 : int[] = prim::ListConstruct(%4548, %4549, %4550, %4551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %query.20 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4552, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4554 : int = aten::size(%query.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.39 : Long() = prim::NumToTensor(%4554), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4556 : int = aten::size(%query.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.40 : Long() = prim::NumToTensor(%4556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4558 : int = aten::size(%query.20, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.29 : Long() = prim::NumToTensor(%4558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4560 : int = aten::size(%query.20, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4561 : Long() = aten::floor_divide(%seq_len.40, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.29 : Long() = aten::sub(%4561, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4563 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.20, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4564 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4565 : int = aten::Int(%4564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4566 : int[] = prim::ListConstruct(%4565, %4556, %4560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.105 : Float(17:512, 512:1, 1:1) = aten::reshape(%4563, %4566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4568 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.10, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4569 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4570 : int = aten::Int(%4569), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4571 : int[] = prim::ListConstruct(%4570, %4556, %4560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.107 : Float(17:512, 512:1, 1:1) = aten::reshape(%4568, %4571), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4573 : int = aten::size(%hidden_states.105, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4574 : int = aten::size(%hidden_states.105, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4575 : Long() = prim::NumToTensor(%4574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4576 : Long() = aten::floor_divide(%4575, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4577 : int = aten::Int(%4576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4578 : int = aten::size(%hidden_states.105, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4579 : int[] = prim::ListConstruct(%4573, %4577, %46, %4578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.106 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.105, %4579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4581 : int = aten::size(%hidden_states.106, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4582 : int = aten::size(%hidden_states.106, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4583 : Long() = prim::NumToTensor(%4582), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4584 : int = aten::size(%hidden_states.106, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4585 : int = aten::size(%hidden_states.106, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4586 : Long() = aten::mul(%4583, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4587 : Long() = aten::sub(%4586, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4588 : int = aten::Int(%4587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4589 : int[] = prim::ListConstruct(%4581, %4588, %4584, %4585), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4590 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4591 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.106, %4589, %4590, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4592 : int = aten::size(%hidden_states.107, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4593 : int = aten::size(%hidden_states.107, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4594 : Long() = prim::NumToTensor(%4593), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4595 : Long() = aten::floor_divide(%4594, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4596 : int = aten::Int(%4595), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4597 : int = aten::size(%hidden_states.107, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4598 : int[] = prim::ListConstruct(%4592, %4596, %46, %4597), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.108 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.107, %4598), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4600 : int = aten::size(%hidden_states.108, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4601 : int = aten::size(%hidden_states.108, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4602 : Long() = prim::NumToTensor(%4601), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4603 : int = aten::size(%hidden_states.108, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4604 : int = aten::size(%hidden_states.108, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4605 : Long() = aten::mul(%4602, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4606 : Long() = aten::sub(%4605, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4607 : int = aten::Int(%4606), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4608 : int[] = prim::ListConstruct(%4600, %4607, %4603, %4604), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4609 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4610 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.108, %4608, %4609, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4611 : Tensor[] = prim::ListConstruct(%4591, %4610), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.117 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %4611), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4613 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.20 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.117, %4613, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4615 : int = aten::size(%hidden_states_padded.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4616 : int = aten::size(%hidden_states_padded.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4617 : int = aten::size(%hidden_states_padded.20, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4618 : int = aten::size(%hidden_states_padded.20, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4619 : int[] = prim::ListConstruct(%4615, %4616, %4617, %4618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.20 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.20, %4619), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4621 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4622 : int = aten::Int(%4621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4623 : Long() = aten::add(%chunks_count.29, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4624 : int = aten::Int(%4623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4625 : int[] = prim::ListConstruct(%4622, %4624, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.20 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.20, %4625, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4627 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4628 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4627, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4629 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4628, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4630 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4629, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4631 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4632 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4631, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4633 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4632, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4634 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4633, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4635 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4636 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4630, %4635), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4637 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4634, %4636, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4638 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4639 : Float(17:262656, 512:513, 513:1) = aten::select(%4638, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4640 : Float(17:262656, 256:513, 513:1) = aten::slice(%4639, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4641 : Float(17:262656, 256:513, 257:1) = aten::slice(%4640, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4642 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4643 : Float(17:262656, 256:513, 513:1) = aten::select(%4642, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4644 : Float(17:262656, 256:513, 513:1) = aten::slice(%4643, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4645 : Float(17:262656, 256:513, 257:1) = aten::slice(%4644, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4646 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4647 : Float(17:262656, 256:513, 257:1) = aten::view(%4641, %4646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4648 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4645, %4647, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4649 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4650 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4649, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4651 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4650, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4652 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4651, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4653 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4654 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4653, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4655 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4654, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4656 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4655, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4657 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4658 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4652, %4657), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4659 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4656, %4658, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4660 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4661 : Float(17:262656, 512:513, 513:1) = aten::select(%4660, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4662 : Float(17:262656, 255:513, 513:1) = aten::slice(%4661, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4663 : Float(17:262656, 255:513, 255:1) = aten::slice(%4662, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4664 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4665 : Float(17:262656, 256:513, 513:1) = aten::select(%4664, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4666 : Float(17:262656, 255:513, 513:1) = aten::slice(%4665, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4667 : Float(17:262656, 255:513, 255:1) = aten::slice(%4666, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4668 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4669 : Float(17:262656, 255:513, 255:1) = aten::view(%4663, %4668), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4670 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4667, %4669, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4671 : int[] = prim::ListConstruct(%4554, %4558, %4556, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4672 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.20, %4671), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.29 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4672, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4674 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4675 : Float(256:257, 257:1) = aten::ones(%4674, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4676 : Float(256:257, 257:1) = aten::tril(%4675, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4677 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.20 : Float(256:257, 257:1) = aten::flip(%4676, %4677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4679 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4680 : Float(1:65792, 256:257, 257:1) = aten::slice(%4679, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4681 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4680, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4681, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4683 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.20, %4683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4685 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4686 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4685, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4687 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4686, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4687, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4689 : int = aten::size(%beginning_input.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4690 : int = aten::size(%beginning_input.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4691 : int = aten::size(%beginning_input.20, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4692 : int = aten::size(%beginning_input.20, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4693 : int[] = prim::ListConstruct(%4689, %4690, %4691, %4692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4694 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.20, %4693, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4695 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4694, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4696 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.20, %4695, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4697 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4698 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4697, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4699 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4698, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4699, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4701 : int = aten::size(%ending_input.20, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4702 : int = aten::size(%ending_input.20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4703 : int = aten::size(%ending_input.20, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4704 : int = aten::size(%ending_input.20, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4705 : int[] = prim::ListConstruct(%4701, %4702, %4703, %4704), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4706 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.20, %4705, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4707 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4706, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4708 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.20, %4707, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.28, %input_tensor.29, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.10, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
  %4711 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.10, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4712 : Bool(17:512, 512:1) = aten::slice(%4711, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4713 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4712, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4714 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4713, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %input.118 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.10, %4714, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.20 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.118, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
  %4717 : int[] = prim::ListConstruct(%4376, %4377, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4718 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.10, %4717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %value.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4718, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %4720 : int = aten::size(%value.10, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.40 : Long() = prim::NumToTensor(%4720), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4722 : int = aten::size(%value.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.41 : Long() = prim::NumToTensor(%4722), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4724 : int = aten::size(%value.10, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.30 : Long() = prim::NumToTensor(%4724), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4726 : int = aten::size(%value.10, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %4727 : Long() = aten::floor_divide(%seq_len.41, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.30 : Long() = aten::sub(%4727, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
  %4729 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.20, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4730 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:546:0
  %4731 : int = aten::Int(%4730), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4732 : Long() = aten::floor_divide(%seq_len.41, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4733 : int = aten::Int(%4732), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4734 : int[] = prim::ListConstruct(%4731, %4733, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.46 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4729, %4734), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4736 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.10, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4737 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4738 : int = aten::Int(%4737), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4739 : int[] = prim::ListConstruct(%4738, %4722, %4726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.119 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4736, %4739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4741 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %padded_value.10 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.119, %4741, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4743 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4744 : int = aten::Int(%4743), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4745 : Long() = aten::add(%chunks_count.30, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4746 : int = aten::Int(%4745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4747 : int[] = prim::ListConstruct(%4744, %4746, %37, %4726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4748 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4749 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.10, %4747, %4748, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
  %4750 : int = aten::size(%chunked_hidden_states.46, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4751 : int = aten::size(%chunked_hidden_states.46, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4752 : int = aten::size(%chunked_hidden_states.46, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.10 : Long() = prim::NumToTensor(%4752), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4754 : int = aten::size(%chunked_hidden_states.46, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.10 : Long() = prim::NumToTensor(%4754), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4756 : Long() = aten::add(%window_overlap.10, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
  %4757 : int = aten::Int(%4756), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4758 : int[] = prim::ListConstruct(%51, %4757), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.47 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.46, %4758, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4760 : int[] = prim::ListConstruct(%4750, %4751, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.48 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.47, %4760), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:424:0
  %4762 : Long() = aten::neg(%window_overlap.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:428:0
  %4763 : int = aten::Int(%4762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4764 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.48, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4765 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4764, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.49 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4765, %44, %51, %4763, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4767 : Long() = aten::add(%window_overlap.10, %hidden_dim.10, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:431:0
  %4768 : int = aten::Int(%4767), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4769 : int[] = prim::ListConstruct(%4750, %4751, %4752, %4768), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.50 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.49, %4769), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:430:0
  %4771 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.50, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4772 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4771, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4773 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4772, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4774 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4773, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4775 : Tensor[] = prim::ListConstruct(%4774, %4749), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %context.10 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %4775), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4777 : int[] = prim::ListConstruct(%4720, %4724, %4722, %4726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4778 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.10, %4777), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.19 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4778, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %4780 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.19, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %4781 : int[] = prim::ListConstruct(%4376, %4377, %4378), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4782 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4780, %4781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.20 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4782, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %input.120 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.20, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
  %4785 : __torch__.torch.nn.modules.normalization.___torch_mangle_6309.LayerNorm = prim::GetAttr[name="LayerNorm"](%4352)
  %4786 : __torch__.torch.nn.modules.linear.___torch_mangle_6308.Linear = prim::GetAttr[name="dense"](%4352)
  %4787 : Tensor = prim::GetAttr[name="bias"](%4786)
  %4788 : Tensor = prim::GetAttr[name="weight"](%4786)
  %4789 : Float(768:1, 768:768) = aten::t(%4788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.120, %4789), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.58, %4787, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.109 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.121, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.122 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.109, %hidden_states.100, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output # transformers/modeling_longformer.py:758:0
  %4794 : Tensor = prim::GetAttr[name="bias"](%4785)
  %4795 : Tensor = prim::GetAttr[name="weight"](%4785)
  %4796 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.30 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.122, %4796, %4795, %4794, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4798 : __torch__.torch.nn.modules.linear.___torch_mangle_6313.Linear = prim::GetAttr[name="dense"](%4350)
  %4799 : Tensor = prim::GetAttr[name="bias"](%4798)
  %4800 : Tensor = prim::GetAttr[name="weight"](%4798)
  %4801 : Float(768:1, 3072:768) = aten::t(%4800), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.30, %4801), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.59, %4799, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.124 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.123), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %4805 : __torch__.torch.nn.modules.normalization.___torch_mangle_6316.LayerNorm = prim::GetAttr[name="LayerNorm"](%4349)
  %4806 : __torch__.torch.nn.modules.linear.___torch_mangle_6315.Linear = prim::GetAttr[name="dense"](%4349)
  %4807 : Tensor = prim::GetAttr[name="bias"](%4806)
  %4808 : Tensor = prim::GetAttr[name="weight"](%4806)
  %4809 : Float(3072:1, 768:3072) = aten::t(%4808), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.124, %4809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.125 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.60, %4807, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.110 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.125, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.126 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.110, %input_tensor.30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output # transformers/modeling_longformer.py:830:0
  %4814 : Tensor = prim::GetAttr[name="bias"](%4805)
  %4815 : Tensor = prim::GetAttr[name="weight"](%4805)
  %4816 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm
  %hidden_states.111 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.126, %4816, %4815, %4814, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %4818 : __torch__.transformers.modeling_longformer.___torch_mangle_6337.LongformerOutput = prim::GetAttr[name="output"](%107)
  %4819 : __torch__.transformers.modeling_longformer.___torch_mangle_6333.LongformerIntermediate = prim::GetAttr[name="intermediate"](%107)
  %4820 : __torch__.transformers.modeling_longformer.___torch_mangle_6331.LongformerAttention = prim::GetAttr[name="attention"](%107)
  %4821 : __torch__.transformers.modeling_longformer.___torch_mangle_6330.LongformerSelfOutput = prim::GetAttr[name="output"](%4820)
  %4822 : __torch__.transformers.modeling_longformer.___torch_mangle_6326.LongformerSelfAttention = prim::GetAttr[name="self"](%4820)
  %4823 : __torch__.torch.nn.modules.linear.___torch_mangle_6322.Linear = prim::GetAttr[name="value"](%4822)
  %4824 : __torch__.torch.nn.modules.linear.___torch_mangle_6321.Linear = prim::GetAttr[name="key"](%4822)
  %4825 : __torch__.torch.nn.modules.linear.___torch_mangle_6320.Linear = prim::GetAttr[name="query"](%4822)
  %4826 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %4827 : Float(17:512, 512:1) = aten::squeeze(%4826, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.11 : Bool(17:512, 512:1) = aten::lt(%4827, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %input.127 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.111, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
  %4830 : Tensor = prim::GetAttr[name="bias"](%4825)
  %4831 : Tensor = prim::GetAttr[name="weight"](%4825)
  %4832 : Float(768:1, 768:768) = aten::t(%4831), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.21 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.61, %4830, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %4835 : Tensor = prim::GetAttr[name="bias"](%4824)
  %4836 : Tensor = prim::GetAttr[name="weight"](%4824)
  %4837 : Float(768:1, 768:768) = aten::t(%4836), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4837), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.62, %4835, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %4840 : Tensor = prim::GetAttr[name="bias"](%4823)
  %4841 : Tensor = prim::GetAttr[name="weight"](%4823)
  %4842 : Float(768:1, 768:768) = aten::t(%4841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4842), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.63, %4840, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %4845 : int = aten::size(%input.127, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4846 : int = aten::size(%input.127, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4847 : int = aten::size(%input.127, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.22 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.21, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:261:0
  %4849 : int[] = prim::ListConstruct(%4845, %4846, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4850 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.22, %4849), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %query.21 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4850, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %4852 : int[] = prim::ListConstruct(%4845, %4846, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4853 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.11, %4852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %key.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4853, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %4855 : int = aten::size(%query.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.42 : Long() = prim::NumToTensor(%4855), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4857 : int = aten::size(%query.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.43 : Long() = prim::NumToTensor(%4857), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4859 : int = aten::size(%query.21, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.31 : Long() = prim::NumToTensor(%4859), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4861 : int = aten::size(%query.21, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %4862 : Long() = aten::floor_divide(%seq_len.43, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.31 : Long() = aten::sub(%4862, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %4864 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.21, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4865 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4866 : int = aten::Int(%4865), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4867 : int[] = prim::ListConstruct(%4866, %4857, %4861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.112 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4864, %4867), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4869 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.11, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4870 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4871 : int = aten::Int(%4870), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4872 : int[] = prim::ListConstruct(%4871, %4857, %4861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.114 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4869, %4872), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4874 : int = aten::size(%hidden_states.112, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4875 : int = aten::size(%hidden_states.112, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4876 : Long() = prim::NumToTensor(%4875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4877 : Long() = aten::floor_divide(%4876, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4878 : int = aten::Int(%4877), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4879 : int = aten::size(%hidden_states.112, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4880 : int[] = prim::ListConstruct(%4874, %4878, %46, %4879), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.113 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.112, %4880), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4882 : int = aten::size(%hidden_states.113, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4883 : int = aten::size(%hidden_states.113, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4884 : Long() = prim::NumToTensor(%4883), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4885 : int = aten::size(%hidden_states.113, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4886 : int = aten::size(%hidden_states.113, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4887 : Long() = aten::mul(%4884, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4888 : Long() = aten::sub(%4887, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4889 : int = aten::Int(%4888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4890 : int[] = prim::ListConstruct(%4882, %4889, %4885, %4886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4891 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4892 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.113, %4890, %4891, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4893 : int = aten::size(%hidden_states.114, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4894 : int = aten::size(%hidden_states.114, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4895 : Long() = prim::NumToTensor(%4894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4896 : Long() = aten::floor_divide(%4895, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4897 : int = aten::Int(%4896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4898 : int = aten::size(%hidden_states.114, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4899 : int[] = prim::ListConstruct(%4893, %4897, %46, %4898), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.115 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.114, %4899), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4901 : int = aten::size(%hidden_states.115, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4902 : int = aten::size(%hidden_states.115, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4903 : Long() = prim::NumToTensor(%4902), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4904 : int = aten::size(%hidden_states.115, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4905 : int = aten::size(%hidden_states.115, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4906 : Long() = aten::mul(%4903, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4907 : Long() = aten::sub(%4906, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4908 : int = aten::Int(%4907), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4909 : int[] = prim::ListConstruct(%4901, %4908, %4904, %4905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4910 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4911 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.115, %4909, %4910, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4912 : Tensor[] = prim::ListConstruct(%4892, %4911), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.128 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %4912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %4914 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.21 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.128, %4914, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %4916 : int = aten::size(%hidden_states_padded.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4917 : int = aten::size(%hidden_states_padded.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4918 : int = aten::size(%hidden_states_padded.21, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4919 : int = aten::size(%hidden_states_padded.21, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4920 : int[] = prim::ListConstruct(%4916, %4917, %4918, %4919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.21 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.21, %4920), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %4922 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4923 : int = aten::Int(%4922), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4924 : Long() = aten::add(%chunks_count.31, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4925 : int = aten::Int(%4924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4926 : int[] = prim::ListConstruct(%4923, %4925, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.21, %4926, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %4928 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4929 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4928, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4930 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4929, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4931 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4930, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4932 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4933 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4932, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4934 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4933, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4935 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4934, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4936 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4937 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4931, %4936), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4938 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4935, %4937, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4939 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4940 : Float(204:262656, 512:513, 513:1) = aten::select(%4939, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4941 : Float(204:262656, 256:513, 513:1) = aten::slice(%4940, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4942 : Float(204:262656, 256:513, 257:1) = aten::slice(%4941, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4943 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4944 : Float(204:262656, 256:513, 513:1) = aten::select(%4943, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4945 : Float(204:262656, 256:513, 513:1) = aten::slice(%4944, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4946 : Float(204:262656, 256:513, 257:1) = aten::slice(%4945, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4947 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4948 : Float(204:262656, 256:513, 257:1) = aten::view(%4942, %4947), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4949 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4946, %4948, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4950 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4951 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4950, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4952 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4951, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4953 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4952, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4954 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4955 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4954, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4956 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4955, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4957 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4956, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4958 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4959 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4953, %4958), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4960 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4957, %4959, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4961 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4962 : Float(204:262656, 512:513, 513:1) = aten::select(%4961, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4963 : Float(204:262656, 255:513, 513:1) = aten::slice(%4962, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4964 : Float(204:262656, 255:513, 255:1) = aten::slice(%4963, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4965 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4966 : Float(204:262656, 256:513, 513:1) = aten::select(%4965, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4967 : Float(204:262656, 255:513, 513:1) = aten::slice(%4966, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4968 : Float(204:262656, 255:513, 255:1) = aten::slice(%4967, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4969 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4970 : Float(204:262656, 255:513, 255:1) = aten::view(%4964, %4969), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4971 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4968, %4970, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4972 : int[] = prim::ListConstruct(%4855, %4859, %4857, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4973 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.21, %4972), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.31 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4973, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %4975 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4976 : Float(256:257, 257:1) = aten::ones(%4975, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4977 : Float(256:257, 257:1) = aten::tril(%4976, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4978 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.21 : Float(256:257, 257:1) = aten::flip(%4977, %4978), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4980 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4981 : Float(1:65792, 256:257, 257:1) = aten::slice(%4980, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4982 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4981, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4982, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4984 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.21, %4984), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %4986 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4987 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4986, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4988 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4987, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4988, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4990 : int = aten::size(%beginning_input.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4991 : int = aten::size(%beginning_input.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4992 : int = aten::size(%beginning_input.21, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4993 : int = aten::size(%beginning_input.21, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4994 : int[] = prim::ListConstruct(%4990, %4991, %4992, %4993), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4995 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.21, %4994, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4996 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4995, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %4997 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.21, %4996, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %4998 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %4999 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4998, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5000 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4999, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5000, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5002 : int = aten::size(%ending_input.21, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5003 : int = aten::size(%ending_input.21, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5004 : int = aten::size(%ending_input.21, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5005 : int = aten::size(%ending_input.21, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5006 : int[] = prim::ListConstruct(%5002, %5003, %5004, %5005), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5007 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.21, %5006, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5008 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5007, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5009 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.21, %5008, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %5010 : Bool(17:512, 512:1) = aten::ne(%4827, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5011 : Bool(17:512, 512:1) = aten::slice(%5010, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5012 : Bool(17:512, 512:1) = aten::slice(%5011, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5013 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5012, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.11 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5013, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5015 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.11, %query.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.11 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5015, %remove_from_windowed_attention_mask.11, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %5017 : int = aten::size(%float_mask.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5018 : int = aten::size(%float_mask.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5019 : int = aten::size(%float_mask.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5020 : int = aten::size(%float_mask.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5021 : int[] = prim::ListConstruct(%5017, %5018, %5019, %5020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %query.22 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5021, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5023 : int = aten::size(%query.22, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.43 : Long() = prim::NumToTensor(%5023), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5025 : int = aten::size(%query.22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.44 : Long() = prim::NumToTensor(%5025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5027 : int = aten::size(%query.22, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.32 : Long() = prim::NumToTensor(%5027), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5029 : int = aten::size(%query.22, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %5030 : Long() = aten::floor_divide(%seq_len.44, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.32 : Long() = aten::sub(%5030, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %5032 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.22, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5033 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5034 : int = aten::Int(%5033), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5035 : int[] = prim::ListConstruct(%5034, %5025, %5029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.116 : Float(17:512, 512:1, 1:1) = aten::reshape(%5032, %5035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5037 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.11, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5038 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5039 : int = aten::Int(%5038), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5040 : int[] = prim::ListConstruct(%5039, %5025, %5029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.118 : Float(17:512, 512:1, 1:1) = aten::reshape(%5037, %5040), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5042 : int = aten::size(%hidden_states.116, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5043 : int = aten::size(%hidden_states.116, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5044 : Long() = prim::NumToTensor(%5043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5045 : Long() = aten::floor_divide(%5044, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5046 : int = aten::Int(%5045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5047 : int = aten::size(%hidden_states.116, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5048 : int[] = prim::ListConstruct(%5042, %5046, %46, %5047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.117 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.116, %5048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5050 : int = aten::size(%hidden_states.117, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5051 : int = aten::size(%hidden_states.117, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5052 : Long() = prim::NumToTensor(%5051), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5053 : int = aten::size(%hidden_states.117, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5054 : int = aten::size(%hidden_states.117, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5055 : Long() = aten::mul(%5052, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5056 : Long() = aten::sub(%5055, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5057 : int = aten::Int(%5056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5058 : int[] = prim::ListConstruct(%5050, %5057, %5053, %5054), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5059 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5060 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.117, %5058, %5059, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5061 : int = aten::size(%hidden_states.118, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5062 : int = aten::size(%hidden_states.118, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5063 : Long() = prim::NumToTensor(%5062), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5064 : Long() = aten::floor_divide(%5063, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5065 : int = aten::Int(%5064), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5066 : int = aten::size(%hidden_states.118, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5067 : int[] = prim::ListConstruct(%5061, %5065, %46, %5066), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.119 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.118, %5067), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5069 : int = aten::size(%hidden_states.119, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5070 : int = aten::size(%hidden_states.119, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5071 : Long() = prim::NumToTensor(%5070), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5072 : int = aten::size(%hidden_states.119, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5073 : int = aten::size(%hidden_states.119, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5074 : Long() = aten::mul(%5071, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5075 : Long() = aten::sub(%5074, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5076 : int = aten::Int(%5075), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5077 : int[] = prim::ListConstruct(%5069, %5076, %5072, %5073), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5078 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5079 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.119, %5077, %5078, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5080 : Tensor[] = prim::ListConstruct(%5060, %5079), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.129 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %5080), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5082 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.22 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.129, %5082, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5084 : int = aten::size(%hidden_states_padded.22, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5085 : int = aten::size(%hidden_states_padded.22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5086 : int = aten::size(%hidden_states_padded.22, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5087 : int = aten::size(%hidden_states_padded.22, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5088 : int[] = prim::ListConstruct(%5084, %5085, %5086, %5087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.22 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.22, %5088), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %5090 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5091 : int = aten::Int(%5090), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5092 : Long() = aten::add(%chunks_count.32, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5093 : int = aten::Int(%5092), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5094 : int[] = prim::ListConstruct(%5091, %5093, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.22 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.22, %5094, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %5096 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5097 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5096, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5098 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5097, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5099 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5098, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5100 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5101 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5100, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5102 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5101, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5103 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5102, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5104 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5105 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5099, %5104), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5106 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5103, %5105, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5107 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5108 : Float(17:262656, 512:513, 513:1) = aten::select(%5107, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5109 : Float(17:262656, 256:513, 513:1) = aten::slice(%5108, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5110 : Float(17:262656, 256:513, 257:1) = aten::slice(%5109, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5111 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5112 : Float(17:262656, 256:513, 513:1) = aten::select(%5111, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5113 : Float(17:262656, 256:513, 513:1) = aten::slice(%5112, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5114 : Float(17:262656, 256:513, 257:1) = aten::slice(%5113, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5115 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5116 : Float(17:262656, 256:513, 257:1) = aten::view(%5110, %5115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5117 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5114, %5116, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5118 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5119 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5118, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5120 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5119, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5121 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5120, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5122 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5123 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5122, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5124 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5123, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5125 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5124, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5126 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5127 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5121, %5126), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5128 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5125, %5127, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5129 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5130 : Float(17:262656, 512:513, 513:1) = aten::select(%5129, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5131 : Float(17:262656, 255:513, 513:1) = aten::slice(%5130, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5132 : Float(17:262656, 255:513, 255:1) = aten::slice(%5131, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5133 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5134 : Float(17:262656, 256:513, 513:1) = aten::select(%5133, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5135 : Float(17:262656, 255:513, 513:1) = aten::slice(%5134, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5136 : Float(17:262656, 255:513, 255:1) = aten::slice(%5135, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5137 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5138 : Float(17:262656, 255:513, 255:1) = aten::view(%5132, %5137), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5139 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5136, %5138, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5140 : int[] = prim::ListConstruct(%5023, %5027, %5025, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5141 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.22, %5140), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.32 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5141, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %5143 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5144 : Float(256:257, 257:1) = aten::ones(%5143, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5145 : Float(256:257, 257:1) = aten::tril(%5144, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5146 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.22 : Float(256:257, 257:1) = aten::flip(%5145, %5146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5148 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.22, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5149 : Float(1:65792, 256:257, 257:1) = aten::slice(%5148, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5150 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5149, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5150, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5152 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.22, %5152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %5154 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5155 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5154, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5156 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5155, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5156, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5158 : int = aten::size(%beginning_input.22, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5159 : int = aten::size(%beginning_input.22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5160 : int = aten::size(%beginning_input.22, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5161 : int = aten::size(%beginning_input.22, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5162 : int[] = prim::ListConstruct(%5158, %5159, %5160, %5161), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5163 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.22, %5162, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5164 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5163, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5165 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.22, %5164, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %5166 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5167 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5166, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5168 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5167, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5168, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5170 : int = aten::size(%ending_input.22, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5171 : int = aten::size(%ending_input.22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5172 : int = aten::size(%ending_input.22, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5173 : int = aten::size(%ending_input.22, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5174 : int[] = prim::ListConstruct(%5170, %5171, %5172, %5173), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5175 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.22, %5174, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5176 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5175, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5177 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.22, %5176, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.11 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.31, %input_tensor.32, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.11 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.11, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
  %5180 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.11, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5181 : Bool(17:512, 512:1) = aten::slice(%5180, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5182 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5181, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5183 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5182, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %input.130 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.11, %5183, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.130, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
  %5186 : int[] = prim::ListConstruct(%4845, %4846, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5187 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.11, %5186), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %value.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5187, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %5189 : int = aten::size(%value.11, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.44 : Long() = prim::NumToTensor(%5189), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5191 : int = aten::size(%value.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.45 : Long() = prim::NumToTensor(%5191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5193 : int = aten::size(%value.11, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.33 : Long() = prim::NumToTensor(%5193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5195 : int = aten::size(%value.11, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %5196 : Long() = aten::floor_divide(%seq_len.45, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.33 : Long() = aten::sub(%5196, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
  %5198 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.22, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5199 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:546:0
  %5200 : int = aten::Int(%5199), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5201 : Long() = aten::floor_divide(%seq_len.45, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5202 : int = aten::Int(%5201), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5203 : int[] = prim::ListConstruct(%5200, %5202, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.51 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5198, %5203), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5205 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.11, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5206 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5207 : int = aten::Int(%5206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5208 : int[] = prim::ListConstruct(%5207, %5191, %5195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.131 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5205, %5208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5210 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %padded_value.11 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.131, %5210, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5212 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5213 : int = aten::Int(%5212), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5214 : Long() = aten::add(%chunks_count.33, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5215 : int = aten::Int(%5214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5216 : int[] = prim::ListConstruct(%5213, %5215, %37, %5195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5217 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5218 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.11, %5216, %5217, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
  %5219 : int = aten::size(%chunked_hidden_states.51, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5220 : int = aten::size(%chunked_hidden_states.51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5221 : int = aten::size(%chunked_hidden_states.51, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.11 : Long() = prim::NumToTensor(%5221), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5223 : int = aten::size(%chunked_hidden_states.51, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.11 : Long() = prim::NumToTensor(%5223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5225 : Long() = aten::add(%window_overlap.11, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
  %5226 : int = aten::Int(%5225), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5227 : int[] = prim::ListConstruct(%51, %5226), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.52 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.51, %5227, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5229 : int[] = prim::ListConstruct(%5219, %5220, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.53 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.52, %5229), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:424:0
  %5231 : Long() = aten::neg(%window_overlap.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:428:0
  %5232 : int = aten::Int(%5231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5233 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.53, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5234 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5233, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.54 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5234, %44, %51, %5232, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5236 : Long() = aten::add(%window_overlap.11, %hidden_dim.11, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:431:0
  %5237 : int = aten::Int(%5236), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5238 : int[] = prim::ListConstruct(%5219, %5220, %5221, %5237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.55 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.54, %5238), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:430:0
  %5240 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.55, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5241 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5240, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5242 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5241, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5243 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5242, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5244 : Tensor[] = prim::ListConstruct(%5243, %5218), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %context.11 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %5244), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5246 : int[] = prim::ListConstruct(%5189, %5193, %5191, %5195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5247 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.11, %5246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.21 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5247, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %5249 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.21, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %5250 : int[] = prim::ListConstruct(%4845, %4846, %4847), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5251 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5249, %5250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.22 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5251, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %input.132 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.22, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
  %5254 : __torch__.torch.nn.modules.normalization.___torch_mangle_6328.LayerNorm = prim::GetAttr[name="LayerNorm"](%4821)
  %5255 : __torch__.torch.nn.modules.linear.___torch_mangle_6327.Linear = prim::GetAttr[name="dense"](%4821)
  %5256 : Tensor = prim::GetAttr[name="bias"](%5255)
  %5257 : Tensor = prim::GetAttr[name="weight"](%5255)
  %5258 : Float(768:1, 768:768) = aten::t(%5257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.132, %5258), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.133 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.64, %5256, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.120 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.133, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.134 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.120, %hidden_states.111, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output # transformers/modeling_longformer.py:758:0
  %5263 : Tensor = prim::GetAttr[name="bias"](%5254)
  %5264 : Tensor = prim::GetAttr[name="weight"](%5254)
  %5265 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.33 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.134, %5265, %5264, %5263, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5267 : __torch__.torch.nn.modules.linear.___torch_mangle_6332.Linear = prim::GetAttr[name="dense"](%4819)
  %5268 : Tensor = prim::GetAttr[name="bias"](%5267)
  %5269 : Tensor = prim::GetAttr[name="weight"](%5267)
  %5270 : Float(768:1, 3072:768) = aten::t(%5269), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.33, %5270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.135 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.65, %5268, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.136 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %5274 : __torch__.torch.nn.modules.normalization.___torch_mangle_6335.LayerNorm = prim::GetAttr[name="LayerNorm"](%4818)
  %5275 : __torch__.torch.nn.modules.linear.___torch_mangle_6334.Linear = prim::GetAttr[name="dense"](%4818)
  %5276 : Tensor = prim::GetAttr[name="bias"](%5275)
  %5277 : Tensor = prim::GetAttr[name="weight"](%5275)
  %5278 : Float(3072:1, 768:3072) = aten::t(%5277), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.136, %5278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.137 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.66, %5276, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.121 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.137, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.138 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.121, %input_tensor.33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output # transformers/modeling_longformer.py:830:0
  %5283 : Tensor = prim::GetAttr[name="bias"](%5274)
  %5284 : Tensor = prim::GetAttr[name="weight"](%5274)
  %5285 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm
  %hidden_states.122 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.138, %5285, %5284, %5283, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %5287 : __torch__.transformers.modeling_longformer.___torch_mangle_6356.LongformerOutput = prim::GetAttr[name="output"](%105)
  %5288 : __torch__.transformers.modeling_longformer.___torch_mangle_6352.LongformerIntermediate = prim::GetAttr[name="intermediate"](%105)
  %5289 : __torch__.transformers.modeling_longformer.___torch_mangle_6350.LongformerAttention = prim::GetAttr[name="attention"](%105)
  %5290 : __torch__.transformers.modeling_longformer.___torch_mangle_6349.LongformerSelfOutput = prim::GetAttr[name="output"](%5289)
  %5291 : __torch__.transformers.modeling_longformer.___torch_mangle_6345.LongformerSelfAttention = prim::GetAttr[name="self"](%5289)
  %5292 : __torch__.torch.nn.modules.linear.___torch_mangle_6341.Linear = prim::GetAttr[name="value"](%5291)
  %5293 : __torch__.torch.nn.modules.linear.___torch_mangle_6340.Linear = prim::GetAttr[name="key"](%5291)
  %5294 : __torch__.torch.nn.modules.linear.___torch_mangle_6339.Linear = prim::GetAttr[name="query"](%5291)
  %5295 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %5296 : Float(17:512, 512:1) = aten::squeeze(%5295, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked : Bool(17:512, 512:1) = aten::lt(%5296, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %input.139 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.122, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
  %5299 : Tensor = prim::GetAttr[name="bias"](%5294)
  %5300 : Tensor = prim::GetAttr[name="weight"](%5294)
  %5301 : Float(768:1, 768:768) = aten::t(%5300), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.23 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.67, %5299, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %5304 : Tensor = prim::GetAttr[name="bias"](%5293)
  %5305 : Tensor = prim::GetAttr[name="weight"](%5293)
  %5306 : Float(768:1, 768:768) = aten::t(%5305), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5306), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.68, %5304, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %5309 : Tensor = prim::GetAttr[name="bias"](%5292)
  %5310 : Tensor = prim::GetAttr[name="weight"](%5292)
  %5311 : Float(768:1, 768:768) = aten::t(%5310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5311), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.69, %5309, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %5314 : int = aten::size(%input.139, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5315 : int = aten::size(%input.139, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5316 : int = aten::size(%input.139, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.23, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:261:0
  %5318 : int[] = prim::ListConstruct(%5314, %5315, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5319 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors, %5318), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %query.23 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5319, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %5321 : int[] = prim::ListConstruct(%5314, %5315, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5322 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors, %5321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %key : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5322, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %5324 : int = aten::size(%query.23, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.46 : Long() = prim::NumToTensor(%5324), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5326 : int = aten::size(%query.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.47 : Long() = prim::NumToTensor(%5326), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5328 : int = aten::size(%query.23, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.34 : Long() = prim::NumToTensor(%5328), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5330 : int = aten::size(%query.23, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5331 : Long() = aten::floor_divide(%seq_len.47, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.34 : Long() = aten::sub(%5331, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5333 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.23, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5334 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5335 : int = aten::Int(%5334), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5336 : int[] = prim::ListConstruct(%5335, %5326, %5330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.123 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5333, %5336), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5338 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5339 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5340 : int = aten::Int(%5339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5341 : int[] = prim::ListConstruct(%5340, %5326, %5330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.125 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5338, %5341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5343 : int = aten::size(%hidden_states.123, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5344 : int = aten::size(%hidden_states.123, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5345 : Long() = prim::NumToTensor(%5344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5346 : Long() = aten::floor_divide(%5345, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5347 : int = aten::Int(%5346), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5348 : int = aten::size(%hidden_states.123, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5349 : int[] = prim::ListConstruct(%5343, %5347, %46, %5348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.124 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.123, %5349), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5351 : int = aten::size(%hidden_states.124, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5352 : int = aten::size(%hidden_states.124, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5353 : Long() = prim::NumToTensor(%5352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5354 : int = aten::size(%hidden_states.124, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5355 : int = aten::size(%hidden_states.124, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5356 : Long() = aten::mul(%5353, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5357 : Long() = aten::sub(%5356, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5358 : int = aten::Int(%5357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5359 : int[] = prim::ListConstruct(%5351, %5358, %5354, %5355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5360 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5361 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.124, %5359, %5360, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5362 : int = aten::size(%hidden_states.125, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5363 : int = aten::size(%hidden_states.125, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5364 : Long() = prim::NumToTensor(%5363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5365 : Long() = aten::floor_divide(%5364, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5366 : int = aten::Int(%5365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5367 : int = aten::size(%hidden_states.125, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5368 : int[] = prim::ListConstruct(%5362, %5366, %46, %5367), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.126 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.125, %5368), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5370 : int = aten::size(%hidden_states.126, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5371 : int = aten::size(%hidden_states.126, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5372 : Long() = prim::NumToTensor(%5371), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5373 : int = aten::size(%hidden_states.126, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5374 : int = aten::size(%hidden_states.126, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5375 : Long() = aten::mul(%5372, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5376 : Long() = aten::sub(%5375, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5377 : int = aten::Int(%5376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5378 : int[] = prim::ListConstruct(%5370, %5377, %5373, %5374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5379 : int[] = prim::ListConstruct(%30, %26, %25, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5380 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.126, %5378, %5379, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5381 : Tensor[] = prim::ListConstruct(%5361, %5380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.140 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %5381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5383 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded.23 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.140, %5383, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5385 : int = aten::size(%hidden_states_padded.23, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5386 : int = aten::size(%hidden_states_padded.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5387 : int = aten::size(%hidden_states_padded.23, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5388 : int = aten::size(%hidden_states_padded.23, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5389 : int[] = prim::ListConstruct(%5385, %5386, %5387, %5388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores.23 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.23, %5389), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5391 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5392 : int = aten::Int(%5391), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5393 : Long() = aten::add(%chunks_count.34, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5394 : int = aten::Int(%5393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5395 : int[] = prim::ListConstruct(%5392, %5394, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores.23 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.23, %5395, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5397 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5398 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5397, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5399 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5398, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5400 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%5399, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5401 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5402 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5401, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5403 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5402, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5404 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%5403, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5405 : int[] = prim::ListConstruct(%19, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5406 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%5400, %5405), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5407 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5404, %5406, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5408 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5409 : Float(204:262656, 512:513, 513:1) = aten::select(%5408, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5410 : Float(204:262656, 256:513, 513:1) = aten::slice(%5409, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5411 : Float(204:262656, 256:513, 257:1) = aten::slice(%5410, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5412 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5413 : Float(204:262656, 256:513, 513:1) = aten::select(%5412, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5414 : Float(204:262656, 256:513, 513:1) = aten::slice(%5413, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5415 : Float(204:262656, 256:513, 257:1) = aten::slice(%5414, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5416 : int[] = prim::ListConstruct(%19, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5417 : Float(204:262656, 256:513, 257:1) = aten::view(%5411, %5416), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5418 : Float(204:262656, 256:513, 257:1) = aten::copy_(%5415, %5417, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5419 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5420 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5419, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5421 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5420, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5422 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%5421, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5423 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5424 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5423, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5425 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5424, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5426 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%5425, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5427 : int[] = prim::ListConstruct(%19, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5428 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%5422, %5427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5429 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5426, %5428, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5430 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5431 : Float(204:262656, 512:513, 513:1) = aten::select(%5430, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5432 : Float(204:262656, 255:513, 513:1) = aten::slice(%5431, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5433 : Float(204:262656, 255:513, 255:1) = aten::slice(%5432, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5434 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5435 : Float(204:262656, 256:513, 513:1) = aten::select(%5434, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5436 : Float(204:262656, 255:513, 513:1) = aten::slice(%5435, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5437 : Float(204:262656, 255:513, 255:1) = aten::slice(%5436, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5438 : int[] = prim::ListConstruct(%19, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5439 : Float(204:262656, 255:513, 255:1) = aten::view(%5433, %5438), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5440 : Float(204:262656, 255:513, 255:1) = aten::copy_(%5437, %5439, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5441 : int[] = prim::ListConstruct(%5324, %5328, %5326, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5442 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.23, %5441), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.34 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%5442, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5444 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5445 : Float(256:257, 257:1) = aten::ones(%5444, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5446 : Float(256:257, 257:1) = aten::tril(%5445, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5447 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d.23 : Float(256:257, 257:1) = aten::flip(%5446, %5447), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5449 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.23, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5450 : Float(1:65792, 256:257, 257:1) = aten::slice(%5449, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5451 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5450, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5451, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5453 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.23, %5453), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5455 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5456 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5455, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5457 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5456, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5457, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5459 : int = aten::size(%beginning_input.23, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5460 : int = aten::size(%beginning_input.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5461 : int = aten::size(%beginning_input.23, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5462 : int = aten::size(%beginning_input.23, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5463 : int[] = prim::ListConstruct(%5459, %5460, %5461, %5462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5464 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.23, %5463, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5465 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5464, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5466 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.23, %5465, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5467 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5468 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5467, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5469 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5468, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5469, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5471 : int = aten::size(%ending_input.23, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5472 : int = aten::size(%ending_input.23, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5473 : int = aten::size(%ending_input.23, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5474 : int = aten::size(%ending_input.23, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5475 : int[] = prim::ListConstruct(%5471, %5472, %5473, %5474), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5476 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.23, %5475, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5477 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5476, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5478 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.23, %5477, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %5479 : Bool(17:512, 512:1) = aten::ne(%5296, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5480 : Bool(17:512, 512:1) = aten::slice(%5479, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5481 : Bool(17:512, 512:1) = aten::slice(%5480, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5482 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5481, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5482, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5484 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask, %query.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5484, %remove_from_windowed_attention_mask, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %5486 : int = aten::size(%float_mask, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5487 : int = aten::size(%float_mask, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5488 : int = aten::size(%float_mask, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5489 : int = aten::size(%float_mask, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5490 : int[] = prim::ListConstruct(%5486, %5487, %5488, %5489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %query : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5490, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5492 : int = aten::size(%query, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.47 : Long() = prim::NumToTensor(%5492), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5494 : int = aten::size(%query, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.48 : Long() = prim::NumToTensor(%5494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5496 : int = aten::size(%query, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.35 : Long() = prim::NumToTensor(%5496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5498 : int = aten::size(%query, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5499 : Long() = aten::floor_divide(%seq_len.48, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.35 : Long() = aten::sub(%5499, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5501 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5502 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5503 : int = aten::Int(%5502), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5504 : int[] = prim::ListConstruct(%5503, %5494, %5498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.127 : Float(17:512, 512:1, 1:1) = aten::reshape(%5501, %5504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5506 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5507 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5508 : int = aten::Int(%5507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5509 : int[] = prim::ListConstruct(%5508, %5494, %5498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.129 : Float(17:512, 512:1, 1:1) = aten::reshape(%5506, %5509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5511 : int = aten::size(%hidden_states.127, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5512 : int = aten::size(%hidden_states.127, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5513 : Long() = prim::NumToTensor(%5512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5514 : Long() = aten::floor_divide(%5513, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5515 : int = aten::Int(%5514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5516 : int = aten::size(%hidden_states.127, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5517 : int[] = prim::ListConstruct(%5511, %5515, %46, %5516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.128 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.127, %5517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5519 : int = aten::size(%hidden_states.128, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5520 : int = aten::size(%hidden_states.128, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5521 : Long() = prim::NumToTensor(%5520), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5522 : int = aten::size(%hidden_states.128, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5523 : int = aten::size(%hidden_states.128, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5524 : Long() = aten::mul(%5521, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5525 : Long() = aten::sub(%5524, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5526 : int = aten::Int(%5525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5527 : int[] = prim::ListConstruct(%5519, %5526, %5522, %5523), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5528 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5529 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.128, %5527, %5528, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5530 : int = aten::size(%hidden_states.129, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5531 : int = aten::size(%hidden_states.129, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5532 : Long() = prim::NumToTensor(%5531), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5533 : Long() = aten::floor_divide(%5532, %28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5534 : int = aten::Int(%5533), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5535 : int = aten::size(%hidden_states.129, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5536 : int[] = prim::ListConstruct(%5530, %5534, %46, %5535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.130 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.129, %5536), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5538 : int = aten::size(%hidden_states.130, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5539 : int = aten::size(%hidden_states.130, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5540 : Long() = prim::NumToTensor(%5539), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5541 : int = aten::size(%hidden_states.130, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5542 : int = aten::size(%hidden_states.130, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5543 : Long() = aten::mul(%5540, %27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5544 : Long() = aten::sub(%5543, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5545 : int = aten::Int(%5544), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5546 : int[] = prim::ListConstruct(%5538, %5545, %5541, %5542), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5547 : int[] = prim::ListConstruct(%46, %22, %50, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5548 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.130, %5546, %5547, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5549 : Tensor[] = prim::ListConstruct(%5529, %5548), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.141 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%24, %5549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5551 : int[] = prim::ListConstruct(%51, %51, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.141, %5551, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5553 : int = aten::size(%hidden_states_padded, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5554 : int = aten::size(%hidden_states_padded, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5555 : int = aten::size(%hidden_states_padded, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5556 : int = aten::size(%hidden_states_padded, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5557 : int[] = prim::ListConstruct(%5553, %5554, %5555, %5556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded, %5557), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5559 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5560 : int = aten::Int(%5559), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5561 : Long() = aten::add(%chunks_count.35, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5562 : int = aten::Int(%5561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5563 : int[] = prim::ListConstruct(%5560, %5562, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores, %5563, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5565 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5566 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5565, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5567 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5566, %44, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5568 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5567, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5569 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5570 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5569, %50, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5571 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5570, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5572 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5571, %43, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5573 : int[] = prim::ListConstruct(%12, %50, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5574 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5568, %5573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5575 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5572, %5574, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5576 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5577 : Float(17:262656, 512:513, 513:1) = aten::select(%5576, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5578 : Float(17:262656, 256:513, 513:1) = aten::slice(%5577, %50, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5579 : Float(17:262656, 256:513, 257:1) = aten::slice(%5578, %44, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5580 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5581 : Float(17:262656, 256:513, 513:1) = aten::select(%5580, %50, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5582 : Float(17:262656, 256:513, 513:1) = aten::slice(%5581, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5583 : Float(17:262656, 256:513, 257:1) = aten::slice(%5582, %44, %22, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5584 : int[] = prim::ListConstruct(%12, %22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5585 : Float(17:262656, 256:513, 257:1) = aten::view(%5579, %5584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5586 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5583, %5585, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5587 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5588 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5587, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5589 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5588, %44, %18, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5590 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5589, %43, %20, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5591 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5592 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5591, %50, %50, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5593 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5592, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5594 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5593, %43, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5595 : int[] = prim::ListConstruct(%12, %50, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5596 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5590, %5595), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5597 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5594, %5596, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5598 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5599 : Float(17:262656, 512:513, 513:1) = aten::select(%5598, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5600 : Float(17:262656, 255:513, 513:1) = aten::slice(%5599, %50, %51, %17, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5601 : Float(17:262656, 255:513, 255:1) = aten::slice(%5600, %44, %16, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5602 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5603 : Float(17:262656, 256:513, 513:1) = aten::select(%5602, %50, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5604 : Float(17:262656, 255:513, 513:1) = aten::slice(%5603, %50, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5605 : Float(17:262656, 255:513, 255:1) = aten::slice(%5604, %44, %50, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5606 : int[] = prim::ListConstruct(%12, %17, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5607 : Float(17:262656, 255:513, 255:1) = aten::view(%5601, %5606), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5608 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5605, %5607, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5609 : int[] = prim::ListConstruct(%5492, %5496, %5494, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5610 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores, %5609), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.35 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5610, %44, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5612 : int[] = prim::ListConstruct(%22, %20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5613 : Float(256:257, 257:1) = aten::ones(%5612, %42, %51, %48, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5614 : Float(256:257, 257:1) = aten::tril(%5613, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5615 : int[] = prim::ListConstruct(%51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d : Float(256:257, 257:1) = aten::flip(%5614, %5615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5617 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5618 : Float(1:65792, 256:257, 257:1) = aten::slice(%5617, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5619 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5618, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5619, %43, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5621 : int[] = prim::ListConstruct(%50, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask, %5621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5623 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5624 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5623, %50, %51, %22, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5625 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5624, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5625, %43, %51, %20, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5627 : int = aten::size(%beginning_input, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5628 : int = aten::size(%beginning_input, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5629 : int = aten::size(%beginning_input, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5630 : int = aten::size(%beginning_input, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5631 : int[] = prim::ListConstruct(%5627, %5628, %5629, %5630), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5632 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask, %5631, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5633 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5632, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5634 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input, %5633, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5635 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5636 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5635, %50, %14, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5637 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5636, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5637, %43, %18, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5639 : int = aten::size(%ending_input, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5640 : int = aten::size(%ending_input, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5641 : int = aten::size(%ending_input, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5642 : int = aten::size(%ending_input, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5643 : int[] = prim::ListConstruct(%5639, %5640, %5641, %5642), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5644 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask, %5643, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5645 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5644, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5646 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input, %5645, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.34, %input_tensor.35, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores, %34, %42), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
  %5649 : Bool(17:512, 512:1) = aten::slice(%is_index_masked, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5650 : Bool(17:512, 512:1) = aten::slice(%5649, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5651 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5650, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5652 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5651, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %input.142 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32, %5652, %11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.142, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
  %5655 : int[] = prim::ListConstruct(%5314, %5315, %31, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5656 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors, %5655), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %value : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5656, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %5658 : int = aten::size(%value, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size : Long() = prim::NumToTensor(%5658), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5660 : int = aten::size(%value, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len : Long() = prim::NumToTensor(%5660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5662 : int = aten::size(%value, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads : Long() = prim::NumToTensor(%5662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5664 : int = aten::size(%value, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %5665 : Long() = aten::floor_divide(%seq_len, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count : Long() = aten::sub(%5665, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
  %5667 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5668 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:546:0
  %5669 : int = aten::Int(%5668), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5670 : Long() = aten::floor_divide(%seq_len, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5671 : int = aten::Int(%5670), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5672 : int[] = prim::ListConstruct(%5669, %5671, %22, %21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.56 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5667, %5672), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5674 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5675 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5676 : int = aten::Int(%5675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5677 : int[] = prim::ListConstruct(%5676, %5660, %5664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.143 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5674, %5677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5679 : int[] = prim::ListConstruct(%51, %51, %22, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %padded_value : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.143, %5679, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5681 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5682 : int = aten::Int(%5681), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5683 : Long() = aten::add(%chunks_count, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5684 : int = aten::Int(%5683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5685 : int[] = prim::ListConstruct(%5682, %5684, %37, %5664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5686 : int[] = prim::ListConstruct(%10, %9, %30, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5687 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value, %5685, %5686, %41), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
  %5688 : int = aten::size(%chunked_hidden_states.56, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5689 : int = aten::size(%chunked_hidden_states.56, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5690 : int = aten::size(%chunked_hidden_states.56, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap : Long() = prim::NumToTensor(%5690), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5692 : int = aten::size(%chunked_hidden_states.56, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim : Long() = prim::NumToTensor(%5692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5694 : Long() = aten::add(%window_overlap, %33, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
  %5695 : int = aten::Int(%5694), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5696 : int[] = prim::ListConstruct(%51, %5695), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.57 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.56, %5696, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5698 : int[] = prim::ListConstruct(%5688, %5689, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.58 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.57, %5698), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:424:0
  %5700 : Long() = aten::neg(%window_overlap), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:428:0
  %5701 : int = aten::Int(%5700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5702 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.58, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5703 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5702, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.59 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5703, %44, %51, %5701, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5705 : Long() = aten::add(%window_overlap, %hidden_dim, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:431:0
  %5706 : int = aten::Int(%5705), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5707 : int[] = prim::ListConstruct(%5688, %5689, %5690, %5706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.59, %5707), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:430:0
  %5709 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states, %51, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5710 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5709, %50, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5711 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5710, %44, %51, %45, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5712 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5711, %43, %51, %34, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5713 : Tensor[] = prim::ListConstruct(%5712, %5687), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %context : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%8, %5713), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5715 : int[] = prim::ListConstruct(%5658, %5662, %5660, %5664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5716 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context, %5715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.23 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5716, %50, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %5718 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.23, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %5719 : int[] = prim::ListConstruct(%5314, %5315, %5316), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5720 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5718, %5719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5720, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %input.144 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output, %51, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
  %5723 : __torch__.torch.nn.modules.normalization.___torch_mangle_6347.LayerNorm = prim::GetAttr[name="LayerNorm"](%5290)
  %5724 : __torch__.torch.nn.modules.linear.___torch_mangle_6346.Linear = prim::GetAttr[name="dense"](%5290)
  %5725 : Tensor = prim::GetAttr[name="bias"](%5724)
  %5726 : Tensor = prim::GetAttr[name="weight"](%5724)
  %5727 : Float(768:1, 768:768) = aten::t(%5726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.144, %5727), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.145 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.70, %5725, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.131 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.145, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.131, %hidden_states.122, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output # transformers/modeling_longformer.py:758:0
  %5732 : Tensor = prim::GetAttr[name="bias"](%5723)
  %5733 : Tensor = prim::GetAttr[name="weight"](%5723)
  %5734 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.146, %5734, %5733, %5732, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5736 : __torch__.torch.nn.modules.linear.___torch_mangle_6351.Linear = prim::GetAttr[name="dense"](%5288)
  %5737 : Tensor = prim::GetAttr[name="bias"](%5736)
  %5738 : Tensor = prim::GetAttr[name="weight"](%5736)
  %5739 : Float(768:1, 3072:768) = aten::t(%5738), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor, %5739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.147 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.71, %5737, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.148 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %5743 : __torch__.torch.nn.modules.normalization.___torch_mangle_6354.LayerNorm = prim::GetAttr[name="LayerNorm"](%5287)
  %5744 : __torch__.torch.nn.modules.linear.___torch_mangle_6353.Linear = prim::GetAttr[name="dense"](%5287)
  %5745 : Tensor = prim::GetAttr[name="bias"](%5744)
  %5746 : Tensor = prim::GetAttr[name="weight"](%5744)
  %5747 : Float(3072:1, 768:3072) = aten::t(%5746), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.148, %5747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.149 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.72, %5745, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.149, %38, %47), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states, %input_tensor, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output # transformers/modeling_longformer.py:830:0
  %5752 : Tensor = prim::GetAttr[name="bias"](%5743)
  %5753 : Tensor = prim::GetAttr[name="weight"](%5743)
  %5754 : int[] = prim::ListConstruct(%37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm
  %sequence_output : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.150, %5754, %5753, %5752, %36, %35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %5756 : Long() = aten::neg(%padding_len), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5757 : int = aten::Int(%5756), scope: __module.longformer
  %5758 : Float(17:393216, 512:768, 768:1) = aten::slice(%sequence_output, %51, %51, %45, %50), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %input.151 : Float(17:393216, 13:768, 768:1) = aten::slice(%5758, %50, %51, %5757, %50), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5760 : int = prim::Constant[value=768](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %5761 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %5762 : bool = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %5763 : int = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1678:0
  %5764 : Tensor = prim::GetAttr[name="bias"](%3)
  %5765 : __torch__.torch.nn.modules.linear.___torch_mangle_6361.Linear = prim::GetAttr[name="decoder"](%3)
  %5766 : __torch__.torch.nn.modules.normalization.___torch_mangle_6360.LayerNorm = prim::GetAttr[name="layer_norm"](%3)
  %5767 : __torch__.torch.nn.modules.linear.___torch_mangle_6359.Linear = prim::GetAttr[name="dense"](%3)
  %5768 : Tensor = prim::GetAttr[name="bias"](%5767)
  %5769 : Tensor = prim::GetAttr[name="weight"](%5767)
  %5770 : Float(768:1, 768:768) = aten::t(%5769), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.151, %5770), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1676:0
  %input.152 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.73, %5768, %5763), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1678:0
  %input.153 : Float(17:9984, 13:768, 768:1) = aten::gelu(%input.152), scope: __module.lm_head # torch/nn/functional.py:1369:0
  %5774 : Tensor = prim::GetAttr[name="bias"](%5766)
  %5775 : Tensor = prim::GetAttr[name="weight"](%5766)
  %5776 : int[] = prim::ListConstruct(%5760), scope: __module.lm_head/__module.lm_head.layer_norm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.153, %5776, %5775, %5774, %5761, %5762), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %5778 : Tensor = prim::GetAttr[name="weight"](%5765)
  %5779 : Float(768:1, 30522:768) = aten::t(%5778), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %output : Float(17:396786, 13:30522, 30522:1) = aten::matmul(%input, %5779), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %5781 : Float(17:396786, 13:30522, 30522:1) = aten::add_(%output, %5764, %5763), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
  %7 : (Float(17:396786, 13:30522, 30522:1)) = prim::TupleConstruct(%5781)
  return (%7)
