graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_41485.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_xlnet.___torch_mangle_41484.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
  %43 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %44 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %45 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %46 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %47 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %48 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %49 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %50 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %51 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %52 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %53 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
  %54 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %55 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
  %56 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %57 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %58 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %59 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %60 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %61 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %62 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %63 : None = prim::Constant(), scope: __module.transformer
  %64 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %65 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %66 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %67 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %68 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %69 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
  %70 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %71 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %72 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %74 : __torch__.transformers.modeling_xlnet.___torch_mangle_41481.XLNetLayer = prim::GetAttr[name="23"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %76 : __torch__.transformers.modeling_xlnet.___torch_mangle_41471.XLNetLayer = prim::GetAttr[name="22"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %78 : __torch__.transformers.modeling_xlnet.___torch_mangle_41461.XLNetLayer = prim::GetAttr[name="21"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %80 : __torch__.transformers.modeling_xlnet.___torch_mangle_41451.XLNetLayer = prim::GetAttr[name="20"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %82 : __torch__.transformers.modeling_xlnet.___torch_mangle_41441.XLNetLayer = prim::GetAttr[name="19"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %84 : __torch__.transformers.modeling_xlnet.___torch_mangle_41431.XLNetLayer = prim::GetAttr[name="18"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %86 : __torch__.transformers.modeling_xlnet.___torch_mangle_41421.XLNetLayer = prim::GetAttr[name="17"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %88 : __torch__.transformers.modeling_xlnet.___torch_mangle_41411.XLNetLayer = prim::GetAttr[name="16"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %90 : __torch__.transformers.modeling_xlnet.___torch_mangle_41401.XLNetLayer = prim::GetAttr[name="15"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %92 : __torch__.transformers.modeling_xlnet.___torch_mangle_41391.XLNetLayer = prim::GetAttr[name="14"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %94 : __torch__.transformers.modeling_xlnet.___torch_mangle_41381.XLNetLayer = prim::GetAttr[name="13"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %96 : __torch__.transformers.modeling_xlnet.___torch_mangle_41371.XLNetLayer = prim::GetAttr[name="12"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %98 : __torch__.transformers.modeling_xlnet.___torch_mangle_41361.XLNetLayer = prim::GetAttr[name="11"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %100 : __torch__.transformers.modeling_xlnet.___torch_mangle_41351.XLNetLayer = prim::GetAttr[name="10"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %102 : __torch__.transformers.modeling_xlnet.___torch_mangle_41341.XLNetLayer = prim::GetAttr[name="9"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %104 : __torch__.transformers.modeling_xlnet.___torch_mangle_41331.XLNetLayer = prim::GetAttr[name="8"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %106 : __torch__.transformers.modeling_xlnet.___torch_mangle_41321.XLNetLayer = prim::GetAttr[name="7"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %108 : __torch__.transformers.modeling_xlnet.___torch_mangle_41311.XLNetLayer = prim::GetAttr[name="6"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %110 : __torch__.transformers.modeling_xlnet.___torch_mangle_41301.XLNetLayer = prim::GetAttr[name="5"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %112 : __torch__.transformers.modeling_xlnet.___torch_mangle_41291.XLNetLayer = prim::GetAttr[name="4"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %114 : __torch__.transformers.modeling_xlnet.___torch_mangle_41281.XLNetLayer = prim::GetAttr[name="3"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %116 : __torch__.transformers.modeling_xlnet.___torch_mangle_41271.XLNetLayer = prim::GetAttr[name="2"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %118 : __torch__.transformers.modeling_xlnet.___torch_mangle_41261.XLNetLayer = prim::GetAttr[name="1"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_41482.ModuleList = prim::GetAttr[name="layer"](%4)
  %120 : __torch__.transformers.modeling_xlnet.___torch_mangle_41251.XLNetLayer = prim::GetAttr[name="0"](%119)
  %121 : __torch__.torch.nn.modules.sparse.___torch_mangle_41241.Embedding = prim::GetAttr[name="word_embedding"](%4)
  %122 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %72, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:17, 17:1) = aten::contiguous(%122, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %124 : int = aten::size(%input_ids, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%124), scope: __module.transformer
  %126 : int = aten::size(%input_ids, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %127 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %72, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %128 : Long(13:17, 17:1) = aten::contiguous(%127, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %klen.1 : Long() = aten::add(%qlen, %70, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %130 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
  %input_mask : Float(13:17, 17:1) = aten::rsub(%128, %69, %71), scope: __module.transformer # torch/tensor.py:396:0
  %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
  %133 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %134 : Float(1:221, 13:17, 17:1) = aten::slice(%133, %71, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %135 : Float(1:221, 13:17, 17:1) = aten::slice(%134, %67, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %136 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%135, %66), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %137 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%136, %72), scope: __module.transformer # torch/tensor.py:22:0
  %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%137, %65, %64, %64, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %139 : Float(13:13, 13:1) = aten::eye(%124, %65, %72, %62, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %140 : Float(13:13, 13:1) = aten::to(%139, %62, %65, %64, %64, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%140), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %142 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %143 : Float(13:13, 13:1) = aten::slice(%142, %71, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %144 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%143, %67), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %145 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%144, %66), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %146 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %145, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %147 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%146, %72), scope: __module.transformer # torch/tensor.py:22:0
  %148 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%147, %62, %65, %64, %64, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %149 : Tensor = prim::GetAttr[name="weight"](%121)
  %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%149, %input_ids, %61, %64, %64), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.1, %60, %64), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %freq_seq : Float(512:1) = aten::arange(%72, %59, %58, %65, %72, %62, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %153 : Float(512:1) = aten::div(%freq_seq, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %154 : Float(512:1) = aten::pow(%56, %153), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %155 : Float(512:1) = aten::reciprocal(%154), scope: __module.transformer # torch/tensor.py:400:0
  %156 : Float(512:1) = aten::mul(%155, %55), scope: __module.transformer # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
  %158 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
  %159 : Float(26:1) = aten::arange(%130, %158, %54, %63, %72, %62, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %160 : Tensor[] = prim::ListConstruct(%159, %156), scope: __module.transformer
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%53, %160), scope: __module.transformer # torch/functional.py:327:0
  %162 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %163 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %164 : Tensor[] = prim::ListConstruct(%162, %163), scope: __module.transformer
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%164, %61), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %166 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %167 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%166, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%167, %67, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %169 : int[] = prim::ListConstruct(%61, %126, %61), scope: __module.transformer
  %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %169, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
  %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %65, %72, %62, %64, %64, %64, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
  %r.1 : Float(26:1024, 17:0, 1024:1) = aten::dropout(%input.2, %60, %64), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%curr_out.1, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %174 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %175 : __torch__.transformers.modeling_xlnet.___torch_mangle_41249.XLNetFeedForward = prim::GetAttr[name="ff"](%120)
  %176 : __torch__.transformers.modeling_xlnet.___torch_mangle_41244.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%120)
  %177 : __torch__.torch.nn.modules.normalization.___torch_mangle_41242.LayerNorm = prim::GetAttr[name="layer_norm"](%176)
  %178 : Tensor = prim::GetAttr[name="o"](%176)
  %179 : Tensor = prim::GetAttr[name="r_r_bias"](%176)
  %180 : Tensor = prim::GetAttr[name="r_w_bias"](%176)
  %181 : Tensor = prim::GetAttr[name="r"](%176)
  %182 : Tensor = prim::GetAttr[name="v"](%176)
  %183 : Tensor = prim::GetAttr[name="k"](%176)
  %184 : Tensor = prim::GetAttr[name="q"](%176)
  %185 : Tensor[] = prim::ListConstruct(%curr_out.1, %184), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %185), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %187 : Tensor[] = prim::ListConstruct(%curr_out.1, %183), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %188 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %187), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %189 : Tensor[] = prim::ListConstruct(%curr_out.1, %182), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %190 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %189), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%r.1, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %192 : Tensor[] = prim::ListConstruct(%r.2, %181), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %193 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %192), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %194 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %180, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %195 : Tensor[] = prim::ListConstruct(%194, %188), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %195), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %197 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %179, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %198 : Tensor[] = prim::ListConstruct(%197, %193), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %198), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %200 : int = aten::size(%ac.1, %66), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %201 : int = aten::size(%x.1, %72), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %202 : int = aten::size(%x.1, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %203 : int = aten::size(%x.1, %67), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %204 : int = aten::size(%x.1, %66), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %205 : Long() = prim::NumToTensor(%204), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %206 : int[] = prim::ListConstruct(%201, %202, %204, %203), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %206), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %208 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %209 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%208, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %210 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%209, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%210, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %212 : Long() = aten::sub(%205, %55, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %213 : int = aten::Int(%212), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %214 : int[] = prim::ListConstruct(%201, %202, %203, %213), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %214), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %216 : Long(13:1) = aten::arange(%200, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %66, %216), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %218 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %219 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%218, %70, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%219, %49), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %221 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %222 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %221), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %223 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%222, %47), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %223, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %66, %63), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %226 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %60, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %227 : Tensor[] = prim::ListConstruct(%226, %190), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %228 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %227), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %229 : Tensor[] = prim::ListConstruct(%228, %178), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %229), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.5, %60, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %233 : Tensor = prim::GetAttr[name="bias"](%177)
  %234 : Tensor = prim::GetAttr[name="weight"](%177)
  %235 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %235, %234, %233, %43, %44), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %237 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %238 : Float(13:17408, 17:1024, 1024:1), %239 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%237)
  %240 : __torch__.torch.nn.modules.normalization.___torch_mangle_41245.LayerNorm = prim::GetAttr[name="layer_norm"](%175)
  %241 : __torch__.torch.nn.modules.linear.___torch_mangle_41247.Linear = prim::GetAttr[name="layer_2"](%175)
  %242 : __torch__.torch.nn.modules.linear.___torch_mangle_41246.Linear = prim::GetAttr[name="layer_1"](%175)
  %243 : Tensor = prim::GetAttr[name="bias"](%242)
  %244 : Tensor = prim::GetAttr[name="weight"](%242)
  %245 : Float(1024:1, 4096:1024) = aten::t(%244), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%238, %245), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %243, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %60, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %250 : Tensor = prim::GetAttr[name="bias"](%241)
  %251 : Tensor = prim::GetAttr[name="weight"](%241)
  %252 : Float(4096:1, 1024:4096) = aten::t(%251), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.9, %252), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %250, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.10, %60, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.3, %238, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %257 : Tensor = prim::GetAttr[name="bias"](%240)
  %258 : Tensor = prim::GetAttr[name="weight"](%240)
  %259 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %259, %258, %257, %43, %44), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %261 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %239)
  %262 : Float(13:17408, 17:1024, 1024:1), %263 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%261)
  %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%262, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %265 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %266 : __torch__.transformers.modeling_xlnet.___torch_mangle_41259.XLNetFeedForward = prim::GetAttr[name="ff"](%118)
  %267 : __torch__.transformers.modeling_xlnet.___torch_mangle_41254.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%118)
  %268 : __torch__.torch.nn.modules.normalization.___torch_mangle_41252.LayerNorm = prim::GetAttr[name="layer_norm"](%267)
  %269 : Tensor = prim::GetAttr[name="o"](%267)
  %270 : Tensor = prim::GetAttr[name="r_r_bias"](%267)
  %271 : Tensor = prim::GetAttr[name="r_w_bias"](%267)
  %272 : Tensor = prim::GetAttr[name="r"](%267)
  %273 : Tensor = prim::GetAttr[name="v"](%267)
  %274 : Tensor = prim::GetAttr[name="k"](%267)
  %275 : Tensor = prim::GetAttr[name="q"](%267)
  %276 : Tensor[] = prim::ListConstruct(%262, %275), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %276), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %278 : Tensor[] = prim::ListConstruct(%262, %274), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %279 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %278), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %280 : Tensor[] = prim::ListConstruct(%262, %273), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %281 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %280), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%263, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %283 : Tensor[] = prim::ListConstruct(%r.3, %272), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %284 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %283), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %285 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %271, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %286 : Tensor[] = prim::ListConstruct(%285, %279), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %286), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %288 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %270, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %289 : Tensor[] = prim::ListConstruct(%288, %284), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %289), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %291 : int = aten::size(%ac.2, %66), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %292 : int = aten::size(%x.5, %72), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %293 : int = aten::size(%x.5, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %294 : int = aten::size(%x.5, %67), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %295 : int = aten::size(%x.5, %66), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %296 : Long() = prim::NumToTensor(%295), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %297 : int[] = prim::ListConstruct(%292, %293, %295, %294), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %297), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %299 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %300 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%299, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %301 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%300, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%301, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %303 : Long() = aten::sub(%296, %55, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %304 : int = aten::Int(%303), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %305 : int[] = prim::ListConstruct(%292, %293, %294, %304), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %305), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %307 : Long(13:1) = aten::arange(%291, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %66, %307), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %309 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %310 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%309, %70, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%310, %49), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %312 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %313 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %312), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %314 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%313, %47), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %314, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %66, %63), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %317 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %60, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %318 : Tensor[] = prim::ListConstruct(%317, %281), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %319 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %318), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %320 : Tensor[] = prim::ListConstruct(%319, %269), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %320), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.14, %60, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.2, %262, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %324 : Tensor = prim::GetAttr[name="bias"](%268)
  %325 : Tensor = prim::GetAttr[name="weight"](%268)
  %326 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %326, %325, %324, %43, %44), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %328 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %329 : Float(13:17408, 17:1024, 1024:1), %330 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%328)
  %331 : __torch__.torch.nn.modules.normalization.___torch_mangle_41255.LayerNorm = prim::GetAttr[name="layer_norm"](%266)
  %332 : __torch__.torch.nn.modules.linear.___torch_mangle_41257.Linear = prim::GetAttr[name="layer_2"](%266)
  %333 : __torch__.torch.nn.modules.linear.___torch_mangle_41256.Linear = prim::GetAttr[name="layer_1"](%266)
  %334 : Tensor = prim::GetAttr[name="bias"](%333)
  %335 : Tensor = prim::GetAttr[name="weight"](%333)
  %336 : Float(1024:1, 4096:1024) = aten::t(%335), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%329, %336), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %334, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %60, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %341 : Tensor = prim::GetAttr[name="bias"](%332)
  %342 : Tensor = prim::GetAttr[name="weight"](%332)
  %343 : Float(4096:1, 1024:4096) = aten::t(%342), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.18, %343), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %341, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.19, %60, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.6, %329, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %348 : Tensor = prim::GetAttr[name="bias"](%331)
  %349 : Tensor = prim::GetAttr[name="weight"](%331)
  %350 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %350, %349, %348, %43, %44), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %352 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %330)
  %353 : Float(13:17408, 17:1024, 1024:1), %354 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%352)
  %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%353, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %356 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %357 : __torch__.transformers.modeling_xlnet.___torch_mangle_41269.XLNetFeedForward = prim::GetAttr[name="ff"](%116)
  %358 : __torch__.transformers.modeling_xlnet.___torch_mangle_41264.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%116)
  %359 : __torch__.torch.nn.modules.normalization.___torch_mangle_41262.LayerNorm = prim::GetAttr[name="layer_norm"](%358)
  %360 : Tensor = prim::GetAttr[name="o"](%358)
  %361 : Tensor = prim::GetAttr[name="r_r_bias"](%358)
  %362 : Tensor = prim::GetAttr[name="r_w_bias"](%358)
  %363 : Tensor = prim::GetAttr[name="r"](%358)
  %364 : Tensor = prim::GetAttr[name="v"](%358)
  %365 : Tensor = prim::GetAttr[name="k"](%358)
  %366 : Tensor = prim::GetAttr[name="q"](%358)
  %367 : Tensor[] = prim::ListConstruct(%353, %366), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %367), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %369 : Tensor[] = prim::ListConstruct(%353, %365), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %370 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %369), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %371 : Tensor[] = prim::ListConstruct(%353, %364), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %372 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %371), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%354, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %374 : Tensor[] = prim::ListConstruct(%r.4, %363), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %375 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %374), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %376 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %362, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %377 : Tensor[] = prim::ListConstruct(%376, %370), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %377), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %379 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %361, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %380 : Tensor[] = prim::ListConstruct(%379, %375), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %380), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %382 : int = aten::size(%ac.3, %66), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %383 : int = aten::size(%x.9, %72), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %384 : int = aten::size(%x.9, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %385 : int = aten::size(%x.9, %67), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %386 : int = aten::size(%x.9, %66), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %387 : Long() = prim::NumToTensor(%386), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %388 : int[] = prim::ListConstruct(%383, %384, %386, %385), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %388), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %390 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %391 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%390, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %392 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%391, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%392, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %394 : Long() = aten::sub(%387, %55, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %395 : int = aten::Int(%394), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %396 : int[] = prim::ListConstruct(%383, %384, %385, %395), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %396), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %398 : Long(13:1) = aten::arange(%382, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %66, %398), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %400 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %401 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%400, %70, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%401, %49), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %403 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %404 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %403), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %405 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%404, %47), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %405, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %66, %63), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %408 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %60, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %409 : Tensor[] = prim::ListConstruct(%408, %372), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %410 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %409), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %411 : Tensor[] = prim::ListConstruct(%410, %360), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %411), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.23, %60, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.3, %353, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %415 : Tensor = prim::GetAttr[name="bias"](%359)
  %416 : Tensor = prim::GetAttr[name="weight"](%359)
  %417 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %417, %416, %415, %43, %44), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %419 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %420 : Float(13:17408, 17:1024, 1024:1), %421 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%419)
  %422 : __torch__.torch.nn.modules.normalization.___torch_mangle_41265.LayerNorm = prim::GetAttr[name="layer_norm"](%357)
  %423 : __torch__.torch.nn.modules.linear.___torch_mangle_41267.Linear = prim::GetAttr[name="layer_2"](%357)
  %424 : __torch__.torch.nn.modules.linear.___torch_mangle_41266.Linear = prim::GetAttr[name="layer_1"](%357)
  %425 : Tensor = prim::GetAttr[name="bias"](%424)
  %426 : Tensor = prim::GetAttr[name="weight"](%424)
  %427 : Float(1024:1, 4096:1024) = aten::t(%426), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%420, %427), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %425, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.25), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %60, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %432 : Tensor = prim::GetAttr[name="bias"](%423)
  %433 : Tensor = prim::GetAttr[name="weight"](%423)
  %434 : Float(4096:1, 1024:4096) = aten::t(%433), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.27, %434), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %432, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.28, %60, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.9, %420, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %439 : Tensor = prim::GetAttr[name="bias"](%422)
  %440 : Tensor = prim::GetAttr[name="weight"](%422)
  %441 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %441, %440, %439, %43, %44), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %443 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %421)
  %444 : Float(13:17408, 17:1024, 1024:1), %445 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%443)
  %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%444, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %447 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %448 : __torch__.transformers.modeling_xlnet.___torch_mangle_41279.XLNetFeedForward = prim::GetAttr[name="ff"](%114)
  %449 : __torch__.transformers.modeling_xlnet.___torch_mangle_41274.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%114)
  %450 : __torch__.torch.nn.modules.normalization.___torch_mangle_41272.LayerNorm = prim::GetAttr[name="layer_norm"](%449)
  %451 : Tensor = prim::GetAttr[name="o"](%449)
  %452 : Tensor = prim::GetAttr[name="r_r_bias"](%449)
  %453 : Tensor = prim::GetAttr[name="r_w_bias"](%449)
  %454 : Tensor = prim::GetAttr[name="r"](%449)
  %455 : Tensor = prim::GetAttr[name="v"](%449)
  %456 : Tensor = prim::GetAttr[name="k"](%449)
  %457 : Tensor = prim::GetAttr[name="q"](%449)
  %458 : Tensor[] = prim::ListConstruct(%444, %457), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %458), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %460 : Tensor[] = prim::ListConstruct(%444, %456), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %461 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %460), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %462 : Tensor[] = prim::ListConstruct(%444, %455), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %463 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %462), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%445, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %465 : Tensor[] = prim::ListConstruct(%r.5, %454), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %466 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %465), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %467 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %453, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %468 : Tensor[] = prim::ListConstruct(%467, %461), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %468), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %470 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %452, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %471 : Tensor[] = prim::ListConstruct(%470, %466), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %471), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %473 : int = aten::size(%ac.4, %66), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %474 : int = aten::size(%x.13, %72), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %475 : int = aten::size(%x.13, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %476 : int = aten::size(%x.13, %67), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %477 : int = aten::size(%x.13, %66), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %478 : Long() = prim::NumToTensor(%477), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %479 : int[] = prim::ListConstruct(%474, %475, %477, %476), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %479), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %481 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %482 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%481, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %483 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%482, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%483, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %485 : Long() = aten::sub(%478, %55, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %486 : int = aten::Int(%485), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %487 : int[] = prim::ListConstruct(%474, %475, %476, %486), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %487), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %489 : Long(13:1) = aten::arange(%473, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %66, %489), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %491 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %492 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%491, %70, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%492, %49), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %494 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %495 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %494), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %496 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%495, %47), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %496, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %66, %63), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %499 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %60, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %500 : Tensor[] = prim::ListConstruct(%499, %463), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %501 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %500), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %502 : Tensor[] = prim::ListConstruct(%501, %451), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %502), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.32, %60, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.4, %444, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %506 : Tensor = prim::GetAttr[name="bias"](%450)
  %507 : Tensor = prim::GetAttr[name="weight"](%450)
  %508 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %508, %507, %506, %43, %44), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %510 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %511 : Float(13:17408, 17:1024, 1024:1), %512 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%510)
  %513 : __torch__.torch.nn.modules.normalization.___torch_mangle_41275.LayerNorm = prim::GetAttr[name="layer_norm"](%448)
  %514 : __torch__.torch.nn.modules.linear.___torch_mangle_41277.Linear = prim::GetAttr[name="layer_2"](%448)
  %515 : __torch__.torch.nn.modules.linear.___torch_mangle_41276.Linear = prim::GetAttr[name="layer_1"](%448)
  %516 : Tensor = prim::GetAttr[name="bias"](%515)
  %517 : Tensor = prim::GetAttr[name="weight"](%515)
  %518 : Float(1024:1, 4096:1024) = aten::t(%517), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%511, %518), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %516, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %60, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %523 : Tensor = prim::GetAttr[name="bias"](%514)
  %524 : Tensor = prim::GetAttr[name="weight"](%514)
  %525 : Float(4096:1, 1024:4096) = aten::t(%524), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.36, %525), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %523, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.37, %60, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.12, %511, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %530 : Tensor = prim::GetAttr[name="bias"](%513)
  %531 : Tensor = prim::GetAttr[name="weight"](%513)
  %532 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %532, %531, %530, %43, %44), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %534 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %512)
  %535 : Float(13:17408, 17:1024, 1024:1), %536 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%534)
  %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%535, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %538 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %539 : __torch__.transformers.modeling_xlnet.___torch_mangle_41289.XLNetFeedForward = prim::GetAttr[name="ff"](%112)
  %540 : __torch__.transformers.modeling_xlnet.___torch_mangle_41284.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%112)
  %541 : __torch__.torch.nn.modules.normalization.___torch_mangle_41282.LayerNorm = prim::GetAttr[name="layer_norm"](%540)
  %542 : Tensor = prim::GetAttr[name="o"](%540)
  %543 : Tensor = prim::GetAttr[name="r_r_bias"](%540)
  %544 : Tensor = prim::GetAttr[name="r_w_bias"](%540)
  %545 : Tensor = prim::GetAttr[name="r"](%540)
  %546 : Tensor = prim::GetAttr[name="v"](%540)
  %547 : Tensor = prim::GetAttr[name="k"](%540)
  %548 : Tensor = prim::GetAttr[name="q"](%540)
  %549 : Tensor[] = prim::ListConstruct(%535, %548), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %549), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %551 : Tensor[] = prim::ListConstruct(%535, %547), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %552 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %551), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %553 : Tensor[] = prim::ListConstruct(%535, %546), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %554 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %553), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%536, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %556 : Tensor[] = prim::ListConstruct(%r.6, %545), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %557 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %556), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %558 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %544, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %559 : Tensor[] = prim::ListConstruct(%558, %552), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %559), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %561 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %543, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %562 : Tensor[] = prim::ListConstruct(%561, %557), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %562), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %564 : int = aten::size(%ac.5, %66), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %565 : int = aten::size(%x.17, %72), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %566 : int = aten::size(%x.17, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %567 : int = aten::size(%x.17, %67), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %568 : int = aten::size(%x.17, %66), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %569 : Long() = prim::NumToTensor(%568), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %570 : int[] = prim::ListConstruct(%565, %566, %568, %567), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %570), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %572 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %573 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%572, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %574 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%573, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%574, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %576 : Long() = aten::sub(%569, %55, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %577 : int = aten::Int(%576), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %578 : int[] = prim::ListConstruct(%565, %566, %567, %577), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %578), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %580 : Long(13:1) = aten::arange(%564, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %66, %580), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %582 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %583 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%582, %70, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%583, %49), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %585 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %586 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %585), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %587 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%586, %47), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %587, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %66, %63), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %590 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %60, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %591 : Tensor[] = prim::ListConstruct(%590, %554), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %592 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %591), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %593 : Tensor[] = prim::ListConstruct(%592, %542), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %593), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.41, %60, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.5, %535, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %597 : Tensor = prim::GetAttr[name="bias"](%541)
  %598 : Tensor = prim::GetAttr[name="weight"](%541)
  %599 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %599, %598, %597, %43, %44), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %601 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %602 : Float(13:17408, 17:1024, 1024:1), %603 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%601)
  %604 : __torch__.torch.nn.modules.normalization.___torch_mangle_41285.LayerNorm = prim::GetAttr[name="layer_norm"](%539)
  %605 : __torch__.torch.nn.modules.linear.___torch_mangle_41287.Linear = prim::GetAttr[name="layer_2"](%539)
  %606 : __torch__.torch.nn.modules.linear.___torch_mangle_41286.Linear = prim::GetAttr[name="layer_1"](%539)
  %607 : Tensor = prim::GetAttr[name="bias"](%606)
  %608 : Tensor = prim::GetAttr[name="weight"](%606)
  %609 : Float(1024:1, 4096:1024) = aten::t(%608), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%602, %609), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %607, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %60, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %614 : Tensor = prim::GetAttr[name="bias"](%605)
  %615 : Tensor = prim::GetAttr[name="weight"](%605)
  %616 : Float(4096:1, 1024:4096) = aten::t(%615), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %616), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %614, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %60, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.15, %602, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %621 : Tensor = prim::GetAttr[name="bias"](%604)
  %622 : Tensor = prim::GetAttr[name="weight"](%604)
  %623 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %623, %622, %621, %43, %44), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %625 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %603)
  %626 : Float(13:17408, 17:1024, 1024:1), %627 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%625)
  %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%626, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %629 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %630 : __torch__.transformers.modeling_xlnet.___torch_mangle_41299.XLNetFeedForward = prim::GetAttr[name="ff"](%110)
  %631 : __torch__.transformers.modeling_xlnet.___torch_mangle_41294.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%110)
  %632 : __torch__.torch.nn.modules.normalization.___torch_mangle_41292.LayerNorm = prim::GetAttr[name="layer_norm"](%631)
  %633 : Tensor = prim::GetAttr[name="o"](%631)
  %634 : Tensor = prim::GetAttr[name="r_r_bias"](%631)
  %635 : Tensor = prim::GetAttr[name="r_w_bias"](%631)
  %636 : Tensor = prim::GetAttr[name="r"](%631)
  %637 : Tensor = prim::GetAttr[name="v"](%631)
  %638 : Tensor = prim::GetAttr[name="k"](%631)
  %639 : Tensor = prim::GetAttr[name="q"](%631)
  %640 : Tensor[] = prim::ListConstruct(%626, %639), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %640), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %642 : Tensor[] = prim::ListConstruct(%626, %638), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %643 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %642), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %644 : Tensor[] = prim::ListConstruct(%626, %637), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %645 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %644), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%627, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %647 : Tensor[] = prim::ListConstruct(%r.7, %636), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %648 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %647), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %649 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %635, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %650 : Tensor[] = prim::ListConstruct(%649, %643), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %650), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %652 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %634, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %653 : Tensor[] = prim::ListConstruct(%652, %648), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %653), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %655 : int = aten::size(%ac.6, %66), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %656 : int = aten::size(%x.21, %72), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %657 : int = aten::size(%x.21, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %658 : int = aten::size(%x.21, %67), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %659 : int = aten::size(%x.21, %66), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %660 : Long() = prim::NumToTensor(%659), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %661 : int[] = prim::ListConstruct(%656, %657, %659, %658), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %661), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %663 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %664 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%663, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %665 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%664, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%665, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %667 : Long() = aten::sub(%660, %55, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %668 : int = aten::Int(%667), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %669 : int[] = prim::ListConstruct(%656, %657, %658, %668), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %669), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %671 : Long(13:1) = aten::arange(%655, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %66, %671), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %673 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %674 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%673, %70, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%674, %49), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %676 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %677 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %676), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %678 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%677, %47), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %678, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %66, %63), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %681 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %60, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %682 : Tensor[] = prim::ListConstruct(%681, %645), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %683 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %682), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %684 : Tensor[] = prim::ListConstruct(%683, %633), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %684), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.50, %60, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.6, %626, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %688 : Tensor = prim::GetAttr[name="bias"](%632)
  %689 : Tensor = prim::GetAttr[name="weight"](%632)
  %690 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %690, %689, %688, %43, %44), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %692 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %693 : Float(13:17408, 17:1024, 1024:1), %694 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%692)
  %695 : __torch__.torch.nn.modules.normalization.___torch_mangle_41295.LayerNorm = prim::GetAttr[name="layer_norm"](%630)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_41297.Linear = prim::GetAttr[name="layer_2"](%630)
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_41296.Linear = prim::GetAttr[name="layer_1"](%630)
  %698 : Tensor = prim::GetAttr[name="bias"](%697)
  %699 : Tensor = prim::GetAttr[name="weight"](%697)
  %700 : Float(1024:1, 4096:1024) = aten::t(%699), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%693, %700), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %698, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %60, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %705 : Tensor = prim::GetAttr[name="bias"](%696)
  %706 : Tensor = prim::GetAttr[name="weight"](%696)
  %707 : Float(4096:1, 1024:4096) = aten::t(%706), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.54, %707), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %705, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.55, %60, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.18, %693, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %712 : Tensor = prim::GetAttr[name="bias"](%695)
  %713 : Tensor = prim::GetAttr[name="weight"](%695)
  %714 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %714, %713, %712, %43, %44), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %716 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %694)
  %717 : Float(13:17408, 17:1024, 1024:1), %718 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%716)
  %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%717, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %720 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %721 : __torch__.transformers.modeling_xlnet.___torch_mangle_41309.XLNetFeedForward = prim::GetAttr[name="ff"](%108)
  %722 : __torch__.transformers.modeling_xlnet.___torch_mangle_41304.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%108)
  %723 : __torch__.torch.nn.modules.normalization.___torch_mangle_41302.LayerNorm = prim::GetAttr[name="layer_norm"](%722)
  %724 : Tensor = prim::GetAttr[name="o"](%722)
  %725 : Tensor = prim::GetAttr[name="r_r_bias"](%722)
  %726 : Tensor = prim::GetAttr[name="r_w_bias"](%722)
  %727 : Tensor = prim::GetAttr[name="r"](%722)
  %728 : Tensor = prim::GetAttr[name="v"](%722)
  %729 : Tensor = prim::GetAttr[name="k"](%722)
  %730 : Tensor = prim::GetAttr[name="q"](%722)
  %731 : Tensor[] = prim::ListConstruct(%717, %730), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %731), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %733 : Tensor[] = prim::ListConstruct(%717, %729), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %734 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %733), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %735 : Tensor[] = prim::ListConstruct(%717, %728), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %736 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %735), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%718, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %738 : Tensor[] = prim::ListConstruct(%r.8, %727), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %739 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %738), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %740 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %726, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %741 : Tensor[] = prim::ListConstruct(%740, %734), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %741), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %743 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %725, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %744 : Tensor[] = prim::ListConstruct(%743, %739), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %744), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %746 : int = aten::size(%ac.7, %66), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %747 : int = aten::size(%x.25, %72), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %748 : int = aten::size(%x.25, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %749 : int = aten::size(%x.25, %67), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %750 : int = aten::size(%x.25, %66), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %751 : Long() = prim::NumToTensor(%750), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %752 : int[] = prim::ListConstruct(%747, %748, %750, %749), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %752), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %754 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %755 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%754, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %756 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%755, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%756, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %758 : Long() = aten::sub(%751, %55, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %759 : int = aten::Int(%758), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %760 : int[] = prim::ListConstruct(%747, %748, %749, %759), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %760), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %762 : Long(13:1) = aten::arange(%746, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %66, %762), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %764 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %765 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%764, %70, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%765, %49), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %767 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %768 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %767), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %769 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%768, %47), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %769, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %66, %63), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %772 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %60, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %773 : Tensor[] = prim::ListConstruct(%772, %736), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %774 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %773), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %775 : Tensor[] = prim::ListConstruct(%774, %724), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %775), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.59, %60, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.7, %717, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %779 : Tensor = prim::GetAttr[name="bias"](%723)
  %780 : Tensor = prim::GetAttr[name="weight"](%723)
  %781 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %781, %780, %779, %43, %44), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %783 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %784 : Float(13:17408, 17:1024, 1024:1), %785 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%783)
  %786 : __torch__.torch.nn.modules.normalization.___torch_mangle_41305.LayerNorm = prim::GetAttr[name="layer_norm"](%721)
  %787 : __torch__.torch.nn.modules.linear.___torch_mangle_41307.Linear = prim::GetAttr[name="layer_2"](%721)
  %788 : __torch__.torch.nn.modules.linear.___torch_mangle_41306.Linear = prim::GetAttr[name="layer_1"](%721)
  %789 : Tensor = prim::GetAttr[name="bias"](%788)
  %790 : Tensor = prim::GetAttr[name="weight"](%788)
  %791 : Float(1024:1, 4096:1024) = aten::t(%790), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%784, %791), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %789, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %60, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %796 : Tensor = prim::GetAttr[name="bias"](%787)
  %797 : Tensor = prim::GetAttr[name="weight"](%787)
  %798 : Float(4096:1, 1024:4096) = aten::t(%797), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.63, %798), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %796, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.64, %60, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.21, %784, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %803 : Tensor = prim::GetAttr[name="bias"](%786)
  %804 : Tensor = prim::GetAttr[name="weight"](%786)
  %805 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %805, %804, %803, %43, %44), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %807 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %785)
  %808 : Float(13:17408, 17:1024, 1024:1), %809 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%807)
  %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%808, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %811 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %812 : __torch__.transformers.modeling_xlnet.___torch_mangle_41319.XLNetFeedForward = prim::GetAttr[name="ff"](%106)
  %813 : __torch__.transformers.modeling_xlnet.___torch_mangle_41314.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%106)
  %814 : __torch__.torch.nn.modules.normalization.___torch_mangle_41312.LayerNorm = prim::GetAttr[name="layer_norm"](%813)
  %815 : Tensor = prim::GetAttr[name="o"](%813)
  %816 : Tensor = prim::GetAttr[name="r_r_bias"](%813)
  %817 : Tensor = prim::GetAttr[name="r_w_bias"](%813)
  %818 : Tensor = prim::GetAttr[name="r"](%813)
  %819 : Tensor = prim::GetAttr[name="v"](%813)
  %820 : Tensor = prim::GetAttr[name="k"](%813)
  %821 : Tensor = prim::GetAttr[name="q"](%813)
  %822 : Tensor[] = prim::ListConstruct(%808, %821), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %822), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %824 : Tensor[] = prim::ListConstruct(%808, %820), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %825 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %824), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %826 : Tensor[] = prim::ListConstruct(%808, %819), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %827 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %826), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%809, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %829 : Tensor[] = prim::ListConstruct(%r.9, %818), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %830 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %829), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %831 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %817, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %832 : Tensor[] = prim::ListConstruct(%831, %825), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %832), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %834 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %816, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %835 : Tensor[] = prim::ListConstruct(%834, %830), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %835), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %837 : int = aten::size(%ac.8, %66), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %838 : int = aten::size(%x.29, %72), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %839 : int = aten::size(%x.29, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %840 : int = aten::size(%x.29, %67), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %841 : int = aten::size(%x.29, %66), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %842 : Long() = prim::NumToTensor(%841), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %843 : int[] = prim::ListConstruct(%838, %839, %841, %840), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %843), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %845 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %846 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%845, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %847 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%846, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%847, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %849 : Long() = aten::sub(%842, %55, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %850 : int = aten::Int(%849), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %851 : int[] = prim::ListConstruct(%838, %839, %840, %850), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %851), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %853 : Long(13:1) = aten::arange(%837, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %66, %853), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %855 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %856 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%855, %70, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%856, %49), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %858 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %859 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %858), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %860 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%859, %47), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %860, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %66, %63), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %863 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %60, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %864 : Tensor[] = prim::ListConstruct(%863, %827), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %865 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %864), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %866 : Tensor[] = prim::ListConstruct(%865, %815), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %866), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %60, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.8, %808, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %870 : Tensor = prim::GetAttr[name="bias"](%814)
  %871 : Tensor = prim::GetAttr[name="weight"](%814)
  %872 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %872, %871, %870, %43, %44), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %874 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %875 : Float(13:17408, 17:1024, 1024:1), %876 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%874)
  %877 : __torch__.torch.nn.modules.normalization.___torch_mangle_41315.LayerNorm = prim::GetAttr[name="layer_norm"](%812)
  %878 : __torch__.torch.nn.modules.linear.___torch_mangle_41317.Linear = prim::GetAttr[name="layer_2"](%812)
  %879 : __torch__.torch.nn.modules.linear.___torch_mangle_41316.Linear = prim::GetAttr[name="layer_1"](%812)
  %880 : Tensor = prim::GetAttr[name="bias"](%879)
  %881 : Tensor = prim::GetAttr[name="weight"](%879)
  %882 : Float(1024:1, 4096:1024) = aten::t(%881), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%875, %882), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %880, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %60, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %887 : Tensor = prim::GetAttr[name="bias"](%878)
  %888 : Tensor = prim::GetAttr[name="weight"](%878)
  %889 : Float(4096:1, 1024:4096) = aten::t(%888), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %889), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %887, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %60, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.24, %875, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %894 : Tensor = prim::GetAttr[name="bias"](%877)
  %895 : Tensor = prim::GetAttr[name="weight"](%877)
  %896 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %896, %895, %894, %43, %44), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %898 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %876)
  %899 : Float(13:17408, 17:1024, 1024:1), %900 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%898)
  %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%899, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %902 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %903 : __torch__.transformers.modeling_xlnet.___torch_mangle_41329.XLNetFeedForward = prim::GetAttr[name="ff"](%104)
  %904 : __torch__.transformers.modeling_xlnet.___torch_mangle_41324.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%104)
  %905 : __torch__.torch.nn.modules.normalization.___torch_mangle_41322.LayerNorm = prim::GetAttr[name="layer_norm"](%904)
  %906 : Tensor = prim::GetAttr[name="o"](%904)
  %907 : Tensor = prim::GetAttr[name="r_r_bias"](%904)
  %908 : Tensor = prim::GetAttr[name="r_w_bias"](%904)
  %909 : Tensor = prim::GetAttr[name="r"](%904)
  %910 : Tensor = prim::GetAttr[name="v"](%904)
  %911 : Tensor = prim::GetAttr[name="k"](%904)
  %912 : Tensor = prim::GetAttr[name="q"](%904)
  %913 : Tensor[] = prim::ListConstruct(%899, %912), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %913), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %915 : Tensor[] = prim::ListConstruct(%899, %911), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %916 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %915), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %917 : Tensor[] = prim::ListConstruct(%899, %910), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %918 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %917), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%900, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %920 : Tensor[] = prim::ListConstruct(%r.10, %909), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %921 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %920), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %922 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %908, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %923 : Tensor[] = prim::ListConstruct(%922, %916), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %923), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %925 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %907, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %926 : Tensor[] = prim::ListConstruct(%925, %921), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %926), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %928 : int = aten::size(%ac.9, %66), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %929 : int = aten::size(%x.33, %72), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %930 : int = aten::size(%x.33, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %931 : int = aten::size(%x.33, %67), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %932 : int = aten::size(%x.33, %66), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %933 : Long() = prim::NumToTensor(%932), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %934 : int[] = prim::ListConstruct(%929, %930, %932, %931), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %934), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %936 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %937 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%936, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %938 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%937, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%938, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %940 : Long() = aten::sub(%933, %55, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %941 : int = aten::Int(%940), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %942 : int[] = prim::ListConstruct(%929, %930, %931, %941), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %942), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %944 : Long(13:1) = aten::arange(%928, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %66, %944), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %946 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %947 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%946, %70, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%947, %49), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %949 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %950 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %949), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %951 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%950, %47), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %951, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %66, %63), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %954 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %60, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %955 : Tensor[] = prim::ListConstruct(%954, %918), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %956 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %955), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %957 : Tensor[] = prim::ListConstruct(%956, %906), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %957), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.77, %60, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.9, %899, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %961 : Tensor = prim::GetAttr[name="bias"](%905)
  %962 : Tensor = prim::GetAttr[name="weight"](%905)
  %963 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %963, %962, %961, %43, %44), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %965 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %966 : Float(13:17408, 17:1024, 1024:1), %967 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%965)
  %968 : __torch__.torch.nn.modules.normalization.___torch_mangle_41325.LayerNorm = prim::GetAttr[name="layer_norm"](%903)
  %969 : __torch__.torch.nn.modules.linear.___torch_mangle_41327.Linear = prim::GetAttr[name="layer_2"](%903)
  %970 : __torch__.torch.nn.modules.linear.___torch_mangle_41326.Linear = prim::GetAttr[name="layer_1"](%903)
  %971 : Tensor = prim::GetAttr[name="bias"](%970)
  %972 : Tensor = prim::GetAttr[name="weight"](%970)
  %973 : Float(1024:1, 4096:1024) = aten::t(%972), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%966, %973), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %971, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.79), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %60, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %978 : Tensor = prim::GetAttr[name="bias"](%969)
  %979 : Tensor = prim::GetAttr[name="weight"](%969)
  %980 : Float(4096:1, 1024:4096) = aten::t(%979), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.81, %980), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %978, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.82, %60, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.27, %966, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %985 : Tensor = prim::GetAttr[name="bias"](%968)
  %986 : Tensor = prim::GetAttr[name="weight"](%968)
  %987 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %987, %986, %985, %43, %44), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %989 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %967)
  %990 : Float(13:17408, 17:1024, 1024:1), %991 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%989)
  %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%990, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %993 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %994 : __torch__.transformers.modeling_xlnet.___torch_mangle_41339.XLNetFeedForward = prim::GetAttr[name="ff"](%102)
  %995 : __torch__.transformers.modeling_xlnet.___torch_mangle_41334.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%102)
  %996 : __torch__.torch.nn.modules.normalization.___torch_mangle_41332.LayerNorm = prim::GetAttr[name="layer_norm"](%995)
  %997 : Tensor = prim::GetAttr[name="o"](%995)
  %998 : Tensor = prim::GetAttr[name="r_r_bias"](%995)
  %999 : Tensor = prim::GetAttr[name="r_w_bias"](%995)
  %1000 : Tensor = prim::GetAttr[name="r"](%995)
  %1001 : Tensor = prim::GetAttr[name="v"](%995)
  %1002 : Tensor = prim::GetAttr[name="k"](%995)
  %1003 : Tensor = prim::GetAttr[name="q"](%995)
  %1004 : Tensor[] = prim::ListConstruct(%990, %1003), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1004), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1006 : Tensor[] = prim::ListConstruct(%990, %1002), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1007 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1006), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1008 : Tensor[] = prim::ListConstruct(%990, %1001), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1009 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1008), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%991, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1011 : Tensor[] = prim::ListConstruct(%r.11, %1000), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1012 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1011), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1013 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %999, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1014 : Tensor[] = prim::ListConstruct(%1013, %1007), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1014), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1016 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %998, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1017 : Tensor[] = prim::ListConstruct(%1016, %1012), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1017), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1019 : int = aten::size(%ac.10, %66), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1020 : int = aten::size(%x.37, %72), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1021 : int = aten::size(%x.37, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1022 : int = aten::size(%x.37, %67), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1023 : int = aten::size(%x.37, %66), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1024 : Long() = prim::NumToTensor(%1023), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1025 : int[] = prim::ListConstruct(%1020, %1021, %1023, %1022), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1025), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1027 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1028 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1027, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1029 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1028, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1029, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1031 : Long() = aten::sub(%1024, %55, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1032 : int = aten::Int(%1031), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1033 : int[] = prim::ListConstruct(%1020, %1021, %1022, %1032), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1033), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1035 : Long(13:1) = aten::arange(%1019, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %66, %1035), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1037 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1038 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1037, %70, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1038, %49), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1040 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1041 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1040), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1042 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1041, %47), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %1042, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %66, %63), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1045 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %60, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1046 : Tensor[] = prim::ListConstruct(%1045, %1009), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1047 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1046), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1048 : Tensor[] = prim::ListConstruct(%1047, %997), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1048), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.86, %60, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.10, %990, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1052 : Tensor = prim::GetAttr[name="bias"](%996)
  %1053 : Tensor = prim::GetAttr[name="weight"](%996)
  %1054 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %1054, %1053, %1052, %43, %44), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1056 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1057 : Float(13:17408, 17:1024, 1024:1), %1058 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1056)
  %1059 : __torch__.torch.nn.modules.normalization.___torch_mangle_41335.LayerNorm = prim::GetAttr[name="layer_norm"](%994)
  %1060 : __torch__.torch.nn.modules.linear.___torch_mangle_41337.Linear = prim::GetAttr[name="layer_2"](%994)
  %1061 : __torch__.torch.nn.modules.linear.___torch_mangle_41336.Linear = prim::GetAttr[name="layer_1"](%994)
  %1062 : Tensor = prim::GetAttr[name="bias"](%1061)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1064 : Float(1024:1, 4096:1024) = aten::t(%1063), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1057, %1064), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %1062, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %60, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%1060)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1060)
  %1071 : Float(4096:1, 1024:4096) = aten::t(%1070), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.90, %1071), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %1069, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.91, %60, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.30, %1057, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1076 : Tensor = prim::GetAttr[name="bias"](%1059)
  %1077 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1078 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %1078, %1077, %1076, %43, %44), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1080 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1058)
  %1081 : Float(13:17408, 17:1024, 1024:1), %1082 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1080)
  %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1081, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1084 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1085 : __torch__.transformers.modeling_xlnet.___torch_mangle_41349.XLNetFeedForward = prim::GetAttr[name="ff"](%100)
  %1086 : __torch__.transformers.modeling_xlnet.___torch_mangle_41344.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%100)
  %1087 : __torch__.torch.nn.modules.normalization.___torch_mangle_41342.LayerNorm = prim::GetAttr[name="layer_norm"](%1086)
  %1088 : Tensor = prim::GetAttr[name="o"](%1086)
  %1089 : Tensor = prim::GetAttr[name="r_r_bias"](%1086)
  %1090 : Tensor = prim::GetAttr[name="r_w_bias"](%1086)
  %1091 : Tensor = prim::GetAttr[name="r"](%1086)
  %1092 : Tensor = prim::GetAttr[name="v"](%1086)
  %1093 : Tensor = prim::GetAttr[name="k"](%1086)
  %1094 : Tensor = prim::GetAttr[name="q"](%1086)
  %1095 : Tensor[] = prim::ListConstruct(%1081, %1094), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1095), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1097 : Tensor[] = prim::ListConstruct(%1081, %1093), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1098 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1097), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1099 : Tensor[] = prim::ListConstruct(%1081, %1092), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1100 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1099), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%1082, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1102 : Tensor[] = prim::ListConstruct(%r.12, %1091), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1103 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1102), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1104 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1090, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1105 : Tensor[] = prim::ListConstruct(%1104, %1098), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1105), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1107 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1089, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1108 : Tensor[] = prim::ListConstruct(%1107, %1103), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1108), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1110 : int = aten::size(%ac.11, %66), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1111 : int = aten::size(%x.41, %72), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1112 : int = aten::size(%x.41, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1113 : int = aten::size(%x.41, %67), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1114 : int = aten::size(%x.41, %66), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1115 : Long() = prim::NumToTensor(%1114), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1116 : int[] = prim::ListConstruct(%1111, %1112, %1114, %1113), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1118 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1119 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1118, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1120 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1119, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1120, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1122 : Long() = aten::sub(%1115, %55, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1123 : int = aten::Int(%1122), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1124 : int[] = prim::ListConstruct(%1111, %1112, %1113, %1123), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1126 : Long(13:1) = aten::arange(%1110, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %66, %1126), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1128 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1128, %70, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1129, %49), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1131 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1132 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1131), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1133 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1132, %47), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %1133, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %66, %63), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1136 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %60, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1137 : Tensor[] = prim::ListConstruct(%1136, %1100), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1138 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1137), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1139 : Tensor[] = prim::ListConstruct(%1138, %1088), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1139), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %60, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.11, %1081, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1143 : Tensor = prim::GetAttr[name="bias"](%1087)
  %1144 : Tensor = prim::GetAttr[name="weight"](%1087)
  %1145 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1145, %1144, %1143, %43, %44), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1147 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1148 : Float(13:17408, 17:1024, 1024:1), %1149 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1147)
  %1150 : __torch__.torch.nn.modules.normalization.___torch_mangle_41345.LayerNorm = prim::GetAttr[name="layer_norm"](%1085)
  %1151 : __torch__.torch.nn.modules.linear.___torch_mangle_41347.Linear = prim::GetAttr[name="layer_2"](%1085)
  %1152 : __torch__.torch.nn.modules.linear.___torch_mangle_41346.Linear = prim::GetAttr[name="layer_1"](%1085)
  %1153 : Tensor = prim::GetAttr[name="bias"](%1152)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1152)
  %1155 : Float(1024:1, 4096:1024) = aten::t(%1154), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1148, %1155), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %1153, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %60, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1160 : Tensor = prim::GetAttr[name="bias"](%1151)
  %1161 : Tensor = prim::GetAttr[name="weight"](%1151)
  %1162 : Float(4096:1, 1024:4096) = aten::t(%1161), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.99, %1162), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %1160, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.100, %60, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.33, %1148, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1167 : Tensor = prim::GetAttr[name="bias"](%1150)
  %1168 : Tensor = prim::GetAttr[name="weight"](%1150)
  %1169 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %1169, %1168, %1167, %43, %44), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1171 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1149)
  %1172 : Float(13:17408, 17:1024, 1024:1), %1173 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1171)
  %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1172, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1175 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1176 : __torch__.transformers.modeling_xlnet.___torch_mangle_41359.XLNetFeedForward = prim::GetAttr[name="ff"](%98)
  %1177 : __torch__.transformers.modeling_xlnet.___torch_mangle_41354.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%98)
  %1178 : __torch__.torch.nn.modules.normalization.___torch_mangle_41352.LayerNorm = prim::GetAttr[name="layer_norm"](%1177)
  %1179 : Tensor = prim::GetAttr[name="o"](%1177)
  %1180 : Tensor = prim::GetAttr[name="r_r_bias"](%1177)
  %1181 : Tensor = prim::GetAttr[name="r_w_bias"](%1177)
  %1182 : Tensor = prim::GetAttr[name="r"](%1177)
  %1183 : Tensor = prim::GetAttr[name="v"](%1177)
  %1184 : Tensor = prim::GetAttr[name="k"](%1177)
  %1185 : Tensor = prim::GetAttr[name="q"](%1177)
  %1186 : Tensor[] = prim::ListConstruct(%1172, %1185), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1186), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1188 : Tensor[] = prim::ListConstruct(%1172, %1184), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1189 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1188), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1190 : Tensor[] = prim::ListConstruct(%1172, %1183), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1191 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1190), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%1173, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1193 : Tensor[] = prim::ListConstruct(%r.13, %1182), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1194 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1193), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1195 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1181, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1196 : Tensor[] = prim::ListConstruct(%1195, %1189), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1196), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1198 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1180, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1199 : Tensor[] = prim::ListConstruct(%1198, %1194), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1199), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1201 : int = aten::size(%ac.12, %66), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1202 : int = aten::size(%x.45, %72), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1203 : int = aten::size(%x.45, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1204 : int = aten::size(%x.45, %67), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1205 : int = aten::size(%x.45, %66), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1206 : Long() = prim::NumToTensor(%1205), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1207 : int[] = prim::ListConstruct(%1202, %1203, %1205, %1204), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1207), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1209 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1210 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1209, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1211 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1210, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1211, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1213 : Long() = aten::sub(%1206, %55, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1214 : int = aten::Int(%1213), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1215 : int[] = prim::ListConstruct(%1202, %1203, %1204, %1214), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1215), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1217 : Long(13:1) = aten::arange(%1201, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %66, %1217), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1219 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1220 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1219, %70, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1220, %49), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1222 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1223 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1222), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1224 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1223, %47), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %1224, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %66, %63), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1227 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %60, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1228 : Tensor[] = prim::ListConstruct(%1227, %1191), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1229 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1228), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1230 : Tensor[] = prim::ListConstruct(%1229, %1179), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1230), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.104, %60, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.12, %1172, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1234 : Tensor = prim::GetAttr[name="bias"](%1178)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1178)
  %1236 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %1236, %1235, %1234, %43, %44), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1238 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1239 : Float(13:17408, 17:1024, 1024:1), %1240 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1238)
  %1241 : __torch__.torch.nn.modules.normalization.___torch_mangle_41355.LayerNorm = prim::GetAttr[name="layer_norm"](%1176)
  %1242 : __torch__.torch.nn.modules.linear.___torch_mangle_41357.Linear = prim::GetAttr[name="layer_2"](%1176)
  %1243 : __torch__.torch.nn.modules.linear.___torch_mangle_41356.Linear = prim::GetAttr[name="layer_1"](%1176)
  %1244 : Tensor = prim::GetAttr[name="bias"](%1243)
  %1245 : Tensor = prim::GetAttr[name="weight"](%1243)
  %1246 : Float(1024:1, 4096:1024) = aten::t(%1245), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1239, %1246), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %1244, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %60, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1251 : Tensor = prim::GetAttr[name="bias"](%1242)
  %1252 : Tensor = prim::GetAttr[name="weight"](%1242)
  %1253 : Float(4096:1, 1024:4096) = aten::t(%1252), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.108, %1253), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %1251, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.109, %60, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.36, %1239, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1258 : Tensor = prim::GetAttr[name="bias"](%1241)
  %1259 : Tensor = prim::GetAttr[name="weight"](%1241)
  %1260 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %1260, %1259, %1258, %43, %44), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1262 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1240)
  %1263 : Float(13:17408, 17:1024, 1024:1), %1264 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1262)
  %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1263, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1266 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1267 : __torch__.transformers.modeling_xlnet.___torch_mangle_41369.XLNetFeedForward = prim::GetAttr[name="ff"](%96)
  %1268 : __torch__.transformers.modeling_xlnet.___torch_mangle_41364.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%96)
  %1269 : __torch__.torch.nn.modules.normalization.___torch_mangle_41362.LayerNorm = prim::GetAttr[name="layer_norm"](%1268)
  %1270 : Tensor = prim::GetAttr[name="o"](%1268)
  %1271 : Tensor = prim::GetAttr[name="r_r_bias"](%1268)
  %1272 : Tensor = prim::GetAttr[name="r_w_bias"](%1268)
  %1273 : Tensor = prim::GetAttr[name="r"](%1268)
  %1274 : Tensor = prim::GetAttr[name="v"](%1268)
  %1275 : Tensor = prim::GetAttr[name="k"](%1268)
  %1276 : Tensor = prim::GetAttr[name="q"](%1268)
  %1277 : Tensor[] = prim::ListConstruct(%1263, %1276), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1277), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1279 : Tensor[] = prim::ListConstruct(%1263, %1275), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1280 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1279), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1281 : Tensor[] = prim::ListConstruct(%1263, %1274), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1282 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1281), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%1264, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1284 : Tensor[] = prim::ListConstruct(%r.14, %1273), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1285 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1284), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1286 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1272, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1287 : Tensor[] = prim::ListConstruct(%1286, %1280), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1287), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1289 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1271, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1290 : Tensor[] = prim::ListConstruct(%1289, %1285), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1290), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1292 : int = aten::size(%ac.13, %66), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1293 : int = aten::size(%x.49, %72), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1294 : int = aten::size(%x.49, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1295 : int = aten::size(%x.49, %67), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1296 : int = aten::size(%x.49, %66), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1297 : Long() = prim::NumToTensor(%1296), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1298 : int[] = prim::ListConstruct(%1293, %1294, %1296, %1295), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1298), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1300 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1301 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1300, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1302 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1301, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1302, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1304 : Long() = aten::sub(%1297, %55, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1305 : int = aten::Int(%1304), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1306 : int[] = prim::ListConstruct(%1293, %1294, %1295, %1305), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1306), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1308 : Long(13:1) = aten::arange(%1292, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %66, %1308), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1310 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1311 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1310, %70, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1311, %49), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1313 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1314 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1313), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1315 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1314, %47), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %1315, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %66, %63), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1318 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %60, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1319 : Tensor[] = prim::ListConstruct(%1318, %1282), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1320 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1319), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1321 : Tensor[] = prim::ListConstruct(%1320, %1270), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1321), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.113, %60, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.13, %1263, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1325 : Tensor = prim::GetAttr[name="bias"](%1269)
  %1326 : Tensor = prim::GetAttr[name="weight"](%1269)
  %1327 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %1327, %1326, %1325, %43, %44), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1329 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1330 : Float(13:17408, 17:1024, 1024:1), %1331 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1329)
  %1332 : __torch__.torch.nn.modules.normalization.___torch_mangle_41365.LayerNorm = prim::GetAttr[name="layer_norm"](%1267)
  %1333 : __torch__.torch.nn.modules.linear.___torch_mangle_41367.Linear = prim::GetAttr[name="layer_2"](%1267)
  %1334 : __torch__.torch.nn.modules.linear.___torch_mangle_41366.Linear = prim::GetAttr[name="layer_1"](%1267)
  %1335 : Tensor = prim::GetAttr[name="bias"](%1334)
  %1336 : Tensor = prim::GetAttr[name="weight"](%1334)
  %1337 : Float(1024:1, 4096:1024) = aten::t(%1336), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1330, %1337), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %1335, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.115), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %60, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1342 : Tensor = prim::GetAttr[name="bias"](%1333)
  %1343 : Tensor = prim::GetAttr[name="weight"](%1333)
  %1344 : Float(4096:1, 1024:4096) = aten::t(%1343), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.117, %1344), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %1342, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.118, %60, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.39, %1330, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1349 : Tensor = prim::GetAttr[name="bias"](%1332)
  %1350 : Tensor = prim::GetAttr[name="weight"](%1332)
  %1351 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %1351, %1350, %1349, %43, %44), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1353 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1331)
  %1354 : Float(13:17408, 17:1024, 1024:1), %1355 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1353)
  %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1354, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1357 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1358 : __torch__.transformers.modeling_xlnet.___torch_mangle_41379.XLNetFeedForward = prim::GetAttr[name="ff"](%94)
  %1359 : __torch__.transformers.modeling_xlnet.___torch_mangle_41374.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%94)
  %1360 : __torch__.torch.nn.modules.normalization.___torch_mangle_41372.LayerNorm = prim::GetAttr[name="layer_norm"](%1359)
  %1361 : Tensor = prim::GetAttr[name="o"](%1359)
  %1362 : Tensor = prim::GetAttr[name="r_r_bias"](%1359)
  %1363 : Tensor = prim::GetAttr[name="r_w_bias"](%1359)
  %1364 : Tensor = prim::GetAttr[name="r"](%1359)
  %1365 : Tensor = prim::GetAttr[name="v"](%1359)
  %1366 : Tensor = prim::GetAttr[name="k"](%1359)
  %1367 : Tensor = prim::GetAttr[name="q"](%1359)
  %1368 : Tensor[] = prim::ListConstruct(%1354, %1367), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1368), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1370 : Tensor[] = prim::ListConstruct(%1354, %1366), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1371 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1370), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1372 : Tensor[] = prim::ListConstruct(%1354, %1365), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1373 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1372), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%1355, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1375 : Tensor[] = prim::ListConstruct(%r.15, %1364), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1376 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1375), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1377 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1363, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1378 : Tensor[] = prim::ListConstruct(%1377, %1371), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1378), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1380 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1362, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1381 : Tensor[] = prim::ListConstruct(%1380, %1376), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1381), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1383 : int = aten::size(%ac.14, %66), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1384 : int = aten::size(%x.53, %72), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1385 : int = aten::size(%x.53, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1386 : int = aten::size(%x.53, %67), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1387 : int = aten::size(%x.53, %66), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1388 : Long() = prim::NumToTensor(%1387), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1389 : int[] = prim::ListConstruct(%1384, %1385, %1387, %1386), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1389), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1391 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1392 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1391, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1393 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1392, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1393, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1395 : Long() = aten::sub(%1388, %55, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1396 : int = aten::Int(%1395), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1397 : int[] = prim::ListConstruct(%1384, %1385, %1386, %1396), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1397), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1399 : Long(13:1) = aten::arange(%1383, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %66, %1399), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1401 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1402 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1401, %70, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1402, %49), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1404 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1405 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1404), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1406 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1405, %47), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %1406, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %66, %63), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1409 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %60, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1410 : Tensor[] = prim::ListConstruct(%1409, %1373), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1411 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1410), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1412 : Tensor[] = prim::ListConstruct(%1411, %1361), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1412), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.122, %60, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.14, %1354, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1416 : Tensor = prim::GetAttr[name="bias"](%1360)
  %1417 : Tensor = prim::GetAttr[name="weight"](%1360)
  %1418 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %1418, %1417, %1416, %43, %44), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1420 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1421 : Float(13:17408, 17:1024, 1024:1), %1422 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1420)
  %1423 : __torch__.torch.nn.modules.normalization.___torch_mangle_41375.LayerNorm = prim::GetAttr[name="layer_norm"](%1358)
  %1424 : __torch__.torch.nn.modules.linear.___torch_mangle_41377.Linear = prim::GetAttr[name="layer_2"](%1358)
  %1425 : __torch__.torch.nn.modules.linear.___torch_mangle_41376.Linear = prim::GetAttr[name="layer_1"](%1358)
  %1426 : Tensor = prim::GetAttr[name="bias"](%1425)
  %1427 : Tensor = prim::GetAttr[name="weight"](%1425)
  %1428 : Float(1024:1, 4096:1024) = aten::t(%1427), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1421, %1428), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %1426, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %60, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1433 : Tensor = prim::GetAttr[name="bias"](%1424)
  %1434 : Tensor = prim::GetAttr[name="weight"](%1424)
  %1435 : Float(4096:1, 1024:4096) = aten::t(%1434), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.126, %1435), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %1433, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.127, %60, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.42, %1421, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1440 : Tensor = prim::GetAttr[name="bias"](%1423)
  %1441 : Tensor = prim::GetAttr[name="weight"](%1423)
  %1442 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %1442, %1441, %1440, %43, %44), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1444 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1422)
  %1445 : Float(13:17408, 17:1024, 1024:1), %1446 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1444)
  %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1445, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1448 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1449 : __torch__.transformers.modeling_xlnet.___torch_mangle_41389.XLNetFeedForward = prim::GetAttr[name="ff"](%92)
  %1450 : __torch__.transformers.modeling_xlnet.___torch_mangle_41384.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%92)
  %1451 : __torch__.torch.nn.modules.normalization.___torch_mangle_41382.LayerNorm = prim::GetAttr[name="layer_norm"](%1450)
  %1452 : Tensor = prim::GetAttr[name="o"](%1450)
  %1453 : Tensor = prim::GetAttr[name="r_r_bias"](%1450)
  %1454 : Tensor = prim::GetAttr[name="r_w_bias"](%1450)
  %1455 : Tensor = prim::GetAttr[name="r"](%1450)
  %1456 : Tensor = prim::GetAttr[name="v"](%1450)
  %1457 : Tensor = prim::GetAttr[name="k"](%1450)
  %1458 : Tensor = prim::GetAttr[name="q"](%1450)
  %1459 : Tensor[] = prim::ListConstruct(%1445, %1458), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1459), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1461 : Tensor[] = prim::ListConstruct(%1445, %1457), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1462 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1461), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1463 : Tensor[] = prim::ListConstruct(%1445, %1456), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1464 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1463), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%1446, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1466 : Tensor[] = prim::ListConstruct(%r.16, %1455), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1467 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1466), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1468 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1454, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1469 : Tensor[] = prim::ListConstruct(%1468, %1462), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1469), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1471 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1453, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %1472 : Tensor[] = prim::ListConstruct(%1471, %1467), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1472), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1474 : int = aten::size(%ac.15, %66), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1475 : int = aten::size(%x.57, %72), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1476 : int = aten::size(%x.57, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1477 : int = aten::size(%x.57, %67), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1478 : int = aten::size(%x.57, %66), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1479 : Long() = prim::NumToTensor(%1478), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1480 : int[] = prim::ListConstruct(%1475, %1476, %1478, %1477), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %1480), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %1482 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1483 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1482, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1484 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1483, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1484, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1486 : Long() = aten::sub(%1479, %55, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1487 : int = aten::Int(%1486), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1488 : int[] = prim::ListConstruct(%1475, %1476, %1477, %1487), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %1488), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1490 : Long(13:1) = aten::arange(%1474, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %66, %1490), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1492 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1493 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1492, %70, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1493, %49), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1495 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1496 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1495), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1497 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1496, %47), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %1497, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %66, %63), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %1500 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %60, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1501 : Tensor[] = prim::ListConstruct(%1500, %1464), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1502 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1501), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1503 : Tensor[] = prim::ListConstruct(%1502, %1452), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1503), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.131, %60, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.15, %1445, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %1507 : Tensor = prim::GetAttr[name="bias"](%1451)
  %1508 : Tensor = prim::GetAttr[name="weight"](%1451)
  %1509 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %1509, %1508, %1507, %43, %44), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1511 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %1512 : Float(13:17408, 17:1024, 1024:1), %1513 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1511)
  %1514 : __torch__.torch.nn.modules.normalization.___torch_mangle_41385.LayerNorm = prim::GetAttr[name="layer_norm"](%1449)
  %1515 : __torch__.torch.nn.modules.linear.___torch_mangle_41387.Linear = prim::GetAttr[name="layer_2"](%1449)
  %1516 : __torch__.torch.nn.modules.linear.___torch_mangle_41386.Linear = prim::GetAttr[name="layer_1"](%1449)
  %1517 : Tensor = prim::GetAttr[name="bias"](%1516)
  %1518 : Tensor = prim::GetAttr[name="weight"](%1516)
  %1519 : Float(1024:1, 4096:1024) = aten::t(%1518), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1512, %1519), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %1517, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %60, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %1524 : Tensor = prim::GetAttr[name="bias"](%1515)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1515)
  %1526 : Float(4096:1, 1024:4096) = aten::t(%1525), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.135, %1526), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %1524, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.136, %60, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.45, %1512, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %1531 : Tensor = prim::GetAttr[name="bias"](%1514)
  %1532 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1533 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %1533, %1532, %1531, %43, %44), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %1535 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %1513)
  %1536 : Float(13:17408, 17:1024, 1024:1), %1537 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1535)
  %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1536, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1539 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1540 : __torch__.transformers.modeling_xlnet.___torch_mangle_41399.XLNetFeedForward = prim::GetAttr[name="ff"](%90)
  %1541 : __torch__.transformers.modeling_xlnet.___torch_mangle_41394.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%90)
  %1542 : __torch__.torch.nn.modules.normalization.___torch_mangle_41392.LayerNorm = prim::GetAttr[name="layer_norm"](%1541)
  %1543 : Tensor = prim::GetAttr[name="o"](%1541)
  %1544 : Tensor = prim::GetAttr[name="r_r_bias"](%1541)
  %1545 : Tensor = prim::GetAttr[name="r_w_bias"](%1541)
  %1546 : Tensor = prim::GetAttr[name="r"](%1541)
  %1547 : Tensor = prim::GetAttr[name="v"](%1541)
  %1548 : Tensor = prim::GetAttr[name="k"](%1541)
  %1549 : Tensor = prim::GetAttr[name="q"](%1541)
  %1550 : Tensor[] = prim::ListConstruct(%1536, %1549), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1550), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1552 : Tensor[] = prim::ListConstruct(%1536, %1548), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1553 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1552), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1554 : Tensor[] = prim::ListConstruct(%1536, %1547), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1555 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1554), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%1537, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %1557 : Tensor[] = prim::ListConstruct(%r.17, %1546), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1558 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1557), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1559 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1545, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %1560 : Tensor[] = prim::ListConstruct(%1559, %1553), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1560), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1562 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1544, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %1563 : Tensor[] = prim::ListConstruct(%1562, %1558), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1563), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1565 : int = aten::size(%ac.16, %66), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %1566 : int = aten::size(%x.61, %72), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1567 : int = aten::size(%x.61, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1568 : int = aten::size(%x.61, %67), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1569 : int = aten::size(%x.61, %66), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1570 : Long() = prim::NumToTensor(%1569), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1571 : int[] = prim::ListConstruct(%1566, %1567, %1569, %1568), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %1571), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %1573 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1574 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1573, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1575 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1574, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1575, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1577 : Long() = aten::sub(%1570, %55, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1578 : int = aten::Int(%1577), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1579 : int[] = prim::ListConstruct(%1566, %1567, %1568, %1578), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %1579), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1581 : Long(13:1) = aten::arange(%1565, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %66, %1581), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %1583 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1584 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1583, %70, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1584, %49), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1586 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1587 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1586), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1588 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1587, %47), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %1588, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %66, %63), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %1591 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %60, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %1592 : Tensor[] = prim::ListConstruct(%1591, %1555), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1593 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1592), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1594 : Tensor[] = prim::ListConstruct(%1593, %1543), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1594), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.140, %60, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.16, %1536, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %1598 : Tensor = prim::GetAttr[name="bias"](%1542)
  %1599 : Tensor = prim::GetAttr[name="weight"](%1542)
  %1600 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %1600, %1599, %1598, %43, %44), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1602 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %1603 : Float(13:17408, 17:1024, 1024:1), %1604 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1602)
  %1605 : __torch__.torch.nn.modules.normalization.___torch_mangle_41395.LayerNorm = prim::GetAttr[name="layer_norm"](%1540)
  %1606 : __torch__.torch.nn.modules.linear.___torch_mangle_41397.Linear = prim::GetAttr[name="layer_2"](%1540)
  %1607 : __torch__.torch.nn.modules.linear.___torch_mangle_41396.Linear = prim::GetAttr[name="layer_1"](%1540)
  %1608 : Tensor = prim::GetAttr[name="bias"](%1607)
  %1609 : Tensor = prim::GetAttr[name="weight"](%1607)
  %1610 : Float(1024:1, 4096:1024) = aten::t(%1609), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1603, %1610), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %1608, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.142), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %60, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %1615 : Tensor = prim::GetAttr[name="bias"](%1606)
  %1616 : Tensor = prim::GetAttr[name="weight"](%1606)
  %1617 : Float(4096:1, 1024:4096) = aten::t(%1616), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1617), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %1615, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.145, %60, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.48, %1603, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %1622 : Tensor = prim::GetAttr[name="bias"](%1605)
  %1623 : Tensor = prim::GetAttr[name="weight"](%1605)
  %1624 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %1624, %1623, %1622, %43, %44), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %1626 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %1604)
  %1627 : Float(13:17408, 17:1024, 1024:1), %1628 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1626)
  %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1627, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1630 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1631 : __torch__.transformers.modeling_xlnet.___torch_mangle_41409.XLNetFeedForward = prim::GetAttr[name="ff"](%88)
  %1632 : __torch__.transformers.modeling_xlnet.___torch_mangle_41404.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%88)
  %1633 : __torch__.torch.nn.modules.normalization.___torch_mangle_41402.LayerNorm = prim::GetAttr[name="layer_norm"](%1632)
  %1634 : Tensor = prim::GetAttr[name="o"](%1632)
  %1635 : Tensor = prim::GetAttr[name="r_r_bias"](%1632)
  %1636 : Tensor = prim::GetAttr[name="r_w_bias"](%1632)
  %1637 : Tensor = prim::GetAttr[name="r"](%1632)
  %1638 : Tensor = prim::GetAttr[name="v"](%1632)
  %1639 : Tensor = prim::GetAttr[name="k"](%1632)
  %1640 : Tensor = prim::GetAttr[name="q"](%1632)
  %1641 : Tensor[] = prim::ListConstruct(%1627, %1640), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1641), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1643 : Tensor[] = prim::ListConstruct(%1627, %1639), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1644 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1643), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1645 : Tensor[] = prim::ListConstruct(%1627, %1638), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1646 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1645), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%1628, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %1648 : Tensor[] = prim::ListConstruct(%r.18, %1637), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1649 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1648), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1650 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1636, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %1651 : Tensor[] = prim::ListConstruct(%1650, %1644), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1651), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1653 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1635, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %1654 : Tensor[] = prim::ListConstruct(%1653, %1649), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1654), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1656 : int = aten::size(%ac.17, %66), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %1657 : int = aten::size(%x.65, %72), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1658 : int = aten::size(%x.65, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1659 : int = aten::size(%x.65, %67), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1660 : int = aten::size(%x.65, %66), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1661 : Long() = prim::NumToTensor(%1660), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1662 : int[] = prim::ListConstruct(%1657, %1658, %1660, %1659), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %1662), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %1664 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1665 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1664, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1666 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1665, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1666, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1668 : Long() = aten::sub(%1661, %55, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1669 : int = aten::Int(%1668), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1670 : int[] = prim::ListConstruct(%1657, %1658, %1659, %1669), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %1670), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1672 : Long(13:1) = aten::arange(%1656, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %66, %1672), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %1674 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1675 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1674, %70, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1675, %49), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1677 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1678 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1677), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1679 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1678, %47), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %1679, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %66, %63), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %1682 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %60, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %1683 : Tensor[] = prim::ListConstruct(%1682, %1646), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1684 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1683), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1685 : Tensor[] = prim::ListConstruct(%1684, %1634), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1685), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.149, %60, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.17, %1627, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %1689 : Tensor = prim::GetAttr[name="bias"](%1633)
  %1690 : Tensor = prim::GetAttr[name="weight"](%1633)
  %1691 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %1691, %1690, %1689, %43, %44), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1693 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %1694 : Float(13:17408, 17:1024, 1024:1), %1695 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1693)
  %1696 : __torch__.torch.nn.modules.normalization.___torch_mangle_41405.LayerNorm = prim::GetAttr[name="layer_norm"](%1631)
  %1697 : __torch__.torch.nn.modules.linear.___torch_mangle_41407.Linear = prim::GetAttr[name="layer_2"](%1631)
  %1698 : __torch__.torch.nn.modules.linear.___torch_mangle_41406.Linear = prim::GetAttr[name="layer_1"](%1631)
  %1699 : Tensor = prim::GetAttr[name="bias"](%1698)
  %1700 : Tensor = prim::GetAttr[name="weight"](%1698)
  %1701 : Float(1024:1, 4096:1024) = aten::t(%1700), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1694, %1701), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %1699, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %60, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %1706 : Tensor = prim::GetAttr[name="bias"](%1697)
  %1707 : Tensor = prim::GetAttr[name="weight"](%1697)
  %1708 : Float(4096:1, 1024:4096) = aten::t(%1707), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1708), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %1706, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %60, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.51, %1694, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %1713 : Tensor = prim::GetAttr[name="bias"](%1696)
  %1714 : Tensor = prim::GetAttr[name="weight"](%1696)
  %1715 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1715, %1714, %1713, %43, %44), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %1717 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %1695)
  %1718 : Float(13:17408, 17:1024, 1024:1), %1719 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1717)
  %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1718, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1721 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1722 : __torch__.transformers.modeling_xlnet.___torch_mangle_41419.XLNetFeedForward = prim::GetAttr[name="ff"](%86)
  %1723 : __torch__.transformers.modeling_xlnet.___torch_mangle_41414.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%86)
  %1724 : __torch__.torch.nn.modules.normalization.___torch_mangle_41412.LayerNorm = prim::GetAttr[name="layer_norm"](%1723)
  %1725 : Tensor = prim::GetAttr[name="o"](%1723)
  %1726 : Tensor = prim::GetAttr[name="r_r_bias"](%1723)
  %1727 : Tensor = prim::GetAttr[name="r_w_bias"](%1723)
  %1728 : Tensor = prim::GetAttr[name="r"](%1723)
  %1729 : Tensor = prim::GetAttr[name="v"](%1723)
  %1730 : Tensor = prim::GetAttr[name="k"](%1723)
  %1731 : Tensor = prim::GetAttr[name="q"](%1723)
  %1732 : Tensor[] = prim::ListConstruct(%1718, %1731), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1732), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1734 : Tensor[] = prim::ListConstruct(%1718, %1730), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1735 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1734), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1736 : Tensor[] = prim::ListConstruct(%1718, %1729), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1737 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1736), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%1719, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %1739 : Tensor[] = prim::ListConstruct(%r.19, %1728), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1740 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1739), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1741 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1727, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %1742 : Tensor[] = prim::ListConstruct(%1741, %1735), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1742), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1744 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1726, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %1745 : Tensor[] = prim::ListConstruct(%1744, %1740), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1745), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1747 : int = aten::size(%ac.18, %66), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %1748 : int = aten::size(%x.69, %72), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1749 : int = aten::size(%x.69, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1750 : int = aten::size(%x.69, %67), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1751 : int = aten::size(%x.69, %66), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1752 : Long() = prim::NumToTensor(%1751), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1753 : int[] = prim::ListConstruct(%1748, %1749, %1751, %1750), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %1753), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %1755 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1756 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1755, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1757 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1756, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1757, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1759 : Long() = aten::sub(%1752, %55, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1760 : int = aten::Int(%1759), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1761 : int[] = prim::ListConstruct(%1748, %1749, %1750, %1760), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %1761), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1763 : Long(13:1) = aten::arange(%1747, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %66, %1763), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %1765 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1766 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1765, %70, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1766, %49), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1768 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1769 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1768), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1770 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1769, %47), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %1770, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %66, %63), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %1773 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %60, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %1774 : Tensor[] = prim::ListConstruct(%1773, %1737), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1775 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1774), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1776 : Tensor[] = prim::ListConstruct(%1775, %1725), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1776), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.158, %60, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.18, %1718, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %1780 : Tensor = prim::GetAttr[name="bias"](%1724)
  %1781 : Tensor = prim::GetAttr[name="weight"](%1724)
  %1782 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %1782, %1781, %1780, %43, %44), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1784 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %1785 : Float(13:17408, 17:1024, 1024:1), %1786 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1784)
  %1787 : __torch__.torch.nn.modules.normalization.___torch_mangle_41415.LayerNorm = prim::GetAttr[name="layer_norm"](%1722)
  %1788 : __torch__.torch.nn.modules.linear.___torch_mangle_41417.Linear = prim::GetAttr[name="layer_2"](%1722)
  %1789 : __torch__.torch.nn.modules.linear.___torch_mangle_41416.Linear = prim::GetAttr[name="layer_1"](%1722)
  %1790 : Tensor = prim::GetAttr[name="bias"](%1789)
  %1791 : Tensor = prim::GetAttr[name="weight"](%1789)
  %1792 : Float(1024:1, 4096:1024) = aten::t(%1791), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1785, %1792), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %1790, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.160), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %60, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %1797 : Tensor = prim::GetAttr[name="bias"](%1788)
  %1798 : Tensor = prim::GetAttr[name="weight"](%1788)
  %1799 : Float(4096:1, 1024:4096) = aten::t(%1798), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.162, %1799), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %1797, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.163, %60, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.54, %1785, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %1804 : Tensor = prim::GetAttr[name="bias"](%1787)
  %1805 : Tensor = prim::GetAttr[name="weight"](%1787)
  %1806 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %1806, %1805, %1804, %43, %44), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %1808 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %1786)
  %1809 : Float(13:17408, 17:1024, 1024:1), %1810 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1808)
  %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1809, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1812 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1813 : __torch__.transformers.modeling_xlnet.___torch_mangle_41429.XLNetFeedForward = prim::GetAttr[name="ff"](%84)
  %1814 : __torch__.transformers.modeling_xlnet.___torch_mangle_41424.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%84)
  %1815 : __torch__.torch.nn.modules.normalization.___torch_mangle_41422.LayerNorm = prim::GetAttr[name="layer_norm"](%1814)
  %1816 : Tensor = prim::GetAttr[name="o"](%1814)
  %1817 : Tensor = prim::GetAttr[name="r_r_bias"](%1814)
  %1818 : Tensor = prim::GetAttr[name="r_w_bias"](%1814)
  %1819 : Tensor = prim::GetAttr[name="r"](%1814)
  %1820 : Tensor = prim::GetAttr[name="v"](%1814)
  %1821 : Tensor = prim::GetAttr[name="k"](%1814)
  %1822 : Tensor = prim::GetAttr[name="q"](%1814)
  %1823 : Tensor[] = prim::ListConstruct(%1809, %1822), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1823), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1825 : Tensor[] = prim::ListConstruct(%1809, %1821), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1826 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1825), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1827 : Tensor[] = prim::ListConstruct(%1809, %1820), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1828 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1827), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%1810, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %1830 : Tensor[] = prim::ListConstruct(%r.20, %1819), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1831 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1830), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1832 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1818, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %1833 : Tensor[] = prim::ListConstruct(%1832, %1826), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1833), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1835 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1817, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %1836 : Tensor[] = prim::ListConstruct(%1835, %1831), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1836), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1838 : int = aten::size(%ac.19, %66), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %1839 : int = aten::size(%x.73, %72), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1840 : int = aten::size(%x.73, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1841 : int = aten::size(%x.73, %67), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1842 : int = aten::size(%x.73, %66), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1843 : Long() = prim::NumToTensor(%1842), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1844 : int[] = prim::ListConstruct(%1839, %1840, %1842, %1841), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %1844), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %1846 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1847 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1846, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1848 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1847, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1848, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1850 : Long() = aten::sub(%1843, %55, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1851 : int = aten::Int(%1850), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1852 : int[] = prim::ListConstruct(%1839, %1840, %1841, %1851), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %1852), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1854 : Long(13:1) = aten::arange(%1838, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %66, %1854), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %1856 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1857 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1856, %70, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1857, %49), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1859 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1860 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1859), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1861 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1860, %47), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %1861, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %66, %63), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %1864 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %60, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %1865 : Tensor[] = prim::ListConstruct(%1864, %1828), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1866 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1865), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1867 : Tensor[] = prim::ListConstruct(%1866, %1816), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1867), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.167, %60, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.19, %1809, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %1871 : Tensor = prim::GetAttr[name="bias"](%1815)
  %1872 : Tensor = prim::GetAttr[name="weight"](%1815)
  %1873 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %1873, %1872, %1871, %43, %44), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1875 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %1876 : Float(13:17408, 17:1024, 1024:1), %1877 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1875)
  %1878 : __torch__.torch.nn.modules.normalization.___torch_mangle_41425.LayerNorm = prim::GetAttr[name="layer_norm"](%1813)
  %1879 : __torch__.torch.nn.modules.linear.___torch_mangle_41427.Linear = prim::GetAttr[name="layer_2"](%1813)
  %1880 : __torch__.torch.nn.modules.linear.___torch_mangle_41426.Linear = prim::GetAttr[name="layer_1"](%1813)
  %1881 : Tensor = prim::GetAttr[name="bias"](%1880)
  %1882 : Tensor = prim::GetAttr[name="weight"](%1880)
  %1883 : Float(1024:1, 4096:1024) = aten::t(%1882), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1876, %1883), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %1881, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.169), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %60, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %1888 : Tensor = prim::GetAttr[name="bias"](%1879)
  %1889 : Tensor = prim::GetAttr[name="weight"](%1879)
  %1890 : Float(4096:1, 1024:4096) = aten::t(%1889), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.171, %1890), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %1888, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.172, %60, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.57, %1876, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %1895 : Tensor = prim::GetAttr[name="bias"](%1878)
  %1896 : Tensor = prim::GetAttr[name="weight"](%1878)
  %1897 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %1897, %1896, %1895, %43, %44), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %1899 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %1877)
  %1900 : Float(13:17408, 17:1024, 1024:1), %1901 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1899)
  %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1900, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1903 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1904 : __torch__.transformers.modeling_xlnet.___torch_mangle_41439.XLNetFeedForward = prim::GetAttr[name="ff"](%82)
  %1905 : __torch__.transformers.modeling_xlnet.___torch_mangle_41434.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%82)
  %1906 : __torch__.torch.nn.modules.normalization.___torch_mangle_41432.LayerNorm = prim::GetAttr[name="layer_norm"](%1905)
  %1907 : Tensor = prim::GetAttr[name="o"](%1905)
  %1908 : Tensor = prim::GetAttr[name="r_r_bias"](%1905)
  %1909 : Tensor = prim::GetAttr[name="r_w_bias"](%1905)
  %1910 : Tensor = prim::GetAttr[name="r"](%1905)
  %1911 : Tensor = prim::GetAttr[name="v"](%1905)
  %1912 : Tensor = prim::GetAttr[name="k"](%1905)
  %1913 : Tensor = prim::GetAttr[name="q"](%1905)
  %1914 : Tensor[] = prim::ListConstruct(%1900, %1913), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1914), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1916 : Tensor[] = prim::ListConstruct(%1900, %1912), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1917 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1916), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1918 : Tensor[] = prim::ListConstruct(%1900, %1911), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1919 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1918), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%1901, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %1921 : Tensor[] = prim::ListConstruct(%r.21, %1910), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1922 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %1921), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1923 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1909, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %1924 : Tensor[] = prim::ListConstruct(%1923, %1917), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %1924), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1926 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1908, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %1927 : Tensor[] = prim::ListConstruct(%1926, %1922), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %1927), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1929 : int = aten::size(%ac.20, %66), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %1930 : int = aten::size(%x.77, %72), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1931 : int = aten::size(%x.77, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1932 : int = aten::size(%x.77, %67), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1933 : int = aten::size(%x.77, %66), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1934 : Long() = prim::NumToTensor(%1933), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1935 : int[] = prim::ListConstruct(%1930, %1931, %1933, %1932), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %1935), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %1937 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1938 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1937, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1939 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1938, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1939, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1941 : Long() = aten::sub(%1934, %55, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1942 : int = aten::Int(%1941), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1943 : int[] = prim::ListConstruct(%1930, %1931, %1932, %1942), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %1943), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1945 : Long(13:1) = aten::arange(%1929, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %66, %1945), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %1947 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1948 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1947, %70, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1948, %49), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1950 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1951 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %1950), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1952 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1951, %47), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %1952, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %66, %63), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %1955 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %60, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %1956 : Tensor[] = prim::ListConstruct(%1955, %1919), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1957 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %1956), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1958 : Tensor[] = prim::ListConstruct(%1957, %1907), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %1958), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.176, %60, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.20, %1900, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %1962 : Tensor = prim::GetAttr[name="bias"](%1906)
  %1963 : Tensor = prim::GetAttr[name="weight"](%1906)
  %1964 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %1964, %1963, %1962, %43, %44), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1966 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %1967 : Float(13:17408, 17:1024, 1024:1), %1968 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1966)
  %1969 : __torch__.torch.nn.modules.normalization.___torch_mangle_41435.LayerNorm = prim::GetAttr[name="layer_norm"](%1904)
  %1970 : __torch__.torch.nn.modules.linear.___torch_mangle_41437.Linear = prim::GetAttr[name="layer_2"](%1904)
  %1971 : __torch__.torch.nn.modules.linear.___torch_mangle_41436.Linear = prim::GetAttr[name="layer_1"](%1904)
  %1972 : Tensor = prim::GetAttr[name="bias"](%1971)
  %1973 : Tensor = prim::GetAttr[name="weight"](%1971)
  %1974 : Float(1024:1, 4096:1024) = aten::t(%1973), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1967, %1974), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %1972, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.178), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %60, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %1979 : Tensor = prim::GetAttr[name="bias"](%1970)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1970)
  %1981 : Float(4096:1, 1024:4096) = aten::t(%1980), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.180, %1981), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %1979, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.181, %60, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.60, %1967, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %1986 : Tensor = prim::GetAttr[name="bias"](%1969)
  %1987 : Tensor = prim::GetAttr[name="weight"](%1969)
  %1988 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %1988, %1987, %1986, %43, %44), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %1990 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %1968)
  %1991 : Float(13:17408, 17:1024, 1024:1), %1992 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1990)
  %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1991, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1994 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1995 : __torch__.transformers.modeling_xlnet.___torch_mangle_41449.XLNetFeedForward = prim::GetAttr[name="ff"](%80)
  %1996 : __torch__.transformers.modeling_xlnet.___torch_mangle_41444.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%80)
  %1997 : __torch__.torch.nn.modules.normalization.___torch_mangle_41442.LayerNorm = prim::GetAttr[name="layer_norm"](%1996)
  %1998 : Tensor = prim::GetAttr[name="o"](%1996)
  %1999 : Tensor = prim::GetAttr[name="r_r_bias"](%1996)
  %2000 : Tensor = prim::GetAttr[name="r_w_bias"](%1996)
  %2001 : Tensor = prim::GetAttr[name="r"](%1996)
  %2002 : Tensor = prim::GetAttr[name="v"](%1996)
  %2003 : Tensor = prim::GetAttr[name="k"](%1996)
  %2004 : Tensor = prim::GetAttr[name="q"](%1996)
  %2005 : Tensor[] = prim::ListConstruct(%1991, %2004), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2005), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2007 : Tensor[] = prim::ListConstruct(%1991, %2003), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2008 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2007), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2009 : Tensor[] = prim::ListConstruct(%1991, %2002), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2010 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2009), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%1992, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2012 : Tensor[] = prim::ListConstruct(%r.22, %2001), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2013 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2012), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2014 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %2000, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2015 : Tensor[] = prim::ListConstruct(%2014, %2008), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %2015), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2017 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %1999, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2018 : Tensor[] = prim::ListConstruct(%2017, %2013), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %2018), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2020 : int = aten::size(%ac.21, %66), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2021 : int = aten::size(%x.81, %72), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2022 : int = aten::size(%x.81, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2023 : int = aten::size(%x.81, %67), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2024 : int = aten::size(%x.81, %66), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2025 : Long() = prim::NumToTensor(%2024), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2026 : int[] = prim::ListConstruct(%2021, %2022, %2024, %2023), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2026), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2028 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2029 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2028, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2030 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2029, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2030, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2032 : Long() = aten::sub(%2025, %55, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2033 : int = aten::Int(%2032), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2034 : int[] = prim::ListConstruct(%2021, %2022, %2023, %2033), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2034), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2036 : Long(13:1) = aten::arange(%2020, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %66, %2036), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2038 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2039 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2038, %70, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2039, %49), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2041 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2042 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %2041), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2043 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2042, %47), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %2043, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %66, %63), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2046 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %60, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2047 : Tensor[] = prim::ListConstruct(%2046, %2010), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2048 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %2047), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2049 : Tensor[] = prim::ListConstruct(%2048, %1998), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %2049), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.185, %60, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.21, %1991, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2053 : Tensor = prim::GetAttr[name="bias"](%1997)
  %2054 : Tensor = prim::GetAttr[name="weight"](%1997)
  %2055 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %2055, %2054, %2053, %43, %44), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2057 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2058 : Float(13:17408, 17:1024, 1024:1), %2059 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2057)
  %2060 : __torch__.torch.nn.modules.normalization.___torch_mangle_41445.LayerNorm = prim::GetAttr[name="layer_norm"](%1995)
  %2061 : __torch__.torch.nn.modules.linear.___torch_mangle_41447.Linear = prim::GetAttr[name="layer_2"](%1995)
  %2062 : __torch__.torch.nn.modules.linear.___torch_mangle_41446.Linear = prim::GetAttr[name="layer_1"](%1995)
  %2063 : Tensor = prim::GetAttr[name="bias"](%2062)
  %2064 : Tensor = prim::GetAttr[name="weight"](%2062)
  %2065 : Float(1024:1, 4096:1024) = aten::t(%2064), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2058, %2065), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2063, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.187), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %60, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2070 : Tensor = prim::GetAttr[name="bias"](%2061)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2061)
  %2072 : Float(4096:1, 1024:4096) = aten::t(%2071), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.189, %2072), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2070, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.190, %60, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.63, %2058, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2077 : Tensor = prim::GetAttr[name="bias"](%2060)
  %2078 : Tensor = prim::GetAttr[name="weight"](%2060)
  %2079 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %2079, %2078, %2077, %43, %44), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2081 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2059)
  %2082 : Float(13:17408, 17:1024, 1024:1), %2083 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2081)
  %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2082, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2085 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2086 : __torch__.transformers.modeling_xlnet.___torch_mangle_41459.XLNetFeedForward = prim::GetAttr[name="ff"](%78)
  %2087 : __torch__.transformers.modeling_xlnet.___torch_mangle_41454.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%78)
  %2088 : __torch__.torch.nn.modules.normalization.___torch_mangle_41452.LayerNorm = prim::GetAttr[name="layer_norm"](%2087)
  %2089 : Tensor = prim::GetAttr[name="o"](%2087)
  %2090 : Tensor = prim::GetAttr[name="r_r_bias"](%2087)
  %2091 : Tensor = prim::GetAttr[name="r_w_bias"](%2087)
  %2092 : Tensor = prim::GetAttr[name="r"](%2087)
  %2093 : Tensor = prim::GetAttr[name="v"](%2087)
  %2094 : Tensor = prim::GetAttr[name="k"](%2087)
  %2095 : Tensor = prim::GetAttr[name="q"](%2087)
  %2096 : Tensor[] = prim::ListConstruct(%2082, %2095), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2096), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2098 : Tensor[] = prim::ListConstruct(%2082, %2094), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2099 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2098), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2100 : Tensor[] = prim::ListConstruct(%2082, %2093), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2101 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2100), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2083, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2103 : Tensor[] = prim::ListConstruct(%r.23, %2092), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2104 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2103), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2105 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2091, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2106 : Tensor[] = prim::ListConstruct(%2105, %2099), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %2106), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2108 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2090, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2109 : Tensor[] = prim::ListConstruct(%2108, %2104), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %2109), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2111 : int = aten::size(%ac.22, %66), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2112 : int = aten::size(%x.85, %72), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2113 : int = aten::size(%x.85, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2114 : int = aten::size(%x.85, %67), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2115 : int = aten::size(%x.85, %66), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2116 : Long() = prim::NumToTensor(%2115), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2117 : int[] = prim::ListConstruct(%2112, %2113, %2115, %2114), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2119 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2120 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2119, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2121 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2120, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2121, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2123 : Long() = aten::sub(%2116, %55, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2124 : int = aten::Int(%2123), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2125 : int[] = prim::ListConstruct(%2112, %2113, %2114, %2124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2125), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2127 : Long(13:1) = aten::arange(%2111, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %66, %2127), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2129, %70, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2130, %49), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2132 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2133 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %2132), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2134 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2133, %47), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %2134, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %66, %63), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2137 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %60, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2138 : Tensor[] = prim::ListConstruct(%2137, %2101), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2139 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %2138), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2140 : Tensor[] = prim::ListConstruct(%2139, %2089), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %2140), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.194, %60, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.22, %2082, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2144 : Tensor = prim::GetAttr[name="bias"](%2088)
  %2145 : Tensor = prim::GetAttr[name="weight"](%2088)
  %2146 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %2146, %2145, %2144, %43, %44), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2148 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2149 : Float(13:17408, 17:1024, 1024:1), %2150 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2148)
  %2151 : __torch__.torch.nn.modules.normalization.___torch_mangle_41455.LayerNorm = prim::GetAttr[name="layer_norm"](%2086)
  %2152 : __torch__.torch.nn.modules.linear.___torch_mangle_41457.Linear = prim::GetAttr[name="layer_2"](%2086)
  %2153 : __torch__.torch.nn.modules.linear.___torch_mangle_41456.Linear = prim::GetAttr[name="layer_1"](%2086)
  %2154 : Tensor = prim::GetAttr[name="bias"](%2153)
  %2155 : Tensor = prim::GetAttr[name="weight"](%2153)
  %2156 : Float(1024:1, 4096:1024) = aten::t(%2155), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2149, %2156), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2154, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.196), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %60, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2161 : Tensor = prim::GetAttr[name="bias"](%2152)
  %2162 : Tensor = prim::GetAttr[name="weight"](%2152)
  %2163 : Float(4096:1, 1024:4096) = aten::t(%2162), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.198, %2163), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2161, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.199, %60, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.66, %2149, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2168 : Tensor = prim::GetAttr[name="bias"](%2151)
  %2169 : Tensor = prim::GetAttr[name="weight"](%2151)
  %2170 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %2170, %2169, %2168, %43, %44), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2172 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2150)
  %2173 : Float(13:17408, 17:1024, 1024:1), %2174 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2172)
  %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2173, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2176 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2177 : __torch__.transformers.modeling_xlnet.___torch_mangle_41469.XLNetFeedForward = prim::GetAttr[name="ff"](%76)
  %2178 : __torch__.transformers.modeling_xlnet.___torch_mangle_41464.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%76)
  %2179 : __torch__.torch.nn.modules.normalization.___torch_mangle_41462.LayerNorm = prim::GetAttr[name="layer_norm"](%2178)
  %2180 : Tensor = prim::GetAttr[name="o"](%2178)
  %2181 : Tensor = prim::GetAttr[name="r_r_bias"](%2178)
  %2182 : Tensor = prim::GetAttr[name="r_w_bias"](%2178)
  %2183 : Tensor = prim::GetAttr[name="r"](%2178)
  %2184 : Tensor = prim::GetAttr[name="v"](%2178)
  %2185 : Tensor = prim::GetAttr[name="k"](%2178)
  %2186 : Tensor = prim::GetAttr[name="q"](%2178)
  %2187 : Tensor[] = prim::ListConstruct(%2173, %2186), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2187), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2189 : Tensor[] = prim::ListConstruct(%2173, %2185), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2190 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2189), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2191 : Tensor[] = prim::ListConstruct(%2173, %2184), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2192 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2191), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2174, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2194 : Tensor[] = prim::ListConstruct(%r, %2183), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2195 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2194), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2196 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2182, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2197 : Tensor[] = prim::ListConstruct(%2196, %2190), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %2197), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2199 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2181, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2200 : Tensor[] = prim::ListConstruct(%2199, %2195), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %2200), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2202 : int = aten::size(%ac.23, %66), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2203 : int = aten::size(%x.89, %72), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2204 : int = aten::size(%x.89, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2205 : int = aten::size(%x.89, %67), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2206 : int = aten::size(%x.89, %66), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2207 : Long() = prim::NumToTensor(%2206), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2208 : int[] = prim::ListConstruct(%2203, %2204, %2206, %2205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2208), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2210 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2211 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2210, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2212 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2211, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2212, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2214 : Long() = aten::sub(%2207, %55, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2215 : int = aten::Int(%2214), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2216 : int[] = prim::ListConstruct(%2203, %2204, %2205, %2215), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2216), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2218 : Long(13:1) = aten::arange(%2202, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %66, %2218), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2220 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2221 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2220, %70, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2221, %49), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2223 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2224 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %2223), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2225 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2224, %47), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %2225, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %66, %63), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2228 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %60, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2229 : Tensor[] = prim::ListConstruct(%2228, %2192), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2230 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %2229), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2231 : Tensor[] = prim::ListConstruct(%2230, %2180), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %2231), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.203, %60, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.23, %2173, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2235 : Tensor = prim::GetAttr[name="bias"](%2179)
  %2236 : Tensor = prim::GetAttr[name="weight"](%2179)
  %2237 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %2237, %2236, %2235, %43, %44), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2239 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2240 : Float(13:17408, 17:1024, 1024:1), %2241 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2239)
  %2242 : __torch__.torch.nn.modules.normalization.___torch_mangle_41465.LayerNorm = prim::GetAttr[name="layer_norm"](%2177)
  %2243 : __torch__.torch.nn.modules.linear.___torch_mangle_41467.Linear = prim::GetAttr[name="layer_2"](%2177)
  %2244 : __torch__.torch.nn.modules.linear.___torch_mangle_41466.Linear = prim::GetAttr[name="layer_1"](%2177)
  %2245 : Tensor = prim::GetAttr[name="bias"](%2244)
  %2246 : Tensor = prim::GetAttr[name="weight"](%2244)
  %2247 : Float(1024:1, 4096:1024) = aten::t(%2246), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2240, %2247), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2245, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %60, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2252 : Tensor = prim::GetAttr[name="bias"](%2243)
  %2253 : Tensor = prim::GetAttr[name="weight"](%2243)
  %2254 : Float(4096:1, 1024:4096) = aten::t(%2253), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.207, %2254), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2252, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.208, %60, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.69, %2240, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2259 : Tensor = prim::GetAttr[name="bias"](%2242)
  %2260 : Tensor = prim::GetAttr[name="weight"](%2242)
  %2261 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
  %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %2261, %2260, %2259, %43, %44), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2263 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2241)
  %2264 : Float(13:17408, 17:1024, 1024:1), %2265 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2263)
  %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2264, %72, %72, %68, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2267 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2268 : __torch__.transformers.modeling_xlnet.___torch_mangle_41479.XLNetFeedForward = prim::GetAttr[name="ff"](%74)
  %2269 : __torch__.transformers.modeling_xlnet.___torch_mangle_41474.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%74)
  %2270 : __torch__.torch.nn.modules.normalization.___torch_mangle_41472.LayerNorm = prim::GetAttr[name="layer_norm"](%2269)
  %2271 : Tensor = prim::GetAttr[name="o"](%2269)
  %2272 : Tensor = prim::GetAttr[name="r_r_bias"](%2269)
  %2273 : Tensor = prim::GetAttr[name="r_w_bias"](%2269)
  %2274 : Tensor = prim::GetAttr[name="r"](%2269)
  %2275 : Tensor = prim::GetAttr[name="v"](%2269)
  %2276 : Tensor = prim::GetAttr[name="k"](%2269)
  %2277 : Tensor = prim::GetAttr[name="q"](%2269)
  %2278 : Tensor[] = prim::ListConstruct(%2264, %2277), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2278), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2280 : Tensor[] = prim::ListConstruct(%2264, %2276), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2281 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2280), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2282 : Tensor[] = prim::ListConstruct(%2264, %2275), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2283 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2282), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2284 : Float(26:1024, 17:0, 1024:1) = aten::to(%2265, %62, %65, %64, %64, %63), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2285 : Tensor[] = prim::ListConstruct(%2284, %2274), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2286 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%52, %2285), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2287 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2273, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2288 : Tensor[] = prim::ListConstruct(%2287, %2281), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%51, %2288), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2290 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2272, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %2291 : Tensor[] = prim::ListConstruct(%2290, %2286), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%51, %2291), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2293 : int = aten::size(%ac, %66), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2294 : int = aten::size(%x.93, %72), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2295 : int = aten::size(%x.93, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2296 : int = aten::size(%x.93, %67), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2297 : int = aten::size(%x.93, %66), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2298 : Long() = prim::NumToTensor(%2297), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2299 : int[] = prim::ListConstruct(%2294, %2295, %2297, %2296), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %2299), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %2301 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %72, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2302 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2301, %71, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2303 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2302, %67, %71, %68, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2303, %66, %72, %68, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2305 : Long() = aten::sub(%2298, %55, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2306 : int = aten::Int(%2305), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2307 : int[] = prim::ListConstruct(%2294, %2295, %2296, %2306), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %2307), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2309 : Long(13:1) = aten::arange(%2293, %50, %72, %62, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %66, %2309), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2311 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2312 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2311, %70, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2312, %49), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2314 : Tensor[] = prim::ListConstruct(%148), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2315 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%48, %2314), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2316 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2315, %47), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %2316, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %66, %63), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %2319 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %60, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2320 : Tensor[] = prim::ListConstruct(%2319, %2283), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2321 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%46, %2320), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2322 : Tensor[] = prim::ListConstruct(%2321, %2271), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%45, %2322), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %60, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out, %2264, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %2326 : Tensor = prim::GetAttr[name="bias"](%2270)
  %2327 : Tensor = prim::GetAttr[name="weight"](%2270)
  %2328 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2328, %2327, %2326, %43, %44), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2330 : __torch__.torch.nn.modules.normalization.___torch_mangle_41475.LayerNorm = prim::GetAttr[name="layer_norm"](%2268)
  %2331 : __torch__.torch.nn.modules.linear.___torch_mangle_41477.Linear = prim::GetAttr[name="layer_2"](%2268)
  %2332 : __torch__.torch.nn.modules.linear.___torch_mangle_41476.Linear = prim::GetAttr[name="layer_1"](%2268)
  %2333 : Tensor = prim::GetAttr[name="bias"](%2332)
  %2334 : Tensor = prim::GetAttr[name="weight"](%2332)
  %2335 : Float(1024:1, 4096:1024) = aten::t(%2334), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input_tensor, %2335), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2333, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.214), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %60, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %2340 : Tensor = prim::GetAttr[name="bias"](%2331)
  %2341 : Tensor = prim::GetAttr[name="weight"](%2331)
  %2342 : Float(4096:1, 1024:4096) = aten::t(%2341), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.216, %2342), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2340, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.217, %60, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.72, %input_tensor, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %2347 : Tensor = prim::GetAttr[name="bias"](%2330)
  %2348 : Tensor = prim::GetAttr[name="weight"](%2330)
  %2349 : int[] = prim::ListConstruct(%59), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %2349, %2348, %2347, %43, %44), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %output_h : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.219, %60, %64), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %2352 : int[] = prim::ListConstruct(%71, %72, %67), scope: __module.transformer
  %2353 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%output_h, %2352), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %input : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%2353, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %2355 : (Float(17:13312, 13:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%input, %174, %265, %356, %447, %538, %629, %720, %811, %902, %993, %1084, %1175, %1266, %1357, %1448, %1539, %1630, %1721, %1812, %1903, %1994, %2085, %2176, %2267)
  %6 : Float(17:13312, 13:1024, 1024:1), %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(13:17408, 17:1024, 1024:1), %9 : Float(13:17408, 17:1024, 1024:1), %10 : Float(13:17408, 17:1024, 1024:1), %11 : Float(13:17408, 17:1024, 1024:1), %12 : Float(13:17408, 17:1024, 1024:1), %13 : Float(13:17408, 17:1024, 1024:1), %14 : Float(13:17408, 17:1024, 1024:1), %15 : Float(13:17408, 17:1024, 1024:1), %16 : Float(13:17408, 17:1024, 1024:1), %17 : Float(13:17408, 17:1024, 1024:1), %18 : Float(13:17408, 17:1024, 1024:1), %19 : Float(13:17408, 17:1024, 1024:1), %20 : Float(13:17408, 17:1024, 1024:1), %21 : Float(13:17408, 17:1024, 1024:1), %22 : Float(13:17408, 17:1024, 1024:1), %23 : Float(13:17408, 17:1024, 1024:1), %24 : Float(13:17408, 17:1024, 1024:1), %25 : Float(13:17408, 17:1024, 1024:1), %26 : Float(13:17408, 17:1024, 1024:1), %27 : Float(13:17408, 17:1024, 1024:1), %28 : Float(13:17408, 17:1024, 1024:1), %29 : Float(13:17408, 17:1024, 1024:1), %30 : Float(13:17408, 17:1024, 1024:1) = prim::TupleUnpack(%2355)
  %2356 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %2357 : Tensor = prim::GetAttr[name="bias"](%3)
  %2358 : Tensor = prim::GetAttr[name="weight"](%3)
  %2359 : Float(1024:1, 2:1024) = aten::t(%2358), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%6, %2359), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %2361 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %2357, %2356), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %32 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %33 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %34 : Tensor[] = aten::split(%2361, %32, %33) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%34)
  %37 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1809:0
  %38 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %37) # transformers/modeling_xlnet.py:1809:0
  %39 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1810:0
  %40 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %39) # transformers/modeling_xlnet.py:1810:0
  %41 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, %28, %29, %30)
  %42 : (Float(17:26, 13:2), Float(17:26, 13:2), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%38, %40, %41)
  return (%42)
