graph(%self.1 : __torch__.transformers.modeling_bart.___torch_mangle_2337.BartModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_bart.___torch_mangle_2336.BartDecoder = prim::GetAttr[name="decoder"](%self.1)
  %4 : __torch__.transformers.modeling_bart.___torch_mangle_2336.BartDecoder = prim::GetAttr[name="decoder"](%self.1)
  %5 : __torch__.torch.nn.modules.sparse.___torch_mangle_2140.Embedding = prim::GetAttr[name="embed_tokens"](%4)
  %6 : __torch__.transformers.modeling_bart.___torch_mangle_2139.BartEncoder = prim::GetAttr[name="encoder"](%self.1)
  %7 : __torch__.torch.nn.modules.sparse.___torch_mangle_2014.Embedding = prim::GetAttr[name="shared"](%self.1)
  %8 : Tensor = prim::GetAttr[name="weight"](%7)
  %9 : None = prim::Constant()
  %prev_output_tokens : Long(17:13, 13:1) = aten::clone(%input_ids, %9) # transformers/modeling_bart.py:207:0
  %11 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:208:0
  %12 : Bool(17:13, 13:1) = aten::ne(%input_ids, %11) # transformers/modeling_bart.py:208:0
  %13 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:208:0
  %14 : int[] = prim::ListConstruct(%13)
  %15 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:208:0
  %16 : None = prim::Constant()
  %17 : Long(17:1) = aten::sum(%12, %14, %15, %16) # transformers/modeling_bart.py:208:0
  %18 : Long() = prim::Constant[value={1}]() # transformers/modeling_bart.py:208:0
  %19 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:208:0
  %20 : Long(17:1) = aten::sub(%17, %18, %19) # transformers/modeling_bart.py:208:0
  %21 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:208:0
  %index_of_eos : Long(17:1, 1:1) = aten::unsqueeze(%20, %21) # transformers/modeling_bart.py:208:0
  %23 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:209:0
  %24 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:209:0
  %25 : Long(17:1, 1:1) = aten::gather(%input_ids, %23, %index_of_eos, %24) # transformers/modeling_bart.py:209:0
  %26 : Long(17:1) = aten::squeeze(%25) # transformers/modeling_bart.py:209:0
  %27 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:209:0
  %28 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:209:0
  %29 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_bart.py:209:0
  %30 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:209:0
  %31 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %27, %28, %29, %30) # transformers/modeling_bart.py:209:0
  %32 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:209:0
  %33 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:209:0
  %34 : Long(17:13) = aten::select(%31, %32, %33) # transformers/modeling_bart.py:209:0
  %35 : int = prim::Constant[value=17]() # transformers/modeling_bart.py:209:0
  %36 : int[] = prim::ListConstruct(%35)
  %37 : Long(17:1) = aten::view(%26, %36) # transformers/modeling_bart.py:209:0
  %38 : bool = prim::Constant[value=0]()
  %39 : Long(17:13) = aten::copy_(%34, %37, %38) # transformers/modeling_bart.py:209:0
  %40 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:210:0
  %41 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:210:0
  %42 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_bart.py:210:0
  %43 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %44 : Long(17:13, 13:1) = aten::slice(%input_ids, %40, %41, %42, %43) # transformers/modeling_bart.py:210:0
  %45 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %46 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:210:0
  %47 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:210:0
  %48 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %49 : Long(17:13, 12:1) = aten::slice(%44, %45, %46, %47, %48) # transformers/modeling_bart.py:210:0
  %50 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:210:0
  %51 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:210:0
  %52 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_bart.py:210:0
  %53 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %54 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %50, %51, %52, %53) # transformers/modeling_bart.py:210:0
  %55 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %56 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %57 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_bart.py:210:0
  %58 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:210:0
  %59 : Long(17:13, 12:1) = aten::slice(%54, %55, %56, %57, %58) # transformers/modeling_bart.py:210:0
  %60 : int = prim::Constant[value=17]() # transformers/modeling_bart.py:210:0
  %61 : int = prim::Constant[value=12]() # transformers/modeling_bart.py:210:0
  %62 : int[] = prim::ListConstruct(%60, %61)
  %63 : Long(17:13, 12:1) = aten::view(%49, %62) # transformers/modeling_bart.py:210:0
  %64 : bool = prim::Constant[value=0]()
  %65 : Long(17:13, 12:1) = aten::copy_(%59, %63, %64) # transformers/modeling_bart.py:210:0
  %66 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:149:0
  %67 : int = aten::size(%prev_output_tokens, %66) # transformers/modeling_bart.py:149:0
  %tgt_len : Long() = prim::NumToTensor(%67)
  %69 : int = aten::Int(%tgt_len)
  %70 : int = aten::Int(%tgt_len)
  %71 : int[] = prim::ListConstruct(%70, %69)
  %72 : int = prim::Constant[value=6]() # transformers/modeling_bart.py:157:0
  %73 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:157:0
  %74 : Device = prim::Constant[value="cpu"]() # transformers/modeling_bart.py:157:0
  %75 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:157:0
  %t.1 : Float(13:13, 13:1) = aten::zeros(%71, %72, %73, %74, %75) # transformers/modeling_bart.py:157:0
  %77 : int = prim::Constant[value=6]() # transformers/modeling_bart.py:838:0
  %78 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:838:0
  %79 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:838:0
  %80 : None = prim::Constant()
  %t.2 : Float(13:13, 13:1) = aten::to(%t.1, %77, %78, %79, %80) # transformers/modeling_bart.py:838:0
  %82 : float = prim::Constant[value=-inf]() # transformers/modeling_bart.py:838:0
  %t : Float(13:13, 13:1) = aten::fill_(%t.2, %82) # transformers/modeling_bart.py:838:0
  %tmp.1 : Float(13:13, 13:1) = aten::type_as(%t, %t) # transformers/modeling_bart.py:838:0
  %85 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:158:0
  %86 : int = aten::size(%tmp.1, %85) # transformers/modeling_bart.py:158:0
  %87 : Long() = prim::NumToTensor(%86)
  %88 : Scalar = aten::ScalarImplicit(%87)
  %89 : None = prim::Constant()
  %90 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:158:0
  %91 : Device = prim::Constant[value="cpu"]() # transformers/modeling_bart.py:158:0
  %92 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:158:0
  %mask : Long(13:1) = aten::arange(%88, %89, %90, %91, %92) # transformers/modeling_bart.py:158:0
  %94 : Long() = prim::Constant[value={1}]() # transformers/modeling_bart.py:159:0
  %95 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:159:0
  %96 : Long(13:1) = aten::add(%mask, %94, %95) # transformers/modeling_bart.py:159:0
  %97 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:159:0
  %98 : int = aten::size(%tmp.1, %97) # transformers/modeling_bart.py:159:0
  %99 : Long() = prim::NumToTensor(%98)
  %100 : int = aten::Int(%99)
  %101 : int = prim::Constant[value=1]() # transformers/modeling_bart.py:159:0
  %102 : int[] = prim::ListConstruct(%100, %101)
  %103 : Long(13:1, 1:1) = aten::view(%96, %102) # transformers/modeling_bart.py:159:0
  %104 : Bool(13:13, 13:1) = aten::lt(%mask, %103) # torch/tensor.py:22:0
  %105 : int = prim::Constant[value=0]() # transformers/modeling_bart.py:159:0
  %tmp : Float(13:13, 13:1) = aten::masked_fill_(%tmp.1, %104, %105) # transformers/modeling_bart.py:159:0
  %107 : Device = prim::Constant[value="cpu"]() # transformers/modeling_bart.py:160:0
  %108 : int = prim::Constant[value=6]() # transformers/modeling_bart.py:160:0
  %109 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:160:0
  %110 : bool = prim::Constant[value=0]() # transformers/modeling_bart.py:160:0
  %111 : None = prim::Constant()
  %attn_mask : Float(13:13, 13:1) = aten::to(%tmp, %107, %108, %109, %110, %111) # transformers/modeling_bart.py:160:0
  %116 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %117 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %118 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
  %119 : int = prim::Constant[value=16](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %120 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %121 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %122 : Long() = prim::Constant[value={16}](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %123 : Double() = prim::Constant[value={0.125}](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %125 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder # torch/nn/functional.py:973:0
  %126 : int = prim::Constant[value=1024](), scope: __module.encoder/__module.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %127 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.encoder/__module.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %128 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %129 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %130 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %131 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %132 : Double() = prim::Constant[value={1}](), scope: __module.encoder # transformers/modeling_bart.py:334:0
  %133 : int = prim::Constant[value=1](), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %134 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %135 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_bart.py:136:0
  %136 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %137 : __torch__.transformers.modeling_bart.___torch_mangle_2136.EncoderLayer = prim::GetAttr[name="11"](%136)
  %138 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %139 : __torch__.transformers.modeling_bart.___torch_mangle_2126.EncoderLayer = prim::GetAttr[name="10"](%138)
  %140 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %141 : __torch__.transformers.modeling_bart.___torch_mangle_2116.EncoderLayer = prim::GetAttr[name="9"](%140)
  %142 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %143 : __torch__.transformers.modeling_bart.___torch_mangle_2106.EncoderLayer = prim::GetAttr[name="8"](%142)
  %144 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %145 : __torch__.transformers.modeling_bart.___torch_mangle_2096.EncoderLayer = prim::GetAttr[name="7"](%144)
  %146 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %147 : __torch__.transformers.modeling_bart.___torch_mangle_2086.EncoderLayer = prim::GetAttr[name="6"](%146)
  %148 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %149 : __torch__.transformers.modeling_bart.___torch_mangle_2076.EncoderLayer = prim::GetAttr[name="5"](%148)
  %150 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %151 : __torch__.transformers.modeling_bart.___torch_mangle_2066.EncoderLayer = prim::GetAttr[name="4"](%150)
  %152 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %153 : __torch__.transformers.modeling_bart.___torch_mangle_2056.EncoderLayer = prim::GetAttr[name="3"](%152)
  %154 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %155 : __torch__.transformers.modeling_bart.___torch_mangle_2046.EncoderLayer = prim::GetAttr[name="2"](%154)
  %156 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %157 : __torch__.transformers.modeling_bart.___torch_mangle_2036.EncoderLayer = prim::GetAttr[name="1"](%156)
  %158 : __torch__.torch.nn.modules.container.___torch_mangle_2137.ModuleList = prim::GetAttr[name="layers"](%6)
  %159 : __torch__.transformers.modeling_bart.___torch_mangle_2026.EncoderLayer = prim::GetAttr[name="0"](%158)
  %160 : __torch__.torch.nn.modules.normalization.___torch_mangle_2138.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%6)
  %161 : __torch__.transformers.modeling_bart.___torch_mangle_2016.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%6)
  %key_padding_mask.1 : Bool(17:13, 13:1) = aten::eq(%attention_mask, %135), scope: __module.encoder # transformers/modeling_bart.py:136:0
  %163 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%8, %input_ids, %133, %134, %134), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::mul(%163, %132), scope: __module.encoder # transformers/modeling_bart.py:334:0
  %165 : Tensor = prim::GetAttr[name="weight"](%161)
  %166 : int = aten::size(%input_ids, %133), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.1 : Long(13:1) = aten::arange(%166, %129, %135, %130, %134), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.1 : Long(13:1) = aten::add(%positions.1, %131, %133), scope: __module.encoder/__module.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %embed_pos : Float(13:1024, 1024:1) = aten::embedding(%165, %input.1, %133, %134, %134), scope: __module.encoder/__module.encoder.embed_positions # torch/nn/functional.py:1814:0
  %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %embed_pos, %133), scope: __module.encoder # transformers/modeling_bart.py:336:0
  %171 : Tensor = prim::GetAttr[name="bias"](%160)
  %172 : Tensor = prim::GetAttr[name="weight"](%160)
  %173 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layernorm_embedding
  %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %173, %172, %171, %127, %128), scope: __module.encoder/__module.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.3, %125, %134), scope: __module.encoder # torch/nn/functional.py:973:0
  %query.1 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.1, %135, %133), scope: __module.encoder # transformers/modeling_bart.py:341:0
  %177 : __torch__.torch.nn.modules.normalization.___torch_mangle_2025.LayerNorm = prim::GetAttr[name="final_layer_norm"](%159)
  %178 : __torch__.torch.nn.modules.linear.___torch_mangle_2024.Linear = prim::GetAttr[name="fc2"](%159)
  %179 : __torch__.torch.nn.modules.linear.___torch_mangle_2023.Linear = prim::GetAttr[name="fc1"](%159)
  %180 : __torch__.torch.nn.modules.normalization.___torch_mangle_2022.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%159)
  %181 : __torch__.transformers.modeling_bart.___torch_mangle_2021.Attention = prim::GetAttr[name="self_attn"](%159)
  %182 : __torch__.torch.nn.modules.linear.___torch_mangle_2020.Linear = prim::GetAttr[name="out_proj"](%181)
  %183 : __torch__.torch.nn.modules.linear.___torch_mangle_2018.Linear = prim::GetAttr[name="v_proj"](%181)
  %184 : __torch__.torch.nn.modules.linear.___torch_mangle_2017.Linear = prim::GetAttr[name="k_proj"](%181)
  %185 : __torch__.torch.nn.modules.linear.___torch_mangle_2019.Linear = prim::GetAttr[name="q_proj"](%181)
  %186 : int = aten::size(%query.1, %135), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %187 : int = aten::size(%query.1, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.1 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %189 : int = aten::size(%query.1, %124), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %190 : Tensor = prim::GetAttr[name="bias"](%185)
  %191 : Tensor = prim::GetAttr[name="weight"](%185)
  %192 : Float(1024:1, 1024:1024) = aten::t(%191), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.1 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %192), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %194 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.1, %190, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%194, %123), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %196 : Tensor = prim::GetAttr[name="bias"](%184)
  %197 : Tensor = prim::GetAttr[name="weight"](%184)
  %198 : Float(1024:1, 1024:1024) = aten::t(%197), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %198), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %196, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %201 : Tensor = prim::GetAttr[name="bias"](%183)
  %202 : Tensor = prim::GetAttr[name="weight"](%183)
  %203 : Float(1024:1, 1024:1024) = aten::t(%202), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %203), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.3, %201, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.1, %135), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %207 : Long() = aten::mul(%bsz.1, %122), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %208 : int = aten::Int(%207), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %209 : int[] = prim::ListConstruct(%186, %208, %121), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %210 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.2, %209), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%210, %135, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.3, %135), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %213 : Long() = aten::mul(%bsz.1, %122), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %214 : int = aten::Int(%213), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %215 : int[] = prim::ListConstruct(%120, %214, %121), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %216 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.4, %215), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%216, %135, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.5, %135), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %219 : Long() = aten::mul(%bsz.1, %122), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %220 : int = aten::Int(%219), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %221 : int[] = prim::ListConstruct(%120, %220, %121), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %222 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.6, %221), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%222, %135, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %224 : int = aten::size(%k.1, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %225 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.1, %133, %124), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.1 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.1, %225), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %227 : int[] = prim::ListConstruct(%187, %119, %186, %224), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %attn_weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.1, %227), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %229 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.1 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%229, %124), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.2, %reshaped.1, %118), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
  %232 : Long() = aten::mul(%bsz.1, %122), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %233 : int = aten::Int(%232), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %234 : int[] = prim::ListConstruct(%233, %186, %224), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %input.4 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.3, %234), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %input.5 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.4, %120, %117), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.4 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.5, %116, %134), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.1 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.4, %v.1), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %239 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.1, %135, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %240 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%239, %135), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %241 : int[] = prim::ListConstruct(%186, %187, %189), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::view(%240, %241), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %243 : Tensor = prim::GetAttr[name="bias"](%182)
  %244 : Tensor = prim::GetAttr[name="weight"](%182)
  %245 : Float(1024:1, 1024:1024) = aten::t(%244), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.4 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.6, %245), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.4, %243, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn/__module.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.7, %125, %134), scope: __module.encoder/__module.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.8 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.1, %x.2, %133), scope: __module.encoder/__module.encoder.layers.0 # transformers/modeling_bart.py:258:0
  %250 : Tensor = prim::GetAttr[name="bias"](%180)
  %251 : Tensor = prim::GetAttr[name="weight"](%180)
  %252 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn_layer_norm
  %input.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.8, %252, %251, %250, %127, %128), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %254 : Tensor = prim::GetAttr[name="bias"](%179)
  %255 : Tensor = prim::GetAttr[name="weight"](%179)
  %256 : Float(1024:1, 4096:1024) = aten::t(%255), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.9, %256), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.5, %254, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.11 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.10), scope: __module.encoder/__module.encoder.layers.0 # torch/nn/functional.py:1369:0
  %input.12 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.11, %116, %134), scope: __module.encoder/__module.encoder.layers.0 # torch/nn/functional.py:973:0
  %261 : Tensor = prim::GetAttr[name="bias"](%178)
  %262 : Tensor = prim::GetAttr[name="weight"](%178)
  %263 : Float(4096:1, 1024:4096) = aten::t(%262), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.12, %263), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.13 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.6, %261, %133), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.13, %125, %134), scope: __module.encoder/__module.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.9, %x.3, %133), scope: __module.encoder/__module.encoder.layers.0 # transformers/modeling_bart.py:269:0
  %268 : Tensor = prim::GetAttr[name="bias"](%177)
  %269 : Tensor = prim::GetAttr[name="weight"](%177)
  %270 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.final_layer_norm
  %query.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.14, %270, %269, %268, %127, %128), scope: __module.encoder/__module.encoder.layers.0/__module.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %272 : __torch__.torch.nn.modules.normalization.___torch_mangle_2035.LayerNorm = prim::GetAttr[name="final_layer_norm"](%157)
  %273 : __torch__.torch.nn.modules.linear.___torch_mangle_2034.Linear = prim::GetAttr[name="fc2"](%157)
  %274 : __torch__.torch.nn.modules.linear.___torch_mangle_2033.Linear = prim::GetAttr[name="fc1"](%157)
  %275 : __torch__.torch.nn.modules.normalization.___torch_mangle_2032.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%157)
  %276 : __torch__.transformers.modeling_bart.___torch_mangle_2031.Attention = prim::GetAttr[name="self_attn"](%157)
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_2030.Linear = prim::GetAttr[name="out_proj"](%276)
  %278 : __torch__.torch.nn.modules.linear.___torch_mangle_2028.Linear = prim::GetAttr[name="v_proj"](%276)
  %279 : __torch__.torch.nn.modules.linear.___torch_mangle_2027.Linear = prim::GetAttr[name="k_proj"](%276)
  %280 : __torch__.torch.nn.modules.linear.___torch_mangle_2029.Linear = prim::GetAttr[name="q_proj"](%276)
  %281 : int = aten::size(%query.2, %135), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %282 : int = aten::size(%query.2, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.2 : Long() = prim::NumToTensor(%282), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %284 : int = aten::size(%query.2, %124), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %285 : Tensor = prim::GetAttr[name="bias"](%280)
  %286 : Tensor = prim::GetAttr[name="weight"](%280)
  %287 : Float(1024:1, 1024:1024) = aten::t(%286), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.7 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %287), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %289 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.7, %285, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%289, %123), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %291 : Tensor = prim::GetAttr[name="bias"](%279)
  %292 : Tensor = prim::GetAttr[name="weight"](%279)
  %293 : Float(1024:1, 1024:1024) = aten::t(%292), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %293), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %291, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %296 : Tensor = prim::GetAttr[name="bias"](%278)
  %297 : Tensor = prim::GetAttr[name="weight"](%278)
  %298 : Float(1024:1, 1024:1024) = aten::t(%297), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %298), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.9, %296, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.7, %135), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %302 : Long() = aten::mul(%bsz.2, %122), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %303 : int = aten::Int(%302), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %304 : int[] = prim::ListConstruct(%281, %303, %121), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %305 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.8, %304), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%305, %135, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.9, %135), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %308 : Long() = aten::mul(%bsz.2, %122), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %309 : int = aten::Int(%308), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %310 : int[] = prim::ListConstruct(%120, %309, %121), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %311 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.10, %310), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%311, %135, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.11, %135), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %314 : Long() = aten::mul(%bsz.2, %122), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %315 : int = aten::Int(%314), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %316 : int[] = prim::ListConstruct(%120, %315, %121), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %317 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.12, %316), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%317, %135, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %319 : int = aten::size(%k.2, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %320 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.2, %133, %124), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.5 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.2, %320), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %322 : int[] = prim::ListConstruct(%282, %119, %281, %319), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %attn_weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.5, %322), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:718:0
  %324 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.2 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%324, %124), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.6, %reshaped.2, %118), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:720:0
  %327 : Long() = aten::mul(%bsz.2, %122), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %328 : int = aten::Int(%327), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %329 : int[] = prim::ListConstruct(%328, %281, %319), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %input.15 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.7, %329), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %input.16 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.15, %120, %117), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.8 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.16, %116, %134), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.2 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.8, %v.2), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %334 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.2, %135, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %335 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%334, %135), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %336 : int[] = prim::ListConstruct(%281, %282, %284), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn
  %input.17 : Float(13:17408, 17:1024, 1024:1) = aten::view(%335, %336), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %338 : Tensor = prim::GetAttr[name="bias"](%277)
  %339 : Tensor = prim::GetAttr[name="weight"](%277)
  %340 : Float(1024:1, 1024:1024) = aten::t(%339), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.10 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.17, %340), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.18 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.10, %338, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn/__module.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.18, %125, %134), scope: __module.encoder/__module.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.2, %x.4, %133), scope: __module.encoder/__module.encoder.layers.1 # transformers/modeling_bart.py:258:0
  %345 : Tensor = prim::GetAttr[name="bias"](%275)
  %346 : Tensor = prim::GetAttr[name="weight"](%275)
  %347 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn_layer_norm
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.19, %347, %346, %345, %127, %128), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %349 : Tensor = prim::GetAttr[name="bias"](%274)
  %350 : Tensor = prim::GetAttr[name="weight"](%274)
  %351 : Float(1024:1, 4096:1024) = aten::t(%350), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.20, %351), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.21 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.11, %349, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.22 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.21), scope: __module.encoder/__module.encoder.layers.1 # torch/nn/functional.py:1369:0
  %input.23 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.22, %116, %134), scope: __module.encoder/__module.encoder.layers.1 # torch/nn/functional.py:973:0
  %356 : Tensor = prim::GetAttr[name="bias"](%273)
  %357 : Tensor = prim::GetAttr[name="weight"](%273)
  %358 : Float(4096:1, 1024:4096) = aten::t(%357), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.23, %358), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.12, %356, %133), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.24, %125, %134), scope: __module.encoder/__module.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.25 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.20, %x.5, %133), scope: __module.encoder/__module.encoder.layers.1 # transformers/modeling_bart.py:269:0
  %363 : Tensor = prim::GetAttr[name="bias"](%272)
  %364 : Tensor = prim::GetAttr[name="weight"](%272)
  %365 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.final_layer_norm
  %query.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.25, %365, %364, %363, %127, %128), scope: __module.encoder/__module.encoder.layers.1/__module.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %367 : __torch__.torch.nn.modules.normalization.___torch_mangle_2045.LayerNorm = prim::GetAttr[name="final_layer_norm"](%155)
  %368 : __torch__.torch.nn.modules.linear.___torch_mangle_2044.Linear = prim::GetAttr[name="fc2"](%155)
  %369 : __torch__.torch.nn.modules.linear.___torch_mangle_2043.Linear = prim::GetAttr[name="fc1"](%155)
  %370 : __torch__.torch.nn.modules.normalization.___torch_mangle_2042.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%155)
  %371 : __torch__.transformers.modeling_bart.___torch_mangle_2041.Attention = prim::GetAttr[name="self_attn"](%155)
  %372 : __torch__.torch.nn.modules.linear.___torch_mangle_2040.Linear = prim::GetAttr[name="out_proj"](%371)
  %373 : __torch__.torch.nn.modules.linear.___torch_mangle_2038.Linear = prim::GetAttr[name="v_proj"](%371)
  %374 : __torch__.torch.nn.modules.linear.___torch_mangle_2037.Linear = prim::GetAttr[name="k_proj"](%371)
  %375 : __torch__.torch.nn.modules.linear.___torch_mangle_2039.Linear = prim::GetAttr[name="q_proj"](%371)
  %376 : int = aten::size(%query.3, %135), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %377 : int = aten::size(%query.3, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.3 : Long() = prim::NumToTensor(%377), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %379 : int = aten::size(%query.3, %124), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %380 : Tensor = prim::GetAttr[name="bias"](%375)
  %381 : Tensor = prim::GetAttr[name="weight"](%375)
  %382 : Float(1024:1, 1024:1024) = aten::t(%381), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %382), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %384 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.13, %380, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%384, %123), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %386 : Tensor = prim::GetAttr[name="bias"](%374)
  %387 : Tensor = prim::GetAttr[name="weight"](%374)
  %388 : Float(1024:1, 1024:1024) = aten::t(%387), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %388), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %386, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %391 : Tensor = prim::GetAttr[name="bias"](%373)
  %392 : Tensor = prim::GetAttr[name="weight"](%373)
  %393 : Float(1024:1, 1024:1024) = aten::t(%392), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %393), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.15, %391, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.13, %135), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %397 : Long() = aten::mul(%bsz.3, %122), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %398 : int = aten::Int(%397), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %399 : int[] = prim::ListConstruct(%376, %398, %121), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %400 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.14, %399), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%400, %135, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.15, %135), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %403 : Long() = aten::mul(%bsz.3, %122), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %404 : int = aten::Int(%403), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %405 : int[] = prim::ListConstruct(%120, %404, %121), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %406 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.16, %405), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%406, %135, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.17, %135), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %409 : Long() = aten::mul(%bsz.3, %122), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %410 : int = aten::Int(%409), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %411 : int[] = prim::ListConstruct(%120, %410, %121), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %412 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.18, %411), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%412, %135, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %414 : int = aten::size(%k.3, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %415 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.3, %133, %124), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.9 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.3, %415), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %417 : int[] = prim::ListConstruct(%377, %119, %376, %414), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %attn_weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.9, %417), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:718:0
  %419 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.3 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%419, %124), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.10, %reshaped.3, %118), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:720:0
  %422 : Long() = aten::mul(%bsz.3, %122), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %423 : int = aten::Int(%422), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %424 : int[] = prim::ListConstruct(%423, %376, %414), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %input.26 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.11, %424), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %input.27 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.26, %120, %117), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.12 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.27, %116, %134), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.3 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.12, %v.3), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %429 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.3, %135, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %430 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%429, %135), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %431 : int[] = prim::ListConstruct(%376, %377, %379), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::view(%430, %431), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %433 : Tensor = prim::GetAttr[name="bias"](%372)
  %434 : Tensor = prim::GetAttr[name="weight"](%372)
  %435 : Float(1024:1, 1024:1024) = aten::t(%434), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.16 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.28, %435), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.16, %433, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn/__module.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.29, %125, %134), scope: __module.encoder/__module.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.30 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.3, %x.6, %133), scope: __module.encoder/__module.encoder.layers.2 # transformers/modeling_bart.py:258:0
  %440 : Tensor = prim::GetAttr[name="bias"](%370)
  %441 : Tensor = prim::GetAttr[name="weight"](%370)
  %442 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn_layer_norm
  %input.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.30, %442, %441, %440, %127, %128), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %444 : Tensor = prim::GetAttr[name="bias"](%369)
  %445 : Tensor = prim::GetAttr[name="weight"](%369)
  %446 : Float(1024:1, 4096:1024) = aten::t(%445), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.31, %446), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.32 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.17, %444, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.33 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.32), scope: __module.encoder/__module.encoder.layers.2 # torch/nn/functional.py:1369:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.33, %116, %134), scope: __module.encoder/__module.encoder.layers.2 # torch/nn/functional.py:973:0
  %451 : Tensor = prim::GetAttr[name="bias"](%368)
  %452 : Tensor = prim::GetAttr[name="weight"](%368)
  %453 : Float(4096:1, 1024:4096) = aten::t(%452), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.34, %453), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.18, %451, %133), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.35, %125, %134), scope: __module.encoder/__module.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.36 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.31, %x.7, %133), scope: __module.encoder/__module.encoder.layers.2 # transformers/modeling_bart.py:269:0
  %458 : Tensor = prim::GetAttr[name="bias"](%367)
  %459 : Tensor = prim::GetAttr[name="weight"](%367)
  %460 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.final_layer_norm
  %query.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.36, %460, %459, %458, %127, %128), scope: __module.encoder/__module.encoder.layers.2/__module.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %462 : __torch__.torch.nn.modules.normalization.___torch_mangle_2055.LayerNorm = prim::GetAttr[name="final_layer_norm"](%153)
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_2054.Linear = prim::GetAttr[name="fc2"](%153)
  %464 : __torch__.torch.nn.modules.linear.___torch_mangle_2053.Linear = prim::GetAttr[name="fc1"](%153)
  %465 : __torch__.torch.nn.modules.normalization.___torch_mangle_2052.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%153)
  %466 : __torch__.transformers.modeling_bart.___torch_mangle_2051.Attention = prim::GetAttr[name="self_attn"](%153)
  %467 : __torch__.torch.nn.modules.linear.___torch_mangle_2050.Linear = prim::GetAttr[name="out_proj"](%466)
  %468 : __torch__.torch.nn.modules.linear.___torch_mangle_2048.Linear = prim::GetAttr[name="v_proj"](%466)
  %469 : __torch__.torch.nn.modules.linear.___torch_mangle_2047.Linear = prim::GetAttr[name="k_proj"](%466)
  %470 : __torch__.torch.nn.modules.linear.___torch_mangle_2049.Linear = prim::GetAttr[name="q_proj"](%466)
  %471 : int = aten::size(%query.4, %135), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %472 : int = aten::size(%query.4, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.4 : Long() = prim::NumToTensor(%472), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %474 : int = aten::size(%query.4, %124), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %475 : Tensor = prim::GetAttr[name="bias"](%470)
  %476 : Tensor = prim::GetAttr[name="weight"](%470)
  %477 : Float(1024:1, 1024:1024) = aten::t(%476), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.19 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %477), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %479 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.19, %475, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%479, %123), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %481 : Tensor = prim::GetAttr[name="bias"](%469)
  %482 : Tensor = prim::GetAttr[name="weight"](%469)
  %483 : Float(1024:1, 1024:1024) = aten::t(%482), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %483), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %481, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %486 : Tensor = prim::GetAttr[name="bias"](%468)
  %487 : Tensor = prim::GetAttr[name="weight"](%468)
  %488 : Float(1024:1, 1024:1024) = aten::t(%487), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %488), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.21, %486, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.19, %135), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %492 : Long() = aten::mul(%bsz.4, %122), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %493 : int = aten::Int(%492), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %494 : int[] = prim::ListConstruct(%471, %493, %121), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %495 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.20, %494), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%495, %135, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.21, %135), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %498 : Long() = aten::mul(%bsz.4, %122), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %499 : int = aten::Int(%498), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %500 : int[] = prim::ListConstruct(%120, %499, %121), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %501 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.22, %500), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%501, %135, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.24 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.23, %135), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %504 : Long() = aten::mul(%bsz.4, %122), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %505 : int = aten::Int(%504), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %506 : int[] = prim::ListConstruct(%120, %505, %121), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %507 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.24, %506), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%507, %135, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %509 : int = aten::size(%k.4, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %510 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.4, %133, %124), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.13 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.4, %510), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %512 : int[] = prim::ListConstruct(%472, %119, %471, %509), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %attn_weights.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.13, %512), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:718:0
  %514 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.4 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%514, %124), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.14, %reshaped.4, %118), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:720:0
  %517 : Long() = aten::mul(%bsz.4, %122), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %518 : int = aten::Int(%517), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %519 : int[] = prim::ListConstruct(%518, %471, %509), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %input.37 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.15, %519), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %input.38 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.37, %120, %117), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.16 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.38, %116, %134), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.4 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.16, %v.4), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %524 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.4, %135, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %525 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%524, %135), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %526 : int[] = prim::ListConstruct(%471, %472, %474), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn
  %input.39 : Float(13:17408, 17:1024, 1024:1) = aten::view(%525, %526), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %528 : Tensor = prim::GetAttr[name="bias"](%467)
  %529 : Tensor = prim::GetAttr[name="weight"](%467)
  %530 : Float(1024:1, 1024:1024) = aten::t(%529), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.22 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.39, %530), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.40 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.22, %528, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn/__module.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.40, %125, %134), scope: __module.encoder/__module.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.4, %x.8, %133), scope: __module.encoder/__module.encoder.layers.3 # transformers/modeling_bart.py:258:0
  %535 : Tensor = prim::GetAttr[name="bias"](%465)
  %536 : Tensor = prim::GetAttr[name="weight"](%465)
  %537 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn_layer_norm
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.41, %537, %536, %535, %127, %128), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %539 : Tensor = prim::GetAttr[name="bias"](%464)
  %540 : Tensor = prim::GetAttr[name="weight"](%464)
  %541 : Float(1024:1, 4096:1024) = aten::t(%540), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.42, %541), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.23, %539, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.encoder/__module.encoder.layers.3 # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %116, %134), scope: __module.encoder/__module.encoder.layers.3 # torch/nn/functional.py:973:0
  %546 : Tensor = prim::GetAttr[name="bias"](%463)
  %547 : Tensor = prim::GetAttr[name="weight"](%463)
  %548 : Float(4096:1, 1024:4096) = aten::t(%547), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %548), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.24, %546, %133), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %125, %134), scope: __module.encoder/__module.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.42, %x.9, %133), scope: __module.encoder/__module.encoder.layers.3 # transformers/modeling_bart.py:269:0
  %553 : Tensor = prim::GetAttr[name="bias"](%462)
  %554 : Tensor = prim::GetAttr[name="weight"](%462)
  %555 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.final_layer_norm
  %query.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %555, %554, %553, %127, %128), scope: __module.encoder/__module.encoder.layers.3/__module.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %557 : __torch__.torch.nn.modules.normalization.___torch_mangle_2065.LayerNorm = prim::GetAttr[name="final_layer_norm"](%151)
  %558 : __torch__.torch.nn.modules.linear.___torch_mangle_2064.Linear = prim::GetAttr[name="fc2"](%151)
  %559 : __torch__.torch.nn.modules.linear.___torch_mangle_2063.Linear = prim::GetAttr[name="fc1"](%151)
  %560 : __torch__.torch.nn.modules.normalization.___torch_mangle_2062.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%151)
  %561 : __torch__.transformers.modeling_bart.___torch_mangle_2061.Attention = prim::GetAttr[name="self_attn"](%151)
  %562 : __torch__.torch.nn.modules.linear.___torch_mangle_2060.Linear = prim::GetAttr[name="out_proj"](%561)
  %563 : __torch__.torch.nn.modules.linear.___torch_mangle_2058.Linear = prim::GetAttr[name="v_proj"](%561)
  %564 : __torch__.torch.nn.modules.linear.___torch_mangle_2057.Linear = prim::GetAttr[name="k_proj"](%561)
  %565 : __torch__.torch.nn.modules.linear.___torch_mangle_2059.Linear = prim::GetAttr[name="q_proj"](%561)
  %566 : int = aten::size(%query.5, %135), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %567 : int = aten::size(%query.5, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.5 : Long() = prim::NumToTensor(%567), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %569 : int = aten::size(%query.5, %124), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %570 : Tensor = prim::GetAttr[name="bias"](%565)
  %571 : Tensor = prim::GetAttr[name="weight"](%565)
  %572 : Float(1024:1, 1024:1024) = aten::t(%571), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.25 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %572), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %574 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.25, %570, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.25 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%574, %123), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %576 : Tensor = prim::GetAttr[name="bias"](%564)
  %577 : Tensor = prim::GetAttr[name="weight"](%564)
  %578 : Float(1024:1, 1024:1024) = aten::t(%577), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %578), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.27 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %576, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %581 : Tensor = prim::GetAttr[name="bias"](%563)
  %582 : Tensor = prim::GetAttr[name="weight"](%563)
  %583 : Float(1024:1, 1024:1024) = aten::t(%582), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %583), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.27, %581, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.26 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.25, %135), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %587 : Long() = aten::mul(%bsz.5, %122), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %588 : int = aten::Int(%587), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %589 : int[] = prim::ListConstruct(%566, %588, %121), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %590 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.26, %589), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%590, %135, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.28 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.27, %135), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %593 : Long() = aten::mul(%bsz.5, %122), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %594 : int = aten::Int(%593), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %595 : int[] = prim::ListConstruct(%120, %594, %121), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %596 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.28, %595), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%596, %135, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.30 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.29, %135), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %599 : Long() = aten::mul(%bsz.5, %122), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %600 : int = aten::Int(%599), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %601 : int[] = prim::ListConstruct(%120, %600, %121), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %602 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.30, %601), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%602, %135, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %604 : int = aten::size(%k.5, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %605 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.5, %133, %124), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.17 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.5, %605), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %607 : int[] = prim::ListConstruct(%567, %119, %566, %604), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %attn_weights.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.17, %607), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:718:0
  %609 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.5 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%609, %124), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.18, %reshaped.5, %118), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:720:0
  %612 : Long() = aten::mul(%bsz.5, %122), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %613 : int = aten::Int(%612), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %614 : int[] = prim::ListConstruct(%613, %566, %604), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %input.48 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.19, %614), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %input.49 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.48, %120, %117), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.20 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.49, %116, %134), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.5 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.20, %v.5), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %619 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.5, %135, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %620 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%619, %135), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %621 : int[] = prim::ListConstruct(%566, %567, %569), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::view(%620, %621), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %623 : Tensor = prim::GetAttr[name="bias"](%562)
  %624 : Tensor = prim::GetAttr[name="weight"](%562)
  %625 : Float(1024:1, 1024:1024) = aten::t(%624), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.50, %625), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.28, %623, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn/__module.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.51, %125, %134), scope: __module.encoder/__module.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.52 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.5, %x.10, %133), scope: __module.encoder/__module.encoder.layers.4 # transformers/modeling_bart.py:258:0
  %630 : Tensor = prim::GetAttr[name="bias"](%560)
  %631 : Tensor = prim::GetAttr[name="weight"](%560)
  %632 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn_layer_norm
  %input.53 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.52, %632, %631, %630, %127, %128), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %634 : Tensor = prim::GetAttr[name="bias"](%559)
  %635 : Tensor = prim::GetAttr[name="weight"](%559)
  %636 : Float(1024:1, 4096:1024) = aten::t(%635), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.53, %636), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.29, %634, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.55 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.54), scope: __module.encoder/__module.encoder.layers.4 # torch/nn/functional.py:1369:0
  %input.56 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.55, %116, %134), scope: __module.encoder/__module.encoder.layers.4 # torch/nn/functional.py:973:0
  %641 : Tensor = prim::GetAttr[name="bias"](%558)
  %642 : Tensor = prim::GetAttr[name="weight"](%558)
  %643 : Float(4096:1, 1024:4096) = aten::t(%642), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.56, %643), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.30, %641, %133), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.57, %125, %134), scope: __module.encoder/__module.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.58 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.53, %x.11, %133), scope: __module.encoder/__module.encoder.layers.4 # transformers/modeling_bart.py:269:0
  %648 : Tensor = prim::GetAttr[name="bias"](%557)
  %649 : Tensor = prim::GetAttr[name="weight"](%557)
  %650 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.final_layer_norm
  %query.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.58, %650, %649, %648, %127, %128), scope: __module.encoder/__module.encoder.layers.4/__module.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %652 : __torch__.torch.nn.modules.normalization.___torch_mangle_2075.LayerNorm = prim::GetAttr[name="final_layer_norm"](%149)
  %653 : __torch__.torch.nn.modules.linear.___torch_mangle_2074.Linear = prim::GetAttr[name="fc2"](%149)
  %654 : __torch__.torch.nn.modules.linear.___torch_mangle_2073.Linear = prim::GetAttr[name="fc1"](%149)
  %655 : __torch__.torch.nn.modules.normalization.___torch_mangle_2072.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%149)
  %656 : __torch__.transformers.modeling_bart.___torch_mangle_2071.Attention = prim::GetAttr[name="self_attn"](%149)
  %657 : __torch__.torch.nn.modules.linear.___torch_mangle_2070.Linear = prim::GetAttr[name="out_proj"](%656)
  %658 : __torch__.torch.nn.modules.linear.___torch_mangle_2068.Linear = prim::GetAttr[name="v_proj"](%656)
  %659 : __torch__.torch.nn.modules.linear.___torch_mangle_2067.Linear = prim::GetAttr[name="k_proj"](%656)
  %660 : __torch__.torch.nn.modules.linear.___torch_mangle_2069.Linear = prim::GetAttr[name="q_proj"](%656)
  %661 : int = aten::size(%query.6, %135), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %662 : int = aten::size(%query.6, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.6 : Long() = prim::NumToTensor(%662), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %664 : int = aten::size(%query.6, %124), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %665 : Tensor = prim::GetAttr[name="bias"](%660)
  %666 : Tensor = prim::GetAttr[name="weight"](%660)
  %667 : Float(1024:1, 1024:1024) = aten::t(%666), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.31 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %667), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %669 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.31, %665, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.31 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%669, %123), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %671 : Tensor = prim::GetAttr[name="bias"](%659)
  %672 : Tensor = prim::GetAttr[name="weight"](%659)
  %673 : Float(1024:1, 1024:1024) = aten::t(%672), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %673), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.33 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %671, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %676 : Tensor = prim::GetAttr[name="bias"](%658)
  %677 : Tensor = prim::GetAttr[name="weight"](%658)
  %678 : Float(1024:1, 1024:1024) = aten::t(%677), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %678), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.33, %676, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.32 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.31, %135), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %682 : Long() = aten::mul(%bsz.6, %122), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %684 : int[] = prim::ListConstruct(%661, %683, %121), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %685 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.32, %684), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%685, %135, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.34 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.33, %135), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %688 : Long() = aten::mul(%bsz.6, %122), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %690 : int[] = prim::ListConstruct(%120, %689, %121), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %691 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.34, %690), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%691, %135, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.36 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.35, %135), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %694 : Long() = aten::mul(%bsz.6, %122), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %696 : int[] = prim::ListConstruct(%120, %695, %121), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %697 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.36, %696), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%697, %135, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %699 : int = aten::size(%k.6, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %700 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.6, %133, %124), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.21 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.6, %700), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %702 : int[] = prim::ListConstruct(%662, %119, %661, %699), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %attn_weights.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.21, %702), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:718:0
  %704 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.6 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%704, %124), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.22, %reshaped.6, %118), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:720:0
  %707 : Long() = aten::mul(%bsz.6, %122), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %708 : int = aten::Int(%707), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %709 : int[] = prim::ListConstruct(%708, %661, %699), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %input.59 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.23, %709), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %input.60 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.59, %120, %117), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.24 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.60, %116, %134), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.6 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.24, %v.6), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %714 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.6, %135, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %715 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%714, %135), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %716 : int[] = prim::ListConstruct(%661, %662, %664), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn
  %input.61 : Float(13:17408, 17:1024, 1024:1) = aten::view(%715, %716), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %718 : Tensor = prim::GetAttr[name="bias"](%657)
  %719 : Tensor = prim::GetAttr[name="weight"](%657)
  %720 : Float(1024:1, 1024:1024) = aten::t(%719), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.34 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.61, %720), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.62 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.34, %718, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn/__module.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.62, %125, %134), scope: __module.encoder/__module.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.63 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.6, %x.12, %133), scope: __module.encoder/__module.encoder.layers.5 # transformers/modeling_bart.py:258:0
  %725 : Tensor = prim::GetAttr[name="bias"](%655)
  %726 : Tensor = prim::GetAttr[name="weight"](%655)
  %727 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn_layer_norm
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.63, %727, %726, %725, %127, %128), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %729 : Tensor = prim::GetAttr[name="bias"](%654)
  %730 : Tensor = prim::GetAttr[name="weight"](%654)
  %731 : Float(1024:1, 4096:1024) = aten::t(%730), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.64, %731), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.65 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.35, %729, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.66 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.65), scope: __module.encoder/__module.encoder.layers.5 # torch/nn/functional.py:1369:0
  %input.67 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.66, %116, %134), scope: __module.encoder/__module.encoder.layers.5 # torch/nn/functional.py:973:0
  %736 : Tensor = prim::GetAttr[name="bias"](%653)
  %737 : Tensor = prim::GetAttr[name="weight"](%653)
  %738 : Float(4096:1, 1024:4096) = aten::t(%737), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.67, %738), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.36, %736, %133), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %125, %134), scope: __module.encoder/__module.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.64, %x.13, %133), scope: __module.encoder/__module.encoder.layers.5 # transformers/modeling_bart.py:269:0
  %743 : Tensor = prim::GetAttr[name="bias"](%652)
  %744 : Tensor = prim::GetAttr[name="weight"](%652)
  %745 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.final_layer_norm
  %query.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %745, %744, %743, %127, %128), scope: __module.encoder/__module.encoder.layers.5/__module.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %747 : __torch__.torch.nn.modules.normalization.___torch_mangle_2085.LayerNorm = prim::GetAttr[name="final_layer_norm"](%147)
  %748 : __torch__.torch.nn.modules.linear.___torch_mangle_2084.Linear = prim::GetAttr[name="fc2"](%147)
  %749 : __torch__.torch.nn.modules.linear.___torch_mangle_2083.Linear = prim::GetAttr[name="fc1"](%147)
  %750 : __torch__.torch.nn.modules.normalization.___torch_mangle_2082.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%147)
  %751 : __torch__.transformers.modeling_bart.___torch_mangle_2081.Attention = prim::GetAttr[name="self_attn"](%147)
  %752 : __torch__.torch.nn.modules.linear.___torch_mangle_2080.Linear = prim::GetAttr[name="out_proj"](%751)
  %753 : __torch__.torch.nn.modules.linear.___torch_mangle_2078.Linear = prim::GetAttr[name="v_proj"](%751)
  %754 : __torch__.torch.nn.modules.linear.___torch_mangle_2077.Linear = prim::GetAttr[name="k_proj"](%751)
  %755 : __torch__.torch.nn.modules.linear.___torch_mangle_2079.Linear = prim::GetAttr[name="q_proj"](%751)
  %756 : int = aten::size(%query.7, %135), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %757 : int = aten::size(%query.7, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.7 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %759 : int = aten::size(%query.7, %124), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %760 : Tensor = prim::GetAttr[name="bias"](%755)
  %761 : Tensor = prim::GetAttr[name="weight"](%755)
  %762 : Float(1024:1, 1024:1024) = aten::t(%761), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.37 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %762), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %764 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.37, %760, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.37 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%764, %123), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %766 : Tensor = prim::GetAttr[name="bias"](%754)
  %767 : Tensor = prim::GetAttr[name="weight"](%754)
  %768 : Float(1024:1, 1024:1024) = aten::t(%767), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %768), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.39 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %766, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %771 : Tensor = prim::GetAttr[name="bias"](%753)
  %772 : Tensor = prim::GetAttr[name="weight"](%753)
  %773 : Float(1024:1, 1024:1024) = aten::t(%772), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %773), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.41 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.39, %771, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.38 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.37, %135), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %777 : Long() = aten::mul(%bsz.7, %122), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %778 : int = aten::Int(%777), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %779 : int[] = prim::ListConstruct(%756, %778, %121), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %780 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.38, %779), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%780, %135, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.40 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.39, %135), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %783 : Long() = aten::mul(%bsz.7, %122), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %784 : int = aten::Int(%783), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %785 : int[] = prim::ListConstruct(%120, %784, %121), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %786 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.40, %785), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%786, %135, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.42 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.41, %135), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %789 : Long() = aten::mul(%bsz.7, %122), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %790 : int = aten::Int(%789), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %791 : int[] = prim::ListConstruct(%120, %790, %121), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %792 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.42, %791), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%792, %135, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %794 : int = aten::size(%k.7, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %795 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.7, %133, %124), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.25 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.7, %795), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %797 : int[] = prim::ListConstruct(%757, %119, %756, %794), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %attn_weights.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.25, %797), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:718:0
  %799 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.7 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%799, %124), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.26, %reshaped.7, %118), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:720:0
  %802 : Long() = aten::mul(%bsz.7, %122), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %803 : int = aten::Int(%802), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %804 : int[] = prim::ListConstruct(%803, %756, %794), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %input.70 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.27, %804), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %input.71 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.70, %120, %117), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.28 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.71, %116, %134), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.7 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.28, %v.7), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %809 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.7, %135, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %810 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%809, %135), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %811 : int[] = prim::ListConstruct(%756, %757, %759), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn
  %input.72 : Float(13:17408, 17:1024, 1024:1) = aten::view(%810, %811), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %813 : Tensor = prim::GetAttr[name="bias"](%752)
  %814 : Tensor = prim::GetAttr[name="weight"](%752)
  %815 : Float(1024:1, 1024:1024) = aten::t(%814), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.40 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %815), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.40, %813, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn/__module.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %125, %134), scope: __module.encoder/__module.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.7, %x.14, %133), scope: __module.encoder/__module.encoder.layers.6 # transformers/modeling_bart.py:258:0
  %820 : Tensor = prim::GetAttr[name="bias"](%750)
  %821 : Tensor = prim::GetAttr[name="weight"](%750)
  %822 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn_layer_norm
  %input.75 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %822, %821, %820, %127, %128), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %824 : Tensor = prim::GetAttr[name="bias"](%749)
  %825 : Tensor = prim::GetAttr[name="weight"](%749)
  %826 : Float(1024:1, 4096:1024) = aten::t(%825), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.75, %826), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.76 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.41, %824, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.77 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.76), scope: __module.encoder/__module.encoder.layers.6 # torch/nn/functional.py:1369:0
  %input.78 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.77, %116, %134), scope: __module.encoder/__module.encoder.layers.6 # torch/nn/functional.py:973:0
  %831 : Tensor = prim::GetAttr[name="bias"](%748)
  %832 : Tensor = prim::GetAttr[name="weight"](%748)
  %833 : Float(4096:1, 1024:4096) = aten::t(%832), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.78, %833), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.42, %831, %133), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.79, %125, %134), scope: __module.encoder/__module.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.80 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.75, %x.15, %133), scope: __module.encoder/__module.encoder.layers.6 # transformers/modeling_bart.py:269:0
  %838 : Tensor = prim::GetAttr[name="bias"](%747)
  %839 : Tensor = prim::GetAttr[name="weight"](%747)
  %840 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.final_layer_norm
  %query.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.80, %840, %839, %838, %127, %128), scope: __module.encoder/__module.encoder.layers.6/__module.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %842 : __torch__.torch.nn.modules.normalization.___torch_mangle_2095.LayerNorm = prim::GetAttr[name="final_layer_norm"](%145)
  %843 : __torch__.torch.nn.modules.linear.___torch_mangle_2094.Linear = prim::GetAttr[name="fc2"](%145)
  %844 : __torch__.torch.nn.modules.linear.___torch_mangle_2093.Linear = prim::GetAttr[name="fc1"](%145)
  %845 : __torch__.torch.nn.modules.normalization.___torch_mangle_2092.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%145)
  %846 : __torch__.transformers.modeling_bart.___torch_mangle_2091.Attention = prim::GetAttr[name="self_attn"](%145)
  %847 : __torch__.torch.nn.modules.linear.___torch_mangle_2090.Linear = prim::GetAttr[name="out_proj"](%846)
  %848 : __torch__.torch.nn.modules.linear.___torch_mangle_2088.Linear = prim::GetAttr[name="v_proj"](%846)
  %849 : __torch__.torch.nn.modules.linear.___torch_mangle_2087.Linear = prim::GetAttr[name="k_proj"](%846)
  %850 : __torch__.torch.nn.modules.linear.___torch_mangle_2089.Linear = prim::GetAttr[name="q_proj"](%846)
  %851 : int = aten::size(%query.8, %135), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %852 : int = aten::size(%query.8, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.8 : Long() = prim::NumToTensor(%852), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %854 : int = aten::size(%query.8, %124), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %855 : Tensor = prim::GetAttr[name="bias"](%850)
  %856 : Tensor = prim::GetAttr[name="weight"](%850)
  %857 : Float(1024:1, 1024:1024) = aten::t(%856), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %857), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %859 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.43, %855, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.43 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%859, %123), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %861 : Tensor = prim::GetAttr[name="bias"](%849)
  %862 : Tensor = prim::GetAttr[name="weight"](%849)
  %863 : Float(1024:1, 1024:1024) = aten::t(%862), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %863), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.45 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %861, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %866 : Tensor = prim::GetAttr[name="bias"](%848)
  %867 : Tensor = prim::GetAttr[name="weight"](%848)
  %868 : Float(1024:1, 1024:1024) = aten::t(%867), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %868), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.47 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.45, %866, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.44 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.43, %135), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %872 : Long() = aten::mul(%bsz.8, %122), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %873 : int = aten::Int(%872), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %874 : int[] = prim::ListConstruct(%851, %873, %121), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %875 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.44, %874), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%875, %135, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.46 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.45, %135), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %878 : Long() = aten::mul(%bsz.8, %122), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %879 : int = aten::Int(%878), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %880 : int[] = prim::ListConstruct(%120, %879, %121), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %881 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.46, %880), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%881, %135, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.48 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.47, %135), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %884 : Long() = aten::mul(%bsz.8, %122), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %885 : int = aten::Int(%884), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %886 : int[] = prim::ListConstruct(%120, %885, %121), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %887 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.48, %886), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%887, %135, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %889 : int = aten::size(%k.8, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %890 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.8, %133, %124), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.29 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.8, %890), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %892 : int[] = prim::ListConstruct(%852, %119, %851, %889), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %attn_weights.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.29, %892), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:718:0
  %894 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.8 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%894, %124), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.30, %reshaped.8, %118), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:720:0
  %897 : Long() = aten::mul(%bsz.8, %122), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %898 : int = aten::Int(%897), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %899 : int[] = prim::ListConstruct(%898, %851, %889), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %input.81 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.31, %899), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %input.82 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.81, %120, %117), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.32 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.82, %116, %134), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.8 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.32, %v.8), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %904 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.8, %135, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %905 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%904, %135), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %906 : int[] = prim::ListConstruct(%851, %852, %854), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::view(%905, %906), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %908 : Tensor = prim::GetAttr[name="bias"](%847)
  %909 : Tensor = prim::GetAttr[name="weight"](%847)
  %910 : Float(1024:1, 1024:1024) = aten::t(%909), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.46 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.83, %910), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.84 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.46, %908, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn/__module.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.84, %125, %134), scope: __module.encoder/__module.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.85 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.8, %x.16, %133), scope: __module.encoder/__module.encoder.layers.7 # transformers/modeling_bart.py:258:0
  %915 : Tensor = prim::GetAttr[name="bias"](%845)
  %916 : Tensor = prim::GetAttr[name="weight"](%845)
  %917 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn_layer_norm
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.85, %917, %916, %915, %127, %128), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %919 : Tensor = prim::GetAttr[name="bias"](%844)
  %920 : Tensor = prim::GetAttr[name="weight"](%844)
  %921 : Float(1024:1, 4096:1024) = aten::t(%920), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.86, %921), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.87 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.47, %919, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.87), scope: __module.encoder/__module.encoder.layers.7 # torch/nn/functional.py:1369:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.88, %116, %134), scope: __module.encoder/__module.encoder.layers.7 # torch/nn/functional.py:973:0
  %926 : Tensor = prim::GetAttr[name="bias"](%843)
  %927 : Tensor = prim::GetAttr[name="weight"](%843)
  %928 : Float(4096:1, 1024:4096) = aten::t(%927), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.89, %928), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.90 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.48, %926, %133), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.90, %125, %134), scope: __module.encoder/__module.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.86, %x.17, %133), scope: __module.encoder/__module.encoder.layers.7 # transformers/modeling_bart.py:269:0
  %933 : Tensor = prim::GetAttr[name="bias"](%842)
  %934 : Tensor = prim::GetAttr[name="weight"](%842)
  %935 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.final_layer_norm
  %query.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.91, %935, %934, %933, %127, %128), scope: __module.encoder/__module.encoder.layers.7/__module.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %937 : __torch__.torch.nn.modules.normalization.___torch_mangle_2105.LayerNorm = prim::GetAttr[name="final_layer_norm"](%143)
  %938 : __torch__.torch.nn.modules.linear.___torch_mangle_2104.Linear = prim::GetAttr[name="fc2"](%143)
  %939 : __torch__.torch.nn.modules.linear.___torch_mangle_2103.Linear = prim::GetAttr[name="fc1"](%143)
  %940 : __torch__.torch.nn.modules.normalization.___torch_mangle_2102.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%143)
  %941 : __torch__.transformers.modeling_bart.___torch_mangle_2101.Attention = prim::GetAttr[name="self_attn"](%143)
  %942 : __torch__.torch.nn.modules.linear.___torch_mangle_2100.Linear = prim::GetAttr[name="out_proj"](%941)
  %943 : __torch__.torch.nn.modules.linear.___torch_mangle_2098.Linear = prim::GetAttr[name="v_proj"](%941)
  %944 : __torch__.torch.nn.modules.linear.___torch_mangle_2097.Linear = prim::GetAttr[name="k_proj"](%941)
  %945 : __torch__.torch.nn.modules.linear.___torch_mangle_2099.Linear = prim::GetAttr[name="q_proj"](%941)
  %946 : int = aten::size(%query.9, %135), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %947 : int = aten::size(%query.9, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.9 : Long() = prim::NumToTensor(%947), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %949 : int = aten::size(%query.9, %124), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %950 : Tensor = prim::GetAttr[name="bias"](%945)
  %951 : Tensor = prim::GetAttr[name="weight"](%945)
  %952 : Float(1024:1, 1024:1024) = aten::t(%951), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.49 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %952), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %954 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.49, %950, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.49 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%954, %123), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %956 : Tensor = prim::GetAttr[name="bias"](%944)
  %957 : Tensor = prim::GetAttr[name="weight"](%944)
  %958 : Float(1024:1, 1024:1024) = aten::t(%957), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %958), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %956, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %961 : Tensor = prim::GetAttr[name="bias"](%943)
  %962 : Tensor = prim::GetAttr[name="weight"](%943)
  %963 : Float(1024:1, 1024:1024) = aten::t(%962), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %963), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.53 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.51, %961, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.50 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.49, %135), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %967 : Long() = aten::mul(%bsz.9, %122), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %968 : int = aten::Int(%967), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %969 : int[] = prim::ListConstruct(%946, %968, %121), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %970 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.50, %969), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%970, %135, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.52 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.51, %135), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %973 : Long() = aten::mul(%bsz.9, %122), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %974 : int = aten::Int(%973), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %975 : int[] = prim::ListConstruct(%120, %974, %121), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %976 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.52, %975), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%976, %135, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.54 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.53, %135), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %979 : Long() = aten::mul(%bsz.9, %122), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %980 : int = aten::Int(%979), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %981 : int[] = prim::ListConstruct(%120, %980, %121), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %982 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.54, %981), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%982, %135, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %984 : int = aten::size(%k.9, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %985 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.9, %133, %124), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.33 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.9, %985), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %987 : int[] = prim::ListConstruct(%947, %119, %946, %984), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %attn_weights.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.33, %987), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:718:0
  %989 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.9 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%989, %124), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.34, %reshaped.9, %118), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:720:0
  %992 : Long() = aten::mul(%bsz.9, %122), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %993 : int = aten::Int(%992), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %994 : int[] = prim::ListConstruct(%993, %946, %984), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %input.92 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.35, %994), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %input.93 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.92, %120, %117), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.36 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.93, %116, %134), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.9 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.36, %v.9), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %999 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.9, %135, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %1000 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%999, %135), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %1001 : int[] = prim::ListConstruct(%946, %947, %949), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn
  %input.94 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1000, %1001), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %1003 : Tensor = prim::GetAttr[name="bias"](%942)
  %1004 : Tensor = prim::GetAttr[name="weight"](%942)
  %1005 : Float(1024:1, 1024:1024) = aten::t(%1004), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.52 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.94, %1005), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.52, %1003, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn/__module.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %125, %134), scope: __module.encoder/__module.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.9, %x.18, %133), scope: __module.encoder/__module.encoder.layers.8 # transformers/modeling_bart.py:258:0
  %1010 : Tensor = prim::GetAttr[name="bias"](%940)
  %1011 : Tensor = prim::GetAttr[name="weight"](%940)
  %1012 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn_layer_norm
  %input.97 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1012, %1011, %1010, %127, %128), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1014 : Tensor = prim::GetAttr[name="bias"](%939)
  %1015 : Tensor = prim::GetAttr[name="weight"](%939)
  %1016 : Float(1024:1, 4096:1024) = aten::t(%1015), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.97, %1016), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.53, %1014, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.98), scope: __module.encoder/__module.encoder.layers.8 # torch/nn/functional.py:1369:0
  %input.100 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.99, %116, %134), scope: __module.encoder/__module.encoder.layers.8 # torch/nn/functional.py:973:0
  %1021 : Tensor = prim::GetAttr[name="bias"](%938)
  %1022 : Tensor = prim::GetAttr[name="weight"](%938)
  %1023 : Float(4096:1, 1024:4096) = aten::t(%1022), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.100, %1023), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.54, %1021, %133), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.101, %125, %134), scope: __module.encoder/__module.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.102 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.97, %x.19, %133), scope: __module.encoder/__module.encoder.layers.8 # transformers/modeling_bart.py:269:0
  %1028 : Tensor = prim::GetAttr[name="bias"](%937)
  %1029 : Tensor = prim::GetAttr[name="weight"](%937)
  %1030 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.final_layer_norm
  %query.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.102, %1030, %1029, %1028, %127, %128), scope: __module.encoder/__module.encoder.layers.8/__module.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %1032 : __torch__.torch.nn.modules.normalization.___torch_mangle_2115.LayerNorm = prim::GetAttr[name="final_layer_norm"](%141)
  %1033 : __torch__.torch.nn.modules.linear.___torch_mangle_2114.Linear = prim::GetAttr[name="fc2"](%141)
  %1034 : __torch__.torch.nn.modules.linear.___torch_mangle_2113.Linear = prim::GetAttr[name="fc1"](%141)
  %1035 : __torch__.torch.nn.modules.normalization.___torch_mangle_2112.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%141)
  %1036 : __torch__.transformers.modeling_bart.___torch_mangle_2111.Attention = prim::GetAttr[name="self_attn"](%141)
  %1037 : __torch__.torch.nn.modules.linear.___torch_mangle_2110.Linear = prim::GetAttr[name="out_proj"](%1036)
  %1038 : __torch__.torch.nn.modules.linear.___torch_mangle_2108.Linear = prim::GetAttr[name="v_proj"](%1036)
  %1039 : __torch__.torch.nn.modules.linear.___torch_mangle_2107.Linear = prim::GetAttr[name="k_proj"](%1036)
  %1040 : __torch__.torch.nn.modules.linear.___torch_mangle_2109.Linear = prim::GetAttr[name="q_proj"](%1036)
  %1041 : int = aten::size(%query.10, %135), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %1042 : int = aten::size(%query.10, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.10 : Long() = prim::NumToTensor(%1042), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1044 : int = aten::size(%query.10, %124), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %1045 : Tensor = prim::GetAttr[name="bias"](%1040)
  %1046 : Tensor = prim::GetAttr[name="weight"](%1040)
  %1047 : Float(1024:1, 1024:1024) = aten::t(%1046), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.55 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %1047), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1049 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.55, %1045, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.55 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1049, %123), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %1051 : Tensor = prim::GetAttr[name="bias"](%1039)
  %1052 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1053 : Float(1024:1, 1024:1024) = aten::t(%1052), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %1053), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %1051, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1056 : Tensor = prim::GetAttr[name="bias"](%1038)
  %1057 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1058 : Float(1024:1, 1024:1024) = aten::t(%1057), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %1058), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.59 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.57, %1056, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.56 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.55, %135), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1062 : Long() = aten::mul(%bsz.10, %122), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1063 : int = aten::Int(%1062), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1064 : int[] = prim::ListConstruct(%1041, %1063, %121), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1065 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.56, %1064), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1065, %135, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.58 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.57, %135), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1068 : Long() = aten::mul(%bsz.10, %122), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1069 : int = aten::Int(%1068), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1070 : int[] = prim::ListConstruct(%120, %1069, %121), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1071 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.58, %1070), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1071, %135, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.60 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.59, %135), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1074 : Long() = aten::mul(%bsz.10, %122), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1075 : int = aten::Int(%1074), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1076 : int[] = prim::ListConstruct(%120, %1075, %121), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1077 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.60, %1076), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1077, %135, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1079 : int = aten::size(%k.10, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %1080 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.10, %133, %124), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.37 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.10, %1080), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %1082 : int[] = prim::ListConstruct(%1042, %119, %1041, %1079), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %attn_weights.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.37, %1082), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:718:0
  %1084 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.10 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1084, %124), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.38, %reshaped.10, %118), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:720:0
  %1087 : Long() = aten::mul(%bsz.10, %122), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %1088 : int = aten::Int(%1087), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %1089 : int[] = prim::ListConstruct(%1088, %1041, %1079), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %input.103 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.39, %1089), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %input.104 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.103, %120, %117), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.40 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.104, %116, %134), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.10 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.40, %v.10), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %1094 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.10, %135, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1095 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1094, %135), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1096 : int[] = prim::ListConstruct(%1041, %1042, %1044), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1095, %1096), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1098 : Tensor = prim::GetAttr[name="bias"](%1037)
  %1099 : Tensor = prim::GetAttr[name="weight"](%1037)
  %1100 : Float(1024:1, 1024:1024) = aten::t(%1099), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.105, %1100), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.106 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.58, %1098, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn/__module.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.106, %125, %134), scope: __module.encoder/__module.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.107 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.10, %x.20, %133), scope: __module.encoder/__module.encoder.layers.9 # transformers/modeling_bart.py:258:0
  %1105 : Tensor = prim::GetAttr[name="bias"](%1035)
  %1106 : Tensor = prim::GetAttr[name="weight"](%1035)
  %1107 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn_layer_norm
  %input.108 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.107, %1107, %1106, %1105, %127, %128), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1109 : Tensor = prim::GetAttr[name="bias"](%1034)
  %1110 : Tensor = prim::GetAttr[name="weight"](%1034)
  %1111 : Float(1024:1, 4096:1024) = aten::t(%1110), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.108, %1111), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.59, %1109, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.110 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.109), scope: __module.encoder/__module.encoder.layers.9 # torch/nn/functional.py:1369:0
  %input.111 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.110, %116, %134), scope: __module.encoder/__module.encoder.layers.9 # torch/nn/functional.py:973:0
  %1116 : Tensor = prim::GetAttr[name="bias"](%1033)
  %1117 : Tensor = prim::GetAttr[name="weight"](%1033)
  %1118 : Float(4096:1, 1024:4096) = aten::t(%1117), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.111, %1118), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.112 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.60, %1116, %133), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.112, %125, %134), scope: __module.encoder/__module.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.108, %x.21, %133), scope: __module.encoder/__module.encoder.layers.9 # transformers/modeling_bart.py:269:0
  %1123 : Tensor = prim::GetAttr[name="bias"](%1032)
  %1124 : Tensor = prim::GetAttr[name="weight"](%1032)
  %1125 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.final_layer_norm
  %query.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.113, %1125, %1124, %1123, %127, %128), scope: __module.encoder/__module.encoder.layers.9/__module.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %1127 : __torch__.torch.nn.modules.normalization.___torch_mangle_2125.LayerNorm = prim::GetAttr[name="final_layer_norm"](%139)
  %1128 : __torch__.torch.nn.modules.linear.___torch_mangle_2124.Linear = prim::GetAttr[name="fc2"](%139)
  %1129 : __torch__.torch.nn.modules.linear.___torch_mangle_2123.Linear = prim::GetAttr[name="fc1"](%139)
  %1130 : __torch__.torch.nn.modules.normalization.___torch_mangle_2122.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%139)
  %1131 : __torch__.transformers.modeling_bart.___torch_mangle_2121.Attention = prim::GetAttr[name="self_attn"](%139)
  %1132 : __torch__.torch.nn.modules.linear.___torch_mangle_2120.Linear = prim::GetAttr[name="out_proj"](%1131)
  %1133 : __torch__.torch.nn.modules.linear.___torch_mangle_2118.Linear = prim::GetAttr[name="v_proj"](%1131)
  %1134 : __torch__.torch.nn.modules.linear.___torch_mangle_2117.Linear = prim::GetAttr[name="k_proj"](%1131)
  %1135 : __torch__.torch.nn.modules.linear.___torch_mangle_2119.Linear = prim::GetAttr[name="q_proj"](%1131)
  %1136 : int = aten::size(%query.11, %135), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1137 : int = aten::size(%query.11, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.11 : Long() = prim::NumToTensor(%1137), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1139 : int = aten::size(%query.11, %124), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1140 : Tensor = prim::GetAttr[name="bias"](%1135)
  %1141 : Tensor = prim::GetAttr[name="weight"](%1135)
  %1142 : Float(1024:1, 1024:1024) = aten::t(%1141), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.61 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1142), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1144 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.61, %1140, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.61 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1144, %123), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %1146 : Tensor = prim::GetAttr[name="bias"](%1134)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1148 : Float(1024:1, 1024:1024) = aten::t(%1147), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1148), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.63 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %1146, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1151 : Tensor = prim::GetAttr[name="bias"](%1133)
  %1152 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1153 : Float(1024:1, 1024:1024) = aten::t(%1152), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1153), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.65 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.63, %1151, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.62 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.61, %135), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1157 : Long() = aten::mul(%bsz.11, %122), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1158 : int = aten::Int(%1157), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1159 : int[] = prim::ListConstruct(%1136, %1158, %121), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1160 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.62, %1159), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1160, %135, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.64 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.63, %135), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1163 : Long() = aten::mul(%bsz.11, %122), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1164 : int = aten::Int(%1163), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1165 : int[] = prim::ListConstruct(%120, %1164, %121), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1166 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.64, %1165), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1166, %135, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.66 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.65, %135), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1169 : Long() = aten::mul(%bsz.11, %122), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1170 : int = aten::Int(%1169), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1171 : int[] = prim::ListConstruct(%120, %1170, %121), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1172 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.66, %1171), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1172, %135, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1174 : int = aten::size(%k.11, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %1175 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.11, %133, %124), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.41 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.11, %1175), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %1177 : int[] = prim::ListConstruct(%1137, %119, %1136, %1174), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %attn_weights.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.41, %1177), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:718:0
  %1179 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.11 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1179, %124), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.42, %reshaped.11, %118), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:720:0
  %1182 : Long() = aten::mul(%bsz.11, %122), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %1183 : int = aten::Int(%1182), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %1184 : int[] = prim::ListConstruct(%1183, %1136, %1174), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %input.114 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.43, %1184), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %input.115 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.114, %120, %117), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.44 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.115, %116, %134), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.11 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.44, %v.11), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %1189 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.11, %135, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1190 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1189, %135), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1191 : int[] = prim::ListConstruct(%1136, %1137, %1139), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn
  %input.116 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1190, %1191), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1193 : Tensor = prim::GetAttr[name="bias"](%1132)
  %1194 : Tensor = prim::GetAttr[name="weight"](%1132)
  %1195 : Float(1024:1, 1024:1024) = aten::t(%1194), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.64 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.116, %1195), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.64, %1193, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn/__module.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.117, %125, %134), scope: __module.encoder/__module.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.11, %x.22, %133), scope: __module.encoder/__module.encoder.layers.10 # transformers/modeling_bart.py:258:0
  %1200 : Tensor = prim::GetAttr[name="bias"](%1130)
  %1201 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1202 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn_layer_norm
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.118, %1202, %1201, %1200, %127, %128), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1204 : Tensor = prim::GetAttr[name="bias"](%1129)
  %1205 : Tensor = prim::GetAttr[name="weight"](%1129)
  %1206 : Float(1024:1, 4096:1024) = aten::t(%1205), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.119, %1206), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.120 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.65, %1204, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.121 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.120), scope: __module.encoder/__module.encoder.layers.10 # torch/nn/functional.py:1369:0
  %input.122 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.121, %116, %134), scope: __module.encoder/__module.encoder.layers.10 # torch/nn/functional.py:973:0
  %1211 : Tensor = prim::GetAttr[name="bias"](%1128)
  %1212 : Tensor = prim::GetAttr[name="weight"](%1128)
  %1213 : Float(4096:1, 1024:4096) = aten::t(%1212), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.122, %1213), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.66, %1211, %133), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.123, %125, %134), scope: __module.encoder/__module.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.124 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.119, %x.23, %133), scope: __module.encoder/__module.encoder.layers.10 # transformers/modeling_bart.py:269:0
  %1218 : Tensor = prim::GetAttr[name="bias"](%1127)
  %1219 : Tensor = prim::GetAttr[name="weight"](%1127)
  %1220 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.final_layer_norm
  %query.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.124, %1220, %1219, %1218, %127, %128), scope: __module.encoder/__module.encoder.layers.10/__module.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %1222 : __torch__.torch.nn.modules.normalization.___torch_mangle_2135.LayerNorm = prim::GetAttr[name="final_layer_norm"](%137)
  %1223 : __torch__.torch.nn.modules.linear.___torch_mangle_2134.Linear = prim::GetAttr[name="fc2"](%137)
  %1224 : __torch__.torch.nn.modules.linear.___torch_mangle_2133.Linear = prim::GetAttr[name="fc1"](%137)
  %1225 : __torch__.torch.nn.modules.normalization.___torch_mangle_2132.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%137)
  %1226 : __torch__.transformers.modeling_bart.___torch_mangle_2131.Attention = prim::GetAttr[name="self_attn"](%137)
  %1227 : __torch__.torch.nn.modules.linear.___torch_mangle_2130.Linear = prim::GetAttr[name="out_proj"](%1226)
  %1228 : __torch__.torch.nn.modules.linear.___torch_mangle_2128.Linear = prim::GetAttr[name="v_proj"](%1226)
  %1229 : __torch__.torch.nn.modules.linear.___torch_mangle_2127.Linear = prim::GetAttr[name="k_proj"](%1226)
  %1230 : __torch__.torch.nn.modules.linear.___torch_mangle_2129.Linear = prim::GetAttr[name="q_proj"](%1226)
  %1231 : int = aten::size(%query.12, %135), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1232 : int = aten::size(%query.12, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.12 : Long() = prim::NumToTensor(%1232), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1234 : int = aten::size(%query.12, %124), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1235 : Tensor = prim::GetAttr[name="bias"](%1230)
  %1236 : Tensor = prim::GetAttr[name="weight"](%1230)
  %1237 : Float(1024:1, 1024:1024) = aten::t(%1236), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.67 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1237), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1239 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.67, %1235, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.67 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1239, %123), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %1241 : Tensor = prim::GetAttr[name="bias"](%1229)
  %1242 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1243 : Float(1024:1, 1024:1024) = aten::t(%1242), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1243), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.69 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %1241, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1246 : Tensor = prim::GetAttr[name="bias"](%1228)
  %1247 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1248 : Float(1024:1, 1024:1024) = aten::t(%1247), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1248), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.71 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.69, %1246, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.68 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.67, %135), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1252 : Long() = aten::mul(%bsz.12, %122), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1253 : int = aten::Int(%1252), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1254 : int[] = prim::ListConstruct(%1231, %1253, %121), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1255 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.68, %1254), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1255, %135, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.70 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.69, %135), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1258 : Long() = aten::mul(%bsz.12, %122), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1259 : int = aten::Int(%1258), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1260 : int[] = prim::ListConstruct(%120, %1259, %121), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1261 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.70, %1260), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1261, %135, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.72 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.71, %135), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1264 : Long() = aten::mul(%bsz.12, %122), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1265 : int = aten::Int(%1264), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1266 : int[] = prim::ListConstruct(%120, %1265, %121), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1267 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.72, %1266), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1267, %135, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1269 : int = aten::size(%k.12, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %1270 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.12, %133, %124), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.45 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.12, %1270), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %1272 : int[] = prim::ListConstruct(%1232, %119, %1231, %1269), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %attn_weights.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.45, %1272), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:718:0
  %1274 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.12 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1274, %124), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.46, %reshaped.12, %118), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:720:0
  %1277 : Long() = aten::mul(%bsz.12, %122), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %1278 : int = aten::Int(%1277), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %1279 : int[] = prim::ListConstruct(%1278, %1231, %1269), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %input.125 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.47, %1279), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %input.126 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.125, %120, %117), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.48 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.126, %116, %134), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.12 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.48, %v.12), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %1284 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.12, %135, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1285 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1284, %135), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1286 : int[] = prim::ListConstruct(%1231, %1232, %1234), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1285, %1286), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1288 : Tensor = prim::GetAttr[name="bias"](%1227)
  %1289 : Tensor = prim::GetAttr[name="weight"](%1227)
  %1290 : Float(1024:1, 1024:1024) = aten::t(%1289), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.127, %1290), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.70, %1288, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn/__module.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.128, %125, %134), scope: __module.encoder/__module.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.129 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.12, %x.24, %133), scope: __module.encoder/__module.encoder.layers.11 # transformers/modeling_bart.py:258:0
  %1295 : Tensor = prim::GetAttr[name="bias"](%1225)
  %1296 : Tensor = prim::GetAttr[name="weight"](%1225)
  %1297 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn_layer_norm
  %input.130 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.129, %1297, %1296, %1295, %127, %128), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1299 : Tensor = prim::GetAttr[name="bias"](%1224)
  %1300 : Tensor = prim::GetAttr[name="weight"](%1224)
  %1301 : Float(1024:1, 4096:1024) = aten::t(%1300), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.130, %1301), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.131 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.71, %1299, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.132 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.131), scope: __module.encoder/__module.encoder.layers.11 # torch/nn/functional.py:1369:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.132, %116, %134), scope: __module.encoder/__module.encoder.layers.11 # torch/nn/functional.py:973:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%1223)
  %1307 : Tensor = prim::GetAttr[name="weight"](%1223)
  %1308 : Float(4096:1, 1024:4096) = aten::t(%1307), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.133, %1308), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.134 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.72, %1306, %133), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.25 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.134, %125, %134), scope: __module.encoder/__module.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.135 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.130, %x.25, %133), scope: __module.encoder/__module.encoder.layers.11 # transformers/modeling_bart.py:269:0
  %1313 : Tensor = prim::GetAttr[name="bias"](%1222)
  %1314 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1315 : int[] = prim::ListConstruct(%126), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.final_layer_norm
  %x.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.135, %1315, %1314, %1313, %127, %128), scope: __module.encoder/__module.encoder.layers.11/__module.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %encoder_hidden_states : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x.26, %135, %133), scope: __module.encoder # transformers/modeling_bart.py:366:0
  %1318 : float = prim::Constant[value=0.](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %1319 : None = prim::Constant(), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1320 : int = prim::Constant[value=16](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %1321 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1322 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1323 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1324 : Double() = prim::Constant[value={0.125}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %1325 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1326 : float = prim::Constant[value=-inf](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
  %1327 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder # torch/nn/functional.py:973:0
  %1328 : int = prim::Constant[value=1024](), scope: __module.decoder/__module.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %1329 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.decoder/__module.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %1330 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %1331 : Double() = prim::Constant[value={1}](), scope: __module.decoder # transformers/modeling_bart.py:556:0
  %1332 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:816:0
  %1333 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %1334 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %1335 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %1336 : Long() = prim::Constant[value={2}](), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:822:0
  %1337 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_bart.py:136:0
  %1338 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1339 : __torch__.transformers.modeling_bart.___torch_mangle_2333.DecoderLayer = prim::GetAttr[name="11"](%1338)
  %1340 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1341 : __torch__.transformers.modeling_bart.___torch_mangle_2317.DecoderLayer = prim::GetAttr[name="10"](%1340)
  %1342 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1343 : __torch__.transformers.modeling_bart.___torch_mangle_2301.DecoderLayer = prim::GetAttr[name="9"](%1342)
  %1344 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1345 : __torch__.transformers.modeling_bart.___torch_mangle_2285.DecoderLayer = prim::GetAttr[name="8"](%1344)
  %1346 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1347 : __torch__.transformers.modeling_bart.___torch_mangle_2269.DecoderLayer = prim::GetAttr[name="7"](%1346)
  %1348 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1349 : __torch__.transformers.modeling_bart.___torch_mangle_2253.DecoderLayer = prim::GetAttr[name="6"](%1348)
  %1350 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1351 : __torch__.transformers.modeling_bart.___torch_mangle_2237.DecoderLayer = prim::GetAttr[name="5"](%1350)
  %1352 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1353 : __torch__.transformers.modeling_bart.___torch_mangle_2221.DecoderLayer = prim::GetAttr[name="4"](%1352)
  %1354 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1355 : __torch__.transformers.modeling_bart.___torch_mangle_2205.DecoderLayer = prim::GetAttr[name="3"](%1354)
  %1356 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1357 : __torch__.transformers.modeling_bart.___torch_mangle_2189.DecoderLayer = prim::GetAttr[name="2"](%1356)
  %1358 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1359 : __torch__.transformers.modeling_bart.___torch_mangle_2173.DecoderLayer = prim::GetAttr[name="1"](%1358)
  %1360 : __torch__.torch.nn.modules.container.___torch_mangle_2334.ModuleList = prim::GetAttr[name="layers"](%3)
  %1361 : __torch__.transformers.modeling_bart.___torch_mangle_2157.DecoderLayer = prim::GetAttr[name="0"](%1360)
  %1362 : __torch__.torch.nn.modules.normalization.___torch_mangle_2335.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%3)
  %1363 : __torch__.transformers.modeling_bart.___torch_mangle_2141.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%3)
  %key_padding_mask : Bool(17:13, 13:1) = aten::eq(%attention_mask, %1337), scope: __module.decoder # transformers/modeling_bart.py:136:0
  %1365 : Tensor = prim::GetAttr[name="weight"](%1363)
  %1366 : int = aten::size(%prev_output_tokens, %1332), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.2 : Long(13:1) = aten::arange(%1366, %1333, %1337, %1334, %1335), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.136 : Long(13:1) = aten::add(%positions.2, %1336, %1332), scope: __module.decoder/__module.decoder.embed_positions # transformers/modeling_bart.py:822:0
  %positions : Float(13:1024, 1024:1) = aten::embedding(%1365, %input.136, %1332, %1335, %1335), scope: __module.decoder/__module.decoder.embed_positions # torch/nn/functional.py:1814:0
  %1370 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%8, %prev_output_tokens, %1332, %1335, %1335), scope: __module.decoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::mul(%1370, %1331), scope: __module.decoder # transformers/modeling_bart.py:556:0
  %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%x.27, %positions, %1332), scope: __module.decoder # transformers/modeling_bart.py:557:0
  %1373 : Tensor = prim::GetAttr[name="bias"](%1362)
  %1374 : Tensor = prim::GetAttr[name="weight"](%1362)
  %1375 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layernorm_embedding
  %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.137, %1375, %1374, %1373, %1329, %1330), scope: __module.decoder/__module.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.138, %1327, %1335), scope: __module.decoder # torch/nn/functional.py:973:0
  %query.13 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.28, %1337, %1332), scope: __module.decoder # transformers/modeling_bart.py:562:0
  %input.144 : Float(13:17408, 17:1024, 1024:1) = aten::transpose(%encoder_hidden_states, %1337, %1332), scope: __module.decoder # transformers/modeling_bart.py:563:0
  %1380 : __torch__.torch.nn.modules.normalization.___torch_mangle_2156.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1361)
  %1381 : __torch__.torch.nn.modules.linear.___torch_mangle_2155.Linear = prim::GetAttr[name="fc2"](%1361)
  %1382 : __torch__.torch.nn.modules.linear.___torch_mangle_2154.Linear = prim::GetAttr[name="fc1"](%1361)
  %1383 : __torch__.torch.nn.modules.normalization.___torch_mangle_2153.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1361)
  %1384 : __torch__.transformers.modeling_bart.___torch_mangle_2152.Attention = prim::GetAttr[name="encoder_attn"](%1361)
  %1385 : __torch__.torch.nn.modules.normalization.___torch_mangle_2147.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1361)
  %1386 : __torch__.transformers.modeling_bart.___torch_mangle_2146.Attention = prim::GetAttr[name="self_attn"](%1361)
  %1387 : __torch__.torch.nn.modules.linear.___torch_mangle_2145.Linear = prim::GetAttr[name="out_proj"](%1386)
  %1388 : __torch__.torch.nn.modules.linear.___torch_mangle_2143.Linear = prim::GetAttr[name="v_proj"](%1386)
  %1389 : __torch__.torch.nn.modules.linear.___torch_mangle_2142.Linear = prim::GetAttr[name="k_proj"](%1386)
  %1390 : __torch__.torch.nn.modules.linear.___torch_mangle_2144.Linear = prim::GetAttr[name="q_proj"](%1386)
  %1391 : int = aten::size(%query.13, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1392 : int = aten::size(%query.13, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.13 : Long() = prim::NumToTensor(%1392), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1394 : int = aten::size(%query.13, %1325), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1395 : Tensor = prim::GetAttr[name="bias"](%1390)
  %1396 : Tensor = prim::GetAttr[name="weight"](%1390)
  %1397 : Float(1024:1, 1024:1024) = aten::t(%1396), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.73 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1397), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1399 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.73, %1395, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.73 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1399, %1324), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %1401 : Tensor = prim::GetAttr[name="bias"](%1389)
  %1402 : Tensor = prim::GetAttr[name="weight"](%1389)
  %1403 : Float(1024:1, 1024:1024) = aten::t(%1402), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.74 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1403), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.75 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.74, %1401, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1406 : Tensor = prim::GetAttr[name="bias"](%1388)
  %1407 : Tensor = prim::GetAttr[name="weight"](%1388)
  %1408 : Float(1024:1, 1024:1024) = aten::t(%1407), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.75 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1408), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.77 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.75, %1406, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.74 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.73, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1412 : Long() = aten::mul(%bsz.13, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1413 : int = aten::Int(%1412), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1414 : int[] = prim::ListConstruct(%1391, %1413, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1415 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.74, %1414), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1415, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.76 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.75, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1418 : Long() = aten::mul(%bsz.13, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1419 : int = aten::Int(%1418), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1420 : int[] = prim::ListConstruct(%1321, %1419, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1421 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.76, %1420), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1421, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.78 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.77, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1424 : Long() = aten::mul(%bsz.13, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1425 : int = aten::Int(%1424), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1426 : int[] = prim::ListConstruct(%1321, %1425, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1427 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.78, %1426), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1427, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1429 : int = aten::size(%k.13, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %1430 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.13, %1332, %1325), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.49 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.13, %1430), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %1432 : int[] = prim::ListConstruct(%1392, %1320, %1391, %1429), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1433 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.49, %1432), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.50 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1433, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %1435 : Long() = aten::mul(%bsz.13, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %1436 : int = aten::Int(%1435), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %1437 : int[] = prim::ListConstruct(%1436, %1391, %1429), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %input.139 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.50, %1437), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %input.140 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.139, %1321, %1319), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.51 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.140, %1318, %1335), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.13 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.51, %v.13), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %1442 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.13, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1443 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1442, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1444 : int[] = prim::ListConstruct(%1391, %1392, %1394), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1443, %1444), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1446 : Tensor = prim::GetAttr[name="bias"](%1387)
  %1447 : Tensor = prim::GetAttr[name="weight"](%1387)
  %1448 : Float(1024:1, 1024:1024) = aten::t(%1447), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.76 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.141, %1448), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.142 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.76, %1446, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn/__module.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.29 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.142, %1327, %1335), scope: __module.decoder/__module.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.143 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.13, %x.29, %1332), scope: __module.decoder/__module.decoder.layers.0 # transformers/modeling_bart.py:427:0
  %1453 : Tensor = prim::GetAttr[name="bias"](%1385)
  %1454 : Tensor = prim::GetAttr[name="weight"](%1385)
  %1455 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn_layer_norm
  %query.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.143, %1455, %1454, %1453, %1329, %1330), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1457 : __torch__.torch.nn.modules.linear.___torch_mangle_2151.Linear = prim::GetAttr[name="out_proj"](%1384)
  %1458 : __torch__.torch.nn.modules.linear.___torch_mangle_2149.Linear = prim::GetAttr[name="v_proj"](%1384)
  %1459 : __torch__.torch.nn.modules.linear.___torch_mangle_2148.Linear = prim::GetAttr[name="k_proj"](%1384)
  %1460 : __torch__.torch.nn.modules.linear.___torch_mangle_2150.Linear = prim::GetAttr[name="q_proj"](%1384)
  %1461 : int = aten::size(%query.14, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1462 : int = aten::size(%query.14, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.14 : Long() = prim::NumToTensor(%1462), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1464 : int = aten::size(%query.14, %1325), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1465 : Tensor = prim::GetAttr[name="bias"](%1460)
  %1466 : Tensor = prim::GetAttr[name="weight"](%1460)
  %1467 : Float(1024:1, 1024:1024) = aten::t(%1466), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.77 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.14, %1467), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1469 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.77, %1465, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.79 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1469, %1324), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:673:0
  %1471 : Tensor = prim::GetAttr[name="bias"](%1459)
  %1472 : Tensor = prim::GetAttr[name="weight"](%1459)
  %1473 : Float(1024:1, 1024:1024) = aten::t(%1472), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.78 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1473), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.81 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.78, %1471, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1476 : Tensor = prim::GetAttr[name="bias"](%1458)
  %1477 : Tensor = prim::GetAttr[name="weight"](%1458)
  %1478 : Float(1024:1, 1024:1024) = aten::t(%1477), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.79 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1478), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.83 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.79, %1476, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.80 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.79, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1482 : Long() = aten::mul(%bsz.14, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1483 : int = aten::Int(%1482), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1484 : int[] = prim::ListConstruct(%1461, %1483, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1485 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.80, %1484), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %q.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1485, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.82 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.81, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1488 : Long() = aten::mul(%bsz.14, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1489 : int = aten::Int(%1488), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1490 : int[] = prim::ListConstruct(%1321, %1489, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1491 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.82, %1490), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %k.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1491, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.84 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.83, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1494 : Long() = aten::mul(%bsz.14, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1495 : int = aten::Int(%1494), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1496 : int[] = prim::ListConstruct(%1321, %1495, %1322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1497 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.84, %1496), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %v.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1497, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1499 : int = aten::size(%k.14, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:701:0
  %1500 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.14, %1332, %1325), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.52 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.14, %1500), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %1502 : int[] = prim::ListConstruct(%1462, %1320, %1461, %1499), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %attn_weights.53 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.52, %1502), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:718:0
  %1504 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.13 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1504, %1325), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.54 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.53, %reshaped.13, %1326), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
  %1507 : Long() = aten::mul(%bsz.14, %1323), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %1508 : int = aten::Int(%1507), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %1509 : int[] = prim::ListConstruct(%1508, %1461, %1499), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %input.145 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.54, %1509), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %input.146 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.145, %1321, %1319), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.55 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.146, %1318, %1335), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.14 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.55, %v.14), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:730:0
  %1514 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.14, %1337, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1515 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1514, %1337), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1516 : int[] = prim::ListConstruct(%1461, %1462, %1464), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn
  %input.147 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1515, %1516), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1518 : Tensor = prim::GetAttr[name="bias"](%1457)
  %1519 : Tensor = prim::GetAttr[name="weight"](%1457)
  %1520 : Float(1024:1, 1024:1024) = aten::t(%1519), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.80 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.147, %1520), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.148 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.80, %1518, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn/__module.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.148, %1327, %1335), scope: __module.decoder/__module.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.14, %x.30, %1332), scope: __module.decoder/__module.decoder.layers.0 # transformers/modeling_bart.py:443:0
  %1525 : Tensor = prim::GetAttr[name="bias"](%1383)
  %1526 : Tensor = prim::GetAttr[name="weight"](%1383)
  %1527 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn_layer_norm
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.149, %1527, %1526, %1525, %1329, %1330), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1529 : Tensor = prim::GetAttr[name="bias"](%1382)
  %1530 : Tensor = prim::GetAttr[name="weight"](%1382)
  %1531 : Float(1024:1, 4096:1024) = aten::t(%1530), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.81 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.150, %1531), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.81, %1529, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.decoder/__module.decoder.layers.0 # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %1318, %1335), scope: __module.decoder/__module.decoder.layers.0 # torch/nn/functional.py:973:0
  %1536 : Tensor = prim::GetAttr[name="bias"](%1381)
  %1537 : Tensor = prim::GetAttr[name="weight"](%1381)
  %1538 : Float(4096:1, 1024:4096) = aten::t(%1537), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.82 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1538), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.82, %1536, %1332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.31 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %1327, %1335), scope: __module.decoder/__module.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.150, %x.31, %1332), scope: __module.decoder/__module.decoder.layers.0 # transformers/modeling_bart.py:455:0
  %1543 : Tensor = prim::GetAttr[name="bias"](%1380)
  %1544 : Tensor = prim::GetAttr[name="weight"](%1380)
  %1545 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.final_layer_norm
  %query.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1545, %1544, %1543, %1329, %1330), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %1547 : __torch__.torch.nn.modules.normalization.___torch_mangle_2172.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1359)
  %1548 : __torch__.torch.nn.modules.linear.___torch_mangle_2171.Linear = prim::GetAttr[name="fc2"](%1359)
  %1549 : __torch__.torch.nn.modules.linear.___torch_mangle_2170.Linear = prim::GetAttr[name="fc1"](%1359)
  %1550 : __torch__.torch.nn.modules.normalization.___torch_mangle_2169.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1359)
  %1551 : __torch__.transformers.modeling_bart.___torch_mangle_2168.Attention = prim::GetAttr[name="encoder_attn"](%1359)
  %1552 : __torch__.torch.nn.modules.normalization.___torch_mangle_2163.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1359)
  %1553 : __torch__.transformers.modeling_bart.___torch_mangle_2162.Attention = prim::GetAttr[name="self_attn"](%1359)
  %1554 : __torch__.torch.nn.modules.linear.___torch_mangle_2161.Linear = prim::GetAttr[name="out_proj"](%1553)
  %1555 : __torch__.torch.nn.modules.linear.___torch_mangle_2159.Linear = prim::GetAttr[name="v_proj"](%1553)
  %1556 : __torch__.torch.nn.modules.linear.___torch_mangle_2158.Linear = prim::GetAttr[name="k_proj"](%1553)
  %1557 : __torch__.torch.nn.modules.linear.___torch_mangle_2160.Linear = prim::GetAttr[name="q_proj"](%1553)
  %1558 : int = aten::size(%query.15, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1559 : int = aten::size(%query.15, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.15 : Long() = prim::NumToTensor(%1559), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1561 : int = aten::size(%query.15, %1325), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1562 : Tensor = prim::GetAttr[name="bias"](%1557)
  %1563 : Tensor = prim::GetAttr[name="weight"](%1557)
  %1564 : Float(1024:1, 1024:1024) = aten::t(%1563), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.83 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1564), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1566 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.83, %1562, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.85 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1566, %1324), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %1568 : Tensor = prim::GetAttr[name="bias"](%1556)
  %1569 : Tensor = prim::GetAttr[name="weight"](%1556)
  %1570 : Float(1024:1, 1024:1024) = aten::t(%1569), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.84 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1570), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.87 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.84, %1568, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1573 : Tensor = prim::GetAttr[name="bias"](%1555)
  %1574 : Tensor = prim::GetAttr[name="weight"](%1555)
  %1575 : Float(1024:1, 1024:1024) = aten::t(%1574), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.85 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1575), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.89 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.85, %1573, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.86 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.85, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1579 : Long() = aten::mul(%bsz.15, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1580 : int = aten::Int(%1579), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1581 : int[] = prim::ListConstruct(%1558, %1580, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1582 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.86, %1581), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1582, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.88 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.87, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1585 : Long() = aten::mul(%bsz.15, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1586 : int = aten::Int(%1585), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1587 : int[] = prim::ListConstruct(%1321, %1586, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1588 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.88, %1587), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1588, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.90 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.89, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1591 : Long() = aten::mul(%bsz.15, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1592 : int = aten::Int(%1591), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1593 : int[] = prim::ListConstruct(%1321, %1592, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1594 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.90, %1593), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1594, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1596 : int = aten::size(%k.15, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %1597 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.15, %1332, %1325), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.56 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.15, %1597), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %1599 : int[] = prim::ListConstruct(%1559, %1320, %1558, %1596), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1600 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.56, %1599), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1600, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %1602 : Long() = aten::mul(%bsz.15, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %1603 : int = aten::Int(%1602), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %1604 : int[] = prim::ListConstruct(%1603, %1558, %1596), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %input.156 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.57, %1604), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %input.157 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.156, %1321, %1319), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.58 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.157, %1318, %1335), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.15 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.58, %v.15), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %1609 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.15, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1610 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1609, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1611 : int[] = prim::ListConstruct(%1558, %1559, %1561), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1610, %1611), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1613 : Tensor = prim::GetAttr[name="bias"](%1554)
  %1614 : Tensor = prim::GetAttr[name="weight"](%1554)
  %1615 : Float(1024:1, 1024:1024) = aten::t(%1614), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.86 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.158, %1615), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.86, %1613, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn/__module.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.32 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.159, %1327, %1335), scope: __module.decoder/__module.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.160 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.15, %x.32, %1332), scope: __module.decoder/__module.decoder.layers.1 # transformers/modeling_bart.py:427:0
  %1620 : Tensor = prim::GetAttr[name="bias"](%1552)
  %1621 : Tensor = prim::GetAttr[name="weight"](%1552)
  %1622 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn_layer_norm
  %query.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.160, %1622, %1621, %1620, %1329, %1330), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1624 : __torch__.torch.nn.modules.linear.___torch_mangle_2167.Linear = prim::GetAttr[name="out_proj"](%1551)
  %1625 : __torch__.torch.nn.modules.linear.___torch_mangle_2165.Linear = prim::GetAttr[name="v_proj"](%1551)
  %1626 : __torch__.torch.nn.modules.linear.___torch_mangle_2164.Linear = prim::GetAttr[name="k_proj"](%1551)
  %1627 : __torch__.torch.nn.modules.linear.___torch_mangle_2166.Linear = prim::GetAttr[name="q_proj"](%1551)
  %1628 : int = aten::size(%query.16, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1629 : int = aten::size(%query.16, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.16 : Long() = prim::NumToTensor(%1629), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1631 : int = aten::size(%query.16, %1325), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1632 : Tensor = prim::GetAttr[name="bias"](%1627)
  %1633 : Tensor = prim::GetAttr[name="weight"](%1627)
  %1634 : Float(1024:1, 1024:1024) = aten::t(%1633), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.87 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.16, %1634), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1636 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.87, %1632, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.91 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1636, %1324), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:673:0
  %1638 : Tensor = prim::GetAttr[name="bias"](%1626)
  %1639 : Tensor = prim::GetAttr[name="weight"](%1626)
  %1640 : Float(1024:1, 1024:1024) = aten::t(%1639), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.88 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1640), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.93 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.88, %1638, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1643 : Tensor = prim::GetAttr[name="bias"](%1625)
  %1644 : Tensor = prim::GetAttr[name="weight"](%1625)
  %1645 : Float(1024:1, 1024:1024) = aten::t(%1644), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.89 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1645), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.89, %1643, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.92 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.91, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1649 : Long() = aten::mul(%bsz.16, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1650 : int = aten::Int(%1649), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1651 : int[] = prim::ListConstruct(%1628, %1650, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1652 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.92, %1651), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %q.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1652, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.94 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.93, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1655 : Long() = aten::mul(%bsz.16, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1656 : int = aten::Int(%1655), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1657 : int[] = prim::ListConstruct(%1321, %1656, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1658 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.94, %1657), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %k.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1658, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.96 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.95, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1661 : Long() = aten::mul(%bsz.16, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1662 : int = aten::Int(%1661), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1663 : int[] = prim::ListConstruct(%1321, %1662, %1322), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1664 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.96, %1663), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %v.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1664, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1666 : int = aten::size(%k.16, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:701:0
  %1667 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.16, %1332, %1325), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.59 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.16, %1667), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %1669 : int[] = prim::ListConstruct(%1629, %1320, %1628, %1666), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %attn_weights.60 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.59, %1669), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:718:0
  %1671 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.14 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1671, %1325), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.61 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.60, %reshaped.14, %1326), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:720:0
  %1674 : Long() = aten::mul(%bsz.16, %1323), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %1675 : int = aten::Int(%1674), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %1676 : int[] = prim::ListConstruct(%1675, %1628, %1666), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %input.161 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.61, %1676), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %input.162 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.161, %1321, %1319), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.62 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.162, %1318, %1335), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.16 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.62, %v.16), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:730:0
  %1681 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.16, %1337, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1682 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1681, %1337), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1683 : int[] = prim::ListConstruct(%1628, %1629, %1631), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1682, %1683), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1685 : Tensor = prim::GetAttr[name="bias"](%1624)
  %1686 : Tensor = prim::GetAttr[name="weight"](%1624)
  %1687 : Float(1024:1, 1024:1024) = aten::t(%1686), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.90 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.163, %1687), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.90, %1685, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn/__module.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.164, %1327, %1335), scope: __module.decoder/__module.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.165 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.16, %x.33, %1332), scope: __module.decoder/__module.decoder.layers.1 # transformers/modeling_bart.py:443:0
  %1692 : Tensor = prim::GetAttr[name="bias"](%1550)
  %1693 : Tensor = prim::GetAttr[name="weight"](%1550)
  %1694 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn_layer_norm
  %input.166 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.165, %1694, %1693, %1692, %1329, %1330), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1696 : Tensor = prim::GetAttr[name="bias"](%1549)
  %1697 : Tensor = prim::GetAttr[name="weight"](%1549)
  %1698 : Float(1024:1, 4096:1024) = aten::t(%1697), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.91 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.166, %1698), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.167 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.91, %1696, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.168 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.167), scope: __module.decoder/__module.decoder.layers.1 # torch/nn/functional.py:1369:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.168, %1318, %1335), scope: __module.decoder/__module.decoder.layers.1 # torch/nn/functional.py:973:0
  %1703 : Tensor = prim::GetAttr[name="bias"](%1548)
  %1704 : Tensor = prim::GetAttr[name="weight"](%1548)
  %1705 : Float(4096:1, 1024:4096) = aten::t(%1704), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.92 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.169, %1705), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.170 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.92, %1703, %1332), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.34 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.170, %1327, %1335), scope: __module.decoder/__module.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.171 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.166, %x.34, %1332), scope: __module.decoder/__module.decoder.layers.1 # transformers/modeling_bart.py:455:0
  %1710 : Tensor = prim::GetAttr[name="bias"](%1547)
  %1711 : Tensor = prim::GetAttr[name="weight"](%1547)
  %1712 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.final_layer_norm
  %query.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.171, %1712, %1711, %1710, %1329, %1330), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %1714 : __torch__.torch.nn.modules.normalization.___torch_mangle_2188.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1357)
  %1715 : __torch__.torch.nn.modules.linear.___torch_mangle_2187.Linear = prim::GetAttr[name="fc2"](%1357)
  %1716 : __torch__.torch.nn.modules.linear.___torch_mangle_2186.Linear = prim::GetAttr[name="fc1"](%1357)
  %1717 : __torch__.torch.nn.modules.normalization.___torch_mangle_2185.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1357)
  %1718 : __torch__.transformers.modeling_bart.___torch_mangle_2184.Attention = prim::GetAttr[name="encoder_attn"](%1357)
  %1719 : __torch__.torch.nn.modules.normalization.___torch_mangle_2179.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1357)
  %1720 : __torch__.transformers.modeling_bart.___torch_mangle_2178.Attention = prim::GetAttr[name="self_attn"](%1357)
  %1721 : __torch__.torch.nn.modules.linear.___torch_mangle_2177.Linear = prim::GetAttr[name="out_proj"](%1720)
  %1722 : __torch__.torch.nn.modules.linear.___torch_mangle_2175.Linear = prim::GetAttr[name="v_proj"](%1720)
  %1723 : __torch__.torch.nn.modules.linear.___torch_mangle_2174.Linear = prim::GetAttr[name="k_proj"](%1720)
  %1724 : __torch__.torch.nn.modules.linear.___torch_mangle_2176.Linear = prim::GetAttr[name="q_proj"](%1720)
  %1725 : int = aten::size(%query.17, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1726 : int = aten::size(%query.17, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.17 : Long() = prim::NumToTensor(%1726), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1728 : int = aten::size(%query.17, %1325), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1729 : Tensor = prim::GetAttr[name="bias"](%1724)
  %1730 : Tensor = prim::GetAttr[name="weight"](%1724)
  %1731 : Float(1024:1, 1024:1024) = aten::t(%1730), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.93 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1731), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1733 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.93, %1729, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.97 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1733, %1324), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %1735 : Tensor = prim::GetAttr[name="bias"](%1723)
  %1736 : Tensor = prim::GetAttr[name="weight"](%1723)
  %1737 : Float(1024:1, 1024:1024) = aten::t(%1736), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.94 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1737), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.99 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.94, %1735, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1740 : Tensor = prim::GetAttr[name="bias"](%1722)
  %1741 : Tensor = prim::GetAttr[name="weight"](%1722)
  %1742 : Float(1024:1, 1024:1024) = aten::t(%1741), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.95 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1742), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.95, %1740, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.98 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.97, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1746 : Long() = aten::mul(%bsz.17, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1747 : int = aten::Int(%1746), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1748 : int[] = prim::ListConstruct(%1725, %1747, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1749 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.98, %1748), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1749, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.100 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.99, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1752 : Long() = aten::mul(%bsz.17, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1753 : int = aten::Int(%1752), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1754 : int[] = prim::ListConstruct(%1321, %1753, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1755 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.100, %1754), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1755, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.102 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.101, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1758 : Long() = aten::mul(%bsz.17, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1759 : int = aten::Int(%1758), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1760 : int[] = prim::ListConstruct(%1321, %1759, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1761 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.102, %1760), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1761, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1763 : int = aten::size(%k.17, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %1764 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.17, %1332, %1325), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.63 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.17, %1764), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %1766 : int[] = prim::ListConstruct(%1726, %1320, %1725, %1763), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1767 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.63, %1766), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.64 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1767, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %1769 : Long() = aten::mul(%bsz.17, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %1770 : int = aten::Int(%1769), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %1771 : int[] = prim::ListConstruct(%1770, %1725, %1763), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %input.172 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.64, %1771), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %input.173 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.172, %1321, %1319), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.65 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.173, %1318, %1335), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.17 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.65, %v.17), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %1776 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.17, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1777 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1776, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1778 : int[] = prim::ListConstruct(%1725, %1726, %1728), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn
  %input.174 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1777, %1778), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1780 : Tensor = prim::GetAttr[name="bias"](%1721)
  %1781 : Tensor = prim::GetAttr[name="weight"](%1721)
  %1782 : Float(1024:1, 1024:1024) = aten::t(%1781), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.96 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.174, %1782), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.175 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.96, %1780, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn/__module.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.35 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.175, %1327, %1335), scope: __module.decoder/__module.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.17, %x.35, %1332), scope: __module.decoder/__module.decoder.layers.2 # transformers/modeling_bart.py:427:0
  %1787 : Tensor = prim::GetAttr[name="bias"](%1719)
  %1788 : Tensor = prim::GetAttr[name="weight"](%1719)
  %1789 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn_layer_norm
  %query.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.176, %1789, %1788, %1787, %1329, %1330), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1791 : __torch__.torch.nn.modules.linear.___torch_mangle_2183.Linear = prim::GetAttr[name="out_proj"](%1718)
  %1792 : __torch__.torch.nn.modules.linear.___torch_mangle_2181.Linear = prim::GetAttr[name="v_proj"](%1718)
  %1793 : __torch__.torch.nn.modules.linear.___torch_mangle_2180.Linear = prim::GetAttr[name="k_proj"](%1718)
  %1794 : __torch__.torch.nn.modules.linear.___torch_mangle_2182.Linear = prim::GetAttr[name="q_proj"](%1718)
  %1795 : int = aten::size(%query.18, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1796 : int = aten::size(%query.18, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.18 : Long() = prim::NumToTensor(%1796), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1798 : int = aten::size(%query.18, %1325), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1799 : Tensor = prim::GetAttr[name="bias"](%1794)
  %1800 : Tensor = prim::GetAttr[name="weight"](%1794)
  %1801 : Float(1024:1, 1024:1024) = aten::t(%1800), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.97 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.18, %1801), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1803 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.97, %1799, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.103 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1803, %1324), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:673:0
  %1805 : Tensor = prim::GetAttr[name="bias"](%1793)
  %1806 : Tensor = prim::GetAttr[name="weight"](%1793)
  %1807 : Float(1024:1, 1024:1024) = aten::t(%1806), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.98 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1807), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.105 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.98, %1805, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1810 : Tensor = prim::GetAttr[name="bias"](%1792)
  %1811 : Tensor = prim::GetAttr[name="weight"](%1792)
  %1812 : Float(1024:1, 1024:1024) = aten::t(%1811), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.99 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1812), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.107 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.99, %1810, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.104 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.103, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1816 : Long() = aten::mul(%bsz.18, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1817 : int = aten::Int(%1816), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1818 : int[] = prim::ListConstruct(%1795, %1817, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1819 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.104, %1818), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %q.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1819, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.106 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.105, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1822 : Long() = aten::mul(%bsz.18, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1823 : int = aten::Int(%1822), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1824 : int[] = prim::ListConstruct(%1321, %1823, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1825 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.106, %1824), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %k.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1825, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.108 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.107, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1828 : Long() = aten::mul(%bsz.18, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1829 : int = aten::Int(%1828), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1830 : int[] = prim::ListConstruct(%1321, %1829, %1322), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1831 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.108, %1830), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %v.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1831, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1833 : int = aten::size(%k.18, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:701:0
  %1834 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.18, %1332, %1325), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.66 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.18, %1834), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %1836 : int[] = prim::ListConstruct(%1796, %1320, %1795, %1833), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %attn_weights.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.66, %1836), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:718:0
  %1838 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.15 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1838, %1325), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.68 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.67, %reshaped.15, %1326), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:720:0
  %1841 : Long() = aten::mul(%bsz.18, %1323), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %1842 : int = aten::Int(%1841), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %1843 : int[] = prim::ListConstruct(%1842, %1795, %1833), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %input.177 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.68, %1843), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %input.178 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.177, %1321, %1319), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.69 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.178, %1318, %1335), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.18 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.69, %v.18), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:730:0
  %1848 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.18, %1337, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1849 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1848, %1337), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1850 : int[] = prim::ListConstruct(%1795, %1796, %1798), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn
  %input.179 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1849, %1850), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1852 : Tensor = prim::GetAttr[name="bias"](%1791)
  %1853 : Tensor = prim::GetAttr[name="weight"](%1791)
  %1854 : Float(1024:1, 1024:1024) = aten::t(%1853), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.100 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.179, %1854), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.180 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.100, %1852, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn/__module.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.180, %1327, %1335), scope: __module.decoder/__module.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.18, %x.36, %1332), scope: __module.decoder/__module.decoder.layers.2 # transformers/modeling_bart.py:443:0
  %1859 : Tensor = prim::GetAttr[name="bias"](%1717)
  %1860 : Tensor = prim::GetAttr[name="weight"](%1717)
  %1861 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn_layer_norm
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.181, %1861, %1860, %1859, %1329, %1330), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1863 : Tensor = prim::GetAttr[name="bias"](%1716)
  %1864 : Tensor = prim::GetAttr[name="weight"](%1716)
  %1865 : Float(1024:1, 4096:1024) = aten::t(%1864), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.101 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.182, %1865), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.183 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.101, %1863, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.184 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.183), scope: __module.decoder/__module.decoder.layers.2 # torch/nn/functional.py:1369:0
  %input.185 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.184, %1318, %1335), scope: __module.decoder/__module.decoder.layers.2 # torch/nn/functional.py:973:0
  %1870 : Tensor = prim::GetAttr[name="bias"](%1715)
  %1871 : Tensor = prim::GetAttr[name="weight"](%1715)
  %1872 : Float(4096:1, 1024:4096) = aten::t(%1871), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.102 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.185, %1872), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.102, %1870, %1332), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.37 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.186, %1327, %1335), scope: __module.decoder/__module.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.187 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.182, %x.37, %1332), scope: __module.decoder/__module.decoder.layers.2 # transformers/modeling_bart.py:455:0
  %1877 : Tensor = prim::GetAttr[name="bias"](%1714)
  %1878 : Tensor = prim::GetAttr[name="weight"](%1714)
  %1879 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.final_layer_norm
  %query.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.187, %1879, %1878, %1877, %1329, %1330), scope: __module.decoder/__module.decoder.layers.2/__module.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %1881 : __torch__.torch.nn.modules.normalization.___torch_mangle_2204.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1355)
  %1882 : __torch__.torch.nn.modules.linear.___torch_mangle_2203.Linear = prim::GetAttr[name="fc2"](%1355)
  %1883 : __torch__.torch.nn.modules.linear.___torch_mangle_2202.Linear = prim::GetAttr[name="fc1"](%1355)
  %1884 : __torch__.torch.nn.modules.normalization.___torch_mangle_2201.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1355)
  %1885 : __torch__.transformers.modeling_bart.___torch_mangle_2200.Attention = prim::GetAttr[name="encoder_attn"](%1355)
  %1886 : __torch__.torch.nn.modules.normalization.___torch_mangle_2195.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1355)
  %1887 : __torch__.transformers.modeling_bart.___torch_mangle_2194.Attention = prim::GetAttr[name="self_attn"](%1355)
  %1888 : __torch__.torch.nn.modules.linear.___torch_mangle_2193.Linear = prim::GetAttr[name="out_proj"](%1887)
  %1889 : __torch__.torch.nn.modules.linear.___torch_mangle_2191.Linear = prim::GetAttr[name="v_proj"](%1887)
  %1890 : __torch__.torch.nn.modules.linear.___torch_mangle_2190.Linear = prim::GetAttr[name="k_proj"](%1887)
  %1891 : __torch__.torch.nn.modules.linear.___torch_mangle_2192.Linear = prim::GetAttr[name="q_proj"](%1887)
  %1892 : int = aten::size(%query.19, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1893 : int = aten::size(%query.19, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.19 : Long() = prim::NumToTensor(%1893), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1895 : int = aten::size(%query.19, %1325), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1896 : Tensor = prim::GetAttr[name="bias"](%1891)
  %1897 : Tensor = prim::GetAttr[name="weight"](%1891)
  %1898 : Float(1024:1, 1024:1024) = aten::t(%1897), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.103 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1898), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1900 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.103, %1896, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.109 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1900, %1324), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %1902 : Tensor = prim::GetAttr[name="bias"](%1890)
  %1903 : Tensor = prim::GetAttr[name="weight"](%1890)
  %1904 : Float(1024:1, 1024:1024) = aten::t(%1903), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.104 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1904), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.111 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.104, %1902, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1907 : Tensor = prim::GetAttr[name="bias"](%1889)
  %1908 : Tensor = prim::GetAttr[name="weight"](%1889)
  %1909 : Float(1024:1, 1024:1024) = aten::t(%1908), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.105 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1909), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.113 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.105, %1907, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.110 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.109, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1913 : Long() = aten::mul(%bsz.19, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1914 : int = aten::Int(%1913), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1915 : int[] = prim::ListConstruct(%1892, %1914, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1916 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.110, %1915), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1916, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.112 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.111, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1919 : Long() = aten::mul(%bsz.19, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1920 : int = aten::Int(%1919), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1921 : int[] = prim::ListConstruct(%1321, %1920, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1922 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.112, %1921), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1922, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.114 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.113, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1925 : Long() = aten::mul(%bsz.19, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1926 : int = aten::Int(%1925), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1927 : int[] = prim::ListConstruct(%1321, %1926, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1928 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.114, %1927), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1928, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1930 : int = aten::size(%k.19, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %1931 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.19, %1332, %1325), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.70 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.19, %1931), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %1933 : int[] = prim::ListConstruct(%1893, %1320, %1892, %1930), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1934 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.70, %1933), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1934, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %1936 : Long() = aten::mul(%bsz.19, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %1937 : int = aten::Int(%1936), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %1938 : int[] = prim::ListConstruct(%1937, %1892, %1930), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %input.188 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.71, %1938), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %input.189 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.188, %1321, %1319), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.72 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.189, %1318, %1335), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.19 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.72, %v.19), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %1943 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.19, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1944 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1943, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1945 : int[] = prim::ListConstruct(%1892, %1893, %1895), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1944, %1945), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1947 : Tensor = prim::GetAttr[name="bias"](%1888)
  %1948 : Tensor = prim::GetAttr[name="weight"](%1888)
  %1949 : Float(1024:1, 1024:1024) = aten::t(%1948), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.106 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.190, %1949), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.106, %1947, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn/__module.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.38 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.191, %1327, %1335), scope: __module.decoder/__module.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.192 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.19, %x.38, %1332), scope: __module.decoder/__module.decoder.layers.3 # transformers/modeling_bart.py:427:0
  %1954 : Tensor = prim::GetAttr[name="bias"](%1886)
  %1955 : Tensor = prim::GetAttr[name="weight"](%1886)
  %1956 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn_layer_norm
  %query.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.192, %1956, %1955, %1954, %1329, %1330), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1958 : __torch__.torch.nn.modules.linear.___torch_mangle_2199.Linear = prim::GetAttr[name="out_proj"](%1885)
  %1959 : __torch__.torch.nn.modules.linear.___torch_mangle_2197.Linear = prim::GetAttr[name="v_proj"](%1885)
  %1960 : __torch__.torch.nn.modules.linear.___torch_mangle_2196.Linear = prim::GetAttr[name="k_proj"](%1885)
  %1961 : __torch__.torch.nn.modules.linear.___torch_mangle_2198.Linear = prim::GetAttr[name="q_proj"](%1885)
  %1962 : int = aten::size(%query.20, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1963 : int = aten::size(%query.20, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.20 : Long() = prim::NumToTensor(%1963), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1965 : int = aten::size(%query.20, %1325), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1966 : Tensor = prim::GetAttr[name="bias"](%1961)
  %1967 : Tensor = prim::GetAttr[name="weight"](%1961)
  %1968 : Float(1024:1, 1024:1024) = aten::t(%1967), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.107 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.20, %1968), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1970 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.107, %1966, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.115 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1970, %1324), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:673:0
  %1972 : Tensor = prim::GetAttr[name="bias"](%1960)
  %1973 : Tensor = prim::GetAttr[name="weight"](%1960)
  %1974 : Float(1024:1, 1024:1024) = aten::t(%1973), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.108 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1974), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.108, %1972, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1977 : Tensor = prim::GetAttr[name="bias"](%1959)
  %1978 : Tensor = prim::GetAttr[name="weight"](%1959)
  %1979 : Float(1024:1, 1024:1024) = aten::t(%1978), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.109 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1979), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.119 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.109, %1977, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.116 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.115, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1983 : Long() = aten::mul(%bsz.20, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1984 : int = aten::Int(%1983), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1985 : int[] = prim::ListConstruct(%1962, %1984, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1986 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.116, %1985), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %q.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1986, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.118 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.117, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1989 : Long() = aten::mul(%bsz.20, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1990 : int = aten::Int(%1989), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1991 : int[] = prim::ListConstruct(%1321, %1990, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1992 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.118, %1991), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %k.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1992, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.120 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.119, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1995 : Long() = aten::mul(%bsz.20, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1996 : int = aten::Int(%1995), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1997 : int[] = prim::ListConstruct(%1321, %1996, %1322), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %1998 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.120, %1997), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %v.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1998, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %2000 : int = aten::size(%k.20, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:701:0
  %2001 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.20, %1332, %1325), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.73 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.20, %2001), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %2003 : int[] = prim::ListConstruct(%1963, %1320, %1962, %2000), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %attn_weights.74 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.73, %2003), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:718:0
  %2005 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.16 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2005, %1325), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.74, %reshaped.16, %1326), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:720:0
  %2008 : Long() = aten::mul(%bsz.20, %1323), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %2009 : int = aten::Int(%2008), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %2010 : int[] = prim::ListConstruct(%2009, %1962, %2000), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %input.193 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.75, %2010), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %input.194 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.193, %1321, %1319), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.76 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.194, %1318, %1335), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.20 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.76, %v.20), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:730:0
  %2015 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.20, %1337, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %2016 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2015, %1337), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %2017 : int[] = prim::ListConstruct(%1962, %1963, %1965), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2016, %2017), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %2019 : Tensor = prim::GetAttr[name="bias"](%1958)
  %2020 : Tensor = prim::GetAttr[name="weight"](%1958)
  %2021 : Float(1024:1, 1024:1024) = aten::t(%2020), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.110 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.195, %2021), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.196 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.110, %2019, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn/__module.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.196, %1327, %1335), scope: __module.decoder/__module.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.197 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.20, %x.39, %1332), scope: __module.decoder/__module.decoder.layers.3 # transformers/modeling_bart.py:443:0
  %2026 : Tensor = prim::GetAttr[name="bias"](%1884)
  %2027 : Tensor = prim::GetAttr[name="weight"](%1884)
  %2028 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn_layer_norm
  %input.198 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.197, %2028, %2027, %2026, %1329, %1330), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2030 : Tensor = prim::GetAttr[name="bias"](%1883)
  %2031 : Tensor = prim::GetAttr[name="weight"](%1883)
  %2032 : Float(1024:1, 4096:1024) = aten::t(%2031), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.111 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.198, %2032), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.111, %2030, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.200 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.199), scope: __module.decoder/__module.decoder.layers.3 # torch/nn/functional.py:1369:0
  %input.201 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.200, %1318, %1335), scope: __module.decoder/__module.decoder.layers.3 # torch/nn/functional.py:973:0
  %2037 : Tensor = prim::GetAttr[name="bias"](%1882)
  %2038 : Tensor = prim::GetAttr[name="weight"](%1882)
  %2039 : Float(4096:1, 1024:4096) = aten::t(%2038), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.112 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.201, %2039), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.202 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.112, %2037, %1332), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.40 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.202, %1327, %1335), scope: __module.decoder/__module.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.198, %x.40, %1332), scope: __module.decoder/__module.decoder.layers.3 # transformers/modeling_bart.py:455:0
  %2044 : Tensor = prim::GetAttr[name="bias"](%1881)
  %2045 : Tensor = prim::GetAttr[name="weight"](%1881)
  %2046 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.final_layer_norm
  %query.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.203, %2046, %2045, %2044, %1329, %1330), scope: __module.decoder/__module.decoder.layers.3/__module.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %2048 : __torch__.torch.nn.modules.normalization.___torch_mangle_2220.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1353)
  %2049 : __torch__.torch.nn.modules.linear.___torch_mangle_2219.Linear = prim::GetAttr[name="fc2"](%1353)
  %2050 : __torch__.torch.nn.modules.linear.___torch_mangle_2218.Linear = prim::GetAttr[name="fc1"](%1353)
  %2051 : __torch__.torch.nn.modules.normalization.___torch_mangle_2217.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1353)
  %2052 : __torch__.transformers.modeling_bart.___torch_mangle_2216.Attention = prim::GetAttr[name="encoder_attn"](%1353)
  %2053 : __torch__.torch.nn.modules.normalization.___torch_mangle_2211.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1353)
  %2054 : __torch__.transformers.modeling_bart.___torch_mangle_2210.Attention = prim::GetAttr[name="self_attn"](%1353)
  %2055 : __torch__.torch.nn.modules.linear.___torch_mangle_2209.Linear = prim::GetAttr[name="out_proj"](%2054)
  %2056 : __torch__.torch.nn.modules.linear.___torch_mangle_2207.Linear = prim::GetAttr[name="v_proj"](%2054)
  %2057 : __torch__.torch.nn.modules.linear.___torch_mangle_2206.Linear = prim::GetAttr[name="k_proj"](%2054)
  %2058 : __torch__.torch.nn.modules.linear.___torch_mangle_2208.Linear = prim::GetAttr[name="q_proj"](%2054)
  %2059 : int = aten::size(%query.21, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %2060 : int = aten::size(%query.21, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.21 : Long() = prim::NumToTensor(%2060), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2062 : int = aten::size(%query.21, %1325), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %2063 : Tensor = prim::GetAttr[name="bias"](%2058)
  %2064 : Tensor = prim::GetAttr[name="weight"](%2058)
  %2065 : Float(1024:1, 1024:1024) = aten::t(%2064), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.113 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %2065), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2067 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.113, %2063, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.121 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2067, %1324), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %2069 : Tensor = prim::GetAttr[name="bias"](%2057)
  %2070 : Tensor = prim::GetAttr[name="weight"](%2057)
  %2071 : Float(1024:1, 1024:1024) = aten::t(%2070), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.114 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %2071), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.114, %2069, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2074 : Tensor = prim::GetAttr[name="bias"](%2056)
  %2075 : Tensor = prim::GetAttr[name="weight"](%2056)
  %2076 : Float(1024:1, 1024:1024) = aten::t(%2075), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.115 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %2076), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.125 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.115, %2074, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.122 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.121, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2080 : Long() = aten::mul(%bsz.21, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2081 : int = aten::Int(%2080), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2082 : int[] = prim::ListConstruct(%2059, %2081, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2083 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.122, %2082), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2083, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.124 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.123, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2086 : Long() = aten::mul(%bsz.21, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2087 : int = aten::Int(%2086), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2088 : int[] = prim::ListConstruct(%1321, %2087, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2089 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.124, %2088), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2089, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.126 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.125, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2092 : Long() = aten::mul(%bsz.21, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2093 : int = aten::Int(%2092), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2094 : int[] = prim::ListConstruct(%1321, %2093, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2095 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.126, %2094), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2095, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2097 : int = aten::size(%k.21, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %2098 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.21, %1332, %1325), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.77 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.21, %2098), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %2100 : int[] = prim::ListConstruct(%2060, %1320, %2059, %2097), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2101 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.77, %2100), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.78 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2101, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %2103 : Long() = aten::mul(%bsz.21, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %2104 : int = aten::Int(%2103), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %2105 : int[] = prim::ListConstruct(%2104, %2059, %2097), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %input.204 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.78, %2105), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %input.205 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.204, %1321, %1319), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.79 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.205, %1318, %1335), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.21 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.79, %v.21), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %2110 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.21, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2111 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2110, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2112 : int[] = prim::ListConstruct(%2059, %2060, %2062), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn
  %input.206 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2111, %2112), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2114 : Tensor = prim::GetAttr[name="bias"](%2055)
  %2115 : Tensor = prim::GetAttr[name="weight"](%2055)
  %2116 : Float(1024:1, 1024:1024) = aten::t(%2115), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.116 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.206, %2116), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.116, %2114, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn/__module.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.41 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.207, %1327, %1335), scope: __module.decoder/__module.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.21, %x.41, %1332), scope: __module.decoder/__module.decoder.layers.4 # transformers/modeling_bart.py:427:0
  %2121 : Tensor = prim::GetAttr[name="bias"](%2053)
  %2122 : Tensor = prim::GetAttr[name="weight"](%2053)
  %2123 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn_layer_norm
  %query.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.208, %2123, %2122, %2121, %1329, %1330), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2125 : __torch__.torch.nn.modules.linear.___torch_mangle_2215.Linear = prim::GetAttr[name="out_proj"](%2052)
  %2126 : __torch__.torch.nn.modules.linear.___torch_mangle_2213.Linear = prim::GetAttr[name="v_proj"](%2052)
  %2127 : __torch__.torch.nn.modules.linear.___torch_mangle_2212.Linear = prim::GetAttr[name="k_proj"](%2052)
  %2128 : __torch__.torch.nn.modules.linear.___torch_mangle_2214.Linear = prim::GetAttr[name="q_proj"](%2052)
  %2129 : int = aten::size(%query.22, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2130 : int = aten::size(%query.22, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.22 : Long() = prim::NumToTensor(%2130), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2132 : int = aten::size(%query.22, %1325), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2133 : Tensor = prim::GetAttr[name="bias"](%2128)
  %2134 : Tensor = prim::GetAttr[name="weight"](%2128)
  %2135 : Float(1024:1, 1024:1024) = aten::t(%2134), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.117 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.22, %2135), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.117, %2133, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.127 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2137, %1324), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:673:0
  %2139 : Tensor = prim::GetAttr[name="bias"](%2127)
  %2140 : Tensor = prim::GetAttr[name="weight"](%2127)
  %2141 : Float(1024:1, 1024:1024) = aten::t(%2140), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.118 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2141), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.129 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.118, %2139, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2144 : Tensor = prim::GetAttr[name="bias"](%2126)
  %2145 : Tensor = prim::GetAttr[name="weight"](%2126)
  %2146 : Float(1024:1, 1024:1024) = aten::t(%2145), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.119 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2146), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.131 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.119, %2144, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.128 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.127, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2150 : Long() = aten::mul(%bsz.22, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2151 : int = aten::Int(%2150), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2152 : int[] = prim::ListConstruct(%2129, %2151, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2153 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.128, %2152), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %q.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2153, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.130 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.129, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2156 : Long() = aten::mul(%bsz.22, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2157 : int = aten::Int(%2156), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2158 : int[] = prim::ListConstruct(%1321, %2157, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2159 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.130, %2158), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %k.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2159, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.132 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.131, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2162 : Long() = aten::mul(%bsz.22, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2163 : int = aten::Int(%2162), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2164 : int[] = prim::ListConstruct(%1321, %2163, %1322), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2165 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.132, %2164), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %v.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2165, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2167 : int = aten::size(%k.22, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:701:0
  %2168 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.22, %1332, %1325), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.80 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.22, %2168), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %2170 : int[] = prim::ListConstruct(%2130, %1320, %2129, %2167), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %attn_weights.81 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.80, %2170), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:718:0
  %2172 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.17 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2172, %1325), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.82 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.81, %reshaped.17, %1326), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:720:0
  %2175 : Long() = aten::mul(%bsz.22, %1323), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %2176 : int = aten::Int(%2175), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %2177 : int[] = prim::ListConstruct(%2176, %2129, %2167), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %input.209 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.82, %2177), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %input.210 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.209, %1321, %1319), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.83 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.210, %1318, %1335), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.22 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.83, %v.22), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:730:0
  %2182 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.22, %1337, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2183 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2182, %1337), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2184 : int[] = prim::ListConstruct(%2129, %2130, %2132), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn
  %input.211 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2183, %2184), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2186 : Tensor = prim::GetAttr[name="bias"](%2125)
  %2187 : Tensor = prim::GetAttr[name="weight"](%2125)
  %2188 : Float(1024:1, 1024:1024) = aten::t(%2187), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.120 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.211, %2188), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.120, %2186, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn/__module.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %1327, %1335), scope: __module.decoder/__module.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.22, %x.42, %1332), scope: __module.decoder/__module.decoder.layers.4 # transformers/modeling_bart.py:443:0
  %2193 : Tensor = prim::GetAttr[name="bias"](%2051)
  %2194 : Tensor = prim::GetAttr[name="weight"](%2051)
  %2195 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn_layer_norm
  %input.214 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2195, %2194, %2193, %1329, %1330), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2197 : Tensor = prim::GetAttr[name="bias"](%2050)
  %2198 : Tensor = prim::GetAttr[name="weight"](%2050)
  %2199 : Float(1024:1, 4096:1024) = aten::t(%2198), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.121 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.214, %2199), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.121, %2197, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.215), scope: __module.decoder/__module.decoder.layers.4 # torch/nn/functional.py:1369:0
  %input.217 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.216, %1318, %1335), scope: __module.decoder/__module.decoder.layers.4 # torch/nn/functional.py:973:0
  %2204 : Tensor = prim::GetAttr[name="bias"](%2049)
  %2205 : Tensor = prim::GetAttr[name="weight"](%2049)
  %2206 : Float(4096:1, 1024:4096) = aten::t(%2205), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.122 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.217, %2206), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.122, %2204, %1332), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.43 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.218, %1327, %1335), scope: __module.decoder/__module.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.214, %x.43, %1332), scope: __module.decoder/__module.decoder.layers.4 # transformers/modeling_bart.py:455:0
  %2211 : Tensor = prim::GetAttr[name="bias"](%2048)
  %2212 : Tensor = prim::GetAttr[name="weight"](%2048)
  %2213 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.final_layer_norm
  %query.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.219, %2213, %2212, %2211, %1329, %1330), scope: __module.decoder/__module.decoder.layers.4/__module.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %2215 : __torch__.torch.nn.modules.normalization.___torch_mangle_2236.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1351)
  %2216 : __torch__.torch.nn.modules.linear.___torch_mangle_2235.Linear = prim::GetAttr[name="fc2"](%1351)
  %2217 : __torch__.torch.nn.modules.linear.___torch_mangle_2234.Linear = prim::GetAttr[name="fc1"](%1351)
  %2218 : __torch__.torch.nn.modules.normalization.___torch_mangle_2233.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1351)
  %2219 : __torch__.transformers.modeling_bart.___torch_mangle_2232.Attention = prim::GetAttr[name="encoder_attn"](%1351)
  %2220 : __torch__.torch.nn.modules.normalization.___torch_mangle_2227.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1351)
  %2221 : __torch__.transformers.modeling_bart.___torch_mangle_2226.Attention = prim::GetAttr[name="self_attn"](%1351)
  %2222 : __torch__.torch.nn.modules.linear.___torch_mangle_2225.Linear = prim::GetAttr[name="out_proj"](%2221)
  %2223 : __torch__.torch.nn.modules.linear.___torch_mangle_2223.Linear = prim::GetAttr[name="v_proj"](%2221)
  %2224 : __torch__.torch.nn.modules.linear.___torch_mangle_2222.Linear = prim::GetAttr[name="k_proj"](%2221)
  %2225 : __torch__.torch.nn.modules.linear.___torch_mangle_2224.Linear = prim::GetAttr[name="q_proj"](%2221)
  %2226 : int = aten::size(%query.23, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2227 : int = aten::size(%query.23, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.23 : Long() = prim::NumToTensor(%2227), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2229 : int = aten::size(%query.23, %1325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2230 : Tensor = prim::GetAttr[name="bias"](%2225)
  %2231 : Tensor = prim::GetAttr[name="weight"](%2225)
  %2232 : Float(1024:1, 1024:1024) = aten::t(%2231), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.123 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2232), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2234 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.123, %2230, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.133 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2234, %1324), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %2236 : Tensor = prim::GetAttr[name="bias"](%2224)
  %2237 : Tensor = prim::GetAttr[name="weight"](%2224)
  %2238 : Float(1024:1, 1024:1024) = aten::t(%2237), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.124 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2238), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.135 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.124, %2236, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2241 : Tensor = prim::GetAttr[name="bias"](%2223)
  %2242 : Tensor = prim::GetAttr[name="weight"](%2223)
  %2243 : Float(1024:1, 1024:1024) = aten::t(%2242), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.125 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2243), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.125, %2241, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.134 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.133, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2247 : Long() = aten::mul(%bsz.23, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2248 : int = aten::Int(%2247), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2249 : int[] = prim::ListConstruct(%2226, %2248, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2250 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.134, %2249), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2250, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.136 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.135, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2253 : Long() = aten::mul(%bsz.23, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2254 : int = aten::Int(%2253), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2255 : int[] = prim::ListConstruct(%1321, %2254, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2256 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.136, %2255), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2256, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.138 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.137, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2259 : Long() = aten::mul(%bsz.23, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2260 : int = aten::Int(%2259), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2261 : int[] = prim::ListConstruct(%1321, %2260, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2262 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.138, %2261), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2262, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2264 : int = aten::size(%k.23, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %2265 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.23, %1332, %1325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.84 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.23, %2265), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %2267 : int[] = prim::ListConstruct(%2227, %1320, %2226, %2264), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2268 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.84, %2267), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2268, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %2270 : Long() = aten::mul(%bsz.23, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %2271 : int = aten::Int(%2270), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %2272 : int[] = prim::ListConstruct(%2271, %2226, %2264), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %input.220 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.85, %2272), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %input.221 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.220, %1321, %1319), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.86 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.221, %1318, %1335), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.23 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.86, %v.23), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %2277 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.23, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2278 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2277, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2279 : int[] = prim::ListConstruct(%2226, %2227, %2229), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn
  %input.222 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2278, %2279), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2281 : Tensor = prim::GetAttr[name="bias"](%2222)
  %2282 : Tensor = prim::GetAttr[name="weight"](%2222)
  %2283 : Float(1024:1, 1024:1024) = aten::t(%2282), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.126 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.222, %2283), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.223 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.126, %2281, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn/__module.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.44 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.223, %1327, %1335), scope: __module.decoder/__module.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.224 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.23, %x.44, %1332), scope: __module.decoder/__module.decoder.layers.5 # transformers/modeling_bart.py:427:0
  %2288 : Tensor = prim::GetAttr[name="bias"](%2220)
  %2289 : Tensor = prim::GetAttr[name="weight"](%2220)
  %2290 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn_layer_norm
  %query.24 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.224, %2290, %2289, %2288, %1329, %1330), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2292 : __torch__.torch.nn.modules.linear.___torch_mangle_2231.Linear = prim::GetAttr[name="out_proj"](%2219)
  %2293 : __torch__.torch.nn.modules.linear.___torch_mangle_2229.Linear = prim::GetAttr[name="v_proj"](%2219)
  %2294 : __torch__.torch.nn.modules.linear.___torch_mangle_2228.Linear = prim::GetAttr[name="k_proj"](%2219)
  %2295 : __torch__.torch.nn.modules.linear.___torch_mangle_2230.Linear = prim::GetAttr[name="q_proj"](%2219)
  %2296 : int = aten::size(%query.24, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2297 : int = aten::size(%query.24, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.24 : Long() = prim::NumToTensor(%2297), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2299 : int = aten::size(%query.24, %1325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2300 : Tensor = prim::GetAttr[name="bias"](%2295)
  %2301 : Tensor = prim::GetAttr[name="weight"](%2295)
  %2302 : Float(1024:1, 1024:1024) = aten::t(%2301), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.127 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.24, %2302), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2304 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.127, %2300, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.139 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2304, %1324), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:673:0
  %2306 : Tensor = prim::GetAttr[name="bias"](%2294)
  %2307 : Tensor = prim::GetAttr[name="weight"](%2294)
  %2308 : Float(1024:1, 1024:1024) = aten::t(%2307), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.128 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2308), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.141 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.128, %2306, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2311 : Tensor = prim::GetAttr[name="bias"](%2293)
  %2312 : Tensor = prim::GetAttr[name="weight"](%2293)
  %2313 : Float(1024:1, 1024:1024) = aten::t(%2312), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.129 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2313), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.143 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.129, %2311, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.140 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.139, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2317 : Long() = aten::mul(%bsz.24, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2318 : int = aten::Int(%2317), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2319 : int[] = prim::ListConstruct(%2296, %2318, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2320 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.140, %2319), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %q.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2320, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.142 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.141, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2323 : Long() = aten::mul(%bsz.24, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2324 : int = aten::Int(%2323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2325 : int[] = prim::ListConstruct(%1321, %2324, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2326 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.142, %2325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %k.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2326, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.144 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.143, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2329 : Long() = aten::mul(%bsz.24, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2330 : int = aten::Int(%2329), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2331 : int[] = prim::ListConstruct(%1321, %2330, %1322), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2332 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.144, %2331), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %v.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2332, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2334 : int = aten::size(%k.24, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:701:0
  %2335 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.24, %1332, %1325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.87 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.24, %2335), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %2337 : int[] = prim::ListConstruct(%2297, %1320, %2296, %2334), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %attn_weights.88 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.87, %2337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:718:0
  %2339 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.18 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2339, %1325), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.89 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.88, %reshaped.18, %1326), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:720:0
  %2342 : Long() = aten::mul(%bsz.24, %1323), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %2343 : int = aten::Int(%2342), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %2344 : int[] = prim::ListConstruct(%2343, %2296, %2334), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %input.225 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.89, %2344), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %input.226 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.225, %1321, %1319), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.90 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.226, %1318, %1335), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.24 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.90, %v.24), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:730:0
  %2349 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.24, %1337, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2350 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2349, %1337), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2351 : int[] = prim::ListConstruct(%2296, %2297, %2299), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn
  %input.227 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2350, %2351), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2353 : Tensor = prim::GetAttr[name="bias"](%2292)
  %2354 : Tensor = prim::GetAttr[name="weight"](%2292)
  %2355 : Float(1024:1, 1024:1024) = aten::t(%2354), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.130 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.227, %2355), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.228 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.130, %2353, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn/__module.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.228, %1327, %1335), scope: __module.decoder/__module.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.229 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.24, %x.45, %1332), scope: __module.decoder/__module.decoder.layers.5 # transformers/modeling_bart.py:443:0
  %2360 : Tensor = prim::GetAttr[name="bias"](%2218)
  %2361 : Tensor = prim::GetAttr[name="weight"](%2218)
  %2362 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn_layer_norm
  %input.230 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.229, %2362, %2361, %2360, %1329, %1330), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2364 : Tensor = prim::GetAttr[name="bias"](%2217)
  %2365 : Tensor = prim::GetAttr[name="weight"](%2217)
  %2366 : Float(1024:1, 4096:1024) = aten::t(%2365), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.131 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.230, %2366), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.231 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.131, %2364, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.232 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.231), scope: __module.decoder/__module.decoder.layers.5 # torch/nn/functional.py:1369:0
  %input.233 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.232, %1318, %1335), scope: __module.decoder/__module.decoder.layers.5 # torch/nn/functional.py:973:0
  %2371 : Tensor = prim::GetAttr[name="bias"](%2216)
  %2372 : Tensor = prim::GetAttr[name="weight"](%2216)
  %2373 : Float(4096:1, 1024:4096) = aten::t(%2372), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.132 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.233, %2373), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.234 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.132, %2371, %1332), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.46 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.234, %1327, %1335), scope: __module.decoder/__module.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.235 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.230, %x.46, %1332), scope: __module.decoder/__module.decoder.layers.5 # transformers/modeling_bart.py:455:0
  %2378 : Tensor = prim::GetAttr[name="bias"](%2215)
  %2379 : Tensor = prim::GetAttr[name="weight"](%2215)
  %2380 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.final_layer_norm
  %query.25 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.235, %2380, %2379, %2378, %1329, %1330), scope: __module.decoder/__module.decoder.layers.5/__module.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %2382 : __torch__.torch.nn.modules.normalization.___torch_mangle_2252.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1349)
  %2383 : __torch__.torch.nn.modules.linear.___torch_mangle_2251.Linear = prim::GetAttr[name="fc2"](%1349)
  %2384 : __torch__.torch.nn.modules.linear.___torch_mangle_2250.Linear = prim::GetAttr[name="fc1"](%1349)
  %2385 : __torch__.torch.nn.modules.normalization.___torch_mangle_2249.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1349)
  %2386 : __torch__.transformers.modeling_bart.___torch_mangle_2248.Attention = prim::GetAttr[name="encoder_attn"](%1349)
  %2387 : __torch__.torch.nn.modules.normalization.___torch_mangle_2243.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1349)
  %2388 : __torch__.transformers.modeling_bart.___torch_mangle_2242.Attention = prim::GetAttr[name="self_attn"](%1349)
  %2389 : __torch__.torch.nn.modules.linear.___torch_mangle_2241.Linear = prim::GetAttr[name="out_proj"](%2388)
  %2390 : __torch__.torch.nn.modules.linear.___torch_mangle_2239.Linear = prim::GetAttr[name="v_proj"](%2388)
  %2391 : __torch__.torch.nn.modules.linear.___torch_mangle_2238.Linear = prim::GetAttr[name="k_proj"](%2388)
  %2392 : __torch__.torch.nn.modules.linear.___torch_mangle_2240.Linear = prim::GetAttr[name="q_proj"](%2388)
  %2393 : int = aten::size(%query.25, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2394 : int = aten::size(%query.25, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.25 : Long() = prim::NumToTensor(%2394), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2396 : int = aten::size(%query.25, %1325), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2397 : Tensor = prim::GetAttr[name="bias"](%2392)
  %2398 : Tensor = prim::GetAttr[name="weight"](%2392)
  %2399 : Float(1024:1, 1024:1024) = aten::t(%2398), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.133 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2399), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2401 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.133, %2397, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.145 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2401, %1324), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %2403 : Tensor = prim::GetAttr[name="bias"](%2391)
  %2404 : Tensor = prim::GetAttr[name="weight"](%2391)
  %2405 : Float(1024:1, 1024:1024) = aten::t(%2404), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.134 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2405), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.147 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.134, %2403, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2408 : Tensor = prim::GetAttr[name="bias"](%2390)
  %2409 : Tensor = prim::GetAttr[name="weight"](%2390)
  %2410 : Float(1024:1, 1024:1024) = aten::t(%2409), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.135 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2410), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.149 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.135, %2408, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.146 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.145, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2414 : Long() = aten::mul(%bsz.25, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2415 : int = aten::Int(%2414), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2416 : int[] = prim::ListConstruct(%2393, %2415, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2417 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.146, %2416), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2417, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.148 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.147, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2420 : Long() = aten::mul(%bsz.25, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2421 : int = aten::Int(%2420), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2422 : int[] = prim::ListConstruct(%1321, %2421, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2423 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.148, %2422), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2423, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.150 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.149, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2426 : Long() = aten::mul(%bsz.25, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2427 : int = aten::Int(%2426), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2428 : int[] = prim::ListConstruct(%1321, %2427, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2429 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.150, %2428), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2429, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2431 : int = aten::size(%k.25, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %2432 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.25, %1332, %1325), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.91 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.25, %2432), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %2434 : int[] = prim::ListConstruct(%2394, %1320, %2393, %2431), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2435 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.91, %2434), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.92 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2435, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %2437 : Long() = aten::mul(%bsz.25, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %2438 : int = aten::Int(%2437), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %2439 : int[] = prim::ListConstruct(%2438, %2393, %2431), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %input.236 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.92, %2439), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %input.237 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.236, %1321, %1319), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.93 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.237, %1318, %1335), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.25 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.93, %v.25), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %2444 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.25, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2445 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2444, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2446 : int[] = prim::ListConstruct(%2393, %2394, %2396), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn
  %input.238 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2445, %2446), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2448 : Tensor = prim::GetAttr[name="bias"](%2389)
  %2449 : Tensor = prim::GetAttr[name="weight"](%2389)
  %2450 : Float(1024:1, 1024:1024) = aten::t(%2449), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.136 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.238, %2450), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.239 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.136, %2448, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn/__module.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.47 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.239, %1327, %1335), scope: __module.decoder/__module.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.240 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.25, %x.47, %1332), scope: __module.decoder/__module.decoder.layers.6 # transformers/modeling_bart.py:427:0
  %2455 : Tensor = prim::GetAttr[name="bias"](%2387)
  %2456 : Tensor = prim::GetAttr[name="weight"](%2387)
  %2457 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn_layer_norm
  %query.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.240, %2457, %2456, %2455, %1329, %1330), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2459 : __torch__.torch.nn.modules.linear.___torch_mangle_2247.Linear = prim::GetAttr[name="out_proj"](%2386)
  %2460 : __torch__.torch.nn.modules.linear.___torch_mangle_2245.Linear = prim::GetAttr[name="v_proj"](%2386)
  %2461 : __torch__.torch.nn.modules.linear.___torch_mangle_2244.Linear = prim::GetAttr[name="k_proj"](%2386)
  %2462 : __torch__.torch.nn.modules.linear.___torch_mangle_2246.Linear = prim::GetAttr[name="q_proj"](%2386)
  %2463 : int = aten::size(%query.26, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2464 : int = aten::size(%query.26, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.26 : Long() = prim::NumToTensor(%2464), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2466 : int = aten::size(%query.26, %1325), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2467 : Tensor = prim::GetAttr[name="bias"](%2462)
  %2468 : Tensor = prim::GetAttr[name="weight"](%2462)
  %2469 : Float(1024:1, 1024:1024) = aten::t(%2468), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.137 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.26, %2469), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2471 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.137, %2467, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.151 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2471, %1324), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:673:0
  %2473 : Tensor = prim::GetAttr[name="bias"](%2461)
  %2474 : Tensor = prim::GetAttr[name="weight"](%2461)
  %2475 : Float(1024:1, 1024:1024) = aten::t(%2474), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.138 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2475), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.153 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.138, %2473, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2478 : Tensor = prim::GetAttr[name="bias"](%2460)
  %2479 : Tensor = prim::GetAttr[name="weight"](%2460)
  %2480 : Float(1024:1, 1024:1024) = aten::t(%2479), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.139 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2480), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.155 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.139, %2478, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.152 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.151, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2484 : Long() = aten::mul(%bsz.26, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2485 : int = aten::Int(%2484), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2486 : int[] = prim::ListConstruct(%2463, %2485, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2487 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.152, %2486), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %q.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2487, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.154 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.153, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2490 : Long() = aten::mul(%bsz.26, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2491 : int = aten::Int(%2490), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2492 : int[] = prim::ListConstruct(%1321, %2491, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2493 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.154, %2492), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %k.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2493, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.156 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.155, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2496 : Long() = aten::mul(%bsz.26, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2497 : int = aten::Int(%2496), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2498 : int[] = prim::ListConstruct(%1321, %2497, %1322), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2499 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.156, %2498), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %v.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2499, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2501 : int = aten::size(%k.26, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:701:0
  %2502 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.26, %1332, %1325), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.94 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.26, %2502), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %2504 : int[] = prim::ListConstruct(%2464, %1320, %2463, %2501), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %attn_weights.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.94, %2504), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:718:0
  %2506 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.19 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2506, %1325), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.95, %reshaped.19, %1326), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:720:0
  %2509 : Long() = aten::mul(%bsz.26, %1323), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %2510 : int = aten::Int(%2509), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %2511 : int[] = prim::ListConstruct(%2510, %2463, %2501), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %input.241 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.96, %2511), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %input.242 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.241, %1321, %1319), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.97 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.242, %1318, %1335), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.26 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.97, %v.26), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:730:0
  %2516 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.26, %1337, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2517 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2516, %1337), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2518 : int[] = prim::ListConstruct(%2463, %2464, %2466), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn
  %input.243 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2517, %2518), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2520 : Tensor = prim::GetAttr[name="bias"](%2459)
  %2521 : Tensor = prim::GetAttr[name="weight"](%2459)
  %2522 : Float(1024:1, 1024:1024) = aten::t(%2521), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.140 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.243, %2522), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.244 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.140, %2520, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn/__module.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.244, %1327, %1335), scope: __module.decoder/__module.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.245 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.26, %x.48, %1332), scope: __module.decoder/__module.decoder.layers.6 # transformers/modeling_bart.py:443:0
  %2527 : Tensor = prim::GetAttr[name="bias"](%2385)
  %2528 : Tensor = prim::GetAttr[name="weight"](%2385)
  %2529 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn_layer_norm
  %input.246 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.245, %2529, %2528, %2527, %1329, %1330), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2531 : Tensor = prim::GetAttr[name="bias"](%2384)
  %2532 : Tensor = prim::GetAttr[name="weight"](%2384)
  %2533 : Float(1024:1, 4096:1024) = aten::t(%2532), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.141 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.246, %2533), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.247 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.141, %2531, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.248 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.247), scope: __module.decoder/__module.decoder.layers.6 # torch/nn/functional.py:1369:0
  %input.249 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.248, %1318, %1335), scope: __module.decoder/__module.decoder.layers.6 # torch/nn/functional.py:973:0
  %2538 : Tensor = prim::GetAttr[name="bias"](%2383)
  %2539 : Tensor = prim::GetAttr[name="weight"](%2383)
  %2540 : Float(4096:1, 1024:4096) = aten::t(%2539), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.142 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.249, %2540), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.250 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.142, %2538, %1332), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.49 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.250, %1327, %1335), scope: __module.decoder/__module.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.251 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.246, %x.49, %1332), scope: __module.decoder/__module.decoder.layers.6 # transformers/modeling_bart.py:455:0
  %2545 : Tensor = prim::GetAttr[name="bias"](%2382)
  %2546 : Tensor = prim::GetAttr[name="weight"](%2382)
  %2547 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.final_layer_norm
  %query.27 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.251, %2547, %2546, %2545, %1329, %1330), scope: __module.decoder/__module.decoder.layers.6/__module.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %2549 : __torch__.torch.nn.modules.normalization.___torch_mangle_2268.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1347)
  %2550 : __torch__.torch.nn.modules.linear.___torch_mangle_2267.Linear = prim::GetAttr[name="fc2"](%1347)
  %2551 : __torch__.torch.nn.modules.linear.___torch_mangle_2266.Linear = prim::GetAttr[name="fc1"](%1347)
  %2552 : __torch__.torch.nn.modules.normalization.___torch_mangle_2265.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1347)
  %2553 : __torch__.transformers.modeling_bart.___torch_mangle_2264.Attention = prim::GetAttr[name="encoder_attn"](%1347)
  %2554 : __torch__.torch.nn.modules.normalization.___torch_mangle_2259.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1347)
  %2555 : __torch__.transformers.modeling_bart.___torch_mangle_2258.Attention = prim::GetAttr[name="self_attn"](%1347)
  %2556 : __torch__.torch.nn.modules.linear.___torch_mangle_2257.Linear = prim::GetAttr[name="out_proj"](%2555)
  %2557 : __torch__.torch.nn.modules.linear.___torch_mangle_2255.Linear = prim::GetAttr[name="v_proj"](%2555)
  %2558 : __torch__.torch.nn.modules.linear.___torch_mangle_2254.Linear = prim::GetAttr[name="k_proj"](%2555)
  %2559 : __torch__.torch.nn.modules.linear.___torch_mangle_2256.Linear = prim::GetAttr[name="q_proj"](%2555)
  %2560 : int = aten::size(%query.27, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2561 : int = aten::size(%query.27, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.27 : Long() = prim::NumToTensor(%2561), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2563 : int = aten::size(%query.27, %1325), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2564 : Tensor = prim::GetAttr[name="bias"](%2559)
  %2565 : Tensor = prim::GetAttr[name="weight"](%2559)
  %2566 : Float(1024:1, 1024:1024) = aten::t(%2565), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.143 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2566), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2568 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.143, %2564, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.157 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2568, %1324), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %2570 : Tensor = prim::GetAttr[name="bias"](%2558)
  %2571 : Tensor = prim::GetAttr[name="weight"](%2558)
  %2572 : Float(1024:1, 1024:1024) = aten::t(%2571), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.144 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2572), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.144, %2570, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2575 : Tensor = prim::GetAttr[name="bias"](%2557)
  %2576 : Tensor = prim::GetAttr[name="weight"](%2557)
  %2577 : Float(1024:1, 1024:1024) = aten::t(%2576), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.145 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2577), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.161 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.145, %2575, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.158 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.157, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2581 : Long() = aten::mul(%bsz.27, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2582 : int = aten::Int(%2581), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2583 : int[] = prim::ListConstruct(%2560, %2582, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2584 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.158, %2583), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2584, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.160 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.159, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2587 : Long() = aten::mul(%bsz.27, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2588 : int = aten::Int(%2587), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2589 : int[] = prim::ListConstruct(%1321, %2588, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2590 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.160, %2589), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2590, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.162 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.161, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2593 : Long() = aten::mul(%bsz.27, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2594 : int = aten::Int(%2593), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2595 : int[] = prim::ListConstruct(%1321, %2594, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2596 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.162, %2595), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2596, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2598 : int = aten::size(%k.27, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %2599 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.27, %1332, %1325), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.98 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.27, %2599), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %2601 : int[] = prim::ListConstruct(%2561, %1320, %2560, %2598), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2602 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.98, %2601), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.99 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2602, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %2604 : Long() = aten::mul(%bsz.27, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %2605 : int = aten::Int(%2604), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %2606 : int[] = prim::ListConstruct(%2605, %2560, %2598), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %input.252 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.99, %2606), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %input.253 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.252, %1321, %1319), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.100 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.253, %1318, %1335), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.27 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.100, %v.27), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %2611 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.27, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2612 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2611, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2613 : int[] = prim::ListConstruct(%2560, %2561, %2563), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn
  %input.254 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2612, %2613), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2615 : Tensor = prim::GetAttr[name="bias"](%2556)
  %2616 : Tensor = prim::GetAttr[name="weight"](%2556)
  %2617 : Float(1024:1, 1024:1024) = aten::t(%2616), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.146 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.254, %2617), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.255 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.146, %2615, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn/__module.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.50 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.255, %1327, %1335), scope: __module.decoder/__module.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.256 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.27, %x.50, %1332), scope: __module.decoder/__module.decoder.layers.7 # transformers/modeling_bart.py:427:0
  %2622 : Tensor = prim::GetAttr[name="bias"](%2554)
  %2623 : Tensor = prim::GetAttr[name="weight"](%2554)
  %2624 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn_layer_norm
  %query.28 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.256, %2624, %2623, %2622, %1329, %1330), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2626 : __torch__.torch.nn.modules.linear.___torch_mangle_2263.Linear = prim::GetAttr[name="out_proj"](%2553)
  %2627 : __torch__.torch.nn.modules.linear.___torch_mangle_2261.Linear = prim::GetAttr[name="v_proj"](%2553)
  %2628 : __torch__.torch.nn.modules.linear.___torch_mangle_2260.Linear = prim::GetAttr[name="k_proj"](%2553)
  %2629 : __torch__.torch.nn.modules.linear.___torch_mangle_2262.Linear = prim::GetAttr[name="q_proj"](%2553)
  %2630 : int = aten::size(%query.28, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2631 : int = aten::size(%query.28, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.28 : Long() = prim::NumToTensor(%2631), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2633 : int = aten::size(%query.28, %1325), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2634 : Tensor = prim::GetAttr[name="bias"](%2629)
  %2635 : Tensor = prim::GetAttr[name="weight"](%2629)
  %2636 : Float(1024:1, 1024:1024) = aten::t(%2635), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.147 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.28, %2636), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2638 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.147, %2634, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.163 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2638, %1324), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:673:0
  %2640 : Tensor = prim::GetAttr[name="bias"](%2628)
  %2641 : Tensor = prim::GetAttr[name="weight"](%2628)
  %2642 : Float(1024:1, 1024:1024) = aten::t(%2641), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.148 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2642), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.165 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.148, %2640, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2645 : Tensor = prim::GetAttr[name="bias"](%2627)
  %2646 : Tensor = prim::GetAttr[name="weight"](%2627)
  %2647 : Float(1024:1, 1024:1024) = aten::t(%2646), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.149 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2647), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.167 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.149, %2645, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.164 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.163, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2651 : Long() = aten::mul(%bsz.28, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2652 : int = aten::Int(%2651), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2653 : int[] = prim::ListConstruct(%2630, %2652, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2654 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.164, %2653), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %q.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2654, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.166 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.165, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2657 : Long() = aten::mul(%bsz.28, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2658 : int = aten::Int(%2657), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2659 : int[] = prim::ListConstruct(%1321, %2658, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2660 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.166, %2659), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %k.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2660, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.168 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.167, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2663 : Long() = aten::mul(%bsz.28, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2664 : int = aten::Int(%2663), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2665 : int[] = prim::ListConstruct(%1321, %2664, %1322), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2666 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.168, %2665), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %v.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2666, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2668 : int = aten::size(%k.28, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:701:0
  %2669 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.28, %1332, %1325), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.101 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.28, %2669), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %2671 : int[] = prim::ListConstruct(%2631, %1320, %2630, %2668), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %attn_weights.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.101, %2671), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:718:0
  %2673 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.20 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2673, %1325), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.102, %reshaped.20, %1326), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:720:0
  %2676 : Long() = aten::mul(%bsz.28, %1323), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %2677 : int = aten::Int(%2676), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %2678 : int[] = prim::ListConstruct(%2677, %2630, %2668), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %input.257 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.103, %2678), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %input.258 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.257, %1321, %1319), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.104 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.258, %1318, %1335), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.28 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.104, %v.28), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:730:0
  %2683 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.28, %1337, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2684 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2683, %1337), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2685 : int[] = prim::ListConstruct(%2630, %2631, %2633), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn
  %input.259 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2684, %2685), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2687 : Tensor = prim::GetAttr[name="bias"](%2626)
  %2688 : Tensor = prim::GetAttr[name="weight"](%2626)
  %2689 : Float(1024:1, 1024:1024) = aten::t(%2688), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.150 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.259, %2689), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.260 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.150, %2687, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn/__module.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.260, %1327, %1335), scope: __module.decoder/__module.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.261 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.28, %x.51, %1332), scope: __module.decoder/__module.decoder.layers.7 # transformers/modeling_bart.py:443:0
  %2694 : Tensor = prim::GetAttr[name="bias"](%2552)
  %2695 : Tensor = prim::GetAttr[name="weight"](%2552)
  %2696 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn_layer_norm
  %input.262 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.261, %2696, %2695, %2694, %1329, %1330), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2698 : Tensor = prim::GetAttr[name="bias"](%2551)
  %2699 : Tensor = prim::GetAttr[name="weight"](%2551)
  %2700 : Float(1024:1, 4096:1024) = aten::t(%2699), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.151 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.262, %2700), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.263 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.151, %2698, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.264 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.263), scope: __module.decoder/__module.decoder.layers.7 # torch/nn/functional.py:1369:0
  %input.265 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.264, %1318, %1335), scope: __module.decoder/__module.decoder.layers.7 # torch/nn/functional.py:973:0
  %2705 : Tensor = prim::GetAttr[name="bias"](%2550)
  %2706 : Tensor = prim::GetAttr[name="weight"](%2550)
  %2707 : Float(4096:1, 1024:4096) = aten::t(%2706), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.152 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.265, %2707), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.266 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.152, %2705, %1332), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.52 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.266, %1327, %1335), scope: __module.decoder/__module.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.267 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.262, %x.52, %1332), scope: __module.decoder/__module.decoder.layers.7 # transformers/modeling_bart.py:455:0
  %2712 : Tensor = prim::GetAttr[name="bias"](%2549)
  %2713 : Tensor = prim::GetAttr[name="weight"](%2549)
  %2714 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.final_layer_norm
  %query.29 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.267, %2714, %2713, %2712, %1329, %1330), scope: __module.decoder/__module.decoder.layers.7/__module.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %2716 : __torch__.torch.nn.modules.normalization.___torch_mangle_2284.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1345)
  %2717 : __torch__.torch.nn.modules.linear.___torch_mangle_2283.Linear = prim::GetAttr[name="fc2"](%1345)
  %2718 : __torch__.torch.nn.modules.linear.___torch_mangle_2282.Linear = prim::GetAttr[name="fc1"](%1345)
  %2719 : __torch__.torch.nn.modules.normalization.___torch_mangle_2281.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1345)
  %2720 : __torch__.transformers.modeling_bart.___torch_mangle_2280.Attention = prim::GetAttr[name="encoder_attn"](%1345)
  %2721 : __torch__.torch.nn.modules.normalization.___torch_mangle_2275.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1345)
  %2722 : __torch__.transformers.modeling_bart.___torch_mangle_2274.Attention = prim::GetAttr[name="self_attn"](%1345)
  %2723 : __torch__.torch.nn.modules.linear.___torch_mangle_2273.Linear = prim::GetAttr[name="out_proj"](%2722)
  %2724 : __torch__.torch.nn.modules.linear.___torch_mangle_2271.Linear = prim::GetAttr[name="v_proj"](%2722)
  %2725 : __torch__.torch.nn.modules.linear.___torch_mangle_2270.Linear = prim::GetAttr[name="k_proj"](%2722)
  %2726 : __torch__.torch.nn.modules.linear.___torch_mangle_2272.Linear = prim::GetAttr[name="q_proj"](%2722)
  %2727 : int = aten::size(%query.29, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2728 : int = aten::size(%query.29, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.29 : Long() = prim::NumToTensor(%2728), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2730 : int = aten::size(%query.29, %1325), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2731 : Tensor = prim::GetAttr[name="bias"](%2726)
  %2732 : Tensor = prim::GetAttr[name="weight"](%2726)
  %2733 : Float(1024:1, 1024:1024) = aten::t(%2732), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.153 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2733), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2735 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.153, %2731, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.169 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2735, %1324), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %2737 : Tensor = prim::GetAttr[name="bias"](%2725)
  %2738 : Tensor = prim::GetAttr[name="weight"](%2725)
  %2739 : Float(1024:1, 1024:1024) = aten::t(%2738), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.154 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2739), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.171 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.154, %2737, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2742 : Tensor = prim::GetAttr[name="bias"](%2724)
  %2743 : Tensor = prim::GetAttr[name="weight"](%2724)
  %2744 : Float(1024:1, 1024:1024) = aten::t(%2743), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.155 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2744), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.173 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.155, %2742, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.170 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.169, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2748 : Long() = aten::mul(%bsz.29, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2749 : int = aten::Int(%2748), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2750 : int[] = prim::ListConstruct(%2727, %2749, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2751 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.170, %2750), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2751, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.172 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.171, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2754 : Long() = aten::mul(%bsz.29, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2755 : int = aten::Int(%2754), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2756 : int[] = prim::ListConstruct(%1321, %2755, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2757 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.172, %2756), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2757, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.174 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.173, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2760 : Long() = aten::mul(%bsz.29, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2761 : int = aten::Int(%2760), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2762 : int[] = prim::ListConstruct(%1321, %2761, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2763 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.174, %2762), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2763, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2765 : int = aten::size(%k.29, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %2766 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.29, %1332, %1325), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.105 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.29, %2766), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %2768 : int[] = prim::ListConstruct(%2728, %1320, %2727, %2765), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2769 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.105, %2768), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2769, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %2771 : Long() = aten::mul(%bsz.29, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %2772 : int = aten::Int(%2771), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %2773 : int[] = prim::ListConstruct(%2772, %2727, %2765), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %input.268 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.106, %2773), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %input.269 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.268, %1321, %1319), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.107 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.269, %1318, %1335), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.29 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.107, %v.29), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %2778 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.29, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2779 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2778, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2780 : int[] = prim::ListConstruct(%2727, %2728, %2730), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn
  %input.270 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2779, %2780), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2782 : Tensor = prim::GetAttr[name="bias"](%2723)
  %2783 : Tensor = prim::GetAttr[name="weight"](%2723)
  %2784 : Float(1024:1, 1024:1024) = aten::t(%2783), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.156 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.270, %2784), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.271 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.156, %2782, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn/__module.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.53 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.271, %1327, %1335), scope: __module.decoder/__module.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.272 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.29, %x.53, %1332), scope: __module.decoder/__module.decoder.layers.8 # transformers/modeling_bart.py:427:0
  %2789 : Tensor = prim::GetAttr[name="bias"](%2721)
  %2790 : Tensor = prim::GetAttr[name="weight"](%2721)
  %2791 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn_layer_norm
  %query.30 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.272, %2791, %2790, %2789, %1329, %1330), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2793 : __torch__.torch.nn.modules.linear.___torch_mangle_2279.Linear = prim::GetAttr[name="out_proj"](%2720)
  %2794 : __torch__.torch.nn.modules.linear.___torch_mangle_2277.Linear = prim::GetAttr[name="v_proj"](%2720)
  %2795 : __torch__.torch.nn.modules.linear.___torch_mangle_2276.Linear = prim::GetAttr[name="k_proj"](%2720)
  %2796 : __torch__.torch.nn.modules.linear.___torch_mangle_2278.Linear = prim::GetAttr[name="q_proj"](%2720)
  %2797 : int = aten::size(%query.30, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2798 : int = aten::size(%query.30, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.30 : Long() = prim::NumToTensor(%2798), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2800 : int = aten::size(%query.30, %1325), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2801 : Tensor = prim::GetAttr[name="bias"](%2796)
  %2802 : Tensor = prim::GetAttr[name="weight"](%2796)
  %2803 : Float(1024:1, 1024:1024) = aten::t(%2802), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.157 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.30, %2803), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2805 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.157, %2801, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.175 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2805, %1324), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:673:0
  %2807 : Tensor = prim::GetAttr[name="bias"](%2795)
  %2808 : Tensor = prim::GetAttr[name="weight"](%2795)
  %2809 : Float(1024:1, 1024:1024) = aten::t(%2808), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.158 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2809), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.177 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.158, %2807, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2812 : Tensor = prim::GetAttr[name="bias"](%2794)
  %2813 : Tensor = prim::GetAttr[name="weight"](%2794)
  %2814 : Float(1024:1, 1024:1024) = aten::t(%2813), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.159 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2814), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.179 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.159, %2812, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.176 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.175, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2818 : Long() = aten::mul(%bsz.30, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2819 : int = aten::Int(%2818), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2820 : int[] = prim::ListConstruct(%2797, %2819, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2821 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.176, %2820), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %q.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2821, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.178 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.177, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2824 : Long() = aten::mul(%bsz.30, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2825 : int = aten::Int(%2824), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2826 : int[] = prim::ListConstruct(%1321, %2825, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2827 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.178, %2826), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %k.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2827, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.180 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.179, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2830 : Long() = aten::mul(%bsz.30, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2831 : int = aten::Int(%2830), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2832 : int[] = prim::ListConstruct(%1321, %2831, %1322), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2833 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.180, %2832), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %v.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2833, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2835 : int = aten::size(%k.30, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:701:0
  %2836 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.30, %1332, %1325), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.108 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.30, %2836), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %2838 : int[] = prim::ListConstruct(%2798, %1320, %2797, %2835), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %attn_weights.109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.108, %2838), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:718:0
  %2840 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.21 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2840, %1325), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.110 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.109, %reshaped.21, %1326), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:720:0
  %2843 : Long() = aten::mul(%bsz.30, %1323), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %2844 : int = aten::Int(%2843), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %2845 : int[] = prim::ListConstruct(%2844, %2797, %2835), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %input.273 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.110, %2845), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %input.274 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.273, %1321, %1319), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.111 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.274, %1318, %1335), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.30 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.111, %v.30), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:730:0
  %2850 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.30, %1337, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2851 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2850, %1337), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2852 : int[] = prim::ListConstruct(%2797, %2798, %2800), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn
  %input.275 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2851, %2852), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2854 : Tensor = prim::GetAttr[name="bias"](%2793)
  %2855 : Tensor = prim::GetAttr[name="weight"](%2793)
  %2856 : Float(1024:1, 1024:1024) = aten::t(%2855), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.160 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.275, %2856), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.276 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.160, %2854, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn/__module.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.276, %1327, %1335), scope: __module.decoder/__module.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.277 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.30, %x.54, %1332), scope: __module.decoder/__module.decoder.layers.8 # transformers/modeling_bart.py:443:0
  %2861 : Tensor = prim::GetAttr[name="bias"](%2719)
  %2862 : Tensor = prim::GetAttr[name="weight"](%2719)
  %2863 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn_layer_norm
  %input.278 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.277, %2863, %2862, %2861, %1329, %1330), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2865 : Tensor = prim::GetAttr[name="bias"](%2718)
  %2866 : Tensor = prim::GetAttr[name="weight"](%2718)
  %2867 : Float(1024:1, 4096:1024) = aten::t(%2866), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.161 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.278, %2867), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.279 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.161, %2865, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.280 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.279), scope: __module.decoder/__module.decoder.layers.8 # torch/nn/functional.py:1369:0
  %input.281 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.280, %1318, %1335), scope: __module.decoder/__module.decoder.layers.8 # torch/nn/functional.py:973:0
  %2872 : Tensor = prim::GetAttr[name="bias"](%2717)
  %2873 : Tensor = prim::GetAttr[name="weight"](%2717)
  %2874 : Float(4096:1, 1024:4096) = aten::t(%2873), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.162 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.281, %2874), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.282 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.162, %2872, %1332), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.55 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.282, %1327, %1335), scope: __module.decoder/__module.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.283 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.278, %x.55, %1332), scope: __module.decoder/__module.decoder.layers.8 # transformers/modeling_bart.py:455:0
  %2879 : Tensor = prim::GetAttr[name="bias"](%2716)
  %2880 : Tensor = prim::GetAttr[name="weight"](%2716)
  %2881 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.final_layer_norm
  %query.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.283, %2881, %2880, %2879, %1329, %1330), scope: __module.decoder/__module.decoder.layers.8/__module.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %2883 : __torch__.torch.nn.modules.normalization.___torch_mangle_2300.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1343)
  %2884 : __torch__.torch.nn.modules.linear.___torch_mangle_2299.Linear = prim::GetAttr[name="fc2"](%1343)
  %2885 : __torch__.torch.nn.modules.linear.___torch_mangle_2298.Linear = prim::GetAttr[name="fc1"](%1343)
  %2886 : __torch__.torch.nn.modules.normalization.___torch_mangle_2297.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1343)
  %2887 : __torch__.transformers.modeling_bart.___torch_mangle_2296.Attention = prim::GetAttr[name="encoder_attn"](%1343)
  %2888 : __torch__.torch.nn.modules.normalization.___torch_mangle_2291.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1343)
  %2889 : __torch__.transformers.modeling_bart.___torch_mangle_2290.Attention = prim::GetAttr[name="self_attn"](%1343)
  %2890 : __torch__.torch.nn.modules.linear.___torch_mangle_2289.Linear = prim::GetAttr[name="out_proj"](%2889)
  %2891 : __torch__.torch.nn.modules.linear.___torch_mangle_2287.Linear = prim::GetAttr[name="v_proj"](%2889)
  %2892 : __torch__.torch.nn.modules.linear.___torch_mangle_2286.Linear = prim::GetAttr[name="k_proj"](%2889)
  %2893 : __torch__.torch.nn.modules.linear.___torch_mangle_2288.Linear = prim::GetAttr[name="q_proj"](%2889)
  %2894 : int = aten::size(%query.31, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2895 : int = aten::size(%query.31, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.31 : Long() = prim::NumToTensor(%2895), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2897 : int = aten::size(%query.31, %1325), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2898 : Tensor = prim::GetAttr[name="bias"](%2893)
  %2899 : Tensor = prim::GetAttr[name="weight"](%2893)
  %2900 : Float(1024:1, 1024:1024) = aten::t(%2899), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.163 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2900), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2902 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.163, %2898, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.181 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2902, %1324), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %2904 : Tensor = prim::GetAttr[name="bias"](%2892)
  %2905 : Tensor = prim::GetAttr[name="weight"](%2892)
  %2906 : Float(1024:1, 1024:1024) = aten::t(%2905), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.164 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2906), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.183 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.164, %2904, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2909 : Tensor = prim::GetAttr[name="bias"](%2891)
  %2910 : Tensor = prim::GetAttr[name="weight"](%2891)
  %2911 : Float(1024:1, 1024:1024) = aten::t(%2910), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.165 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2911), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.185 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.165, %2909, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.182 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.181, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2915 : Long() = aten::mul(%bsz.31, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2916 : int = aten::Int(%2915), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2917 : int[] = prim::ListConstruct(%2894, %2916, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2918 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.182, %2917), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2918, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.184 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.183, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2921 : Long() = aten::mul(%bsz.31, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2922 : int = aten::Int(%2921), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2923 : int[] = prim::ListConstruct(%1321, %2922, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2924 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.184, %2923), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2924, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.186 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.185, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2927 : Long() = aten::mul(%bsz.31, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2928 : int = aten::Int(%2927), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2929 : int[] = prim::ListConstruct(%1321, %2928, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2930 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.186, %2929), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2930, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2932 : int = aten::size(%k.31, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %2933 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.31, %1332, %1325), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.112 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.31, %2933), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %2935 : int[] = prim::ListConstruct(%2895, %1320, %2894, %2932), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2936 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.112, %2935), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.113 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2936, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %2938 : Long() = aten::mul(%bsz.31, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %2939 : int = aten::Int(%2938), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %2940 : int[] = prim::ListConstruct(%2939, %2894, %2932), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %input.284 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.113, %2940), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %input.285 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.284, %1321, %1319), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.114 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.285, %1318, %1335), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.31 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.114, %v.31), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %2945 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.31, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2946 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2945, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2947 : int[] = prim::ListConstruct(%2894, %2895, %2897), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn
  %input.286 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2946, %2947), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2949 : Tensor = prim::GetAttr[name="bias"](%2890)
  %2950 : Tensor = prim::GetAttr[name="weight"](%2890)
  %2951 : Float(1024:1, 1024:1024) = aten::t(%2950), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.166 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.286, %2951), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.287 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.166, %2949, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn/__module.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.56 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.287, %1327, %1335), scope: __module.decoder/__module.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.288 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.31, %x.56, %1332), scope: __module.decoder/__module.decoder.layers.9 # transformers/modeling_bart.py:427:0
  %2956 : Tensor = prim::GetAttr[name="bias"](%2888)
  %2957 : Tensor = prim::GetAttr[name="weight"](%2888)
  %2958 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn_layer_norm
  %query.32 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.288, %2958, %2957, %2956, %1329, %1330), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2960 : __torch__.torch.nn.modules.linear.___torch_mangle_2295.Linear = prim::GetAttr[name="out_proj"](%2887)
  %2961 : __torch__.torch.nn.modules.linear.___torch_mangle_2293.Linear = prim::GetAttr[name="v_proj"](%2887)
  %2962 : __torch__.torch.nn.modules.linear.___torch_mangle_2292.Linear = prim::GetAttr[name="k_proj"](%2887)
  %2963 : __torch__.torch.nn.modules.linear.___torch_mangle_2294.Linear = prim::GetAttr[name="q_proj"](%2887)
  %2964 : int = aten::size(%query.32, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2965 : int = aten::size(%query.32, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.32 : Long() = prim::NumToTensor(%2965), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2967 : int = aten::size(%query.32, %1325), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2968 : Tensor = prim::GetAttr[name="bias"](%2963)
  %2969 : Tensor = prim::GetAttr[name="weight"](%2963)
  %2970 : Float(1024:1, 1024:1024) = aten::t(%2969), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.167 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.32, %2970), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2972 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.167, %2968, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.187 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2972, %1324), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:673:0
  %2974 : Tensor = prim::GetAttr[name="bias"](%2962)
  %2975 : Tensor = prim::GetAttr[name="weight"](%2962)
  %2976 : Float(1024:1, 1024:1024) = aten::t(%2975), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.168 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2976), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.189 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.168, %2974, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2979 : Tensor = prim::GetAttr[name="bias"](%2961)
  %2980 : Tensor = prim::GetAttr[name="weight"](%2961)
  %2981 : Float(1024:1, 1024:1024) = aten::t(%2980), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.169 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2981), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.169, %2979, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.188 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.187, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2985 : Long() = aten::mul(%bsz.32, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2986 : int = aten::Int(%2985), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2987 : int[] = prim::ListConstruct(%2964, %2986, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2988 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.188, %2987), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %q.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2988, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.190 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.189, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2991 : Long() = aten::mul(%bsz.32, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2992 : int = aten::Int(%2991), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2993 : int[] = prim::ListConstruct(%1321, %2992, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2994 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.190, %2993), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %k.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2994, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.192 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.191, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2997 : Long() = aten::mul(%bsz.32, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2998 : int = aten::Int(%2997), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %2999 : int[] = prim::ListConstruct(%1321, %2998, %1322), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %3000 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.192, %2999), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %v.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3000, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %3002 : int = aten::size(%k.32, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:701:0
  %3003 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.32, %1332, %1325), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.115 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.32, %3003), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %3005 : int[] = prim::ListConstruct(%2965, %1320, %2964, %3002), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %attn_weights.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.115, %3005), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:718:0
  %3007 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.22 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3007, %1325), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.117 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.116, %reshaped.22, %1326), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:720:0
  %3010 : Long() = aten::mul(%bsz.32, %1323), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %3011 : int = aten::Int(%3010), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %3012 : int[] = prim::ListConstruct(%3011, %2964, %3002), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %input.289 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.117, %3012), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %input.290 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.289, %1321, %1319), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.118 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.290, %1318, %1335), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.32 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.118, %v.32), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:730:0
  %3017 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.32, %1337, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %3018 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3017, %1337), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %3019 : int[] = prim::ListConstruct(%2964, %2965, %2967), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn
  %input.291 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3018, %3019), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %3021 : Tensor = prim::GetAttr[name="bias"](%2960)
  %3022 : Tensor = prim::GetAttr[name="weight"](%2960)
  %3023 : Float(1024:1, 1024:1024) = aten::t(%3022), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.170 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.291, %3023), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.292 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.170, %3021, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn/__module.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.292, %1327, %1335), scope: __module.decoder/__module.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.293 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.32, %x.57, %1332), scope: __module.decoder/__module.decoder.layers.9 # transformers/modeling_bart.py:443:0
  %3028 : Tensor = prim::GetAttr[name="bias"](%2886)
  %3029 : Tensor = prim::GetAttr[name="weight"](%2886)
  %3030 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn_layer_norm
  %input.294 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.293, %3030, %3029, %3028, %1329, %1330), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3032 : Tensor = prim::GetAttr[name="bias"](%2885)
  %3033 : Tensor = prim::GetAttr[name="weight"](%2885)
  %3034 : Float(1024:1, 4096:1024) = aten::t(%3033), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.171 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.294, %3034), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.295 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.171, %3032, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.296 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.295), scope: __module.decoder/__module.decoder.layers.9 # torch/nn/functional.py:1369:0
  %input.297 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.296, %1318, %1335), scope: __module.decoder/__module.decoder.layers.9 # torch/nn/functional.py:973:0
  %3039 : Tensor = prim::GetAttr[name="bias"](%2884)
  %3040 : Tensor = prim::GetAttr[name="weight"](%2884)
  %3041 : Float(4096:1, 1024:4096) = aten::t(%3040), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.172 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.297, %3041), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.298 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.172, %3039, %1332), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.58 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.298, %1327, %1335), scope: __module.decoder/__module.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.299 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.294, %x.58, %1332), scope: __module.decoder/__module.decoder.layers.9 # transformers/modeling_bart.py:455:0
  %3046 : Tensor = prim::GetAttr[name="bias"](%2883)
  %3047 : Tensor = prim::GetAttr[name="weight"](%2883)
  %3048 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.final_layer_norm
  %query.33 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.299, %3048, %3047, %3046, %1329, %1330), scope: __module.decoder/__module.decoder.layers.9/__module.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %3050 : __torch__.torch.nn.modules.normalization.___torch_mangle_2316.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1341)
  %3051 : __torch__.torch.nn.modules.linear.___torch_mangle_2315.Linear = prim::GetAttr[name="fc2"](%1341)
  %3052 : __torch__.torch.nn.modules.linear.___torch_mangle_2314.Linear = prim::GetAttr[name="fc1"](%1341)
  %3053 : __torch__.torch.nn.modules.normalization.___torch_mangle_2313.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1341)
  %3054 : __torch__.transformers.modeling_bart.___torch_mangle_2312.Attention = prim::GetAttr[name="encoder_attn"](%1341)
  %3055 : __torch__.torch.nn.modules.normalization.___torch_mangle_2307.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1341)
  %3056 : __torch__.transformers.modeling_bart.___torch_mangle_2306.Attention = prim::GetAttr[name="self_attn"](%1341)
  %3057 : __torch__.torch.nn.modules.linear.___torch_mangle_2305.Linear = prim::GetAttr[name="out_proj"](%3056)
  %3058 : __torch__.torch.nn.modules.linear.___torch_mangle_2303.Linear = prim::GetAttr[name="v_proj"](%3056)
  %3059 : __torch__.torch.nn.modules.linear.___torch_mangle_2302.Linear = prim::GetAttr[name="k_proj"](%3056)
  %3060 : __torch__.torch.nn.modules.linear.___torch_mangle_2304.Linear = prim::GetAttr[name="q_proj"](%3056)
  %3061 : int = aten::size(%query.33, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %3062 : int = aten::size(%query.33, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.33 : Long() = prim::NumToTensor(%3062), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3064 : int = aten::size(%query.33, %1325), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %3065 : Tensor = prim::GetAttr[name="bias"](%3060)
  %3066 : Tensor = prim::GetAttr[name="weight"](%3060)
  %3067 : Float(1024:1, 1024:1024) = aten::t(%3066), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.173 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %3067), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %3069 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.173, %3065, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.193 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3069, %1324), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %3071 : Tensor = prim::GetAttr[name="bias"](%3059)
  %3072 : Tensor = prim::GetAttr[name="weight"](%3059)
  %3073 : Float(1024:1, 1024:1024) = aten::t(%3072), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.174 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %3073), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.195 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.174, %3071, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %3076 : Tensor = prim::GetAttr[name="bias"](%3058)
  %3077 : Tensor = prim::GetAttr[name="weight"](%3058)
  %3078 : Float(1024:1, 1024:1024) = aten::t(%3077), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.175 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %3078), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.197 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.175, %3076, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.194 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.193, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3082 : Long() = aten::mul(%bsz.33, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3083 : int = aten::Int(%3082), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3084 : int[] = prim::ListConstruct(%3061, %3083, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3085 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.194, %3084), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3085, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.196 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.195, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3088 : Long() = aten::mul(%bsz.33, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3089 : int = aten::Int(%3088), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3090 : int[] = prim::ListConstruct(%1321, %3089, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3091 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.196, %3090), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3091, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.198 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.197, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3094 : Long() = aten::mul(%bsz.33, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3095 : int = aten::Int(%3094), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3096 : int[] = prim::ListConstruct(%1321, %3095, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3097 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.198, %3096), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3097, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3099 : int = aten::size(%k.33, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %3100 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.33, %1332, %1325), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.119 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.33, %3100), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %3102 : int[] = prim::ListConstruct(%3062, %1320, %3061, %3099), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.119, %3102), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3103, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %3105 : Long() = aten::mul(%bsz.33, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %3106 : int = aten::Int(%3105), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %3107 : int[] = prim::ListConstruct(%3106, %3061, %3099), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %input.300 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.120, %3107), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %input.301 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.300, %1321, %1319), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.121 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.301, %1318, %1335), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.33 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.121, %v.33), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %3112 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.33, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3113 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3112, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3114 : int[] = prim::ListConstruct(%3061, %3062, %3064), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn
  %input.302 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3113, %3114), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3116 : Tensor = prim::GetAttr[name="bias"](%3057)
  %3117 : Tensor = prim::GetAttr[name="weight"](%3057)
  %3118 : Float(1024:1, 1024:1024) = aten::t(%3117), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.176 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.302, %3118), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.303 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.176, %3116, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn/__module.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.59 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.303, %1327, %1335), scope: __module.decoder/__module.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.304 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.33, %x.59, %1332), scope: __module.decoder/__module.decoder.layers.10 # transformers/modeling_bart.py:427:0
  %3123 : Tensor = prim::GetAttr[name="bias"](%3055)
  %3124 : Tensor = prim::GetAttr[name="weight"](%3055)
  %3125 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn_layer_norm
  %query.34 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.304, %3125, %3124, %3123, %1329, %1330), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3127 : __torch__.torch.nn.modules.linear.___torch_mangle_2311.Linear = prim::GetAttr[name="out_proj"](%3054)
  %3128 : __torch__.torch.nn.modules.linear.___torch_mangle_2309.Linear = prim::GetAttr[name="v_proj"](%3054)
  %3129 : __torch__.torch.nn.modules.linear.___torch_mangle_2308.Linear = prim::GetAttr[name="k_proj"](%3054)
  %3130 : __torch__.torch.nn.modules.linear.___torch_mangle_2310.Linear = prim::GetAttr[name="q_proj"](%3054)
  %3131 : int = aten::size(%query.34, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3132 : int = aten::size(%query.34, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.34 : Long() = prim::NumToTensor(%3132), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3134 : int = aten::size(%query.34, %1325), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3135 : Tensor = prim::GetAttr[name="bias"](%3130)
  %3136 : Tensor = prim::GetAttr[name="weight"](%3130)
  %3137 : Float(1024:1, 1024:1024) = aten::t(%3136), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.177 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.34, %3137), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3139 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.177, %3135, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.199 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3139, %1324), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:673:0
  %3141 : Tensor = prim::GetAttr[name="bias"](%3129)
  %3142 : Tensor = prim::GetAttr[name="weight"](%3129)
  %3143 : Float(1024:1, 1024:1024) = aten::t(%3142), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.178 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3143), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.201 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.178, %3141, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3146 : Tensor = prim::GetAttr[name="bias"](%3128)
  %3147 : Tensor = prim::GetAttr[name="weight"](%3128)
  %3148 : Float(1024:1, 1024:1024) = aten::t(%3147), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.179 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3148), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.203 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.179, %3146, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.200 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.199, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3152 : Long() = aten::mul(%bsz.34, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3153 : int = aten::Int(%3152), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3154 : int[] = prim::ListConstruct(%3131, %3153, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3155 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.200, %3154), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %q.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3155, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.202 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.201, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3158 : Long() = aten::mul(%bsz.34, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3159 : int = aten::Int(%3158), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3160 : int[] = prim::ListConstruct(%1321, %3159, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3161 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.202, %3160), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %k.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3161, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.204 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.203, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3164 : Long() = aten::mul(%bsz.34, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3165 : int = aten::Int(%3164), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3166 : int[] = prim::ListConstruct(%1321, %3165, %1322), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3167 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.204, %3166), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %v.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3167, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3169 : int = aten::size(%k.34, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:701:0
  %3170 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.34, %1332, %1325), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.122 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.34, %3170), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %3172 : int[] = prim::ListConstruct(%3132, %1320, %3131, %3169), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %attn_weights.123 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.122, %3172), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:718:0
  %3174 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.23 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3174, %1325), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.124 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.123, %reshaped.23, %1326), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:720:0
  %3177 : Long() = aten::mul(%bsz.34, %1323), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %3178 : int = aten::Int(%3177), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %3179 : int[] = prim::ListConstruct(%3178, %3131, %3169), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %input.305 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.124, %3179), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %input.306 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.305, %1321, %1319), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.125 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.306, %1318, %1335), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.34 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.125, %v.34), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:730:0
  %3184 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.34, %1337, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3185 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3184, %1337), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3186 : int[] = prim::ListConstruct(%3131, %3132, %3134), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn
  %input.307 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3185, %3186), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3188 : Tensor = prim::GetAttr[name="bias"](%3127)
  %3189 : Tensor = prim::GetAttr[name="weight"](%3127)
  %3190 : Float(1024:1, 1024:1024) = aten::t(%3189), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.180 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.307, %3190), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.308 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.180, %3188, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn/__module.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.308, %1327, %1335), scope: __module.decoder/__module.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.309 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.34, %x.60, %1332), scope: __module.decoder/__module.decoder.layers.10 # transformers/modeling_bart.py:443:0
  %3195 : Tensor = prim::GetAttr[name="bias"](%3053)
  %3196 : Tensor = prim::GetAttr[name="weight"](%3053)
  %3197 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn_layer_norm
  %input.310 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.309, %3197, %3196, %3195, %1329, %1330), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3199 : Tensor = prim::GetAttr[name="bias"](%3052)
  %3200 : Tensor = prim::GetAttr[name="weight"](%3052)
  %3201 : Float(1024:1, 4096:1024) = aten::t(%3200), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.181 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.310, %3201), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.311 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.181, %3199, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.312 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.311), scope: __module.decoder/__module.decoder.layers.10 # torch/nn/functional.py:1369:0
  %input.313 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.312, %1318, %1335), scope: __module.decoder/__module.decoder.layers.10 # torch/nn/functional.py:973:0
  %3206 : Tensor = prim::GetAttr[name="bias"](%3051)
  %3207 : Tensor = prim::GetAttr[name="weight"](%3051)
  %3208 : Float(4096:1, 1024:4096) = aten::t(%3207), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.182 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.313, %3208), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.314 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.182, %3206, %1332), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.61 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.314, %1327, %1335), scope: __module.decoder/__module.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.315 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.310, %x.61, %1332), scope: __module.decoder/__module.decoder.layers.10 # transformers/modeling_bart.py:455:0
  %3213 : Tensor = prim::GetAttr[name="bias"](%3050)
  %3214 : Tensor = prim::GetAttr[name="weight"](%3050)
  %3215 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.final_layer_norm
  %query.35 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.315, %3215, %3214, %3213, %1329, %1330), scope: __module.decoder/__module.decoder.layers.10/__module.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %3217 : __torch__.torch.nn.modules.normalization.___torch_mangle_2332.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1339)
  %3218 : __torch__.torch.nn.modules.linear.___torch_mangle_2331.Linear = prim::GetAttr[name="fc2"](%1339)
  %3219 : __torch__.torch.nn.modules.linear.___torch_mangle_2330.Linear = prim::GetAttr[name="fc1"](%1339)
  %3220 : __torch__.torch.nn.modules.normalization.___torch_mangle_2329.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1339)
  %3221 : __torch__.transformers.modeling_bart.___torch_mangle_2328.Attention = prim::GetAttr[name="encoder_attn"](%1339)
  %3222 : __torch__.torch.nn.modules.normalization.___torch_mangle_2323.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1339)
  %3223 : __torch__.transformers.modeling_bart.___torch_mangle_2322.Attention = prim::GetAttr[name="self_attn"](%1339)
  %3224 : __torch__.torch.nn.modules.linear.___torch_mangle_2321.Linear = prim::GetAttr[name="out_proj"](%3223)
  %3225 : __torch__.torch.nn.modules.linear.___torch_mangle_2319.Linear = prim::GetAttr[name="v_proj"](%3223)
  %3226 : __torch__.torch.nn.modules.linear.___torch_mangle_2318.Linear = prim::GetAttr[name="k_proj"](%3223)
  %3227 : __torch__.torch.nn.modules.linear.___torch_mangle_2320.Linear = prim::GetAttr[name="q_proj"](%3223)
  %3228 : int = aten::size(%query.35, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3229 : int = aten::size(%query.35, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.35 : Long() = prim::NumToTensor(%3229), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3231 : int = aten::size(%query.35, %1325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3232 : Tensor = prim::GetAttr[name="bias"](%3227)
  %3233 : Tensor = prim::GetAttr[name="weight"](%3227)
  %3234 : Float(1024:1, 1024:1024) = aten::t(%3233), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.183 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3234), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %3236 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.183, %3232, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.205 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3236, %1324), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %3238 : Tensor = prim::GetAttr[name="bias"](%3226)
  %3239 : Tensor = prim::GetAttr[name="weight"](%3226)
  %3240 : Float(1024:1, 1024:1024) = aten::t(%3239), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.184 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3240), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.184, %3238, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %3243 : Tensor = prim::GetAttr[name="bias"](%3225)
  %3244 : Tensor = prim::GetAttr[name="weight"](%3225)
  %3245 : Float(1024:1, 1024:1024) = aten::t(%3244), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.185 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3245), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.209 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.185, %3243, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.206 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.205, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3249 : Long() = aten::mul(%bsz.35, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3250 : int = aten::Int(%3249), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3251 : int[] = prim::ListConstruct(%3228, %3250, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3252 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.206, %3251), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3252, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.208 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.207, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3255 : Long() = aten::mul(%bsz.35, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3256 : int = aten::Int(%3255), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3257 : int[] = prim::ListConstruct(%1321, %3256, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3258 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.208, %3257), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3258, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.210 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.209, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3261 : Long() = aten::mul(%bsz.35, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3262 : int = aten::Int(%3261), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3263 : int[] = prim::ListConstruct(%1321, %3262, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3264 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.210, %3263), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3264, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3266 : int = aten::size(%k.35, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %3267 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.35, %1332, %1325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.126 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.35, %3267), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %3269 : int[] = prim::ListConstruct(%3229, %1320, %3228, %3266), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3270 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.126, %3269), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.127 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3270, %attn_mask, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %3272 : Long() = aten::mul(%bsz.35, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %3273 : int = aten::Int(%3272), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %3274 : int[] = prim::ListConstruct(%3273, %3228, %3266), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %input.316 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.127, %3274), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %input.317 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.316, %1321, %1319), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.128 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.317, %1318, %1335), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.35 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.128, %v.35), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %3279 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.35, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3280 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3279, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3281 : int[] = prim::ListConstruct(%3228, %3229, %3231), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn
  %input.318 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3280, %3281), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3283 : Tensor = prim::GetAttr[name="bias"](%3224)
  %3284 : Tensor = prim::GetAttr[name="weight"](%3224)
  %3285 : Float(1024:1, 1024:1024) = aten::t(%3284), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.186 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.318, %3285), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.319 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.186, %3283, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn/__module.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.62 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.319, %1327, %1335), scope: __module.decoder/__module.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.320 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.35, %x.62, %1332), scope: __module.decoder/__module.decoder.layers.11 # transformers/modeling_bart.py:427:0
  %3290 : Tensor = prim::GetAttr[name="bias"](%3222)
  %3291 : Tensor = prim::GetAttr[name="weight"](%3222)
  %3292 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn_layer_norm
  %query : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.320, %3292, %3291, %3290, %1329, %1330), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3294 : __torch__.torch.nn.modules.linear.___torch_mangle_2327.Linear = prim::GetAttr[name="out_proj"](%3221)
  %3295 : __torch__.torch.nn.modules.linear.___torch_mangle_2325.Linear = prim::GetAttr[name="v_proj"](%3221)
  %3296 : __torch__.torch.nn.modules.linear.___torch_mangle_2324.Linear = prim::GetAttr[name="k_proj"](%3221)
  %3297 : __torch__.torch.nn.modules.linear.___torch_mangle_2326.Linear = prim::GetAttr[name="q_proj"](%3221)
  %3298 : int = aten::size(%query, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3299 : int = aten::size(%query, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz : Long() = prim::NumToTensor(%3299), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3301 : int = aten::size(%query, %1325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3302 : Tensor = prim::GetAttr[name="bias"](%3297)
  %3303 : Tensor = prim::GetAttr[name="weight"](%3297)
  %3304 : Float(1024:1, 1024:1024) = aten::t(%3303), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.187 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query, %3304), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3306 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.187, %3302, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.211 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3306, %1324), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:673:0
  %3308 : Tensor = prim::GetAttr[name="bias"](%3296)
  %3309 : Tensor = prim::GetAttr[name="weight"](%3296)
  %3310 : Float(1024:1, 1024:1024) = aten::t(%3309), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.188 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3310), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.213 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.188, %3308, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3313 : Tensor = prim::GetAttr[name="bias"](%3295)
  %3314 : Tensor = prim::GetAttr[name="weight"](%3295)
  %3315 : Float(1024:1, 1024:1024) = aten::t(%3314), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.189 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3315), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.215 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.189, %3313, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.212 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.211, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3319 : Long() = aten::mul(%bsz, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3320 : int = aten::Int(%3319), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3321 : int[] = prim::ListConstruct(%3298, %3320, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3322 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.212, %3321), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %q : Float(272:64, 13:17408, 64:1) = aten::transpose(%3322, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.214 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.213, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3325 : Long() = aten::mul(%bsz, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3326 : int = aten::Int(%3325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3327 : int[] = prim::ListConstruct(%1321, %3326, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3328 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.214, %3327), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %k : Float(272:64, 13:17408, 64:1) = aten::transpose(%3328, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.215, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3331 : Long() = aten::mul(%bsz, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3332 : int = aten::Int(%3331), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3333 : int[] = prim::ListConstruct(%1321, %3332, %1322), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3334 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor, %3333), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %v : Float(272:64, 13:17408, 64:1) = aten::transpose(%3334, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3336 : int = aten::size(%k, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:701:0
  %3337 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k, %1332, %1325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.129 : Float(272:169, 13:13, 13:1) = aten::bmm(%q, %3337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %3339 : int[] = prim::ListConstruct(%3299, %1320, %3298, %3336), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %attn_weights.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.129, %3339), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:718:0
  %3341 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3341, %1325), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.131 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.130, %reshaped, %1326), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:720:0
  %3344 : Long() = aten::mul(%bsz, %1323), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %3345 : int = aten::Int(%3344), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %3346 : int[] = prim::ListConstruct(%3345, %3298, %3336), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %input.321 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.131, %3346), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %input.322 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.321, %1321, %1319), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights : Float(272:169, 13:13, 13:1) = aten::dropout(%input.322, %1318, %1335), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
  %attn_output : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights, %v), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:730:0
  %3351 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output, %1337, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3352 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3351, %1337), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3353 : int[] = prim::ListConstruct(%3298, %3299, %3301), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn
  %input.323 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3352, %3353), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3355 : Tensor = prim::GetAttr[name="bias"](%3294)
  %3356 : Tensor = prim::GetAttr[name="weight"](%3294)
  %3357 : Float(1024:1, 1024:1024) = aten::t(%3356), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.190 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.323, %3357), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.324 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.190, %3355, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn/__module.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.324, %1327, %1335), scope: __module.decoder/__module.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.325 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query, %x.63, %1332), scope: __module.decoder/__module.decoder.layers.11 # transformers/modeling_bart.py:443:0
  %3362 : Tensor = prim::GetAttr[name="bias"](%3220)
  %3363 : Tensor = prim::GetAttr[name="weight"](%3220)
  %3364 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn_layer_norm
  %input.326 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.325, %3364, %3363, %3362, %1329, %1330), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3366 : Tensor = prim::GetAttr[name="bias"](%3219)
  %3367 : Tensor = prim::GetAttr[name="weight"](%3219)
  %3368 : Float(1024:1, 4096:1024) = aten::t(%3367), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.191 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.326, %3368), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.327 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.191, %3366, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.328 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.327), scope: __module.decoder/__module.decoder.layers.11 # torch/nn/functional.py:1369:0
  %input.329 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.328, %1318, %1335), scope: __module.decoder/__module.decoder.layers.11 # torch/nn/functional.py:973:0
  %3373 : Tensor = prim::GetAttr[name="bias"](%3218)
  %3374 : Tensor = prim::GetAttr[name="weight"](%3218)
  %3375 : Float(4096:1, 1024:4096) = aten::t(%3374), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.329, %3375), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.330 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output, %3373, %1332), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.64 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.330, %1327, %1335), scope: __module.decoder/__module.decoder.layers.11 # torch/nn/functional.py:973:0
  %input : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.326, %x.64, %1332), scope: __module.decoder/__module.decoder.layers.11 # transformers/modeling_bart.py:455:0
  %3380 : Tensor = prim::GetAttr[name="bias"](%3217)
  %3381 : Tensor = prim::GetAttr[name="weight"](%3217)
  %3382 : int[] = prim::ListConstruct(%1328), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.final_layer_norm
  %x : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input, %3382, %3381, %3380, %1329, %1330), scope: __module.decoder/__module.decoder.layers.11/__module.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %3384 : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x, %1337, %1332), scope: __module.decoder # transformers/modeling_bart.py:601:0
  %115 : (Float(17:1024, 13:17408, 1024:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%3384, %encoder_hidden_states)
  return (%115)
