graph(%self.1 : __torch__.transformers.modeling_roberta.RobertaForMultipleChoice,
      %input_ids.1 : Long(17:91, 7:13, 13:1),
      %token_type_ids : Long(17:91, 7:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_32018.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_32017.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_roberta.___torch_mangle_32016.RobertaModel = prim::GetAttr[name="roberta"](%self.1)
  %6 : int = prim::Constant[value=1]() # transformers/modeling_roberta.py:1069:0
  %7 : int = aten::size(%input_ids.1, %6) # transformers/modeling_roberta.py:1069:0
  %num_choices : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%num_choices)
  %10 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1071:0
  %11 : int = aten::size(%input_ids.1, %10) # transformers/modeling_roberta.py:1071:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1071:0
  %15 : int[] = prim::ListConstruct(%14, %13)
  %input_ids : Long(119:13, 13:1) = aten::view(%input_ids.1, %15) # transformers/modeling_roberta.py:1071:0
  %17 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1073:0
  %18 : int = aten::size(%token_type_ids, %17) # transformers/modeling_roberta.py:1073:0
  %19 : Long() = prim::NumToTensor(%18)
  %20 : int = aten::Int(%19)
  %21 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1073:0
  %22 : int[] = prim::ListConstruct(%21, %20)
  %input.2 : Long(119:13, 13:1) = aten::view(%token_type_ids, %22) # transformers/modeling_roberta.py:1073:0
  %31 : Double() = prim::Constant[value={8}](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %32 : int = prim::Constant[value=-2](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %33 : int = prim::Constant[value=64](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %34 : int = prim::Constant[value=12](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %35 : int = prim::Constant[value=4](), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %36 : Long() = prim::Constant[value={1}](), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %37 : int = prim::Constant[value=-1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %38 : bool = prim::Constant[value=1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %39 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %40 : int = prim::Constant[value=768](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %41 : float = prim::Constant[value=0.10000000000000001](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %42 : Double() = prim::Constant[value={-10000}](), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %43 : float = prim::Constant[value=1.](), scope: __module.roberta # torch/tensor.py:396:0
  %44 : None = prim::Constant(), scope: __module.roberta
  %45 : int = prim::Constant[value=3](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %46 : int = prim::Constant[value=2](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %47 : int = prim::Constant[value=9223372036854775807](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %48 : bool = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:648:0
  %49 : Device = prim::Constant[value="cpu"](), scope: __module.roberta # transformers/modeling_roberta.py:648:0
  %50 : int = prim::Constant[value=6](), scope: __module.roberta # transformers/modeling_roberta.py:648:0
  %51 : int = prim::Constant[value=1](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %52 : int = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %53 : __torch__.transformers.modeling_roberta.RobertaPooler = prim::GetAttr[name="pooler"](%5)
  %54 : __torch__.transformers.modeling_roberta.___torch_mangle_32013.RobertaEncoder = prim::GetAttr[name="encoder"](%5)
  %55 : __torch__.transformers.modeling_roberta.___torch_mangle_31807.RobertaEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %56 : int = aten::size(%input_ids, %52), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %57 : int = aten::size(%input_ids, %51), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %58 : int[] = prim::ListConstruct(%56, %57), scope: __module.roberta
  %attention_mask.1 : Float(119:13, 13:1) = aten::ones(%58, %50, %52, %49, %48), scope: __module.roberta # transformers/modeling_roberta.py:648:0
  %60 : Float(119:13, 13:1) = aten::slice(%attention_mask.1, %52, %52, %47, %51), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %61 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%60, %51), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %62 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%61, %46), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Float(119:13, 1:13, 1:13, 13:1) = aten::slice(%62, %45, %52, %47, %51), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %64 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %50, %48, %48, %44), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %65 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%64, %43, %51), scope: __module.roberta # torch/tensor.py:396:0
  %attention_mask : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%65, %42), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %67 : __torch__.torch.nn.modules.normalization.___torch_mangle_31805.LayerNorm = prim::GetAttr[name="LayerNorm"](%55)
  %68 : __torch__.torch.nn.modules.sparse.___torch_mangle_31804.Embedding = prim::GetAttr[name="token_type_embeddings"](%55)
  %69 : __torch__.torch.nn.modules.sparse.___torch_mangle_31803.Embedding = prim::GetAttr[name="position_embeddings"](%55)
  %70 : __torch__.torch.nn.modules.sparse.___torch_mangle_31802.Embedding = prim::GetAttr[name="word_embeddings"](%55)
  %71 : Bool(119:13, 13:1) = aten::ne(%input_ids, %51), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %mask : Int(119:13, 13:1) = aten::to(%71, %45, %48, %48, %44), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %73 : Long(119:13, 13:1) = aten::cumsum(%mask, %51, %44), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %74 : Int(119:13, 13:1) = aten::type_as(%73, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %incremental_indices : Int(119:13, 13:1) = aten::mul(%74, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %76 : Long(119:13, 13:1) = aten::to(%incremental_indices, %35, %48, %48, %44), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %77 : Long(119:13, 13:1) = aten::add(%76, %36, %51), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %input.1 : Long(119:13, 13:1) = aten::to(%77, %35, %52, %49, %48, %48, %48, %44), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:98:0
  %79 : Tensor = prim::GetAttr[name="weight"](%70)
  %inputs_embeds : Float(119:9984, 13:768, 768:1) = aten::embedding(%79, %input_ids, %51, %48, %48), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %81 : Tensor = prim::GetAttr[name="weight"](%69)
  %position_embeddings : Float(119:9984, 13:768, 768:1) = aten::embedding(%81, %input.1, %51, %48, %48), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %83 : Tensor = prim::GetAttr[name="weight"](%68)
  %token_type_embeddings : Float(119:9984, 13:768, 768:1) = aten::embedding(%83, %input.2, %37, %48, %48), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %85 : Float(119:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %51), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %input.3 : Float(119:9984, 13:768, 768:1) = aten::add(%85, %token_type_embeddings, %51), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %87 : Tensor = prim::GetAttr[name="bias"](%67)
  %88 : Tensor = prim::GetAttr[name="weight"](%67)
  %89 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm
  %input.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %89, %88, %87, %39, %38), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.4, %41, %48), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %93 : __torch__.transformers.modeling_roberta.___torch_mangle_32011.RobertaLayer = prim::GetAttr[name="11"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %95 : __torch__.transformers.modeling_roberta.___torch_mangle_31994.RobertaLayer = prim::GetAttr[name="10"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %97 : __torch__.transformers.modeling_roberta.___torch_mangle_31977.RobertaLayer = prim::GetAttr[name="9"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %99 : __torch__.transformers.modeling_roberta.___torch_mangle_31960.RobertaLayer = prim::GetAttr[name="8"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %101 : __torch__.transformers.modeling_roberta.___torch_mangle_31943.RobertaLayer = prim::GetAttr[name="7"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %103 : __torch__.transformers.modeling_roberta.___torch_mangle_31926.RobertaLayer = prim::GetAttr[name="6"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %105 : __torch__.transformers.modeling_roberta.___torch_mangle_31909.RobertaLayer = prim::GetAttr[name="5"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %107 : __torch__.transformers.modeling_roberta.___torch_mangle_31892.RobertaLayer = prim::GetAttr[name="4"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %109 : __torch__.transformers.modeling_roberta.___torch_mangle_31875.RobertaLayer = prim::GetAttr[name="3"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %111 : __torch__.transformers.modeling_roberta.___torch_mangle_31858.RobertaLayer = prim::GetAttr[name="2"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %113 : __torch__.transformers.modeling_roberta.___torch_mangle_31841.RobertaLayer = prim::GetAttr[name="1"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_32012.ModuleList = prim::GetAttr[name="layer"](%54)
  %115 : __torch__.transformers.modeling_roberta.___torch_mangle_31824.RobertaLayer = prim::GetAttr[name="0"](%114)
  %116 : __torch__.transformers.modeling_roberta.___torch_mangle_31823.RobertaOutput = prim::GetAttr[name="output"](%115)
  %117 : __torch__.transformers.modeling_roberta.___torch_mangle_31819.RobertaIntermediate = prim::GetAttr[name="intermediate"](%115)
  %118 : __torch__.transformers.modeling_roberta.___torch_mangle_31817.RobertaAttention = prim::GetAttr[name="attention"](%115)
  %119 : __torch__.transformers.modeling_roberta.___torch_mangle_31816.RobertaSelfOutput = prim::GetAttr[name="output"](%118)
  %120 : __torch__.transformers.modeling_roberta.___torch_mangle_31812.RobertaSelfAttention = prim::GetAttr[name="self"](%118)
  %121 : __torch__.torch.nn.modules.linear.___torch_mangle_31810.Linear = prim::GetAttr[name="value"](%120)
  %122 : __torch__.torch.nn.modules.linear.___torch_mangle_31809.Linear = prim::GetAttr[name="key"](%120)
  %123 : __torch__.torch.nn.modules.linear.___torch_mangle_31808.Linear = prim::GetAttr[name="query"](%120)
  %124 : Tensor = prim::GetAttr[name="bias"](%123)
  %125 : Tensor = prim::GetAttr[name="weight"](%123)
  %126 : Float(768:1, 768:768) = aten::t(%125), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %126), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.1, %124, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %129 : Tensor = prim::GetAttr[name="bias"](%122)
  %130 : Tensor = prim::GetAttr[name="weight"](%122)
  %131 : Float(768:1, 768:768) = aten::t(%130), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %131), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.2, %129, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %134 : Tensor = prim::GetAttr[name="bias"](%121)
  %135 : Tensor = prim::GetAttr[name="weight"](%121)
  %136 : Float(768:1, 768:768) = aten::t(%135), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %136), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.3, %134, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %139 : int = aten::size(%x.1, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %140 : int = aten::size(%x.1, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %141 : int[] = prim::ListConstruct(%139, %140, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.2 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %141), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %143 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %query_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %143), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %145 : int = aten::size(%x.3, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %146 : int = aten::size(%x.3, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %147 : int[] = prim::ListConstruct(%145, %146, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.4 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %147), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %149 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %key_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %149), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %151 : int = aten::size(%x.5, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %152 : int = aten::size(%x.5, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %153 : int[] = prim::ListConstruct(%151, %152, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.6 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %153), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %155 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %value_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %155), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %157 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %157), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %input.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:198:0
  %input.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:211:0
  %164 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %165 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %164), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.2 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%165, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %167 : int = aten::size(%context_layer.2, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %168 : int = aten::size(%context_layer.2, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %169 : int[] = prim::ListConstruct(%167, %168, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %input.8 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.2, %169), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %171 : __torch__.torch.nn.modules.normalization.___torch_mangle_31814.LayerNorm = prim::GetAttr[name="LayerNorm"](%119)
  %172 : __torch__.torch.nn.modules.linear.___torch_mangle_31813.Linear = prim::GetAttr[name="dense"](%119)
  %173 : Tensor = prim::GetAttr[name="bias"](%172)
  %174 : Tensor = prim::GetAttr[name="weight"](%172)
  %175 : Float(768:1, 768:768) = aten::t(%174), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.8, %175), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.4, %173, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.9, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output # transformers/modeling_roberta.py:232:0
  %180 : Tensor = prim::GetAttr[name="bias"](%171)
  %181 : Tensor = prim::GetAttr[name="weight"](%171)
  %182 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %182, %181, %180, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %184 : __torch__.torch.nn.modules.linear.___torch_mangle_31818.Linear = prim::GetAttr[name="dense"](%117)
  %185 : Tensor = prim::GetAttr[name="bias"](%184)
  %186 : Tensor = prim::GetAttr[name="weight"](%184)
  %187 : Float(768:1, 3072:768) = aten::t(%186), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %187), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.5, %185, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %191 : __torch__.torch.nn.modules.normalization.___torch_mangle_31821.LayerNorm = prim::GetAttr[name="LayerNorm"](%116)
  %192 : __torch__.torch.nn.modules.linear.___torch_mangle_31820.Linear = prim::GetAttr[name="dense"](%116)
  %193 : Tensor = prim::GetAttr[name="bias"](%192)
  %194 : Tensor = prim::GetAttr[name="weight"](%192)
  %195 : Float(3072:1, 768:3072) = aten::t(%194), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.12, %195), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.6, %193, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.13, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output # transformers/modeling_roberta.py:311:0
  %200 : Tensor = prim::GetAttr[name="bias"](%191)
  %201 : Tensor = prim::GetAttr[name="weight"](%191)
  %202 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm
  %input.15 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %202, %201, %200, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %204 : __torch__.transformers.modeling_roberta.___torch_mangle_31840.RobertaOutput = prim::GetAttr[name="output"](%113)
  %205 : __torch__.transformers.modeling_roberta.___torch_mangle_31836.RobertaIntermediate = prim::GetAttr[name="intermediate"](%113)
  %206 : __torch__.transformers.modeling_roberta.___torch_mangle_31834.RobertaAttention = prim::GetAttr[name="attention"](%113)
  %207 : __torch__.transformers.modeling_roberta.___torch_mangle_31833.RobertaSelfOutput = prim::GetAttr[name="output"](%206)
  %208 : __torch__.transformers.modeling_roberta.___torch_mangle_31829.RobertaSelfAttention = prim::GetAttr[name="self"](%206)
  %209 : __torch__.torch.nn.modules.linear.___torch_mangle_31827.Linear = prim::GetAttr[name="value"](%208)
  %210 : __torch__.torch.nn.modules.linear.___torch_mangle_31826.Linear = prim::GetAttr[name="key"](%208)
  %211 : __torch__.torch.nn.modules.linear.___torch_mangle_31825.Linear = prim::GetAttr[name="query"](%208)
  %212 : Tensor = prim::GetAttr[name="bias"](%211)
  %213 : Tensor = prim::GetAttr[name="weight"](%211)
  %214 : Float(768:1, 768:768) = aten::t(%213), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %214), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.7, %212, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %217 : Tensor = prim::GetAttr[name="bias"](%210)
  %218 : Tensor = prim::GetAttr[name="weight"](%210)
  %219 : Float(768:1, 768:768) = aten::t(%218), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %219), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.8, %217, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %222 : Tensor = prim::GetAttr[name="bias"](%209)
  %223 : Tensor = prim::GetAttr[name="weight"](%209)
  %224 : Float(768:1, 768:768) = aten::t(%223), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %224), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.9, %222, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %227 : int = aten::size(%x.7, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %228 : int = aten::size(%x.7, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %229 : int[] = prim::ListConstruct(%227, %228, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.8 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %229), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %231 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %query_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %231), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %233 : int = aten::size(%x.9, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %234 : int = aten::size(%x.9, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %235 : int[] = prim::ListConstruct(%233, %234, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.10 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %235), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %237 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %key_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %237), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %239 : int = aten::size(%x.11, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %240 : int = aten::size(%x.11, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %241 : int[] = prim::ListConstruct(%239, %240, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.12 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %241), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %243 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %value_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %243), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %245 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %245), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:195:0
  %input.16 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:198:0
  %input.17 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:211:0
  %252 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %253 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %252), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.4 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%253, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %255 : int = aten::size(%context_layer.4, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %256 : int = aten::size(%context_layer.4, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %257 : int[] = prim::ListConstruct(%255, %256, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %input.18 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.4, %257), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:215:0
  %259 : __torch__.torch.nn.modules.normalization.___torch_mangle_31831.LayerNorm = prim::GetAttr[name="LayerNorm"](%207)
  %260 : __torch__.torch.nn.modules.linear.___torch_mangle_31830.Linear = prim::GetAttr[name="dense"](%207)
  %261 : Tensor = prim::GetAttr[name="bias"](%260)
  %262 : Tensor = prim::GetAttr[name="weight"](%260)
  %263 : Float(768:1, 768:768) = aten::t(%262), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.18, %263), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.10, %261, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.19, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output # transformers/modeling_roberta.py:232:0
  %268 : Tensor = prim::GetAttr[name="bias"](%259)
  %269 : Tensor = prim::GetAttr[name="weight"](%259)
  %270 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %270, %269, %268, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %272 : __torch__.torch.nn.modules.linear.___torch_mangle_31835.Linear = prim::GetAttr[name="dense"](%205)
  %273 : Tensor = prim::GetAttr[name="bias"](%272)
  %274 : Tensor = prim::GetAttr[name="weight"](%272)
  %275 : Float(768:1, 3072:768) = aten::t(%274), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %275), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.11, %273, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %279 : __torch__.torch.nn.modules.normalization.___torch_mangle_31838.LayerNorm = prim::GetAttr[name="LayerNorm"](%204)
  %280 : __torch__.torch.nn.modules.linear.___torch_mangle_31837.Linear = prim::GetAttr[name="dense"](%204)
  %281 : Tensor = prim::GetAttr[name="bias"](%280)
  %282 : Tensor = prim::GetAttr[name="weight"](%280)
  %283 : Float(3072:1, 768:3072) = aten::t(%282), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.22, %283), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.12, %281, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.23, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output # transformers/modeling_roberta.py:311:0
  %288 : Tensor = prim::GetAttr[name="bias"](%279)
  %289 : Tensor = prim::GetAttr[name="weight"](%279)
  %290 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm
  %input.25 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %290, %289, %288, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %292 : __torch__.transformers.modeling_roberta.___torch_mangle_31857.RobertaOutput = prim::GetAttr[name="output"](%111)
  %293 : __torch__.transformers.modeling_roberta.___torch_mangle_31853.RobertaIntermediate = prim::GetAttr[name="intermediate"](%111)
  %294 : __torch__.transformers.modeling_roberta.___torch_mangle_31851.RobertaAttention = prim::GetAttr[name="attention"](%111)
  %295 : __torch__.transformers.modeling_roberta.___torch_mangle_31850.RobertaSelfOutput = prim::GetAttr[name="output"](%294)
  %296 : __torch__.transformers.modeling_roberta.___torch_mangle_31846.RobertaSelfAttention = prim::GetAttr[name="self"](%294)
  %297 : __torch__.torch.nn.modules.linear.___torch_mangle_31844.Linear = prim::GetAttr[name="value"](%296)
  %298 : __torch__.torch.nn.modules.linear.___torch_mangle_31843.Linear = prim::GetAttr[name="key"](%296)
  %299 : __torch__.torch.nn.modules.linear.___torch_mangle_31842.Linear = prim::GetAttr[name="query"](%296)
  %300 : Tensor = prim::GetAttr[name="bias"](%299)
  %301 : Tensor = prim::GetAttr[name="weight"](%299)
  %302 : Float(768:1, 768:768) = aten::t(%301), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %302), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.13, %300, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %305 : Tensor = prim::GetAttr[name="bias"](%298)
  %306 : Tensor = prim::GetAttr[name="weight"](%298)
  %307 : Float(768:1, 768:768) = aten::t(%306), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %307), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.14, %305, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %310 : Tensor = prim::GetAttr[name="bias"](%297)
  %311 : Tensor = prim::GetAttr[name="weight"](%297)
  %312 : Float(768:1, 768:768) = aten::t(%311), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %312), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.15, %310, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %315 : int = aten::size(%x.13, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %316 : int = aten::size(%x.13, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %317 : int[] = prim::ListConstruct(%315, %316, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.14 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %317), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %319 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %query_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %319), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %321 : int = aten::size(%x.15, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %322 : int = aten::size(%x.15, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %323 : int[] = prim::ListConstruct(%321, %322, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.16 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %323), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %325 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %key_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %325), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %327 : int = aten::size(%x.17, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %328 : int = aten::size(%x.17, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %329 : int[] = prim::ListConstruct(%327, %328, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.18 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %329), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %331 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %value_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %331), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %333 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %333), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:195:0
  %input.26 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:198:0
  %input.27 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:211:0
  %340 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %341 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %340), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.6 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%341, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %343 : int = aten::size(%context_layer.6, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %344 : int = aten::size(%context_layer.6, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %345 : int[] = prim::ListConstruct(%343, %344, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %input.28 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.6, %345), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:215:0
  %347 : __torch__.torch.nn.modules.normalization.___torch_mangle_31848.LayerNorm = prim::GetAttr[name="LayerNorm"](%295)
  %348 : __torch__.torch.nn.modules.linear.___torch_mangle_31847.Linear = prim::GetAttr[name="dense"](%295)
  %349 : Tensor = prim::GetAttr[name="bias"](%348)
  %350 : Tensor = prim::GetAttr[name="weight"](%348)
  %351 : Float(768:1, 768:768) = aten::t(%350), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.28, %351), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.16, %349, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.29, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output # transformers/modeling_roberta.py:232:0
  %356 : Tensor = prim::GetAttr[name="bias"](%347)
  %357 : Tensor = prim::GetAttr[name="weight"](%347)
  %358 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %358, %357, %356, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %360 : __torch__.torch.nn.modules.linear.___torch_mangle_31852.Linear = prim::GetAttr[name="dense"](%293)
  %361 : Tensor = prim::GetAttr[name="bias"](%360)
  %362 : Tensor = prim::GetAttr[name="weight"](%360)
  %363 : Float(768:1, 3072:768) = aten::t(%362), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %363), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.17, %361, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %367 : __torch__.torch.nn.modules.normalization.___torch_mangle_31855.LayerNorm = prim::GetAttr[name="LayerNorm"](%292)
  %368 : __torch__.torch.nn.modules.linear.___torch_mangle_31854.Linear = prim::GetAttr[name="dense"](%292)
  %369 : Tensor = prim::GetAttr[name="bias"](%368)
  %370 : Tensor = prim::GetAttr[name="weight"](%368)
  %371 : Float(3072:1, 768:3072) = aten::t(%370), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.32, %371), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.18, %369, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.33, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output # transformers/modeling_roberta.py:311:0
  %376 : Tensor = prim::GetAttr[name="bias"](%367)
  %377 : Tensor = prim::GetAttr[name="weight"](%367)
  %378 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm
  %input.35 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %378, %377, %376, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %380 : __torch__.transformers.modeling_roberta.___torch_mangle_31874.RobertaOutput = prim::GetAttr[name="output"](%109)
  %381 : __torch__.transformers.modeling_roberta.___torch_mangle_31870.RobertaIntermediate = prim::GetAttr[name="intermediate"](%109)
  %382 : __torch__.transformers.modeling_roberta.___torch_mangle_31868.RobertaAttention = prim::GetAttr[name="attention"](%109)
  %383 : __torch__.transformers.modeling_roberta.___torch_mangle_31867.RobertaSelfOutput = prim::GetAttr[name="output"](%382)
  %384 : __torch__.transformers.modeling_roberta.___torch_mangle_31863.RobertaSelfAttention = prim::GetAttr[name="self"](%382)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_31861.Linear = prim::GetAttr[name="value"](%384)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_31860.Linear = prim::GetAttr[name="key"](%384)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_31859.Linear = prim::GetAttr[name="query"](%384)
  %388 : Tensor = prim::GetAttr[name="bias"](%387)
  %389 : Tensor = prim::GetAttr[name="weight"](%387)
  %390 : Float(768:1, 768:768) = aten::t(%389), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %390), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.19, %388, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %393 : Tensor = prim::GetAttr[name="bias"](%386)
  %394 : Tensor = prim::GetAttr[name="weight"](%386)
  %395 : Float(768:1, 768:768) = aten::t(%394), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %395), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.20, %393, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %398 : Tensor = prim::GetAttr[name="bias"](%385)
  %399 : Tensor = prim::GetAttr[name="weight"](%385)
  %400 : Float(768:1, 768:768) = aten::t(%399), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %400), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.21, %398, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %403 : int = aten::size(%x.19, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %404 : int = aten::size(%x.19, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %405 : int[] = prim::ListConstruct(%403, %404, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.20 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %405), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %407 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %query_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %407), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %409 : int = aten::size(%x.21, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %410 : int = aten::size(%x.21, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %411 : int[] = prim::ListConstruct(%409, %410, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.22 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %411), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %413 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %key_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %413), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %415 : int = aten::size(%x.23, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %416 : int = aten::size(%x.23, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %417 : int[] = prim::ListConstruct(%415, %416, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.24 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %417), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %419 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %value_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %419), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %421 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %421), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:195:0
  %input.36 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:198:0
  %input.37 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:211:0
  %428 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %429 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %428), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.8 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%429, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %431 : int = aten::size(%context_layer.8, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %432 : int = aten::size(%context_layer.8, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %433 : int[] = prim::ListConstruct(%431, %432, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %input.38 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.8, %433), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:215:0
  %435 : __torch__.torch.nn.modules.normalization.___torch_mangle_31865.LayerNorm = prim::GetAttr[name="LayerNorm"](%383)
  %436 : __torch__.torch.nn.modules.linear.___torch_mangle_31864.Linear = prim::GetAttr[name="dense"](%383)
  %437 : Tensor = prim::GetAttr[name="bias"](%436)
  %438 : Tensor = prim::GetAttr[name="weight"](%436)
  %439 : Float(768:1, 768:768) = aten::t(%438), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.38, %439), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.22, %437, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.39, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output # transformers/modeling_roberta.py:232:0
  %444 : Tensor = prim::GetAttr[name="bias"](%435)
  %445 : Tensor = prim::GetAttr[name="weight"](%435)
  %446 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %446, %445, %444, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %448 : __torch__.torch.nn.modules.linear.___torch_mangle_31869.Linear = prim::GetAttr[name="dense"](%381)
  %449 : Tensor = prim::GetAttr[name="bias"](%448)
  %450 : Tensor = prim::GetAttr[name="weight"](%448)
  %451 : Float(768:1, 3072:768) = aten::t(%450), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %451), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.23, %449, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %455 : __torch__.torch.nn.modules.normalization.___torch_mangle_31872.LayerNorm = prim::GetAttr[name="LayerNorm"](%380)
  %456 : __torch__.torch.nn.modules.linear.___torch_mangle_31871.Linear = prim::GetAttr[name="dense"](%380)
  %457 : Tensor = prim::GetAttr[name="bias"](%456)
  %458 : Tensor = prim::GetAttr[name="weight"](%456)
  %459 : Float(3072:1, 768:3072) = aten::t(%458), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.42, %459), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.24, %457, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.43, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output # transformers/modeling_roberta.py:311:0
  %464 : Tensor = prim::GetAttr[name="bias"](%455)
  %465 : Tensor = prim::GetAttr[name="weight"](%455)
  %466 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm
  %input.45 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %466, %465, %464, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %468 : __torch__.transformers.modeling_roberta.___torch_mangle_31891.RobertaOutput = prim::GetAttr[name="output"](%107)
  %469 : __torch__.transformers.modeling_roberta.___torch_mangle_31887.RobertaIntermediate = prim::GetAttr[name="intermediate"](%107)
  %470 : __torch__.transformers.modeling_roberta.___torch_mangle_31885.RobertaAttention = prim::GetAttr[name="attention"](%107)
  %471 : __torch__.transformers.modeling_roberta.___torch_mangle_31884.RobertaSelfOutput = prim::GetAttr[name="output"](%470)
  %472 : __torch__.transformers.modeling_roberta.___torch_mangle_31880.RobertaSelfAttention = prim::GetAttr[name="self"](%470)
  %473 : __torch__.torch.nn.modules.linear.___torch_mangle_31878.Linear = prim::GetAttr[name="value"](%472)
  %474 : __torch__.torch.nn.modules.linear.___torch_mangle_31877.Linear = prim::GetAttr[name="key"](%472)
  %475 : __torch__.torch.nn.modules.linear.___torch_mangle_31876.Linear = prim::GetAttr[name="query"](%472)
  %476 : Tensor = prim::GetAttr[name="bias"](%475)
  %477 : Tensor = prim::GetAttr[name="weight"](%475)
  %478 : Float(768:1, 768:768) = aten::t(%477), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %478), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.25, %476, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %481 : Tensor = prim::GetAttr[name="bias"](%474)
  %482 : Tensor = prim::GetAttr[name="weight"](%474)
  %483 : Float(768:1, 768:768) = aten::t(%482), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %483), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.26, %481, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %486 : Tensor = prim::GetAttr[name="bias"](%473)
  %487 : Tensor = prim::GetAttr[name="weight"](%473)
  %488 : Float(768:1, 768:768) = aten::t(%487), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %488), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.27, %486, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %491 : int = aten::size(%x.25, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %492 : int = aten::size(%x.25, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %493 : int[] = prim::ListConstruct(%491, %492, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.26 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %493), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %495 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %query_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %495), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %497 : int = aten::size(%x.27, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %498 : int = aten::size(%x.27, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %499 : int[] = prim::ListConstruct(%497, %498, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.28 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %499), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %501 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %key_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %501), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %503 : int = aten::size(%x.29, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %504 : int = aten::size(%x.29, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %505 : int[] = prim::ListConstruct(%503, %504, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.30 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %505), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %507 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %value_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %507), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %509 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.9 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %509), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.10 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:195:0
  %input.46 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:198:0
  %input.47 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:211:0
  %516 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %517 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %516), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.10 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%517, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %519 : int = aten::size(%context_layer.10, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %520 : int = aten::size(%context_layer.10, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %521 : int[] = prim::ListConstruct(%519, %520, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %input.48 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.10, %521), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:215:0
  %523 : __torch__.torch.nn.modules.normalization.___torch_mangle_31882.LayerNorm = prim::GetAttr[name="LayerNorm"](%471)
  %524 : __torch__.torch.nn.modules.linear.___torch_mangle_31881.Linear = prim::GetAttr[name="dense"](%471)
  %525 : Tensor = prim::GetAttr[name="bias"](%524)
  %526 : Tensor = prim::GetAttr[name="weight"](%524)
  %527 : Float(768:1, 768:768) = aten::t(%526), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.48, %527), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.28, %525, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.49, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output # transformers/modeling_roberta.py:232:0
  %532 : Tensor = prim::GetAttr[name="bias"](%523)
  %533 : Tensor = prim::GetAttr[name="weight"](%523)
  %534 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %534, %533, %532, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %536 : __torch__.torch.nn.modules.linear.___torch_mangle_31886.Linear = prim::GetAttr[name="dense"](%469)
  %537 : Tensor = prim::GetAttr[name="bias"](%536)
  %538 : Tensor = prim::GetAttr[name="weight"](%536)
  %539 : Float(768:1, 3072:768) = aten::t(%538), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %539), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.29, %537, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %543 : __torch__.torch.nn.modules.normalization.___torch_mangle_31889.LayerNorm = prim::GetAttr[name="LayerNorm"](%468)
  %544 : __torch__.torch.nn.modules.linear.___torch_mangle_31888.Linear = prim::GetAttr[name="dense"](%468)
  %545 : Tensor = prim::GetAttr[name="bias"](%544)
  %546 : Tensor = prim::GetAttr[name="weight"](%544)
  %547 : Float(3072:1, 768:3072) = aten::t(%546), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.52, %547), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.30, %545, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.53, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output # transformers/modeling_roberta.py:311:0
  %552 : Tensor = prim::GetAttr[name="bias"](%543)
  %553 : Tensor = prim::GetAttr[name="weight"](%543)
  %554 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm
  %input.55 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %554, %553, %552, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %556 : __torch__.transformers.modeling_roberta.___torch_mangle_31908.RobertaOutput = prim::GetAttr[name="output"](%105)
  %557 : __torch__.transformers.modeling_roberta.___torch_mangle_31904.RobertaIntermediate = prim::GetAttr[name="intermediate"](%105)
  %558 : __torch__.transformers.modeling_roberta.___torch_mangle_31902.RobertaAttention = prim::GetAttr[name="attention"](%105)
  %559 : __torch__.transformers.modeling_roberta.___torch_mangle_31901.RobertaSelfOutput = prim::GetAttr[name="output"](%558)
  %560 : __torch__.transformers.modeling_roberta.___torch_mangle_31897.RobertaSelfAttention = prim::GetAttr[name="self"](%558)
  %561 : __torch__.torch.nn.modules.linear.___torch_mangle_31895.Linear = prim::GetAttr[name="value"](%560)
  %562 : __torch__.torch.nn.modules.linear.___torch_mangle_31894.Linear = prim::GetAttr[name="key"](%560)
  %563 : __torch__.torch.nn.modules.linear.___torch_mangle_31893.Linear = prim::GetAttr[name="query"](%560)
  %564 : Tensor = prim::GetAttr[name="bias"](%563)
  %565 : Tensor = prim::GetAttr[name="weight"](%563)
  %566 : Float(768:1, 768:768) = aten::t(%565), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %566), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.31, %564, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %569 : Tensor = prim::GetAttr[name="bias"](%562)
  %570 : Tensor = prim::GetAttr[name="weight"](%562)
  %571 : Float(768:1, 768:768) = aten::t(%570), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %571), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.32, %569, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %574 : Tensor = prim::GetAttr[name="bias"](%561)
  %575 : Tensor = prim::GetAttr[name="weight"](%561)
  %576 : Float(768:1, 768:768) = aten::t(%575), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %576), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.33, %574, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %579 : int = aten::size(%x.31, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %580 : int = aten::size(%x.31, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %581 : int[] = prim::ListConstruct(%579, %580, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.32 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %581), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %583 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %query_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %583), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %585 : int = aten::size(%x.33, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %586 : int = aten::size(%x.33, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %587 : int[] = prim::ListConstruct(%585, %586, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.34 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %587), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %589 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %key_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %589), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %591 : int = aten::size(%x.35, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %592 : int = aten::size(%x.35, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %593 : int[] = prim::ListConstruct(%591, %592, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.36 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %593), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %595 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %value_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %595), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %597 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.11 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %597), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.12 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:195:0
  %input.56 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:198:0
  %input.57 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:211:0
  %604 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %605 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %604), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.12 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%605, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %607 : int = aten::size(%context_layer.12, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %608 : int = aten::size(%context_layer.12, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %609 : int[] = prim::ListConstruct(%607, %608, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %input.58 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.12, %609), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:215:0
  %611 : __torch__.torch.nn.modules.normalization.___torch_mangle_31899.LayerNorm = prim::GetAttr[name="LayerNorm"](%559)
  %612 : __torch__.torch.nn.modules.linear.___torch_mangle_31898.Linear = prim::GetAttr[name="dense"](%559)
  %613 : Tensor = prim::GetAttr[name="bias"](%612)
  %614 : Tensor = prim::GetAttr[name="weight"](%612)
  %615 : Float(768:1, 768:768) = aten::t(%614), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.58, %615), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.34, %613, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.59, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output # transformers/modeling_roberta.py:232:0
  %620 : Tensor = prim::GetAttr[name="bias"](%611)
  %621 : Tensor = prim::GetAttr[name="weight"](%611)
  %622 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %622, %621, %620, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_31903.Linear = prim::GetAttr[name="dense"](%557)
  %625 : Tensor = prim::GetAttr[name="bias"](%624)
  %626 : Tensor = prim::GetAttr[name="weight"](%624)
  %627 : Float(768:1, 3072:768) = aten::t(%626), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %627), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.35, %625, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %631 : __torch__.torch.nn.modules.normalization.___torch_mangle_31906.LayerNorm = prim::GetAttr[name="LayerNorm"](%556)
  %632 : __torch__.torch.nn.modules.linear.___torch_mangle_31905.Linear = prim::GetAttr[name="dense"](%556)
  %633 : Tensor = prim::GetAttr[name="bias"](%632)
  %634 : Tensor = prim::GetAttr[name="weight"](%632)
  %635 : Float(3072:1, 768:3072) = aten::t(%634), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.62, %635), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.36, %633, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.63, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output # transformers/modeling_roberta.py:311:0
  %640 : Tensor = prim::GetAttr[name="bias"](%631)
  %641 : Tensor = prim::GetAttr[name="weight"](%631)
  %642 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm
  %input.65 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %642, %641, %640, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %644 : __torch__.transformers.modeling_roberta.___torch_mangle_31925.RobertaOutput = prim::GetAttr[name="output"](%103)
  %645 : __torch__.transformers.modeling_roberta.___torch_mangle_31921.RobertaIntermediate = prim::GetAttr[name="intermediate"](%103)
  %646 : __torch__.transformers.modeling_roberta.___torch_mangle_31919.RobertaAttention = prim::GetAttr[name="attention"](%103)
  %647 : __torch__.transformers.modeling_roberta.___torch_mangle_31918.RobertaSelfOutput = prim::GetAttr[name="output"](%646)
  %648 : __torch__.transformers.modeling_roberta.___torch_mangle_31914.RobertaSelfAttention = prim::GetAttr[name="self"](%646)
  %649 : __torch__.torch.nn.modules.linear.___torch_mangle_31912.Linear = prim::GetAttr[name="value"](%648)
  %650 : __torch__.torch.nn.modules.linear.___torch_mangle_31911.Linear = prim::GetAttr[name="key"](%648)
  %651 : __torch__.torch.nn.modules.linear.___torch_mangle_31910.Linear = prim::GetAttr[name="query"](%648)
  %652 : Tensor = prim::GetAttr[name="bias"](%651)
  %653 : Tensor = prim::GetAttr[name="weight"](%651)
  %654 : Float(768:1, 768:768) = aten::t(%653), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %654), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.37, %652, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %657 : Tensor = prim::GetAttr[name="bias"](%650)
  %658 : Tensor = prim::GetAttr[name="weight"](%650)
  %659 : Float(768:1, 768:768) = aten::t(%658), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %659), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.38, %657, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %662 : Tensor = prim::GetAttr[name="bias"](%649)
  %663 : Tensor = prim::GetAttr[name="weight"](%649)
  %664 : Float(768:1, 768:768) = aten::t(%663), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %664), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.39, %662, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %667 : int = aten::size(%x.37, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %668 : int = aten::size(%x.37, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %669 : int[] = prim::ListConstruct(%667, %668, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.38 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %669), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %671 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %query_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %671), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %673 : int = aten::size(%x.39, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %674 : int = aten::size(%x.39, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %675 : int[] = prim::ListConstruct(%673, %674, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %675), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %677 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %key_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %677), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %679 : int = aten::size(%x.41, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %680 : int = aten::size(%x.41, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %681 : int[] = prim::ListConstruct(%679, %680, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.42 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %681), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %683 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %value_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %683), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %685 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.13 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %685), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.14 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:195:0
  %input.66 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:198:0
  %input.67 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:211:0
  %692 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %693 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %692), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.14 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%693, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %695 : int = aten::size(%context_layer.14, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %696 : int = aten::size(%context_layer.14, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %697 : int[] = prim::ListConstruct(%695, %696, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %input.68 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.14, %697), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:215:0
  %699 : __torch__.torch.nn.modules.normalization.___torch_mangle_31916.LayerNorm = prim::GetAttr[name="LayerNorm"](%647)
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_31915.Linear = prim::GetAttr[name="dense"](%647)
  %701 : Tensor = prim::GetAttr[name="bias"](%700)
  %702 : Tensor = prim::GetAttr[name="weight"](%700)
  %703 : Float(768:1, 768:768) = aten::t(%702), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.68, %703), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.40, %701, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.69, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output # transformers/modeling_roberta.py:232:0
  %708 : Tensor = prim::GetAttr[name="bias"](%699)
  %709 : Tensor = prim::GetAttr[name="weight"](%699)
  %710 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %710, %709, %708, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %712 : __torch__.torch.nn.modules.linear.___torch_mangle_31920.Linear = prim::GetAttr[name="dense"](%645)
  %713 : Tensor = prim::GetAttr[name="bias"](%712)
  %714 : Tensor = prim::GetAttr[name="weight"](%712)
  %715 : Float(768:1, 3072:768) = aten::t(%714), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %715), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.41, %713, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %719 : __torch__.torch.nn.modules.normalization.___torch_mangle_31923.LayerNorm = prim::GetAttr[name="LayerNorm"](%644)
  %720 : __torch__.torch.nn.modules.linear.___torch_mangle_31922.Linear = prim::GetAttr[name="dense"](%644)
  %721 : Tensor = prim::GetAttr[name="bias"](%720)
  %722 : Tensor = prim::GetAttr[name="weight"](%720)
  %723 : Float(3072:1, 768:3072) = aten::t(%722), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.72, %723), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.42, %721, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.73, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output # transformers/modeling_roberta.py:311:0
  %728 : Tensor = prim::GetAttr[name="bias"](%719)
  %729 : Tensor = prim::GetAttr[name="weight"](%719)
  %730 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm
  %input.75 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %730, %729, %728, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %732 : __torch__.transformers.modeling_roberta.___torch_mangle_31942.RobertaOutput = prim::GetAttr[name="output"](%101)
  %733 : __torch__.transformers.modeling_roberta.___torch_mangle_31938.RobertaIntermediate = prim::GetAttr[name="intermediate"](%101)
  %734 : __torch__.transformers.modeling_roberta.___torch_mangle_31936.RobertaAttention = prim::GetAttr[name="attention"](%101)
  %735 : __torch__.transformers.modeling_roberta.___torch_mangle_31935.RobertaSelfOutput = prim::GetAttr[name="output"](%734)
  %736 : __torch__.transformers.modeling_roberta.___torch_mangle_31931.RobertaSelfAttention = prim::GetAttr[name="self"](%734)
  %737 : __torch__.torch.nn.modules.linear.___torch_mangle_31929.Linear = prim::GetAttr[name="value"](%736)
  %738 : __torch__.torch.nn.modules.linear.___torch_mangle_31928.Linear = prim::GetAttr[name="key"](%736)
  %739 : __torch__.torch.nn.modules.linear.___torch_mangle_31927.Linear = prim::GetAttr[name="query"](%736)
  %740 : Tensor = prim::GetAttr[name="bias"](%739)
  %741 : Tensor = prim::GetAttr[name="weight"](%739)
  %742 : Float(768:1, 768:768) = aten::t(%741), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %742), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.43, %740, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %745 : Tensor = prim::GetAttr[name="bias"](%738)
  %746 : Tensor = prim::GetAttr[name="weight"](%738)
  %747 : Float(768:1, 768:768) = aten::t(%746), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %747), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.44, %745, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %750 : Tensor = prim::GetAttr[name="bias"](%737)
  %751 : Tensor = prim::GetAttr[name="weight"](%737)
  %752 : Float(768:1, 768:768) = aten::t(%751), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %752), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.45, %750, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %755 : int = aten::size(%x.43, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %756 : int = aten::size(%x.43, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %757 : int[] = prim::ListConstruct(%755, %756, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.44 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %757), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %759 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %query_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %759), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %761 : int = aten::size(%x.45, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %762 : int = aten::size(%x.45, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %763 : int[] = prim::ListConstruct(%761, %762, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.46 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %763), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %765 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %key_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %765), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %767 : int = aten::size(%x.47, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %768 : int = aten::size(%x.47, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %769 : int[] = prim::ListConstruct(%767, %768, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.48 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %769), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %771 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %value_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %771), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %773 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.15 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %773), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.16 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:195:0
  %input.76 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:198:0
  %input.77 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:211:0
  %780 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %781 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %780), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.16 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%781, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %783 : int = aten::size(%context_layer.16, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %784 : int = aten::size(%context_layer.16, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %785 : int[] = prim::ListConstruct(%783, %784, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %input.78 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.16, %785), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:215:0
  %787 : __torch__.torch.nn.modules.normalization.___torch_mangle_31933.LayerNorm = prim::GetAttr[name="LayerNorm"](%735)
  %788 : __torch__.torch.nn.modules.linear.___torch_mangle_31932.Linear = prim::GetAttr[name="dense"](%735)
  %789 : Tensor = prim::GetAttr[name="bias"](%788)
  %790 : Tensor = prim::GetAttr[name="weight"](%788)
  %791 : Float(768:1, 768:768) = aten::t(%790), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.78, %791), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.46, %789, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.79, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output # transformers/modeling_roberta.py:232:0
  %796 : Tensor = prim::GetAttr[name="bias"](%787)
  %797 : Tensor = prim::GetAttr[name="weight"](%787)
  %798 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %798, %797, %796, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %800 : __torch__.torch.nn.modules.linear.___torch_mangle_31937.Linear = prim::GetAttr[name="dense"](%733)
  %801 : Tensor = prim::GetAttr[name="bias"](%800)
  %802 : Tensor = prim::GetAttr[name="weight"](%800)
  %803 : Float(768:1, 3072:768) = aten::t(%802), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %803), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.47, %801, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %807 : __torch__.torch.nn.modules.normalization.___torch_mangle_31940.LayerNorm = prim::GetAttr[name="LayerNorm"](%732)
  %808 : __torch__.torch.nn.modules.linear.___torch_mangle_31939.Linear = prim::GetAttr[name="dense"](%732)
  %809 : Tensor = prim::GetAttr[name="bias"](%808)
  %810 : Tensor = prim::GetAttr[name="weight"](%808)
  %811 : Float(3072:1, 768:3072) = aten::t(%810), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.82, %811), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.48, %809, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.83, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output # transformers/modeling_roberta.py:311:0
  %816 : Tensor = prim::GetAttr[name="bias"](%807)
  %817 : Tensor = prim::GetAttr[name="weight"](%807)
  %818 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm
  %input.85 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %818, %817, %816, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %820 : __torch__.transformers.modeling_roberta.___torch_mangle_31959.RobertaOutput = prim::GetAttr[name="output"](%99)
  %821 : __torch__.transformers.modeling_roberta.___torch_mangle_31955.RobertaIntermediate = prim::GetAttr[name="intermediate"](%99)
  %822 : __torch__.transformers.modeling_roberta.___torch_mangle_31953.RobertaAttention = prim::GetAttr[name="attention"](%99)
  %823 : __torch__.transformers.modeling_roberta.___torch_mangle_31952.RobertaSelfOutput = prim::GetAttr[name="output"](%822)
  %824 : __torch__.transformers.modeling_roberta.___torch_mangle_31948.RobertaSelfAttention = prim::GetAttr[name="self"](%822)
  %825 : __torch__.torch.nn.modules.linear.___torch_mangle_31946.Linear = prim::GetAttr[name="value"](%824)
  %826 : __torch__.torch.nn.modules.linear.___torch_mangle_31945.Linear = prim::GetAttr[name="key"](%824)
  %827 : __torch__.torch.nn.modules.linear.___torch_mangle_31944.Linear = prim::GetAttr[name="query"](%824)
  %828 : Tensor = prim::GetAttr[name="bias"](%827)
  %829 : Tensor = prim::GetAttr[name="weight"](%827)
  %830 : Float(768:1, 768:768) = aten::t(%829), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %830), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.49, %828, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %833 : Tensor = prim::GetAttr[name="bias"](%826)
  %834 : Tensor = prim::GetAttr[name="weight"](%826)
  %835 : Float(768:1, 768:768) = aten::t(%834), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %835), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.50, %833, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %838 : Tensor = prim::GetAttr[name="bias"](%825)
  %839 : Tensor = prim::GetAttr[name="weight"](%825)
  %840 : Float(768:1, 768:768) = aten::t(%839), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %840), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.51, %838, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %843 : int = aten::size(%x.49, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %844 : int = aten::size(%x.49, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %845 : int[] = prim::ListConstruct(%843, %844, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.50 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %845), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %847 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %query_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %847), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %849 : int = aten::size(%x.51, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %850 : int = aten::size(%x.51, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %851 : int[] = prim::ListConstruct(%849, %850, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.52 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %851), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %853 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %key_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %853), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %855 : int = aten::size(%x.53, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %856 : int = aten::size(%x.53, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %857 : int[] = prim::ListConstruct(%855, %856, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.54 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %857), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %859 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %value_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %859), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %861 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.17 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %861), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.18 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:195:0
  %input.86 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:198:0
  %input.87 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:211:0
  %868 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %869 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %868), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.18 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%869, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %871 : int = aten::size(%context_layer.18, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %872 : int = aten::size(%context_layer.18, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %873 : int[] = prim::ListConstruct(%871, %872, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %input.88 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.18, %873), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:215:0
  %875 : __torch__.torch.nn.modules.normalization.___torch_mangle_31950.LayerNorm = prim::GetAttr[name="LayerNorm"](%823)
  %876 : __torch__.torch.nn.modules.linear.___torch_mangle_31949.Linear = prim::GetAttr[name="dense"](%823)
  %877 : Tensor = prim::GetAttr[name="bias"](%876)
  %878 : Tensor = prim::GetAttr[name="weight"](%876)
  %879 : Float(768:1, 768:768) = aten::t(%878), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.88, %879), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.52, %877, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.89, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output # transformers/modeling_roberta.py:232:0
  %884 : Tensor = prim::GetAttr[name="bias"](%875)
  %885 : Tensor = prim::GetAttr[name="weight"](%875)
  %886 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %886, %885, %884, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_31954.Linear = prim::GetAttr[name="dense"](%821)
  %889 : Tensor = prim::GetAttr[name="bias"](%888)
  %890 : Tensor = prim::GetAttr[name="weight"](%888)
  %891 : Float(768:1, 3072:768) = aten::t(%890), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %891), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.53, %889, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %895 : __torch__.torch.nn.modules.normalization.___torch_mangle_31957.LayerNorm = prim::GetAttr[name="LayerNorm"](%820)
  %896 : __torch__.torch.nn.modules.linear.___torch_mangle_31956.Linear = prim::GetAttr[name="dense"](%820)
  %897 : Tensor = prim::GetAttr[name="bias"](%896)
  %898 : Tensor = prim::GetAttr[name="weight"](%896)
  %899 : Float(3072:1, 768:3072) = aten::t(%898), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.92, %899), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.54, %897, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.93, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output # transformers/modeling_roberta.py:311:0
  %904 : Tensor = prim::GetAttr[name="bias"](%895)
  %905 : Tensor = prim::GetAttr[name="weight"](%895)
  %906 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm
  %input.95 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %906, %905, %904, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %908 : __torch__.transformers.modeling_roberta.___torch_mangle_31976.RobertaOutput = prim::GetAttr[name="output"](%97)
  %909 : __torch__.transformers.modeling_roberta.___torch_mangle_31972.RobertaIntermediate = prim::GetAttr[name="intermediate"](%97)
  %910 : __torch__.transformers.modeling_roberta.___torch_mangle_31970.RobertaAttention = prim::GetAttr[name="attention"](%97)
  %911 : __torch__.transformers.modeling_roberta.___torch_mangle_31969.RobertaSelfOutput = prim::GetAttr[name="output"](%910)
  %912 : __torch__.transformers.modeling_roberta.___torch_mangle_31965.RobertaSelfAttention = prim::GetAttr[name="self"](%910)
  %913 : __torch__.torch.nn.modules.linear.___torch_mangle_31963.Linear = prim::GetAttr[name="value"](%912)
  %914 : __torch__.torch.nn.modules.linear.___torch_mangle_31962.Linear = prim::GetAttr[name="key"](%912)
  %915 : __torch__.torch.nn.modules.linear.___torch_mangle_31961.Linear = prim::GetAttr[name="query"](%912)
  %916 : Tensor = prim::GetAttr[name="bias"](%915)
  %917 : Tensor = prim::GetAttr[name="weight"](%915)
  %918 : Float(768:1, 768:768) = aten::t(%917), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %918), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.55, %916, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %921 : Tensor = prim::GetAttr[name="bias"](%914)
  %922 : Tensor = prim::GetAttr[name="weight"](%914)
  %923 : Float(768:1, 768:768) = aten::t(%922), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %923), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.56, %921, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %926 : Tensor = prim::GetAttr[name="bias"](%913)
  %927 : Tensor = prim::GetAttr[name="weight"](%913)
  %928 : Float(768:1, 768:768) = aten::t(%927), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %928), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.57, %926, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %931 : int = aten::size(%x.55, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %932 : int = aten::size(%x.55, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %933 : int[] = prim::ListConstruct(%931, %932, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.56 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %933), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %935 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %query_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %935), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %937 : int = aten::size(%x.57, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %938 : int = aten::size(%x.57, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %939 : int[] = prim::ListConstruct(%937, %938, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.58 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %939), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %941 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %key_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %941), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %943 : int = aten::size(%x.59, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %944 : int = aten::size(%x.59, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %945 : int[] = prim::ListConstruct(%943, %944, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.60 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %945), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %947 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %value_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %947), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %949 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.19 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %949), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.20 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:195:0
  %input.96 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:198:0
  %input.97 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:211:0
  %956 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %957 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %956), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.20 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%957, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %959 : int = aten::size(%context_layer.20, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %960 : int = aten::size(%context_layer.20, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %961 : int[] = prim::ListConstruct(%959, %960, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %input.98 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.20, %961), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:215:0
  %963 : __torch__.torch.nn.modules.normalization.___torch_mangle_31967.LayerNorm = prim::GetAttr[name="LayerNorm"](%911)
  %964 : __torch__.torch.nn.modules.linear.___torch_mangle_31966.Linear = prim::GetAttr[name="dense"](%911)
  %965 : Tensor = prim::GetAttr[name="bias"](%964)
  %966 : Tensor = prim::GetAttr[name="weight"](%964)
  %967 : Float(768:1, 768:768) = aten::t(%966), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.98, %967), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.58, %965, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.99, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output # transformers/modeling_roberta.py:232:0
  %972 : Tensor = prim::GetAttr[name="bias"](%963)
  %973 : Tensor = prim::GetAttr[name="weight"](%963)
  %974 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %974, %973, %972, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %976 : __torch__.torch.nn.modules.linear.___torch_mangle_31971.Linear = prim::GetAttr[name="dense"](%909)
  %977 : Tensor = prim::GetAttr[name="bias"](%976)
  %978 : Tensor = prim::GetAttr[name="weight"](%976)
  %979 : Float(768:1, 3072:768) = aten::t(%978), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %979), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.59, %977, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %983 : __torch__.torch.nn.modules.normalization.___torch_mangle_31974.LayerNorm = prim::GetAttr[name="LayerNorm"](%908)
  %984 : __torch__.torch.nn.modules.linear.___torch_mangle_31973.Linear = prim::GetAttr[name="dense"](%908)
  %985 : Tensor = prim::GetAttr[name="bias"](%984)
  %986 : Tensor = prim::GetAttr[name="weight"](%984)
  %987 : Float(3072:1, 768:3072) = aten::t(%986), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.102, %987), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.60, %985, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.103, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output # transformers/modeling_roberta.py:311:0
  %992 : Tensor = prim::GetAttr[name="bias"](%983)
  %993 : Tensor = prim::GetAttr[name="weight"](%983)
  %994 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm
  %input.105 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %994, %993, %992, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %996 : __torch__.transformers.modeling_roberta.___torch_mangle_31993.RobertaOutput = prim::GetAttr[name="output"](%95)
  %997 : __torch__.transformers.modeling_roberta.___torch_mangle_31989.RobertaIntermediate = prim::GetAttr[name="intermediate"](%95)
  %998 : __torch__.transformers.modeling_roberta.___torch_mangle_31987.RobertaAttention = prim::GetAttr[name="attention"](%95)
  %999 : __torch__.transformers.modeling_roberta.___torch_mangle_31986.RobertaSelfOutput = prim::GetAttr[name="output"](%998)
  %1000 : __torch__.transformers.modeling_roberta.___torch_mangle_31982.RobertaSelfAttention = prim::GetAttr[name="self"](%998)
  %1001 : __torch__.torch.nn.modules.linear.___torch_mangle_31980.Linear = prim::GetAttr[name="value"](%1000)
  %1002 : __torch__.torch.nn.modules.linear.___torch_mangle_31979.Linear = prim::GetAttr[name="key"](%1000)
  %1003 : __torch__.torch.nn.modules.linear.___torch_mangle_31978.Linear = prim::GetAttr[name="query"](%1000)
  %1004 : Tensor = prim::GetAttr[name="bias"](%1003)
  %1005 : Tensor = prim::GetAttr[name="weight"](%1003)
  %1006 : Float(768:1, 768:768) = aten::t(%1005), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1006), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.61, %1004, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%1002)
  %1010 : Tensor = prim::GetAttr[name="weight"](%1002)
  %1011 : Float(768:1, 768:768) = aten::t(%1010), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1011), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.62, %1009, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1014 : Tensor = prim::GetAttr[name="bias"](%1001)
  %1015 : Tensor = prim::GetAttr[name="weight"](%1001)
  %1016 : Float(768:1, 768:768) = aten::t(%1015), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1016), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.63, %1014, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1019 : int = aten::size(%x.61, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1020 : int = aten::size(%x.61, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1021 : int[] = prim::ListConstruct(%1019, %1020, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.62 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %1021), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1023 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %query_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1023), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1025 : int = aten::size(%x.63, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1026 : int = aten::size(%x.63, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1027 : int[] = prim::ListConstruct(%1025, %1026, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.64 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1027), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1029 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %key_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1029), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1031 : int = aten::size(%x.65, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1032 : int = aten::size(%x.65, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1033 : int[] = prim::ListConstruct(%1031, %1032, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.66 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1033), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1035 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %value_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1035), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1037 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.21 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1037), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.22 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:195:0
  %input.106 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:198:0
  %input.107 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:211:0
  %1044 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %1045 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1044), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.22 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1045, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %1047 : int = aten::size(%context_layer.22, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1048 : int = aten::size(%context_layer.22, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1049 : int[] = prim::ListConstruct(%1047, %1048, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %input.108 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1049), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:215:0
  %1051 : __torch__.torch.nn.modules.normalization.___torch_mangle_31984.LayerNorm = prim::GetAttr[name="LayerNorm"](%999)
  %1052 : __torch__.torch.nn.modules.linear.___torch_mangle_31983.Linear = prim::GetAttr[name="dense"](%999)
  %1053 : Tensor = prim::GetAttr[name="bias"](%1052)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1052)
  %1055 : Float(768:1, 768:768) = aten::t(%1054), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.108, %1055), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.64, %1053, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.109, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output # transformers/modeling_roberta.py:232:0
  %1060 : Tensor = prim::GetAttr[name="bias"](%1051)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1051)
  %1062 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1062, %1061, %1060, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1064 : __torch__.torch.nn.modules.linear.___torch_mangle_31988.Linear = prim::GetAttr[name="dense"](%997)
  %1065 : Tensor = prim::GetAttr[name="bias"](%1064)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1064)
  %1067 : Float(768:1, 3072:768) = aten::t(%1066), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1067), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1065, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1071 : __torch__.torch.nn.modules.normalization.___torch_mangle_31991.LayerNorm = prim::GetAttr[name="LayerNorm"](%996)
  %1072 : __torch__.torch.nn.modules.linear.___torch_mangle_31990.Linear = prim::GetAttr[name="dense"](%996)
  %1073 : Tensor = prim::GetAttr[name="bias"](%1072)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1075 : Float(3072:1, 768:3072) = aten::t(%1074), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.112, %1075), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.66, %1073, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.113, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output # transformers/modeling_roberta.py:311:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1082 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm
  %input.115 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1082, %1081, %1080, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1084 : __torch__.transformers.modeling_roberta.___torch_mangle_32010.RobertaOutput = prim::GetAttr[name="output"](%93)
  %1085 : __torch__.transformers.modeling_roberta.___torch_mangle_32006.RobertaIntermediate = prim::GetAttr[name="intermediate"](%93)
  %1086 : __torch__.transformers.modeling_roberta.___torch_mangle_32004.RobertaAttention = prim::GetAttr[name="attention"](%93)
  %1087 : __torch__.transformers.modeling_roberta.___torch_mangle_32003.RobertaSelfOutput = prim::GetAttr[name="output"](%1086)
  %1088 : __torch__.transformers.modeling_roberta.___torch_mangle_31999.RobertaSelfAttention = prim::GetAttr[name="self"](%1086)
  %1089 : __torch__.torch.nn.modules.linear.___torch_mangle_31997.Linear = prim::GetAttr[name="value"](%1088)
  %1090 : __torch__.torch.nn.modules.linear.___torch_mangle_31996.Linear = prim::GetAttr[name="key"](%1088)
  %1091 : __torch__.torch.nn.modules.linear.___torch_mangle_31995.Linear = prim::GetAttr[name="query"](%1088)
  %1092 : Tensor = prim::GetAttr[name="bias"](%1091)
  %1093 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1094 : Float(768:1, 768:768) = aten::t(%1093), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1094), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.67, %1092, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1097 : Tensor = prim::GetAttr[name="bias"](%1090)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1090)
  %1099 : Float(768:1, 768:768) = aten::t(%1098), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1099), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.68, %1097, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1102 : Tensor = prim::GetAttr[name="bias"](%1089)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1089)
  %1104 : Float(768:1, 768:768) = aten::t(%1103), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1104), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.69, %1102, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1107 : int = aten::size(%x.67, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1108 : int = aten::size(%x.67, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1109 : int[] = prim::ListConstruct(%1107, %1108, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.68 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1109), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1111 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %query_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1113 : int = aten::size(%x.69, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1114 : int = aten::size(%x.69, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1115 : int[] = prim::ListConstruct(%1113, %1114, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.70 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1115), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1117 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %key_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1117), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1119 : int = aten::size(%x.71, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1120 : int = aten::size(%x.71, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1121 : int[] = prim::ListConstruct(%1119, %1120, %34, %33), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1123 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %value_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1123), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1125 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %37, %32), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.23 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1125), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:195:0
  %input.116 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:198:0
  %input.117 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %37, %44), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:211:0
  %1132 : int[] = prim::ListConstruct(%52, %46, %51, %45), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %1133 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1132), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1133, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %1135 : int = aten::size(%context_layer, %52), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1136 : int = aten::size(%context_layer, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1137 : int[] = prim::ListConstruct(%1135, %1136, %40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %input.118 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer, %1137), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:215:0
  %1139 : __torch__.torch.nn.modules.normalization.___torch_mangle_32001.LayerNorm = prim::GetAttr[name="LayerNorm"](%1087)
  %1140 : __torch__.torch.nn.modules.linear.___torch_mangle_32000.Linear = prim::GetAttr[name="dense"](%1087)
  %1141 : Tensor = prim::GetAttr[name="bias"](%1140)
  %1142 : Tensor = prim::GetAttr[name="weight"](%1140)
  %1143 : Float(768:1, 768:768) = aten::t(%1142), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.118, %1143), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.70, %1141, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.119, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output # transformers/modeling_roberta.py:232:0
  %1148 : Tensor = prim::GetAttr[name="bias"](%1139)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1139)
  %1150 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1150, %1149, %1148, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1152 : __torch__.torch.nn.modules.linear.___torch_mangle_32005.Linear = prim::GetAttr[name="dense"](%1085)
  %1153 : Tensor = prim::GetAttr[name="bias"](%1152)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1152)
  %1155 : Float(768:1, 3072:768) = aten::t(%1154), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1155), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1153, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1159 : __torch__.torch.nn.modules.normalization.___torch_mangle_32008.LayerNorm = prim::GetAttr[name="LayerNorm"](%1084)
  %1160 : __torch__.torch.nn.modules.linear.___torch_mangle_32007.Linear = prim::GetAttr[name="dense"](%1084)
  %1161 : Tensor = prim::GetAttr[name="bias"](%1160)
  %1162 : Tensor = prim::GetAttr[name="weight"](%1160)
  %1163 : Float(3072:1, 768:3072) = aten::t(%1162), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.122, %1163), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(119:9984, 13:768, 768:1) = aten::add_(%output, %1161, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.24 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.123, %41, %48), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.24, %input_tensor, %51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output # transformers/modeling_roberta.py:311:0
  %1168 : Tensor = prim::GetAttr[name="bias"](%1159)
  %1169 : Tensor = prim::GetAttr[name="weight"](%1159)
  %1170 : int[] = prim::ListConstruct(%40), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm
  %hidden_states : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1170, %1169, %1168, %39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1172 : __torch__.torch.nn.modules.linear.___torch_mangle_32014.Linear = prim::GetAttr[name="dense"](%53)
  %1173 : Float(119:9984, 13:768, 768:1) = aten::slice(%hidden_states, %52, %52, %47, %51), scope: __module.roberta/__module.roberta.pooler # transformers/modeling_roberta.py:450:0
  %input.125 : Float(119:9984, 768:1) = aten::select(%1173, %51, %52), scope: __module.roberta/__module.roberta.pooler # transformers/modeling_roberta.py:450:0
  %1175 : Tensor = prim::GetAttr[name="bias"](%1172)
  %1176 : Tensor = prim::GetAttr[name="weight"](%1172)
  %1177 : Float(768:1, 768:768) = aten::t(%1176), scope: __module.roberta/__module.roberta.pooler/__module.roberta.pooler.dense # torch/nn/functional.py:1674:0
  %input.126 : Float(119:768, 768:1) = aten::addmm(%1175, %input.125, %1177, %51, %51), scope: __module.roberta/__module.roberta.pooler/__module.roberta.pooler.dense # torch/nn/functional.py:1674:0
  %input.127 : Float(119:768, 768:1) = aten::tanh(%input.126), scope: __module.roberta/__module.roberta.pooler/__module.roberta.pooler.activation # torch/nn/modules/activation.py:350:0
  %1180 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1181 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(119:768, 768:1) = aten::dropout(%input.127, %1181, %1180), scope: __module.dropout # torch/nn/functional.py:973:0
  %1183 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
  %1184 : Tensor = prim::GetAttr[name="bias"](%3)
  %1185 : Tensor = prim::GetAttr[name="weight"](%3)
  %1186 : Float(768:1, 1:768) = aten::t(%1185), scope: __module.classifier # torch/nn/functional.py:1674:0
  %logits : Float(119:1, 1:1) = aten::addmm(%1184, %input, %1186, %1183, %1183), scope: __module.classifier # torch/nn/functional.py:1674:0
  %27 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1096:0
  %28 : int[] = prim::ListConstruct(%27, %9)
  %29 : Float(17:7, 7:1) = aten::view(%logits, %28) # transformers/modeling_roberta.py:1096:0
  %30 : (Float(17:7, 7:1)) = prim::TupleConstruct(%29)
  return (%30)
