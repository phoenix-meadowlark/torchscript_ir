XLNetForMultipleChoice(
  (transformer): XLNetModel(
    (word_embedding): Embedding(32000, 1024)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (12): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (13): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (14): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (15): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (16): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (17): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (18): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (19): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (20): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (21): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (22): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (23): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (sequence_summary): SequenceSummary(
    (summary): Linear(in_features=1024, out_features=1024, bias=True)
    (first_dropout): Identity()
    (last_dropout): Dropout(p=0.1, inplace=False)
  )
  (logits_proj): Linear(in_features=1024, out_features=1, bias=True)
)

XLNetForMultipleChoice._actual_script_module
XLNetForMultipleChoice.forward
  graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForMultipleChoice,
        %input_ids.1 : Long(17:91, 7:13, 13:1),
        %token_type_ids.1 : Long(17:91, 7:13, 13:1)):
    %6349 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="logits_proj"](%self.1)
    %6346 : __torch__.transformers.modeling_utils.SequenceSummary = prim::GetAttr[name="sequence_summary"](%self.1)
    %6340 : __torch__.transformers.modeling_xlnet.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
    %919 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1684:0
    %920 : int = aten::size(%input_ids.1, %919) # transformers/modeling_xlnet.py:1684:0
    %num_choices : Long() = prim::NumToTensor(%920)
    %5409 : int = aten::Int(%num_choices)
    %925 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1686:0
    %926 : int = aten::size(%input_ids.1, %925) # transformers/modeling_xlnet.py:1686:0
    %927 : Long() = prim::NumToTensor(%926)
    %928 : int = aten::Int(%927)
    %929 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1686:0
    %930 : int[] = prim::ListConstruct(%929, %928)
    %input_ids.2 : Long(119:13, 13:1) = aten::view(%input_ids.1, %930) # transformers/modeling_xlnet.py:1686:0
    %932 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1687:0
    %933 : int = aten::size(%token_type_ids.1, %932) # transformers/modeling_xlnet.py:1687:0
    %934 : Long() = prim::NumToTensor(%933)
    %935 : int = aten::Int(%934)
    %936 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1687:0
    %937 : int[] = prim::ListConstruct(%936, %935)
    %token_type_ids.2 : Long(119:13, 13:1) = aten::view(%token_type_ids.1, %937) # transformers/modeling_xlnet.py:1687:0
    %6906 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%6340, %input_ids.2, %token_type_ids.2)
    %6879 : Float(119:13312, 13:1024, 1024:1), %6880 : Float(13:121856, 119:1024, 1024:1), %6881 : Float(13:121856, 119:1024, 1024:1), %6882 : Float(13:121856, 119:1024, 1024:1), %6883 : Float(13:121856, 119:1024, 1024:1), %6884 : Float(13:121856, 119:1024, 1024:1), %6885 : Float(13:121856, 119:1024, 1024:1), %6886 : Float(13:121856, 119:1024, 1024:1), %6887 : Float(13:121856, 119:1024, 1024:1), %6888 : Float(13:121856, 119:1024, 1024:1), %6889 : Float(13:121856, 119:1024, 1024:1), %6890 : Float(13:121856, 119:1024, 1024:1), %6891 : Float(13:121856, 119:1024, 1024:1), %6892 : Float(13:121856, 119:1024, 1024:1), %6893 : Float(13:121856, 119:1024, 1024:1), %6894 : Float(13:121856, 119:1024, 1024:1), %6895 : Float(13:121856, 119:1024, 1024:1), %6896 : Float(13:121856, 119:1024, 1024:1), %6897 : Float(13:121856, 119:1024, 1024:1), %6898 : Float(13:121856, 119:1024, 1024:1), %6899 : Float(13:121856, 119:1024, 1024:1), %6900 : Float(13:121856, 119:1024, 1024:1), %6901 : Float(13:121856, 119:1024, 1024:1), %6902 : Float(13:121856, 119:1024, 1024:1), %6903 : Float(13:121856, 119:1024, 1024:1) = prim::TupleUnpack(%6906)
    %6907 : Tensor = prim::CallMethod[name="forward"](%6346, %6879)
    %6908 : Tensor = prim::CallMethod[name="forward"](%6349, %6907)
    %5410 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1716:0
    %5411 : int[] = prim::ListConstruct(%5410, %5409)
    %5412 : Float(17:7, 7:1) = aten::view(%6908, %5411) # transformers/modeling_xlnet.py:1716:0
    %5413 : (Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1)) = prim::TupleConstruct(%6880, %6881, %6882, %6883, %6884, %6885, %6886, %6887, %6888, %6889, %6890, %6891, %6892, %6893, %6894, %6895, %6896, %6897, %6898, %6899, %6900, %6901, %6902, %6903)
    %5414 : (Float(17:7, 7:1), (Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1))) = prim::TupleConstruct(%5412, %5413)
    return (%5414)

XLNetForMultipleChoice.logits_proj
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %7 : Float(119:1024, 1024:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : Float(1024:1, 1:1024) = aten::t(%2), scope: __module.logits_proj # torch/nn/functional.py:1674:0
    %4 : int = prim::Constant[value=1](), scope: __module.logits_proj # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.logits_proj # torch/nn/functional.py:1674:0
    %logits : Float(119:1, 1:1) = aten::addmm(%1, %7, %3, %4, %5), scope: __module.logits_proj # torch/nn/functional.py:1674:0
    return (%logits)

XLNetForMultipleChoice.sequence_summary
SequenceSummary._actual_script_module
  graph(%self.271 : __torch__.transformers.modeling_utils.SequenceSummary,
        %9 : Float(119:13312, 13:1024, 1024:1)):
    %1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="last_dropout"](%self.271)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="summary"](%self.271)
    %3 : __torch__.torch.nn.modules.linear.Identity = prim::GetAttr[name="first_dropout"](%self.271)
    %4 : int = prim::Constant[value=0](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %5 : int = prim::Constant[value=0](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %6 : int = prim::Constant[value=9223372036854775807](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %7 : int = prim::Constant[value=1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %8 : Float(119:13312, 13:1024, 1024:1) = aten::slice(%9, %4, %5, %6, %7), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %10 : int = prim::Constant[value=1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %11 : int = prim::Constant[value=-1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %input.220 : Float(119:13312, 1024:1) = aten::select(%8, %10, %11), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
    %17 : None = prim::CallMethod[name="forward"](%3)
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.220)
    %input.221 : Float(119:1024, 1024:1) = aten::tanh(%18), scope: __module.sequence_summary # transformers/modeling_utils.py:1509:0
    %19 : Tensor = prim::CallMethod[name="forward"](%1, %input.221)
    return (%19)

XLNetForMultipleChoice.transformer
XLNetModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_xlnet.XLNetModel,
        %input_ids.2 : Long(119:13, 13:1),
        %token_type_ids.2 : Long(119:13, 13:1)):
    %1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %2 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="23"](%1)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %4 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="22"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %6 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="21"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %8 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="20"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %10 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="19"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %12 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="18"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %14 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="17"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %16 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="16"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %18 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="15"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %20 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="14"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %22 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="13"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %24 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="12"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %26 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="11"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %28 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="10"](%27)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %30 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="9"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %32 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="8"](%31)
    %33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %34 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="7"](%33)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %36 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="6"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %38 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="5"](%37)
    %39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %40 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="4"](%39)
    %41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %42 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="3"](%41)
    %43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %44 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="2"](%43)
    %45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %46 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="1"](%45)
    %47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %48 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="0"](%47)
    %49 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.2)
    %50 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embedding"](%self.2)
    %51 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %52 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %53 : Long(13:1, 119:13) = aten::transpose(%input_ids.2, %51, %52), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %input_ids : Long(13:119, 119:1) = aten::contiguous(%53, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %57 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %58 : int = aten::size(%input_ids, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %qlen : Long() = prim::NumToTensor(%58), scope: __module.transformer
    %66 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %67 : int = aten::size(%input_ids, %66), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %bsz : Long() = prim::NumToTensor(%67), scope: __module.transformer
    %69 : int = aten::Int(%bsz), scope: __module.transformer
    %70 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
    %71 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
    %72 : Long(13:1, 119:13) = aten::transpose(%token_type_ids.2, %70, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
    %74 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
    %token_type_ids : Long(13:119, 119:1) = aten::contiguous(%72, %74), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
    %76 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %klen.1 : Long() = aten::add(%qlen, %76, %77), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %79 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
    %389 : Tensor = prim::CallMethod[name="forward"](%50, %input_ids)
    %390 : Tensor = prim::CallMethod[name="forward"](%49, %389)
    %82 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %86 : Long(13:119, 119:1) = aten::slice(%token_type_ids, %82, %83, %84, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %87 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %88 : Long(13:119, 1:119, 119:1) = aten::unsqueeze(%86, %87), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %89 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %90 : Long(1:1547, 13:119, 119:1) = aten::unsqueeze(%token_type_ids, %89), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %91 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %92 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %93 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %94 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %95 : Long(1:1547, 13:119, 119:1) = aten::slice(%90, %91, %92, %93, %94), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %96 : Bool(13:1547, 13:119, 119:1) = aten::ne(%88, %95), scope: __module.transformer # torch/tensor.py:22:0
    %97 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %98 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %99 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %100 : None = prim::Constant(), scope: __module.transformer
    %seg_mat : Long(13:1547, 13:119, 119:1) = aten::to(%96, %97, %98, %99, %100), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
    %102 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %103 : Long(13:3094, 13:238, 119:2, 2:1) = aten::one_hot(%seg_mat, %102), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %104 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %105 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %107 : None = prim::Constant(), scope: __module.transformer
    %108 : Float(13:3094, 13:238, 119:2, 2:1) = aten::to(%103, %104, %105, %106, %107), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
    %109 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %110 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %111 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %112 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %113 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %114 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %115 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %freq_seq : Float(512:1) = aten::arange(%109, %110, %111, %112, %113, %114, %115), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %117 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %118 : Float(512:1) = aten::div(%freq_seq, %117), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %119 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %120 : Float(512:1) = aten::pow(%119, %118), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %121 : Float(512:1) = aten::reciprocal(%120), scope: __module.transformer # torch/tensor.py:400:0
    %122 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
    %123 : Float(512:1) = aten::mul(%121, %122), scope: __module.transformer # torch/tensor.py:400:0
    %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
    %125 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
    %126 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %127 : None = prim::Constant(), scope: __module.transformer
    %128 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %129 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %130 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %131 : Float(26:1) = aten::arange(%79, %125, %126, %127, %128, %129, %130), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %132 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
    %133 : Tensor[] = prim::ListConstruct(%131, %123), scope: __module.transformer
    %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%132, %133), scope: __module.transformer # torch/functional.py:327:0
    %135 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %136 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %137 : Tensor[] = prim::ListConstruct(%135, %136), scope: __module.transformer
    %138 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%137, %138), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %140 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %141 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %142 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %143 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %144 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %140, %141, %142, %143), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %145 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %146 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%144, %145), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %147 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %148 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %149 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %150 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%146, %147, %148, %149, %150), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %152 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %153 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %154 : int[] = prim::ListConstruct(%152, %69, %153), scope: __module.transformer
    %155 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %pos_emb : Float(26:1024, 119:0, 1024:1) = aten::expand(%pos_emb.2, %154, %155), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %157 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %158 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %159 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %160 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %161 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %162 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %163 : None = prim::Constant(), scope: __module.transformer
    %input.2 : Float(26:1024, 119:0, 1024:1) = aten::to(%pos_emb, %157, %158, %159, %160, %161, %162, %163), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %391 : Tensor = prim::CallMethod[name="forward1"](%49, %input.2)
    %166 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %167 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %168 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %169 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.1 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%390, %166, %167, %168, %169), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %171 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %392 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%48, %390, %391, %108)
    %173 : Float(13:121856, 119:1024, 1024:1), %174 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%392)
    %175 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %176 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %177 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %178 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.2 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%173, %175, %176, %177, %178), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %180 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %393 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%46, %173, %174, %108)
    %182 : Float(13:121856, 119:1024, 1024:1), %183 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%393)
    %184 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %185 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %186 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %187 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.3 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%182, %184, %185, %186, %187), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %189 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %394 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%44, %182, %183, %108)
    %191 : Float(13:121856, 119:1024, 1024:1), %192 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%394)
    %193 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %194 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %195 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %196 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.4 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%191, %193, %194, %195, %196), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %198 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %395 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%42, %191, %192, %108)
    %200 : Float(13:121856, 119:1024, 1024:1), %201 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%395)
    %202 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %203 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %204 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %205 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.5 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%200, %202, %203, %204, %205), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %207 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %396 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%40, %200, %201, %108)
    %209 : Float(13:121856, 119:1024, 1024:1), %210 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%396)
    %211 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %212 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %213 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %214 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.6 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%209, %211, %212, %213, %214), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %216 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %397 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%38, %209, %210, %108)
    %218 : Float(13:121856, 119:1024, 1024:1), %219 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%397)
    %220 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %221 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %222 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %223 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.7 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%218, %220, %221, %222, %223), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %225 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %398 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%36, %218, %219, %108)
    %227 : Float(13:121856, 119:1024, 1024:1), %228 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%398)
    %229 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %230 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %231 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %232 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.8 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%227, %229, %230, %231, %232), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %234 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %399 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%34, %227, %228, %108)
    %236 : Float(13:121856, 119:1024, 1024:1), %237 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%399)
    %238 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %239 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %240 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %241 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.9 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%236, %238, %239, %240, %241), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %243 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %400 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%32, %236, %237, %108)
    %245 : Float(13:121856, 119:1024, 1024:1), %246 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%400)
    %247 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %248 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %249 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %250 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.10 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%245, %247, %248, %249, %250), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %252 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %401 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%30, %245, %246, %108)
    %254 : Float(13:121856, 119:1024, 1024:1), %255 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%401)
    %256 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %257 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %258 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %259 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.11 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%254, %256, %257, %258, %259), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %261 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %402 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%28, %254, %255, %108)
    %263 : Float(13:121856, 119:1024, 1024:1), %264 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%402)
    %265 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %266 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %267 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %268 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.12 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%263, %265, %266, %267, %268), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %270 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %403 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%26, %263, %264, %108)
    %272 : Float(13:121856, 119:1024, 1024:1), %273 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%403)
    %274 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %275 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %276 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %277 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.13 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%272, %274, %275, %276, %277), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %279 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %404 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%24, %272, %273, %108)
    %281 : Float(13:121856, 119:1024, 1024:1), %282 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%404)
    %283 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %284 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %285 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %286 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.14 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%281, %283, %284, %285, %286), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %288 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %405 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%22, %281, %282, %108)
    %290 : Float(13:121856, 119:1024, 1024:1), %291 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%405)
    %292 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %293 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %294 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %295 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.15 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%290, %292, %293, %294, %295), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %297 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %406 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%20, %290, %291, %108)
    %299 : Float(13:121856, 119:1024, 1024:1), %300 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%406)
    %301 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %302 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %303 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %304 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.16 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%299, %301, %302, %303, %304), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %306 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %407 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%18, %299, %300, %108)
    %308 : Float(13:121856, 119:1024, 1024:1), %309 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%407)
    %310 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %311 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %312 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %313 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.17 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%308, %310, %311, %312, %313), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %315 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %408 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%16, %308, %309, %108)
    %317 : Float(13:121856, 119:1024, 1024:1), %318 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%408)
    %319 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %320 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %321 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %322 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.18 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%317, %319, %320, %321, %322), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %324 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %409 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%14, %317, %318, %108)
    %326 : Float(13:121856, 119:1024, 1024:1), %327 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%409)
    %328 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %329 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %330 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %331 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.19 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%326, %328, %329, %330, %331), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %333 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %410 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %326, %327, %108)
    %335 : Float(13:121856, 119:1024, 1024:1), %336 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%410)
    %337 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %338 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %339 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %340 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.20 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%335, %337, %338, %339, %340), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %342 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %411 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %335, %336, %108)
    %344 : Float(13:121856, 119:1024, 1024:1), %345 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%411)
    %346 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %347 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %348 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %349 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.21 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%344, %346, %347, %348, %349), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %351 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %412 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %344, %345, %108)
    %353 : Float(13:121856, 119:1024, 1024:1), %354 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%412)
    %355 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %356 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %357 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %358 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.22 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%353, %355, %356, %357, %358), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %360 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %413 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %353, %354, %108)
    %362 : Float(13:121856, 119:1024, 1024:1), %363 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%413)
    %364 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %365 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %366 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %367 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.23 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%362, %364, %365, %366, %367), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %369 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %414 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %362, %363, %108)
    %371 : Float(13:121856, 119:1024, 1024:1), %372 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%414)
    %373 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %374 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %375 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %376 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem : Float(13:121856, 119:1024, 1024:1) = aten::slice(%371, %373, %374, %375, %376), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %378 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %415 : Tensor = prim::CallMethod[name="forward"](%2, %371, %372, %108)
    %416 : Tensor = prim::CallMethod[name="forward2"](%49, %415)
    %381 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %382 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %383 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %384 : int[] = prim::ListConstruct(%381, %382, %383), scope: __module.transformer
    %385 : Float(119:1024, 13:121856, 1024:1) = aten::permute(%416, %384), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %386 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %hidden_states : Float(119:13312, 13:1024, 1024:1) = aten::contiguous(%385, %386), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %388 : (Float(119:13312, 13:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1)) = prim::TupleConstruct(%hidden_states, %171, %180, %189, %198, %207, %216, %225, %234, %243, %252, %261, %270, %279, %288, %297, %306, %315, %324, %333, %342, %351, %360, %369, %378)
    return (%388)

XLNetModel.dropout
Dropout._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    %curr_out.1 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    return (%curr_out.1)

XLNetModel.word_embedding
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(13:119, 119:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %3 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %input.1 : Float(13:121856, 119:1024, 1024:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    return (%input.1)

ModuleList.*
Dropout.*
  module had no methods with graph attrs.

XLNetLayer._actual_script_module
  graph(%self.6 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.6)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.6)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.11)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.11)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.11)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.11)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.8 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.8)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %input.11 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.11)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.7)
    %5 : Tensor = prim::GetAttr[name="o"](%self.7)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.7)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.7)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.7)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.7)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.7)
    %11 : Tensor = prim::GetAttr[name="r"](%self.7)
    %12 : Tensor = prim::GetAttr[name="v"](%self.7)
    %13 : Tensor = prim::GetAttr[name="k"](%self.7)
    %14 : Tensor = prim::GetAttr[name="q"](%self.7)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %q_head.1 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %r.2 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.2, %11), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %10, %33), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %ac.1 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %9, %38), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.1 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.1, %52), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.2 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.2), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.1, %56), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.1, %61), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.1, %66), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.1, %71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.2 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %75), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.3 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.4 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %101), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.1 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %108, %107), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %8, %110), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %ef.1 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %118), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.1, %120), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %input.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %124, %125), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.4)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %input.5 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.5)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.6 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.6)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.2)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.4 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %2, %3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.6 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.6, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.8 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %input.9 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.8, %2, %3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    return (%input.9)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.1 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.7 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.1, %2, %6), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.7)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.2 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.10 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.2, %2, %6), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.10)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.11 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.2 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.11, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.2)

XLNetLayer._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.17)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.17)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.22)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.22)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.22)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.22)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.17 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.17)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %input.20 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.20)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.18 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.18)
    %5 : Tensor = prim::GetAttr[name="o"](%self.18)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.18)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.18)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.18)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.18)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.18)
    %11 : Tensor = prim::GetAttr[name="r"](%self.18)
    %12 : Tensor = prim::GetAttr[name="v"](%self.18)
    %13 : Tensor = prim::GetAttr[name="k"](%self.18)
    %14 : Tensor = prim::GetAttr[name="q"](%self.18)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %q_head.2 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %r.3 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.3, %11), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %10, %33), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %ac.2 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %9, %38), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.5 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.2, %52), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.3 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.5, %56), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.5, %61), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.5, %66), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.5, %71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.6 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %75), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.7 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.8 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %101), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.2 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %108, %107), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %8, %110), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %ef.2 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %118), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.2, %120), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %input.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %124, %125), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.13)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %input.14 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.14)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.15 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.15)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.3)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.13 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %2, %3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.15 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.15, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.17 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %input.18 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.17, %2, %3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    return (%input.18)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.4 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.16 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.4, %2, %6), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.16)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.5 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.19 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.5, %2, %6), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.19)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.20 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.3 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.20, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.3)

XLNetLayer._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.28)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.28)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.33 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.33)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.33)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.33)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.33)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.26 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.26)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %input.29 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.29)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.29 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.29)
    %5 : Tensor = prim::GetAttr[name="o"](%self.29)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.29)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.29)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.29)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.29)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.29)
    %11 : Tensor = prim::GetAttr[name="r"](%self.29)
    %12 : Tensor = prim::GetAttr[name="v"](%self.29)
    %13 : Tensor = prim::GetAttr[name="k"](%self.29)
    %14 : Tensor = prim::GetAttr[name="q"](%self.29)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %q_head.3 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %r.4 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.4, %11), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %10, %33), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %ac.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %9, %38), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.9 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.3, %52), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.4 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.9, %56), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.9, %61), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.9, %66), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.9, %71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.10 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %75), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.11 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.12 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %101), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %108, %107), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %8, %110), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %ef.3 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %118), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.3, %120), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %input.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %124, %125), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.22)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %input.23 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.23)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.24 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.4)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.22 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %2, %3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.24 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.24, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.26 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %input.27 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.26, %2, %3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    return (%input.27)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.7 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.25 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.7, %2, %6), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.25)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.8 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.28 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.8, %2, %6), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.28)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.4 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.29, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.4)

XLNetLayer._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.39)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.39)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.44 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.44)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.44)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.44)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.44)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.35 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.35)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %input.38 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.38)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.40 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.40)
    %5 : Tensor = prim::GetAttr[name="o"](%self.40)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.40)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.40)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.40)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.40)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.40)
    %11 : Tensor = prim::GetAttr[name="r"](%self.40)
    %12 : Tensor = prim::GetAttr[name="v"](%self.40)
    %13 : Tensor = prim::GetAttr[name="k"](%self.40)
    %14 : Tensor = prim::GetAttr[name="q"](%self.40)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %q_head.4 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %r.5 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.5, %11), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %10, %33), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %ac.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %9, %38), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.13 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.4, %52), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.5 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.5), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.13, %56), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.13, %61), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.13, %66), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.13, %71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.14 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %75), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.15 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.16 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %101), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %108, %107), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %8, %110), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %ef.4 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %118), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.4, %120), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.30 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %input.31 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %124, %125), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.31)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %input.32 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.32)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.33 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.33)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.5)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.31 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %2, %3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.33, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.35 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %input.36 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.35, %2, %3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    return (%input.36)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.10 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.34 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.10, %2, %6), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.34)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.11 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.37 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.11, %2, %6), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.37)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.38 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.5 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.38, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.5)

XLNetLayer._actual_script_module
  graph(%self.50 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.50)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.50)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.55)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.55)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.55)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.55)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.44 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %input.47 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.47)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.51 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.51)
    %5 : Tensor = prim::GetAttr[name="o"](%self.51)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.51)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.51)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.51)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.51)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.51)
    %11 : Tensor = prim::GetAttr[name="r"](%self.51)
    %12 : Tensor = prim::GetAttr[name="v"](%self.51)
    %13 : Tensor = prim::GetAttr[name="k"](%self.51)
    %14 : Tensor = prim::GetAttr[name="q"](%self.51)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %q_head.5 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %r.6 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.6, %11), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %10, %33), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %ac.5 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %9, %38), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.17 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.5, %52), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.6 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.17, %56), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.17, %61), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.17, %66), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.17, %71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.18 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %75), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.19 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.20 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %101), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.5 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %108, %107), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %8, %110), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %ef.5 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %118), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.5, %120), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.39 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %input.40 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %124, %125), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.40)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %input.41 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.41)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.42 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.42)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.6)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.40 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %2, %3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.42 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.42, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.44 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %input.45 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.44, %2, %3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    return (%input.45)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.13 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.43 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.13, %2, %6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.43)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.14 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.46 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.14, %2, %6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.46)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.47 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.6 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.47, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.6)

XLNetLayer._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.61)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.61)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.66 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.66)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.66)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.66)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.66)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.53 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.53)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %input.56 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.56)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.62)
    %5 : Tensor = prim::GetAttr[name="o"](%self.62)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.62)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.62)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.62)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.62)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.62)
    %11 : Tensor = prim::GetAttr[name="r"](%self.62)
    %12 : Tensor = prim::GetAttr[name="v"](%self.62)
    %13 : Tensor = prim::GetAttr[name="k"](%self.62)
    %14 : Tensor = prim::GetAttr[name="q"](%self.62)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %q_head.6 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %r.7 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.7, %11), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %10, %33), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %ac.6 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %9, %38), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.21 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.6, %52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.7 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.21, %56), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.21, %61), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.21, %66), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.21, %71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.22 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %75), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.23 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.24 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %101), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.6 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %108, %107), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %8, %110), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %ef.6 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %118), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.6, %120), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.48 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %input.49 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %124, %125), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.49)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %input.50 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.50)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.51 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.51)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.7)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.49 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %2, %3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.51 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.51, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.53 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %input.54 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.53, %2, %3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    return (%input.54)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.16 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.52 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.16, %2, %6), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.52)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.17 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.55 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.17, %2, %6), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.55)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.56 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.71)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.71)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.7 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.56, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.7)

XLNetLayer._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.72)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.72)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.77 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.77)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.77)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.77)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.77)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.62 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.62)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %input.65 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.65)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.73)
    %5 : Tensor = prim::GetAttr[name="o"](%self.73)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.73)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.73)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.73)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.73)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.73)
    %11 : Tensor = prim::GetAttr[name="r"](%self.73)
    %12 : Tensor = prim::GetAttr[name="v"](%self.73)
    %13 : Tensor = prim::GetAttr[name="k"](%self.73)
    %14 : Tensor = prim::GetAttr[name="q"](%self.73)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %q_head.7 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %r.8 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.8, %11), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %10, %33), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %ac.7 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %9, %38), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.25 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.7, %52), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.8 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.8), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.25, %56), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.25, %61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.25, %66), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.25, %71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.26 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %75), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.27 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.28 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %101), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.7 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %108, %107), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %8, %110), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %ef.7 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %118), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.7, %120), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.57 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %input.58 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %124, %125), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.58)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %input.59 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.59)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.60 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.60)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.8)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.58 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %2, %3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.60 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.60, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.62 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %input.63 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.62, %2, %3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    return (%input.63)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.19 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.61 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.19, %2, %6), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.61)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.20 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.64 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.20, %2, %6), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.64)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.65 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.8 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.65, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.8)

XLNetLayer._actual_script_module
  graph(%self.83 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.83)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.83)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.88)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.88)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.88)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.88)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.71 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.71)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %input.74 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.74)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.84 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.84)
    %5 : Tensor = prim::GetAttr[name="o"](%self.84)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.84)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.84)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.84)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.84)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.84)
    %11 : Tensor = prim::GetAttr[name="r"](%self.84)
    %12 : Tensor = prim::GetAttr[name="v"](%self.84)
    %13 : Tensor = prim::GetAttr[name="k"](%self.84)
    %14 : Tensor = prim::GetAttr[name="q"](%self.84)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %q_head.8 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %r.9 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.9, %11), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %10, %33), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %ac.8 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %9, %38), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.29 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.8, %52), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.9 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.9), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.29, %56), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.29, %61), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.29, %66), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.29, %71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.30 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %75), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.31 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.32 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %101), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.8 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %108, %107), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %8, %110), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %ef.8 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %118), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.8, %120), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.66 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %input.67 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %124, %125), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.67)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %input.68 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.68)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.69 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.69)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.9)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.67 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %2, %3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.71 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %input.72 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.71, %2, %3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    return (%input.72)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.22 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.70 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.22, %2, %6), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.70)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.23 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.73 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.23, %2, %6), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.73)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.74 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.9 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.74, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.9)

XLNetLayer._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.94)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.94)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.99 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.99)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.99)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.99)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.99)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.80 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.80)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %input.83 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.83)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.95)
    %5 : Tensor = prim::GetAttr[name="o"](%self.95)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.95)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.95)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.95)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.95)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.95)
    %11 : Tensor = prim::GetAttr[name="r"](%self.95)
    %12 : Tensor = prim::GetAttr[name="v"](%self.95)
    %13 : Tensor = prim::GetAttr[name="k"](%self.95)
    %14 : Tensor = prim::GetAttr[name="q"](%self.95)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %q_head.9 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %r.10 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.10, %11), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %10, %33), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %ac.9 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %9, %38), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.33 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.9, %52), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.10 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.10), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.33, %56), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.33, %61), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.33, %66), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.33, %71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.34 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %75), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.35 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.36 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %101), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.9 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %108, %107), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %8, %110), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %ef.9 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %118), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.9, %120), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.75 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %input.76 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %124, %125), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.76)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %input.77 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.77)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.78 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.78)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.10)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.76 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %2, %3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.78 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.78, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.80 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %input.81 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.80, %2, %3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    return (%input.81)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.25 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.79 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.25, %2, %6), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.79)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.26 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.82 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.26, %2, %6), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.82)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.10 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.83, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.10)

XLNetLayer._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.105)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.105)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.110 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.110)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.110)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.110)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.110)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.89 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.89)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %input.92 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.92)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.106 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.106)
    %5 : Tensor = prim::GetAttr[name="o"](%self.106)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.106)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.106)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.106)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.106)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.106)
    %11 : Tensor = prim::GetAttr[name="r"](%self.106)
    %12 : Tensor = prim::GetAttr[name="v"](%self.106)
    %13 : Tensor = prim::GetAttr[name="k"](%self.106)
    %14 : Tensor = prim::GetAttr[name="q"](%self.106)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %q_head.10 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %r.11 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.11, %11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %10, %33), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %ac.10 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %9, %38), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.37 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.10, %52), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.11 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.37, %56), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.37, %61), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.37, %66), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.37, %71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.38 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %75), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.39 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.40 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %101), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.10 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %108, %107), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %8, %110), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %ef.10 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %118), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.10, %120), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.84 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %input.85 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %124, %125), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.85)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %input.86 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.86)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.87 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.87)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.11)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.85 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %2, %3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.87 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.87, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.89 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %input.90 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.89, %2, %3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    return (%input.90)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.111)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.111)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.28 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.88 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.28, %2, %6), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.88)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.29 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.91 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.29, %2, %6), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.91)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.92 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.115)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.11 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.92, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.11)

XLNetLayer._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.116)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.116)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.121)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.121)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.121)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.121)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.98 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.98)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %input.101 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.101)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.117 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.117)
    %5 : Tensor = prim::GetAttr[name="o"](%self.117)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.117)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.117)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.117)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.117)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.117)
    %11 : Tensor = prim::GetAttr[name="r"](%self.117)
    %12 : Tensor = prim::GetAttr[name="v"](%self.117)
    %13 : Tensor = prim::GetAttr[name="k"](%self.117)
    %14 : Tensor = prim::GetAttr[name="q"](%self.117)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %q_head.11 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %r.12 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.12, %11), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %10, %33), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %ac.11 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %9, %38), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.41 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.11, %52), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.12 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.12), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.41, %56), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.41, %61), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.41, %66), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.41, %71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.42 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %75), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.43 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.44 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %101), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.11 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %108, %107), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %8, %110), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %ef.11 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %118), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.11, %120), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.93 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %input.94 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %124, %125), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.94)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %input.95 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.95)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.96 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.96)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.12)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.94 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %2, %3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.96 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.96, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.98 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %input.99 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.98, %2, %3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    return (%input.99)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.122)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.31 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.97 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.31, %2, %6), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.97)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.124)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.32 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.100 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.32, %2, %6), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.100)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.101 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.126)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.126)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.12 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.101, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.12)

XLNetLayer._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.127)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.127)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.132 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.132)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.132)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.132)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.132)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.107 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.107)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %input.110 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.110)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.128)
    %5 : Tensor = prim::GetAttr[name="o"](%self.128)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.128)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.128)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.128)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.128)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.128)
    %11 : Tensor = prim::GetAttr[name="r"](%self.128)
    %12 : Tensor = prim::GetAttr[name="v"](%self.128)
    %13 : Tensor = prim::GetAttr[name="k"](%self.128)
    %14 : Tensor = prim::GetAttr[name="q"](%self.128)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %q_head.12 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %r.13 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.13, %11), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %10, %33), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %ac.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %9, %38), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.45 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.12, %52), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.13 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.13), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.45, %56), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.45, %61), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.45, %66), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.45, %71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.46 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %75), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.47 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.48 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %101), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %108, %107), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %8, %110), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %ef.12 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %118), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.12, %120), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.102 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %input.103 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %124, %125), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.103)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %input.104 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.104)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.105 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.105)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.13)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.103 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %2, %3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.131 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.105 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.131)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.131)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.12 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.105, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.12)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.107 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %input.108 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.107, %2, %3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    return (%input.108)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.34 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.106 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.34, %2, %6), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.106)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.135)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.135)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.35 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.109 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.35, %2, %6), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.109)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.110 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.137)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.137)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.13 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.110, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.13)

XLNetLayer._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.138)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.138)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.143)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.143)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.143)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.143)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.116 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.116)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %input.119 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.119)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.139 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.139)
    %5 : Tensor = prim::GetAttr[name="o"](%self.139)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.139)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.139)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.139)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.139)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.139)
    %11 : Tensor = prim::GetAttr[name="r"](%self.139)
    %12 : Tensor = prim::GetAttr[name="v"](%self.139)
    %13 : Tensor = prim::GetAttr[name="k"](%self.139)
    %14 : Tensor = prim::GetAttr[name="q"](%self.139)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %q_head.13 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %r.14 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.14, %11), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %10, %33), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %ac.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %9, %38), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.49 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.13, %52), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.14 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.49, %56), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.49, %61), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.49, %66), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.49, %71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.50 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %75), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.51 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.52 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %101), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %108, %107), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %8, %110), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %ef.13 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %118), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.13, %120), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.111 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %input.112 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %124, %125), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.112)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %input.113 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.113)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.114 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.14)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.112 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %2, %3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.114 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.13 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.114, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.13)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.116 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %input.117 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.116, %2, %3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    return (%input.117)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.37 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.115 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.37, %2, %6), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.115)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.38 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.118 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.38, %2, %6), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.118)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.14 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.119, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.14)

XLNetLayer._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.149)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.149)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.154)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.154)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.154)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.154)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.125 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.125)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %input.128 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.128)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.150 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.150)
    %5 : Tensor = prim::GetAttr[name="o"](%self.150)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.150)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.150)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.150)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.150)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.150)
    %11 : Tensor = prim::GetAttr[name="r"](%self.150)
    %12 : Tensor = prim::GetAttr[name="v"](%self.150)
    %13 : Tensor = prim::GetAttr[name="k"](%self.150)
    %14 : Tensor = prim::GetAttr[name="q"](%self.150)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %q_head.14 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %r.15 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.15, %11), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %10, %33), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %ac.14 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %9, %38), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.53 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.14, %52), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.15 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.15), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.53, %56), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.53, %61), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.53, %66), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.53, %71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.54 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %75), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.55 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.56 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %101), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.14 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %108, %107), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %8, %110), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %ef.14 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %118), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.14, %120), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.120 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %input.121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %124, %125), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.121)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %input.122 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.122)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.123 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.123)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.15)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.121 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %2, %3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.14 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.14)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.125 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %input.126 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.125, %2, %3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    return (%input.126)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.40 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.124 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.40, %2, %6), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.124)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.41 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.127 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.41, %2, %6), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.127)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.128 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.15 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.128, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.15)

XLNetLayer._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.160)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.160)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.165 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.165)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.165)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.165)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.165)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.134 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.134)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %input.137 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.137)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.161 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.161)
    %5 : Tensor = prim::GetAttr[name="o"](%self.161)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.161)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.161)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.161)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.161)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.161)
    %11 : Tensor = prim::GetAttr[name="r"](%self.161)
    %12 : Tensor = prim::GetAttr[name="v"](%self.161)
    %13 : Tensor = prim::GetAttr[name="k"](%self.161)
    %14 : Tensor = prim::GetAttr[name="q"](%self.161)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %q_head.15 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %r.16 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.16, %11), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %10, %33), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %ac.15 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %9, %38), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.57 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.15, %52), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.16 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.16), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.57, %56), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.57, %61), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.57, %66), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.57, %71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.58 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %75), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.59 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.60 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %101), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.15 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %108, %107), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %8, %110), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %ef.15 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %118), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.15, %120), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.129 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %input.130 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %124, %125), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.130)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %input.131 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.131)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.132 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.132)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.16)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.130 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %2, %3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.164 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.132 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.164)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.15 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.132, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.15)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.134 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %input.135 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.134, %2, %3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    return (%input.135)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.43 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.133 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.43, %2, %6), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.133)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.44 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.136 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.44, %2, %6), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.136)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.137 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.170)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.16 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.137, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.16)

XLNetLayer._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.171)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.171)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.176 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.176)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.176)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.176)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.176)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.143 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.143)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %input.146 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.146)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.172 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.172)
    %5 : Tensor = prim::GetAttr[name="o"](%self.172)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.172)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.172)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.172)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.172)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.172)
    %11 : Tensor = prim::GetAttr[name="r"](%self.172)
    %12 : Tensor = prim::GetAttr[name="v"](%self.172)
    %13 : Tensor = prim::GetAttr[name="k"](%self.172)
    %14 : Tensor = prim::GetAttr[name="q"](%self.172)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %q_head.16 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %r.17 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.17, %11), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %10, %33), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %ac.16 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %9, %38), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.61 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.16, %52), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.17 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.17), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.61, %56), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.61, %61), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.61, %66), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.61, %71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.62 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %75), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.63 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.64 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %101), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.16 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %108, %107), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %8, %110), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %ef.16 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %118), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.16, %120), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.138 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %input.139 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %124, %125), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.139)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %input.140 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.140)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.141 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.141)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.17)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.139 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %2, %3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.175 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.141 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.175)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.175)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.16 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.141, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.16)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.143 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %input.144 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.143, %2, %3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    return (%input.144)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.177)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.177)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.46 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.142 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.46, %2, %6), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.142)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.179 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.179)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.179)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.47 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.145 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.47, %2, %6), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.145)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.181 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.146 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.181)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.181)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.17 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.146, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.17)

XLNetLayer._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.182)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.182)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.187 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.187)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.187)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.187)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.187)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.152 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.152)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %input.155 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.155)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.183 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.183)
    %5 : Tensor = prim::GetAttr[name="o"](%self.183)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.183)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.183)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.183)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.183)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.183)
    %11 : Tensor = prim::GetAttr[name="r"](%self.183)
    %12 : Tensor = prim::GetAttr[name="v"](%self.183)
    %13 : Tensor = prim::GetAttr[name="k"](%self.183)
    %14 : Tensor = prim::GetAttr[name="q"](%self.183)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %q_head.17 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %r.18 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.18, %11), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %10, %33), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %ac.17 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %9, %38), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.65 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.17, %52), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.18 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.18), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.65, %56), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.65, %61), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.65, %66), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.65, %71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.66 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %75), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.67 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.68 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %101), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.17 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %108, %107), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %8, %110), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %ef.17 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %118), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.17, %120), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.147 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %input.148 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %124, %125), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.148)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %input.149 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.149)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.150 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.150)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.18)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.148 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %2, %3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.186 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.150 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.186)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.186)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.17 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.150, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.17)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.152 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %input.153 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.152, %2, %3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    return (%input.153)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.188)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.188)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.49 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.151 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.49, %2, %6), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.151)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.190 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.190)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.190)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.50 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.154 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.50, %2, %6), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.154)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.155 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.192)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.192)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.18 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.155, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.18)

XLNetLayer._actual_script_module
  graph(%self.193 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.193)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.193)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.198 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.198)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.198)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.198)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.198)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.161 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.161)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %input.164 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.164)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.194 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.194)
    %5 : Tensor = prim::GetAttr[name="o"](%self.194)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.194)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.194)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.194)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.194)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.194)
    %11 : Tensor = prim::GetAttr[name="r"](%self.194)
    %12 : Tensor = prim::GetAttr[name="v"](%self.194)
    %13 : Tensor = prim::GetAttr[name="k"](%self.194)
    %14 : Tensor = prim::GetAttr[name="q"](%self.194)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %q_head.18 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %r.19 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.19, %11), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %10, %33), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %ac.18 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %9, %38), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.69 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.18, %52), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.19 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.19), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.69, %56), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.69, %61), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.69, %66), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.69, %71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.70 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %75), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.71 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.72 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %101), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.18 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %108, %107), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %8, %110), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %ef.18 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %118), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.18, %120), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.156 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %input.157 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %124, %125), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.157)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %input.158 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.158)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.159 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.159)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.19)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.157 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %2, %3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.159 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.18 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.159, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.18)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.161 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %input.162 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.161, %2, %3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    return (%input.162)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.199)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.52 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.160 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.52, %2, %6), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.160)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.53 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.163 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.53, %2, %6), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.163)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.164 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.203)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.19 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.164, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.19)

XLNetLayer._actual_script_module
  graph(%self.204 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.204)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.204)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.209)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.209)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.209)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.209)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.170 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.170)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %input.173 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.173)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.205 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.205)
    %5 : Tensor = prim::GetAttr[name="o"](%self.205)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.205)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.205)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.205)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.205)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.205)
    %11 : Tensor = prim::GetAttr[name="r"](%self.205)
    %12 : Tensor = prim::GetAttr[name="v"](%self.205)
    %13 : Tensor = prim::GetAttr[name="k"](%self.205)
    %14 : Tensor = prim::GetAttr[name="q"](%self.205)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %q_head.19 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %r.20 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.20, %11), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %10, %33), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %ac.19 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %9, %38), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.73 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.19, %52), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.20 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.20), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.73, %56), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.73, %61), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.73, %66), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.73, %71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.74 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %75), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.75 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.76 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %101), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.19 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %108, %107), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %8, %110), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %ef.19 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %118), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.19, %120), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.165 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %input.166 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %124, %125), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.166)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %input.167 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.167)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.168 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.168)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.20)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.166 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %2, %3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.208 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.168 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.19 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.168, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.19)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.170 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %input.171 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.170, %2, %3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    return (%input.171)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.210)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.55 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.169 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.55, %2, %6), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.169)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.212)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.56 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.172 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.56, %2, %6), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.172)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.214 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.173 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.20 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.173, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.20)

XLNetLayer._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.215)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.215)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.220 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.220)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.220)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.220)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.220)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.179 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.179)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %input.182 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.182)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.216 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.216)
    %5 : Tensor = prim::GetAttr[name="o"](%self.216)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.216)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.216)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.216)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.216)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.216)
    %11 : Tensor = prim::GetAttr[name="r"](%self.216)
    %12 : Tensor = prim::GetAttr[name="v"](%self.216)
    %13 : Tensor = prim::GetAttr[name="k"](%self.216)
    %14 : Tensor = prim::GetAttr[name="q"](%self.216)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %q_head.20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %r.21 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.21, %11), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %10, %33), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %ac.20 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %9, %38), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.77 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.20, %52), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.21 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.21), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.77, %56), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.77, %61), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.77, %66), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.77, %71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.78 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %75), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.79 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.80 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %101), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.20 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %108, %107), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %8, %110), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %ef.20 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %118), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.20, %120), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.174 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %input.175 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %124, %125), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.175)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %input.176 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.176)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.177 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.177)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.21)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.175 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %2, %3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.177 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.219)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.219)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.20 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.177, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.20)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.179 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %input.180 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.179, %2, %3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    return (%input.180)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.221)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.221)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.58 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.178 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.58, %2, %6), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.178)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.223 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.223)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.223)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.59 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.181 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.59, %2, %6), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.181)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.225 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.182 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.225)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.21 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.182, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.21)

XLNetLayer._actual_script_module
  graph(%self.226 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.226)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.226)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.231 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.231)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.231)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.231)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.231)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.188 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.188)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %input.191 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.191)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.227 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.227)
    %5 : Tensor = prim::GetAttr[name="o"](%self.227)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.227)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.227)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.227)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.227)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.227)
    %11 : Tensor = prim::GetAttr[name="r"](%self.227)
    %12 : Tensor = prim::GetAttr[name="v"](%self.227)
    %13 : Tensor = prim::GetAttr[name="k"](%self.227)
    %14 : Tensor = prim::GetAttr[name="q"](%self.227)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %q_head.21 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %r.22 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.22, %11), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %10, %33), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %ac.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %9, %38), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.81 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.21, %52), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.22 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.22), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.81, %56), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.81, %61), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.81, %66), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.81, %71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.82 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %75), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.83 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.84 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %101), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %108, %107), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %8, %110), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %ef.21 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %118), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.21, %120), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.183 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %input.184 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %124, %125), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.184)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %input.185 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.185)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.186 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.186)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.22)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.228 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.184 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %2, %3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.230 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.186 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.230)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.230)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.21 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.186, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.21)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.188 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %input.189 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.188, %2, %3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    return (%input.189)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.232 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.232)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.232)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.61 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.187 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.61, %2, %6), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.187)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.234 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.234)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.234)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.62 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.190 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.62, %2, %6), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.190)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.191 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.236)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.236)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.22 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.191, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.22)

XLNetLayer._actual_script_module
  graph(%self.237 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.237)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.237)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.242 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.242)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.242)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.242)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.242)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.197 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.197)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %input.200 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.200)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.238 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.238)
    %5 : Tensor = prim::GetAttr[name="o"](%self.238)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.238)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.238)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.238)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.238)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.238)
    %11 : Tensor = prim::GetAttr[name="r"](%self.238)
    %12 : Tensor = prim::GetAttr[name="v"](%self.238)
    %13 : Tensor = prim::GetAttr[name="k"](%self.238)
    %14 : Tensor = prim::GetAttr[name="q"](%self.238)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %q_head.22 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %r.23 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r.23, %11), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %10, %33), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %ac.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %9, %38), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.85 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.22, %52), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.23 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.23), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.85, %56), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.85, %61), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.85, %66), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.85, %71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %75), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.87 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.88 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %101), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %108, %107), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %8, %110), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %ef.22 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %118), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.22, %120), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.192 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %input.193 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %124, %125), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.193)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %input.194 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.194)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.195 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.195)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r.23)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.239 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.193 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %2, %3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.241 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.195 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.241)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.241)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.22 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.195, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.22)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.244 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.197 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %input.198 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.197, %2, %3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    return (%input.198)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.243 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.243)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.243)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.64 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.196 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.64, %2, %6), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.196)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.245)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.245)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.65 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.199 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.65, %2, %6), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.199)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.247 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.200 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.247)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.247)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.23 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.200, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.23)

XLNetLayer._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.248)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.248)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:121856, 119:1024, 1024:1), %8 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.253 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.253)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.253)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.253)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.253)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.206 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.206)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %input.209 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.209)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.249 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.249)
    %5 : Tensor = prim::GetAttr[name="o"](%self.249)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.249)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.249)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.249)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.249)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.249)
    %11 : Tensor = prim::GetAttr[name="r"](%self.249)
    %12 : Tensor = prim::GetAttr[name="v"](%self.249)
    %13 : Tensor = prim::GetAttr[name="k"](%self.249)
    %14 : Tensor = prim::GetAttr[name="q"](%self.249)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %q_head.23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %r : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%r, %11), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %10, %33), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %ac.23 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %9, %38), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.89 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac.23, %52), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.24 : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen.24), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.89, %56), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.89, %61), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.89, %66), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.89, %71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.90 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %75), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.92 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %101), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.23 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %108, %107), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %8, %110), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %ef.23 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %118), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef.23, %120), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.201 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %input.202 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %124, %125), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %139 : Tensor = prim::CallMethod[name="forward"](%6, %input.202)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%139, %23), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %input.203 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %140 : Tensor = prim::CallMethod[name="forward1"](%6, %input.203)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.204 : Float(13:121856, 119:1024, 1024:1) = aten::add(%140, %1, %135), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %141 : Tensor = prim::CallMethod[name="forward"](%4, %input.204)
    %138 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%141, %r)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.250 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.202 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %2, %3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.252 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.204 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.252)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.252)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.23 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.204, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.23)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.206 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %input.207 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.206, %2, %3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    return (%input.207)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.254 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.254)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.254)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.67 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.205 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.67, %2, %6), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.205)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.256 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.256)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.256)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.68 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.208 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.68, %2, %6), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.208)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.258 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.209 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.258)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.258)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.209, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out)

XLNetLayer._actual_script_module
  graph(%self.259 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.259)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.259)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %29)
    return (%30)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.264 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.264)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.264)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.264)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.264)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.215 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.215)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %input.218 : Float(13:121856, 119:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.218)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.260 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:121856, 119:1024, 1024:1),
        %2 : Float(26:1024, 119:0, 1024:1),
        %3 : Float(13:3094, 13:238, 119:2, 2:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.260)
    %5 : Tensor = prim::GetAttr[name="o"](%self.260)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.260)
    %7 : Tensor = prim::GetAttr[name="seg_embed"](%self.260)
    %8 : Tensor = prim::GetAttr[name="r_s_bias"](%self.260)
    %9 : Tensor = prim::GetAttr[name="r_r_bias"](%self.260)
    %10 : Tensor = prim::GetAttr[name="r_w_bias"](%self.260)
    %11 : Tensor = prim::GetAttr[name="r"](%self.260)
    %12 : Tensor = prim::GetAttr[name="v"](%self.260)
    %13 : Tensor = prim::GetAttr[name="k"](%self.260)
    %14 : Tensor = prim::GetAttr[name="q"](%self.260)
    %15 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %16 : Tensor[] = prim::ListConstruct(%1, %14), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %q_head : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%15, %16), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %18 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %19 : Tensor[] = prim::ListConstruct(%1, %13), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%18, %19), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %21 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %22 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%21, %22), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %27 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %29 : Float(26:1024, 119:0, 1024:1) = aten::to(%2, %24, %25, %26, %27, %28), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %30 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %31 : Tensor[] = prim::ListConstruct(%29, %11), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %32 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%30, %31), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %33 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %34 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %10, %33), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %35 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %36 : Tensor[] = prim::ListConstruct(%34, %20), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %ac : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%35, %36), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %38 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %39 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %9, %38), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %40 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %41 : Tensor[] = prim::ListConstruct(%39, %32), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x.93 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%40, %41), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %53 : int = aten::size(%ac, %52), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen : Long() = prim::NumToTensor(%53), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %55 : Scalar = aten::ScalarImplicit(%klen), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %56 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %57 : int = aten::size(%x.93, %56), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %59 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %60 : int = aten::Int(%58), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %61 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %62 : int = aten::size(%x.93, %61), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %63 : Long() = prim::NumToTensor(%62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %64 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %65 : int = aten::Int(%63), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %66 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %67 : int = aten::size(%x.93, %66), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %68 : Long() = prim::NumToTensor(%67), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %69 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %70 : int = aten::Int(%68), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %71 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %72 : int = aten::size(%x.93, %71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %73 : Long() = prim::NumToTensor(%72), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %74 : int = aten::Int(%73), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %75 : int[] = prim::ListConstruct(%60, %65, %74, %70), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x.94 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %75), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
    %77 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %77, %78, %79, %80), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%81, %82, %83, %84, %85), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.95 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %97 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %99 : Long() = aten::sub(%73, %97, %98), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %100 : int = aten::Int(%99), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %101 : int[] = prim::ListConstruct(%59, %64, %69, %100), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %101), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %107 : Long(13:1) = aten::arange(%55, %103, %104, %105, %106), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %108, %107), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %110 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:294:0
    %111 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %8, %110), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:294:0
    %112 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %113 : Tensor[] = prim::ListConstruct(%111, %7), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %114 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%112, %113), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %115 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3, %114), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %ef : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %118 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %119 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %118), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%119, %ef, %120), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %122 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %input.210 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%121, %122), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %124 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %125 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %input.211 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %124, %125), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %138 : Tensor = prim::CallMethod[name="forward"](%6, %input.211)
    %128 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %129 : Tensor[] = prim::ListConstruct(%138, %23), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %130 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%128, %129), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %131 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %132 : Tensor[] = prim::ListConstruct(%130, %5), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %input.212 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%131, %132), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %139 : Tensor = prim::CallMethod[name="forward1"](%6, %input.212)
    %135 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.213 : Float(13:121856, 119:1024, 1024:1) = aten::add(%139, %1, %135), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %140 : Tensor = prim::CallMethod[name="forward"](%4, %input.213)
    return (%140)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.261 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.211 : Float(119:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %2, %3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.263 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.213 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.263)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.263)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.213, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.266 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.215 : Float(13:487424, 119:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %input.216 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.215, %2, %3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    return (%input.216)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.265 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.265)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.265)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.70 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.214 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.70, %2, %6), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.214)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.267 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:487424, 119:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.267)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.267)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.71 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.217 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.71, %2, %6), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.217)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.269 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.218 : Float(13:121856, 119:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.269)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.269)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %input.219 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.218, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%input.219)

SequenceSummary.first_dropout
Identity._actual_script_module
  graph(%self.272 : __torch__.torch.nn.modules.linear.Identity):
    %1 : None = prim::Constant()
    return (%1)

SequenceSummary.last_dropout
Dropout._actual_script_module
  graph(%self.274 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.221 : Float(119:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
    %input : Float(119:1024, 1024:1) = aten::dropout(%input.221, %2, %3), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
    return (%input)

SequenceSummary.summary
Linear._actual_script_module
  graph(%self.273 : __torch__.torch.nn.modules.linear.Linear,
        %input.220 : Float(119:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.273)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.273)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
    %6 : int = prim::Constant[value=1](), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
    %output : Float(119:1024, 1024:1) = aten::addmm(%2, %input.220, %4, %5, %6), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
    return (%output)

