graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForMultipleChoice,
      %input_ids.1 : Long(17:91, 7:13, 13:1),
      %token_type_ids.1 : Long(17:91, 7:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_40479.Linear = prim::GetAttr[name="logits_proj"](%self.1)
  %4 : __torch__.transformers.modeling_utils.___torch_mangle_40478.SequenceSummary = prim::GetAttr[name="sequence_summary"](%self.1)
  %5 : __torch__.transformers.modeling_xlnet.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
  %6 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1684:0
  %7 : int = aten::size(%input_ids.1, %6) # transformers/modeling_xlnet.py:1684:0
  %num_choices : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%num_choices)
  %10 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1686:0
  %11 : int = aten::size(%input_ids.1, %10) # transformers/modeling_xlnet.py:1686:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1686:0
  %15 : int[] = prim::ListConstruct(%14, %13)
  %input_ids.2 : Long(119:13, 13:1) = aten::view(%input_ids.1, %15) # transformers/modeling_xlnet.py:1686:0
  %17 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1687:0
  %18 : int = aten::size(%token_type_ids.1, %17) # transformers/modeling_xlnet.py:1687:0
  %19 : Long() = prim::NumToTensor(%18)
  %20 : int = aten::Int(%19)
  %21 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1687:0
  %22 : int[] = prim::ListConstruct(%21, %20)
  %token_type_ids.2 : Long(119:13, 13:1) = aten::view(%token_type_ids.1, %22) # transformers/modeling_xlnet.py:1687:0
  %57 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %58 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %59 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %60 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %61 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %62 : str = prim::Constant[value="ijbs,ibns->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %63 : str = prim::Constant[value="ibnd,snd->ibns"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %64 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %65 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %66 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %67 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
  %68 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %69 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
  %70 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %71 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %72 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %73 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %74 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %75 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
  %76 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
  %77 : None = prim::Constant(), scope: __module.transformer
  %78 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %80 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %81 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %82 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %83 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %84 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %85 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %87 : __torch__.transformers.modeling_xlnet.___torch_mangle_40472.XLNetLayer = prim::GetAttr[name="23"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %89 : __torch__.transformers.modeling_xlnet.___torch_mangle_40462.XLNetLayer = prim::GetAttr[name="22"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %91 : __torch__.transformers.modeling_xlnet.___torch_mangle_40452.XLNetLayer = prim::GetAttr[name="21"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %93 : __torch__.transformers.modeling_xlnet.___torch_mangle_40442.XLNetLayer = prim::GetAttr[name="20"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %95 : __torch__.transformers.modeling_xlnet.___torch_mangle_40432.XLNetLayer = prim::GetAttr[name="19"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %97 : __torch__.transformers.modeling_xlnet.___torch_mangle_40422.XLNetLayer = prim::GetAttr[name="18"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %99 : __torch__.transformers.modeling_xlnet.___torch_mangle_40412.XLNetLayer = prim::GetAttr[name="17"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %101 : __torch__.transformers.modeling_xlnet.___torch_mangle_40402.XLNetLayer = prim::GetAttr[name="16"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %103 : __torch__.transformers.modeling_xlnet.___torch_mangle_40392.XLNetLayer = prim::GetAttr[name="15"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %105 : __torch__.transformers.modeling_xlnet.___torch_mangle_40382.XLNetLayer = prim::GetAttr[name="14"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %107 : __torch__.transformers.modeling_xlnet.___torch_mangle_40372.XLNetLayer = prim::GetAttr[name="13"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %109 : __torch__.transformers.modeling_xlnet.___torch_mangle_40362.XLNetLayer = prim::GetAttr[name="12"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %111 : __torch__.transformers.modeling_xlnet.___torch_mangle_40352.XLNetLayer = prim::GetAttr[name="11"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %113 : __torch__.transformers.modeling_xlnet.___torch_mangle_40342.XLNetLayer = prim::GetAttr[name="10"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %115 : __torch__.transformers.modeling_xlnet.___torch_mangle_40332.XLNetLayer = prim::GetAttr[name="9"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %117 : __torch__.transformers.modeling_xlnet.___torch_mangle_40322.XLNetLayer = prim::GetAttr[name="8"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %119 : __torch__.transformers.modeling_xlnet.___torch_mangle_40312.XLNetLayer = prim::GetAttr[name="7"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %121 : __torch__.transformers.modeling_xlnet.___torch_mangle_40302.XLNetLayer = prim::GetAttr[name="6"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %123 : __torch__.transformers.modeling_xlnet.___torch_mangle_40292.XLNetLayer = prim::GetAttr[name="5"](%122)
  %124 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %125 : __torch__.transformers.modeling_xlnet.___torch_mangle_40282.XLNetLayer = prim::GetAttr[name="4"](%124)
  %126 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %127 : __torch__.transformers.modeling_xlnet.___torch_mangle_40272.XLNetLayer = prim::GetAttr[name="3"](%126)
  %128 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %129 : __torch__.transformers.modeling_xlnet.___torch_mangle_40262.XLNetLayer = prim::GetAttr[name="2"](%128)
  %130 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %131 : __torch__.transformers.modeling_xlnet.___torch_mangle_40252.XLNetLayer = prim::GetAttr[name="1"](%130)
  %132 : __torch__.torch.nn.modules.container.___torch_mangle_40473.ModuleList = prim::GetAttr[name="layer"](%5)
  %133 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="0"](%132)
  %134 : __torch__.torch.nn.modules.sparse.___torch_mangle_40235.Embedding = prim::GetAttr[name="word_embedding"](%5)
  %135 : Long(13:1, 119:13) = aten::transpose(%input_ids.2, %85, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:119, 119:1) = aten::contiguous(%135, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %137 : int = aten::size(%input_ids, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%137), scope: __module.transformer
  %139 : int = aten::size(%input_ids, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %140 : Long(13:1, 119:13) = aten::transpose(%token_type_ids.2, %85, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
  %token_type_ids : Long(13:119, 119:1) = aten::contiguous(%140, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1109:0
  %klen.1 : Long() = aten::add(%qlen, %83, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %143 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
  %144 : Tensor = prim::GetAttr[name="weight"](%134)
  %input.1 : Float(13:121856, 119:1024, 1024:1) = aten::embedding(%144, %input_ids, %81, %82, %82), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %curr_out.1 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.1, %80, %82), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %147 : Long(13:119, 119:1) = aten::slice(%token_type_ids, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %148 : Long(13:119, 1:119, 119:1) = aten::unsqueeze(%147, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %149 : Long(1:1547, 13:119, 119:1) = aten::unsqueeze(%token_type_ids, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %150 : Long(1:1547, 13:119, 119:1) = aten::slice(%149, %84, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %151 : Bool(13:1547, 13:119, 119:1) = aten::ne(%148, %150), scope: __module.transformer # torch/tensor.py:22:0
  %seg_mat : Long(13:1547, 13:119, 119:1) = aten::to(%151, %78, %82, %82, %77), scope: __module.transformer # transformers/modeling_xlnet.py:1191:0
  %153 : Long(13:3094, 13:238, 119:2, 2:1) = aten::one_hot(%seg_mat, %76), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
  %154 : Float(13:3094, 13:238, 119:2, 2:1) = aten::to(%153, %75, %82, %82, %77), scope: __module.transformer # transformers/modeling_xlnet.py:1192:0
  %freq_seq : Float(512:1) = aten::arange(%85, %74, %73, %75, %85, %72, %82), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %156 : Float(512:1) = aten::div(%freq_seq, %71), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %157 : Float(512:1) = aten::pow(%70, %156), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %158 : Float(512:1) = aten::reciprocal(%157), scope: __module.transformer # torch/tensor.py:400:0
  %159 : Float(512:1) = aten::mul(%158, %69), scope: __module.transformer # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
  %161 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
  %162 : Float(26:1) = aten::arange(%143, %161, %68, %77, %85, %72, %82), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %163 : Tensor[] = prim::ListConstruct(%162, %159), scope: __module.transformer
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%67, %163), scope: __module.transformer # torch/functional.py:327:0
  %165 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %166 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %167 : Tensor[] = prim::ListConstruct(%165, %166), scope: __module.transformer
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%167, %81), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %169 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %170 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%169, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%170, %76, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %172 : int[] = prim::ListConstruct(%81, %139, %81), scope: __module.transformer
  %pos_emb : Float(26:1024, 119:0, 1024:1) = aten::expand(%pos_emb.2, %172, %82), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
  %input.2 : Float(26:1024, 119:0, 1024:1) = aten::to(%pos_emb, %75, %85, %72, %82, %82, %82, %77), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
  %r.1 : Float(26:1024, 119:0, 1024:1) = aten::dropout(%input.2, %80, %82), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %new_mem.1 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%curr_out.1, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %177 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %178 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%133)
  %179 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%133)
  %180 : __torch__.torch.nn.modules.normalization.___torch_mangle_40236.LayerNorm = prim::GetAttr[name="layer_norm"](%179)
  %181 : Tensor = prim::GetAttr[name="o"](%179)
  %182 : Tensor = prim::GetAttr[name="seg_embed"](%179)
  %183 : Tensor = prim::GetAttr[name="r_s_bias"](%179)
  %184 : Tensor = prim::GetAttr[name="r_r_bias"](%179)
  %185 : Tensor = prim::GetAttr[name="r_w_bias"](%179)
  %186 : Tensor = prim::GetAttr[name="r"](%179)
  %187 : Tensor = prim::GetAttr[name="v"](%179)
  %188 : Tensor = prim::GetAttr[name="k"](%179)
  %189 : Tensor = prim::GetAttr[name="q"](%179)
  %190 : Tensor[] = prim::ListConstruct(%curr_out.1, %189), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %q_head.1 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %190), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %192 : Tensor[] = prim::ListConstruct(%curr_out.1, %188), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %193 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %192), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %194 : Tensor[] = prim::ListConstruct(%curr_out.1, %187), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %195 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %194), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 119:0, 1024:1) = aten::to(%r.1, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %197 : Tensor[] = prim::ListConstruct(%r.2, %186), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %198 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %197), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %199 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %185, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %200 : Tensor[] = prim::ListConstruct(%199, %193), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ac.1 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %200), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %202 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %184, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %203 : Tensor[] = prim::ListConstruct(%202, %198), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.1 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %203), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %205 : int = aten::size(%ac.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %206 : int = aten::size(%x.1, %85), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %207 : int = aten::size(%x.1, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %208 : int = aten::size(%x.1, %76), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %209 : int = aten::size(%x.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %210 : Long() = prim::NumToTensor(%209), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %211 : int[] = prim::ListConstruct(%206, %207, %209, %208), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.2 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %211), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %213 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %214 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%213, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %215 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%214, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%215, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %217 : Long() = aten::sub(%210, %69, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %218 : int = aten::Int(%217), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %219 : int[] = prim::ListConstruct(%206, %207, %208, %218), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.4 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %219), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %221 : Long(13:1) = aten::arange(%205, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %64, %221), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %223 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.1, %183, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:294:0
  %224 : Tensor[] = prim::ListConstruct(%223, %182), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %225 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %224), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %226 : Tensor[] = prim::ListConstruct(%154, %225), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ef.1 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %226), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %228 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %229 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%228, %ef.1, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%229, %61), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %64, %77), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %232 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %80, %82), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %233 : Tensor[] = prim::ListConstruct(%232, %195), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %234 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %233), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %235 : Tensor[] = prim::ListConstruct(%234, %181), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %input.5 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %235), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.5, %80, %82), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %239 : Tensor = prim::GetAttr[name="bias"](%180)
  %240 : Tensor = prim::GetAttr[name="weight"](%180)
  %241 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.6, %241, %240, %239, %57, %58), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %243 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %244 : Float(13:121856, 119:1024, 1024:1), %245 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%243)
  %246 : __torch__.torch.nn.modules.normalization.___torch_mangle_40238.LayerNorm = prim::GetAttr[name="layer_norm"](%178)
  %247 : __torch__.torch.nn.modules.linear.___torch_mangle_40240.Linear = prim::GetAttr[name="layer_2"](%178)
  %248 : __torch__.torch.nn.modules.linear.___torch_mangle_40239.Linear = prim::GetAttr[name="layer_1"](%178)
  %249 : Tensor = prim::GetAttr[name="bias"](%248)
  %250 : Tensor = prim::GetAttr[name="weight"](%248)
  %251 : Float(1024:1, 4096:1024) = aten::t(%250), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%244, %251), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.1, %249, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.8, %80, %82), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %256 : Tensor = prim::GetAttr[name="bias"](%247)
  %257 : Tensor = prim::GetAttr[name="weight"](%247)
  %258 : Float(4096:1, 1024:4096) = aten::t(%257), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.9, %258), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.2, %256, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.10, %80, %82), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.3, %244, %84), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %263 : Tensor = prim::GetAttr[name="bias"](%246)
  %264 : Tensor = prim::GetAttr[name="weight"](%246)
  %265 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.11, %265, %264, %263, %57, %58), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %267 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %245)
  %268 : Float(13:121856, 119:1024, 1024:1), %269 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%267)
  %new_mem.2 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%268, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %271 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %272 : __torch__.transformers.modeling_xlnet.___torch_mangle_40250.XLNetFeedForward = prim::GetAttr[name="ff"](%131)
  %273 : __torch__.transformers.modeling_xlnet.___torch_mangle_40245.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%131)
  %274 : __torch__.torch.nn.modules.normalization.___torch_mangle_40243.LayerNorm = prim::GetAttr[name="layer_norm"](%273)
  %275 : Tensor = prim::GetAttr[name="o"](%273)
  %276 : Tensor = prim::GetAttr[name="seg_embed"](%273)
  %277 : Tensor = prim::GetAttr[name="r_s_bias"](%273)
  %278 : Tensor = prim::GetAttr[name="r_r_bias"](%273)
  %279 : Tensor = prim::GetAttr[name="r_w_bias"](%273)
  %280 : Tensor = prim::GetAttr[name="r"](%273)
  %281 : Tensor = prim::GetAttr[name="v"](%273)
  %282 : Tensor = prim::GetAttr[name="k"](%273)
  %283 : Tensor = prim::GetAttr[name="q"](%273)
  %284 : Tensor[] = prim::ListConstruct(%268, %283), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %q_head.2 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %284), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %286 : Tensor[] = prim::ListConstruct(%268, %282), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %287 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %286), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %288 : Tensor[] = prim::ListConstruct(%268, %281), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %289 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %288), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 119:0, 1024:1) = aten::to(%269, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %291 : Tensor[] = prim::ListConstruct(%r.3, %280), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %292 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %291), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %293 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %279, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %294 : Tensor[] = prim::ListConstruct(%293, %287), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ac.2 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %294), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %296 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %278, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %297 : Tensor[] = prim::ListConstruct(%296, %292), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.5 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %297), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %299 : int = aten::size(%ac.2, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %300 : int = aten::size(%x.5, %85), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %301 : int = aten::size(%x.5, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %302 : int = aten::size(%x.5, %76), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %303 : int = aten::size(%x.5, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %304 : Long() = prim::NumToTensor(%303), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %305 : int[] = prim::ListConstruct(%300, %301, %303, %302), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.6 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %305), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %307 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %308 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%307, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %309 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%308, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%309, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %311 : Long() = aten::sub(%304, %69, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %312 : int = aten::Int(%311), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %313 : int[] = prim::ListConstruct(%300, %301, %302, %312), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.8 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %313), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %315 : Long(13:1) = aten::arange(%299, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %64, %315), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %317 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.2, %277, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:294:0
  %318 : Tensor[] = prim::ListConstruct(%317, %276), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %319 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %318), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %320 : Tensor[] = prim::ListConstruct(%154, %319), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ef.2 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %320), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %322 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %323 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%322, %ef.2, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%323, %61), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %64, %77), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %326 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %80, %82), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %327 : Tensor[] = prim::ListConstruct(%326, %289), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %328 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %327), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %329 : Tensor[] = prim::ListConstruct(%328, %275), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %input.14 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %329), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.14, %80, %82), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.2, %268, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %333 : Tensor = prim::GetAttr[name="bias"](%274)
  %334 : Tensor = prim::GetAttr[name="weight"](%274)
  %335 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.15, %335, %334, %333, %57, %58), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %337 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %338 : Float(13:121856, 119:1024, 1024:1), %339 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%337)
  %340 : __torch__.torch.nn.modules.normalization.___torch_mangle_40246.LayerNorm = prim::GetAttr[name="layer_norm"](%272)
  %341 : __torch__.torch.nn.modules.linear.___torch_mangle_40248.Linear = prim::GetAttr[name="layer_2"](%272)
  %342 : __torch__.torch.nn.modules.linear.___torch_mangle_40247.Linear = prim::GetAttr[name="layer_1"](%272)
  %343 : Tensor = prim::GetAttr[name="bias"](%342)
  %344 : Tensor = prim::GetAttr[name="weight"](%342)
  %345 : Float(1024:1, 4096:1024) = aten::t(%344), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%338, %345), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.4, %343, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.17, %80, %82), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %350 : Tensor = prim::GetAttr[name="bias"](%341)
  %351 : Tensor = prim::GetAttr[name="weight"](%341)
  %352 : Float(4096:1, 1024:4096) = aten::t(%351), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.18, %352), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.5, %350, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.19, %80, %82), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.6, %338, %84), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %357 : Tensor = prim::GetAttr[name="bias"](%340)
  %358 : Tensor = prim::GetAttr[name="weight"](%340)
  %359 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.20, %359, %358, %357, %57, %58), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %361 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %339)
  %362 : Float(13:121856, 119:1024, 1024:1), %363 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%361)
  %new_mem.3 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%362, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %365 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %366 : __torch__.transformers.modeling_xlnet.___torch_mangle_40260.XLNetFeedForward = prim::GetAttr[name="ff"](%129)
  %367 : __torch__.transformers.modeling_xlnet.___torch_mangle_40255.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%129)
  %368 : __torch__.torch.nn.modules.normalization.___torch_mangle_40253.LayerNorm = prim::GetAttr[name="layer_norm"](%367)
  %369 : Tensor = prim::GetAttr[name="o"](%367)
  %370 : Tensor = prim::GetAttr[name="seg_embed"](%367)
  %371 : Tensor = prim::GetAttr[name="r_s_bias"](%367)
  %372 : Tensor = prim::GetAttr[name="r_r_bias"](%367)
  %373 : Tensor = prim::GetAttr[name="r_w_bias"](%367)
  %374 : Tensor = prim::GetAttr[name="r"](%367)
  %375 : Tensor = prim::GetAttr[name="v"](%367)
  %376 : Tensor = prim::GetAttr[name="k"](%367)
  %377 : Tensor = prim::GetAttr[name="q"](%367)
  %378 : Tensor[] = prim::ListConstruct(%362, %377), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %q_head.3 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %378), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %380 : Tensor[] = prim::ListConstruct(%362, %376), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %381 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %380), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %382 : Tensor[] = prim::ListConstruct(%362, %375), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %383 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %382), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 119:0, 1024:1) = aten::to(%363, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %385 : Tensor[] = prim::ListConstruct(%r.4, %374), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %386 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %385), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %387 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %373, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %388 : Tensor[] = prim::ListConstruct(%387, %381), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ac.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %388), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %390 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %372, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %391 : Tensor[] = prim::ListConstruct(%390, %386), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.9 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %391), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %393 : int = aten::size(%ac.3, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %394 : int = aten::size(%x.9, %85), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %395 : int = aten::size(%x.9, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %396 : int = aten::size(%x.9, %76), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %397 : int = aten::size(%x.9, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %398 : Long() = prim::NumToTensor(%397), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %399 : int[] = prim::ListConstruct(%394, %395, %397, %396), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.10 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %399), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %401 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %402 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%401, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %403 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%402, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%403, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %405 : Long() = aten::sub(%398, %69, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %406 : int = aten::Int(%405), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %407 : int[] = prim::ListConstruct(%394, %395, %396, %406), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.12 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %407), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %409 : Long(13:1) = aten::arange(%393, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %64, %409), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %411 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.3, %371, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:294:0
  %412 : Tensor[] = prim::ListConstruct(%411, %370), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %413 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %412), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %414 : Tensor[] = prim::ListConstruct(%154, %413), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ef.3 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %414), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %416 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %417 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%416, %ef.3, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%417, %61), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %64, %77), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %420 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %80, %82), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %421 : Tensor[] = prim::ListConstruct(%420, %383), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %422 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %421), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %423 : Tensor[] = prim::ListConstruct(%422, %369), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %input.23 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %423), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.23, %80, %82), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.3, %362, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %427 : Tensor = prim::GetAttr[name="bias"](%368)
  %428 : Tensor = prim::GetAttr[name="weight"](%368)
  %429 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.24, %429, %428, %427, %57, %58), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %431 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %432 : Float(13:121856, 119:1024, 1024:1), %433 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%431)
  %434 : __torch__.torch.nn.modules.normalization.___torch_mangle_40256.LayerNorm = prim::GetAttr[name="layer_norm"](%366)
  %435 : __torch__.torch.nn.modules.linear.___torch_mangle_40258.Linear = prim::GetAttr[name="layer_2"](%366)
  %436 : __torch__.torch.nn.modules.linear.___torch_mangle_40257.Linear = prim::GetAttr[name="layer_1"](%366)
  %437 : Tensor = prim::GetAttr[name="bias"](%436)
  %438 : Tensor = prim::GetAttr[name="weight"](%436)
  %439 : Float(1024:1, 4096:1024) = aten::t(%438), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%432, %439), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.7, %437, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.25), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.26, %80, %82), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %444 : Tensor = prim::GetAttr[name="bias"](%435)
  %445 : Tensor = prim::GetAttr[name="weight"](%435)
  %446 : Float(4096:1, 1024:4096) = aten::t(%445), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.27, %446), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.8, %444, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.28, %80, %82), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.9, %432, %84), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %451 : Tensor = prim::GetAttr[name="bias"](%434)
  %452 : Tensor = prim::GetAttr[name="weight"](%434)
  %453 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.29, %453, %452, %451, %57, %58), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %455 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %433)
  %456 : Float(13:121856, 119:1024, 1024:1), %457 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%455)
  %new_mem.4 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%456, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %459 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %460 : __torch__.transformers.modeling_xlnet.___torch_mangle_40270.XLNetFeedForward = prim::GetAttr[name="ff"](%127)
  %461 : __torch__.transformers.modeling_xlnet.___torch_mangle_40265.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%127)
  %462 : __torch__.torch.nn.modules.normalization.___torch_mangle_40263.LayerNorm = prim::GetAttr[name="layer_norm"](%461)
  %463 : Tensor = prim::GetAttr[name="o"](%461)
  %464 : Tensor = prim::GetAttr[name="seg_embed"](%461)
  %465 : Tensor = prim::GetAttr[name="r_s_bias"](%461)
  %466 : Tensor = prim::GetAttr[name="r_r_bias"](%461)
  %467 : Tensor = prim::GetAttr[name="r_w_bias"](%461)
  %468 : Tensor = prim::GetAttr[name="r"](%461)
  %469 : Tensor = prim::GetAttr[name="v"](%461)
  %470 : Tensor = prim::GetAttr[name="k"](%461)
  %471 : Tensor = prim::GetAttr[name="q"](%461)
  %472 : Tensor[] = prim::ListConstruct(%456, %471), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %q_head.4 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %472), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %474 : Tensor[] = prim::ListConstruct(%456, %470), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %475 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %474), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %476 : Tensor[] = prim::ListConstruct(%456, %469), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %477 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %476), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 119:0, 1024:1) = aten::to(%457, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %479 : Tensor[] = prim::ListConstruct(%r.5, %468), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %480 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %479), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %481 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %467, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %482 : Tensor[] = prim::ListConstruct(%481, %475), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ac.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %482), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %484 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %466, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %485 : Tensor[] = prim::ListConstruct(%484, %480), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.13 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %485), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %487 : int = aten::size(%ac.4, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %488 : int = aten::size(%x.13, %85), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %489 : int = aten::size(%x.13, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %490 : int = aten::size(%x.13, %76), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %491 : int = aten::size(%x.13, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %492 : Long() = prim::NumToTensor(%491), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %493 : int[] = prim::ListConstruct(%488, %489, %491, %490), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.14 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %493), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %495 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %496 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%495, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %497 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%496, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%497, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %499 : Long() = aten::sub(%492, %69, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %500 : int = aten::Int(%499), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %501 : int[] = prim::ListConstruct(%488, %489, %490, %500), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.16 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %501), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %503 : Long(13:1) = aten::arange(%487, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %64, %503), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %505 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.4, %465, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:294:0
  %506 : Tensor[] = prim::ListConstruct(%505, %464), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %507 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %506), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %508 : Tensor[] = prim::ListConstruct(%154, %507), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ef.4 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %508), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %510 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %511 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%510, %ef.4, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.30 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%511, %61), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.31 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %64, %77), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %514 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %80, %82), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %515 : Tensor[] = prim::ListConstruct(%514, %477), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %516 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %515), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %517 : Tensor[] = prim::ListConstruct(%516, %463), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %input.32 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %517), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.32, %80, %82), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.4, %456, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %521 : Tensor = prim::GetAttr[name="bias"](%462)
  %522 : Tensor = prim::GetAttr[name="weight"](%462)
  %523 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.33, %523, %522, %521, %57, %58), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %525 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %526 : Float(13:121856, 119:1024, 1024:1), %527 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%525)
  %528 : __torch__.torch.nn.modules.normalization.___torch_mangle_40266.LayerNorm = prim::GetAttr[name="layer_norm"](%460)
  %529 : __torch__.torch.nn.modules.linear.___torch_mangle_40268.Linear = prim::GetAttr[name="layer_2"](%460)
  %530 : __torch__.torch.nn.modules.linear.___torch_mangle_40267.Linear = prim::GetAttr[name="layer_1"](%460)
  %531 : Tensor = prim::GetAttr[name="bias"](%530)
  %532 : Tensor = prim::GetAttr[name="weight"](%530)
  %533 : Float(1024:1, 4096:1024) = aten::t(%532), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%526, %533), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.10, %531, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.35, %80, %82), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %538 : Tensor = prim::GetAttr[name="bias"](%529)
  %539 : Tensor = prim::GetAttr[name="weight"](%529)
  %540 : Float(4096:1, 1024:4096) = aten::t(%539), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.36, %540), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.11, %538, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.37, %80, %82), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.12, %526, %84), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %545 : Tensor = prim::GetAttr[name="bias"](%528)
  %546 : Tensor = prim::GetAttr[name="weight"](%528)
  %547 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.38, %547, %546, %545, %57, %58), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %549 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %527)
  %550 : Float(13:121856, 119:1024, 1024:1), %551 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%549)
  %new_mem.5 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%550, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %553 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %554 : __torch__.transformers.modeling_xlnet.___torch_mangle_40280.XLNetFeedForward = prim::GetAttr[name="ff"](%125)
  %555 : __torch__.transformers.modeling_xlnet.___torch_mangle_40275.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%125)
  %556 : __torch__.torch.nn.modules.normalization.___torch_mangle_40273.LayerNorm = prim::GetAttr[name="layer_norm"](%555)
  %557 : Tensor = prim::GetAttr[name="o"](%555)
  %558 : Tensor = prim::GetAttr[name="seg_embed"](%555)
  %559 : Tensor = prim::GetAttr[name="r_s_bias"](%555)
  %560 : Tensor = prim::GetAttr[name="r_r_bias"](%555)
  %561 : Tensor = prim::GetAttr[name="r_w_bias"](%555)
  %562 : Tensor = prim::GetAttr[name="r"](%555)
  %563 : Tensor = prim::GetAttr[name="v"](%555)
  %564 : Tensor = prim::GetAttr[name="k"](%555)
  %565 : Tensor = prim::GetAttr[name="q"](%555)
  %566 : Tensor[] = prim::ListConstruct(%550, %565), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %q_head.5 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %566), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %568 : Tensor[] = prim::ListConstruct(%550, %564), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %569 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %568), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %570 : Tensor[] = prim::ListConstruct(%550, %563), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %571 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %570), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 119:0, 1024:1) = aten::to(%551, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %573 : Tensor[] = prim::ListConstruct(%r.6, %562), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %574 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %573), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %575 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %561, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %576 : Tensor[] = prim::ListConstruct(%575, %569), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ac.5 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %576), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %578 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %560, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %579 : Tensor[] = prim::ListConstruct(%578, %574), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.17 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %579), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %581 : int = aten::size(%ac.5, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %582 : int = aten::size(%x.17, %85), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %583 : int = aten::size(%x.17, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %584 : int = aten::size(%x.17, %76), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %585 : int = aten::size(%x.17, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %586 : Long() = prim::NumToTensor(%585), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %587 : int[] = prim::ListConstruct(%582, %583, %585, %584), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.18 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %587), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %589 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %590 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%589, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %591 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%590, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%591, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %593 : Long() = aten::sub(%586, %69, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %594 : int = aten::Int(%593), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %595 : int[] = prim::ListConstruct(%582, %583, %584, %594), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.20 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %595), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %597 : Long(13:1) = aten::arange(%581, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %64, %597), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %599 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.5, %559, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:294:0
  %600 : Tensor[] = prim::ListConstruct(%599, %558), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %601 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %600), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %602 : Tensor[] = prim::ListConstruct(%154, %601), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ef.5 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %602), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %604 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %605 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%604, %ef.5, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.39 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%605, %61), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.40 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %64, %77), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %608 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %80, %82), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %609 : Tensor[] = prim::ListConstruct(%608, %571), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %610 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %609), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %611 : Tensor[] = prim::ListConstruct(%610, %557), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %input.41 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %611), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.41, %80, %82), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.5, %550, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %615 : Tensor = prim::GetAttr[name="bias"](%556)
  %616 : Tensor = prim::GetAttr[name="weight"](%556)
  %617 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.42, %617, %616, %615, %57, %58), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %619 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %620 : Float(13:121856, 119:1024, 1024:1), %621 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%619)
  %622 : __torch__.torch.nn.modules.normalization.___torch_mangle_40276.LayerNorm = prim::GetAttr[name="layer_norm"](%554)
  %623 : __torch__.torch.nn.modules.linear.___torch_mangle_40278.Linear = prim::GetAttr[name="layer_2"](%554)
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_40277.Linear = prim::GetAttr[name="layer_1"](%554)
  %625 : Tensor = prim::GetAttr[name="bias"](%624)
  %626 : Tensor = prim::GetAttr[name="weight"](%624)
  %627 : Float(1024:1, 4096:1024) = aten::t(%626), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%620, %627), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.13, %625, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.43), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.44, %80, %82), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %632 : Tensor = prim::GetAttr[name="bias"](%623)
  %633 : Tensor = prim::GetAttr[name="weight"](%623)
  %634 : Float(4096:1, 1024:4096) = aten::t(%633), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.45, %634), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.14, %632, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.46, %80, %82), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.15, %620, %84), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %639 : Tensor = prim::GetAttr[name="bias"](%622)
  %640 : Tensor = prim::GetAttr[name="weight"](%622)
  %641 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.47, %641, %640, %639, %57, %58), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %643 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %621)
  %644 : Float(13:121856, 119:1024, 1024:1), %645 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%643)
  %new_mem.6 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%644, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %647 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %648 : __torch__.transformers.modeling_xlnet.___torch_mangle_40290.XLNetFeedForward = prim::GetAttr[name="ff"](%123)
  %649 : __torch__.transformers.modeling_xlnet.___torch_mangle_40285.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%123)
  %650 : __torch__.torch.nn.modules.normalization.___torch_mangle_40283.LayerNorm = prim::GetAttr[name="layer_norm"](%649)
  %651 : Tensor = prim::GetAttr[name="o"](%649)
  %652 : Tensor = prim::GetAttr[name="seg_embed"](%649)
  %653 : Tensor = prim::GetAttr[name="r_s_bias"](%649)
  %654 : Tensor = prim::GetAttr[name="r_r_bias"](%649)
  %655 : Tensor = prim::GetAttr[name="r_w_bias"](%649)
  %656 : Tensor = prim::GetAttr[name="r"](%649)
  %657 : Tensor = prim::GetAttr[name="v"](%649)
  %658 : Tensor = prim::GetAttr[name="k"](%649)
  %659 : Tensor = prim::GetAttr[name="q"](%649)
  %660 : Tensor[] = prim::ListConstruct(%644, %659), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %q_head.6 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %660), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %662 : Tensor[] = prim::ListConstruct(%644, %658), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %663 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %662), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %664 : Tensor[] = prim::ListConstruct(%644, %657), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %665 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %664), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 119:0, 1024:1) = aten::to(%645, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %667 : Tensor[] = prim::ListConstruct(%r.7, %656), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %668 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %667), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %669 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %655, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %670 : Tensor[] = prim::ListConstruct(%669, %663), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ac.6 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %670), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %672 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %654, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %673 : Tensor[] = prim::ListConstruct(%672, %668), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.21 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %673), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %675 : int = aten::size(%ac.6, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %676 : int = aten::size(%x.21, %85), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %677 : int = aten::size(%x.21, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %678 : int = aten::size(%x.21, %76), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %679 : int = aten::size(%x.21, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %680 : Long() = prim::NumToTensor(%679), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %681 : int[] = prim::ListConstruct(%676, %677, %679, %678), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.22 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %681), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %683 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %684 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%683, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %685 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%684, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%685, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %687 : Long() = aten::sub(%680, %69, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %688 : int = aten::Int(%687), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %689 : int[] = prim::ListConstruct(%676, %677, %678, %688), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.24 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %689), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %691 : Long(13:1) = aten::arange(%675, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %64, %691), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %693 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.6, %653, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:294:0
  %694 : Tensor[] = prim::ListConstruct(%693, %652), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %695 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %694), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %696 : Tensor[] = prim::ListConstruct(%154, %695), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ef.6 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %696), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %698 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %699 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%698, %ef.6, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.48 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%699, %61), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.49 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %64, %77), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %702 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %80, %82), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %703 : Tensor[] = prim::ListConstruct(%702, %665), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %704 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %703), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %705 : Tensor[] = prim::ListConstruct(%704, %651), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %input.50 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %705), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.50, %80, %82), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.6, %644, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %709 : Tensor = prim::GetAttr[name="bias"](%650)
  %710 : Tensor = prim::GetAttr[name="weight"](%650)
  %711 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.51, %711, %710, %709, %57, %58), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %713 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %714 : Float(13:121856, 119:1024, 1024:1), %715 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%713)
  %716 : __torch__.torch.nn.modules.normalization.___torch_mangle_40286.LayerNorm = prim::GetAttr[name="layer_norm"](%648)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_40288.Linear = prim::GetAttr[name="layer_2"](%648)
  %718 : __torch__.torch.nn.modules.linear.___torch_mangle_40287.Linear = prim::GetAttr[name="layer_1"](%648)
  %719 : Tensor = prim::GetAttr[name="bias"](%718)
  %720 : Tensor = prim::GetAttr[name="weight"](%718)
  %721 : Float(1024:1, 4096:1024) = aten::t(%720), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%714, %721), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.16, %719, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.53, %80, %82), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %726 : Tensor = prim::GetAttr[name="bias"](%717)
  %727 : Tensor = prim::GetAttr[name="weight"](%717)
  %728 : Float(4096:1, 1024:4096) = aten::t(%727), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.54, %728), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.17, %726, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.55, %80, %82), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.18, %714, %84), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %733 : Tensor = prim::GetAttr[name="bias"](%716)
  %734 : Tensor = prim::GetAttr[name="weight"](%716)
  %735 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.56, %735, %734, %733, %57, %58), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %737 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %715)
  %738 : Float(13:121856, 119:1024, 1024:1), %739 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%737)
  %new_mem.7 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%738, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %741 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %742 : __torch__.transformers.modeling_xlnet.___torch_mangle_40300.XLNetFeedForward = prim::GetAttr[name="ff"](%121)
  %743 : __torch__.transformers.modeling_xlnet.___torch_mangle_40295.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%121)
  %744 : __torch__.torch.nn.modules.normalization.___torch_mangle_40293.LayerNorm = prim::GetAttr[name="layer_norm"](%743)
  %745 : Tensor = prim::GetAttr[name="o"](%743)
  %746 : Tensor = prim::GetAttr[name="seg_embed"](%743)
  %747 : Tensor = prim::GetAttr[name="r_s_bias"](%743)
  %748 : Tensor = prim::GetAttr[name="r_r_bias"](%743)
  %749 : Tensor = prim::GetAttr[name="r_w_bias"](%743)
  %750 : Tensor = prim::GetAttr[name="r"](%743)
  %751 : Tensor = prim::GetAttr[name="v"](%743)
  %752 : Tensor = prim::GetAttr[name="k"](%743)
  %753 : Tensor = prim::GetAttr[name="q"](%743)
  %754 : Tensor[] = prim::ListConstruct(%738, %753), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %q_head.7 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %754), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %756 : Tensor[] = prim::ListConstruct(%738, %752), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %757 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %756), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %758 : Tensor[] = prim::ListConstruct(%738, %751), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %759 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %758), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 119:0, 1024:1) = aten::to(%739, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %761 : Tensor[] = prim::ListConstruct(%r.8, %750), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %762 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %761), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %763 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %749, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %764 : Tensor[] = prim::ListConstruct(%763, %757), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ac.7 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %764), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %766 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %748, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %767 : Tensor[] = prim::ListConstruct(%766, %762), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.25 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %767), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %769 : int = aten::size(%ac.7, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %770 : int = aten::size(%x.25, %85), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %771 : int = aten::size(%x.25, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %772 : int = aten::size(%x.25, %76), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %773 : int = aten::size(%x.25, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %774 : Long() = prim::NumToTensor(%773), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %775 : int[] = prim::ListConstruct(%770, %771, %773, %772), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.26 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %775), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %777 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %778 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%777, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %779 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%778, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%779, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %781 : Long() = aten::sub(%774, %69, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %782 : int = aten::Int(%781), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %783 : int[] = prim::ListConstruct(%770, %771, %772, %782), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.28 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %783), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %785 : Long(13:1) = aten::arange(%769, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %64, %785), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %787 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.7, %747, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:294:0
  %788 : Tensor[] = prim::ListConstruct(%787, %746), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %789 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %788), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %790 : Tensor[] = prim::ListConstruct(%154, %789), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ef.7 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %790), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %792 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %793 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%792, %ef.7, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.57 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%793, %61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.58 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %64, %77), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %796 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %80, %82), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %797 : Tensor[] = prim::ListConstruct(%796, %759), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %798 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %797), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %799 : Tensor[] = prim::ListConstruct(%798, %745), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %input.59 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %799), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.59, %80, %82), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.7, %738, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %803 : Tensor = prim::GetAttr[name="bias"](%744)
  %804 : Tensor = prim::GetAttr[name="weight"](%744)
  %805 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.60, %805, %804, %803, %57, %58), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %807 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %808 : Float(13:121856, 119:1024, 1024:1), %809 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%807)
  %810 : __torch__.torch.nn.modules.normalization.___torch_mangle_40296.LayerNorm = prim::GetAttr[name="layer_norm"](%742)
  %811 : __torch__.torch.nn.modules.linear.___torch_mangle_40298.Linear = prim::GetAttr[name="layer_2"](%742)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_40297.Linear = prim::GetAttr[name="layer_1"](%742)
  %813 : Tensor = prim::GetAttr[name="bias"](%812)
  %814 : Tensor = prim::GetAttr[name="weight"](%812)
  %815 : Float(1024:1, 4096:1024) = aten::t(%814), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%808, %815), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.19, %813, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.62, %80, %82), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %820 : Tensor = prim::GetAttr[name="bias"](%811)
  %821 : Tensor = prim::GetAttr[name="weight"](%811)
  %822 : Float(4096:1, 1024:4096) = aten::t(%821), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.63, %822), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.20, %820, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.64, %80, %82), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.21, %808, %84), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %827 : Tensor = prim::GetAttr[name="bias"](%810)
  %828 : Tensor = prim::GetAttr[name="weight"](%810)
  %829 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.65, %829, %828, %827, %57, %58), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %831 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %809)
  %832 : Float(13:121856, 119:1024, 1024:1), %833 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%831)
  %new_mem.8 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%832, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %835 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %836 : __torch__.transformers.modeling_xlnet.___torch_mangle_40310.XLNetFeedForward = prim::GetAttr[name="ff"](%119)
  %837 : __torch__.transformers.modeling_xlnet.___torch_mangle_40305.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%119)
  %838 : __torch__.torch.nn.modules.normalization.___torch_mangle_40303.LayerNorm = prim::GetAttr[name="layer_norm"](%837)
  %839 : Tensor = prim::GetAttr[name="o"](%837)
  %840 : Tensor = prim::GetAttr[name="seg_embed"](%837)
  %841 : Tensor = prim::GetAttr[name="r_s_bias"](%837)
  %842 : Tensor = prim::GetAttr[name="r_r_bias"](%837)
  %843 : Tensor = prim::GetAttr[name="r_w_bias"](%837)
  %844 : Tensor = prim::GetAttr[name="r"](%837)
  %845 : Tensor = prim::GetAttr[name="v"](%837)
  %846 : Tensor = prim::GetAttr[name="k"](%837)
  %847 : Tensor = prim::GetAttr[name="q"](%837)
  %848 : Tensor[] = prim::ListConstruct(%832, %847), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %q_head.8 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %848), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %850 : Tensor[] = prim::ListConstruct(%832, %846), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %851 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %850), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %852 : Tensor[] = prim::ListConstruct(%832, %845), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %853 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %852), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 119:0, 1024:1) = aten::to(%833, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %855 : Tensor[] = prim::ListConstruct(%r.9, %844), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %856 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %855), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %857 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %843, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %858 : Tensor[] = prim::ListConstruct(%857, %851), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ac.8 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %858), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %860 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %842, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %861 : Tensor[] = prim::ListConstruct(%860, %856), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.29 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %861), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %863 : int = aten::size(%ac.8, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %864 : int = aten::size(%x.29, %85), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %865 : int = aten::size(%x.29, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %866 : int = aten::size(%x.29, %76), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %867 : int = aten::size(%x.29, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %868 : Long() = prim::NumToTensor(%867), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %869 : int[] = prim::ListConstruct(%864, %865, %867, %866), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.30 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %869), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %871 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %872 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%871, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %873 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%872, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%873, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %875 : Long() = aten::sub(%868, %69, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %876 : int = aten::Int(%875), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %877 : int[] = prim::ListConstruct(%864, %865, %866, %876), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.32 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %877), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %879 : Long(13:1) = aten::arange(%863, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %64, %879), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %881 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.8, %841, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:294:0
  %882 : Tensor[] = prim::ListConstruct(%881, %840), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %883 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %882), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %884 : Tensor[] = prim::ListConstruct(%154, %883), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ef.8 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %884), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %886 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %887 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%886, %ef.8, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.66 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%887, %61), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.67 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %64, %77), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %890 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %80, %82), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %891 : Tensor[] = prim::ListConstruct(%890, %853), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %892 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %891), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %893 : Tensor[] = prim::ListConstruct(%892, %839), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %input.68 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %893), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.68, %80, %82), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.8, %832, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %897 : Tensor = prim::GetAttr[name="bias"](%838)
  %898 : Tensor = prim::GetAttr[name="weight"](%838)
  %899 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.69, %899, %898, %897, %57, %58), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %901 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %902 : Float(13:121856, 119:1024, 1024:1), %903 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%901)
  %904 : __torch__.torch.nn.modules.normalization.___torch_mangle_40306.LayerNorm = prim::GetAttr[name="layer_norm"](%836)
  %905 : __torch__.torch.nn.modules.linear.___torch_mangle_40308.Linear = prim::GetAttr[name="layer_2"](%836)
  %906 : __torch__.torch.nn.modules.linear.___torch_mangle_40307.Linear = prim::GetAttr[name="layer_1"](%836)
  %907 : Tensor = prim::GetAttr[name="bias"](%906)
  %908 : Tensor = prim::GetAttr[name="weight"](%906)
  %909 : Float(1024:1, 4096:1024) = aten::t(%908), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%902, %909), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.22, %907, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.71, %80, %82), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %914 : Tensor = prim::GetAttr[name="bias"](%905)
  %915 : Tensor = prim::GetAttr[name="weight"](%905)
  %916 : Float(4096:1, 1024:4096) = aten::t(%915), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.72, %916), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.23, %914, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.73, %80, %82), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.24, %902, %84), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %921 : Tensor = prim::GetAttr[name="bias"](%904)
  %922 : Tensor = prim::GetAttr[name="weight"](%904)
  %923 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.74, %923, %922, %921, %57, %58), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %925 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %903)
  %926 : Float(13:121856, 119:1024, 1024:1), %927 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%925)
  %new_mem.9 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%926, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %929 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %930 : __torch__.transformers.modeling_xlnet.___torch_mangle_40320.XLNetFeedForward = prim::GetAttr[name="ff"](%117)
  %931 : __torch__.transformers.modeling_xlnet.___torch_mangle_40315.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%117)
  %932 : __torch__.torch.nn.modules.normalization.___torch_mangle_40313.LayerNorm = prim::GetAttr[name="layer_norm"](%931)
  %933 : Tensor = prim::GetAttr[name="o"](%931)
  %934 : Tensor = prim::GetAttr[name="seg_embed"](%931)
  %935 : Tensor = prim::GetAttr[name="r_s_bias"](%931)
  %936 : Tensor = prim::GetAttr[name="r_r_bias"](%931)
  %937 : Tensor = prim::GetAttr[name="r_w_bias"](%931)
  %938 : Tensor = prim::GetAttr[name="r"](%931)
  %939 : Tensor = prim::GetAttr[name="v"](%931)
  %940 : Tensor = prim::GetAttr[name="k"](%931)
  %941 : Tensor = prim::GetAttr[name="q"](%931)
  %942 : Tensor[] = prim::ListConstruct(%926, %941), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %q_head.9 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %942), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %944 : Tensor[] = prim::ListConstruct(%926, %940), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %945 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %944), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %946 : Tensor[] = prim::ListConstruct(%926, %939), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %947 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %946), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 119:0, 1024:1) = aten::to(%927, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %949 : Tensor[] = prim::ListConstruct(%r.10, %938), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %950 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %949), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %951 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %937, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %952 : Tensor[] = prim::ListConstruct(%951, %945), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ac.9 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %952), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %954 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %936, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %955 : Tensor[] = prim::ListConstruct(%954, %950), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.33 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %955), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %957 : int = aten::size(%ac.9, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %958 : int = aten::size(%x.33, %85), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %959 : int = aten::size(%x.33, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %960 : int = aten::size(%x.33, %76), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %961 : int = aten::size(%x.33, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %962 : Long() = prim::NumToTensor(%961), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %963 : int[] = prim::ListConstruct(%958, %959, %961, %960), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.34 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %963), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %965 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %966 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%965, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %967 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%966, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%967, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %969 : Long() = aten::sub(%962, %69, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %970 : int = aten::Int(%969), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %971 : int[] = prim::ListConstruct(%958, %959, %960, %970), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.36 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %971), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %973 : Long(13:1) = aten::arange(%957, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %64, %973), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %975 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.9, %935, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:294:0
  %976 : Tensor[] = prim::ListConstruct(%975, %934), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %977 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %976), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %978 : Tensor[] = prim::ListConstruct(%154, %977), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ef.9 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %978), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %980 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %981 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%980, %ef.9, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.75 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%981, %61), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.76 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %64, %77), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %984 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %80, %82), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %985 : Tensor[] = prim::ListConstruct(%984, %947), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %986 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %985), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %987 : Tensor[] = prim::ListConstruct(%986, %933), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %input.77 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %987), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.77, %80, %82), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.9, %926, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %991 : Tensor = prim::GetAttr[name="bias"](%932)
  %992 : Tensor = prim::GetAttr[name="weight"](%932)
  %993 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.78, %993, %992, %991, %57, %58), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %995 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %996 : Float(13:121856, 119:1024, 1024:1), %997 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%995)
  %998 : __torch__.torch.nn.modules.normalization.___torch_mangle_40316.LayerNorm = prim::GetAttr[name="layer_norm"](%930)
  %999 : __torch__.torch.nn.modules.linear.___torch_mangle_40318.Linear = prim::GetAttr[name="layer_2"](%930)
  %1000 : __torch__.torch.nn.modules.linear.___torch_mangle_40317.Linear = prim::GetAttr[name="layer_1"](%930)
  %1001 : Tensor = prim::GetAttr[name="bias"](%1000)
  %1002 : Tensor = prim::GetAttr[name="weight"](%1000)
  %1003 : Float(1024:1, 4096:1024) = aten::t(%1002), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%996, %1003), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.25, %1001, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.79), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.80, %80, %82), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %1008 : Tensor = prim::GetAttr[name="bias"](%999)
  %1009 : Tensor = prim::GetAttr[name="weight"](%999)
  %1010 : Float(4096:1, 1024:4096) = aten::t(%1009), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.81, %1010), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.26, %1008, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.82, %80, %82), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.27, %996, %84), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %1015 : Tensor = prim::GetAttr[name="bias"](%998)
  %1016 : Tensor = prim::GetAttr[name="weight"](%998)
  %1017 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.83, %1017, %1016, %1015, %57, %58), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %1019 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %997)
  %1020 : Float(13:121856, 119:1024, 1024:1), %1021 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1019)
  %new_mem.10 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1020, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1023 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1024 : __torch__.transformers.modeling_xlnet.___torch_mangle_40330.XLNetFeedForward = prim::GetAttr[name="ff"](%115)
  %1025 : __torch__.transformers.modeling_xlnet.___torch_mangle_40325.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%115)
  %1026 : __torch__.torch.nn.modules.normalization.___torch_mangle_40323.LayerNorm = prim::GetAttr[name="layer_norm"](%1025)
  %1027 : Tensor = prim::GetAttr[name="o"](%1025)
  %1028 : Tensor = prim::GetAttr[name="seg_embed"](%1025)
  %1029 : Tensor = prim::GetAttr[name="r_s_bias"](%1025)
  %1030 : Tensor = prim::GetAttr[name="r_r_bias"](%1025)
  %1031 : Tensor = prim::GetAttr[name="r_w_bias"](%1025)
  %1032 : Tensor = prim::GetAttr[name="r"](%1025)
  %1033 : Tensor = prim::GetAttr[name="v"](%1025)
  %1034 : Tensor = prim::GetAttr[name="k"](%1025)
  %1035 : Tensor = prim::GetAttr[name="q"](%1025)
  %1036 : Tensor[] = prim::ListConstruct(%1020, %1035), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %q_head.10 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1036), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1038 : Tensor[] = prim::ListConstruct(%1020, %1034), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1039 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1038), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1040 : Tensor[] = prim::ListConstruct(%1020, %1033), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1041 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1040), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 119:0, 1024:1) = aten::to(%1021, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1043 : Tensor[] = prim::ListConstruct(%r.11, %1032), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1044 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1043), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1045 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %1031, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1046 : Tensor[] = prim::ListConstruct(%1045, %1039), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ac.10 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1046), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1048 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %1030, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1049 : Tensor[] = prim::ListConstruct(%1048, %1044), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.37 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1049), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1051 : int = aten::size(%ac.10, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1052 : int = aten::size(%x.37, %85), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1053 : int = aten::size(%x.37, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1054 : int = aten::size(%x.37, %76), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1055 : int = aten::size(%x.37, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1056 : Long() = prim::NumToTensor(%1055), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1057 : int[] = prim::ListConstruct(%1052, %1053, %1055, %1054), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.38 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1057), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1059 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1060 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1059, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1061 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1060, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1061, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1063 : Long() = aten::sub(%1056, %69, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1064 : int = aten::Int(%1063), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1065 : int[] = prim::ListConstruct(%1052, %1053, %1054, %1064), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.40 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1065), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1067 : Long(13:1) = aten::arange(%1051, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %64, %1067), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1069 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.10, %1029, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:294:0
  %1070 : Tensor[] = prim::ListConstruct(%1069, %1028), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1071 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1070), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1072 : Tensor[] = prim::ListConstruct(%154, %1071), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ef.10 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1072), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1074 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1075 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1074, %ef.10, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.84 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1075, %61), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.85 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %64, %77), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1078 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %80, %82), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1079 : Tensor[] = prim::ListConstruct(%1078, %1041), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1080 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1079), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1081 : Tensor[] = prim::ListConstruct(%1080, %1027), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %input.86 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1081), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.86, %80, %82), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.10, %1020, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1085 : Tensor = prim::GetAttr[name="bias"](%1026)
  %1086 : Tensor = prim::GetAttr[name="weight"](%1026)
  %1087 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.87, %1087, %1086, %1085, %57, %58), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1089 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1090 : Float(13:121856, 119:1024, 1024:1), %1091 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1089)
  %1092 : __torch__.torch.nn.modules.normalization.___torch_mangle_40326.LayerNorm = prim::GetAttr[name="layer_norm"](%1024)
  %1093 : __torch__.torch.nn.modules.linear.___torch_mangle_40328.Linear = prim::GetAttr[name="layer_2"](%1024)
  %1094 : __torch__.torch.nn.modules.linear.___torch_mangle_40327.Linear = prim::GetAttr[name="layer_1"](%1024)
  %1095 : Tensor = prim::GetAttr[name="bias"](%1094)
  %1096 : Tensor = prim::GetAttr[name="weight"](%1094)
  %1097 : Float(1024:1, 4096:1024) = aten::t(%1096), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1090, %1097), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.28, %1095, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.89, %80, %82), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1102 : Tensor = prim::GetAttr[name="bias"](%1093)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1093)
  %1104 : Float(4096:1, 1024:4096) = aten::t(%1103), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.90, %1104), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.29, %1102, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.91, %80, %82), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.30, %1090, %84), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1109 : Tensor = prim::GetAttr[name="bias"](%1092)
  %1110 : Tensor = prim::GetAttr[name="weight"](%1092)
  %1111 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.92, %1111, %1110, %1109, %57, %58), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1113 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1091)
  %1114 : Float(13:121856, 119:1024, 1024:1), %1115 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1113)
  %new_mem.11 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1114, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1117 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1118 : __torch__.transformers.modeling_xlnet.___torch_mangle_40340.XLNetFeedForward = prim::GetAttr[name="ff"](%113)
  %1119 : __torch__.transformers.modeling_xlnet.___torch_mangle_40335.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%113)
  %1120 : __torch__.torch.nn.modules.normalization.___torch_mangle_40333.LayerNorm = prim::GetAttr[name="layer_norm"](%1119)
  %1121 : Tensor = prim::GetAttr[name="o"](%1119)
  %1122 : Tensor = prim::GetAttr[name="seg_embed"](%1119)
  %1123 : Tensor = prim::GetAttr[name="r_s_bias"](%1119)
  %1124 : Tensor = prim::GetAttr[name="r_r_bias"](%1119)
  %1125 : Tensor = prim::GetAttr[name="r_w_bias"](%1119)
  %1126 : Tensor = prim::GetAttr[name="r"](%1119)
  %1127 : Tensor = prim::GetAttr[name="v"](%1119)
  %1128 : Tensor = prim::GetAttr[name="k"](%1119)
  %1129 : Tensor = prim::GetAttr[name="q"](%1119)
  %1130 : Tensor[] = prim::ListConstruct(%1114, %1129), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %q_head.11 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1130), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1132 : Tensor[] = prim::ListConstruct(%1114, %1128), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1133 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1132), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1134 : Tensor[] = prim::ListConstruct(%1114, %1127), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1135 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1134), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 119:0, 1024:1) = aten::to(%1115, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1137 : Tensor[] = prim::ListConstruct(%r.12, %1126), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1138 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1137), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1139 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %1125, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1140 : Tensor[] = prim::ListConstruct(%1139, %1133), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ac.11 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1140), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1142 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %1124, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1143 : Tensor[] = prim::ListConstruct(%1142, %1138), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.41 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1143), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1145 : int = aten::size(%ac.11, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1146 : int = aten::size(%x.41, %85), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1147 : int = aten::size(%x.41, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1148 : int = aten::size(%x.41, %76), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1149 : int = aten::size(%x.41, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1150 : Long() = prim::NumToTensor(%1149), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1151 : int[] = prim::ListConstruct(%1146, %1147, %1149, %1148), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.42 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1151), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1153 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1154 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1153, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1155 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1154, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1155, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1157 : Long() = aten::sub(%1150, %69, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1158 : int = aten::Int(%1157), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1159 : int[] = prim::ListConstruct(%1146, %1147, %1148, %1158), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.44 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1159), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1161 : Long(13:1) = aten::arange(%1145, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %64, %1161), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1163 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.11, %1123, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:294:0
  %1164 : Tensor[] = prim::ListConstruct(%1163, %1122), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1165 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1164), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1166 : Tensor[] = prim::ListConstruct(%154, %1165), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ef.11 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1166), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1168 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1169 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1168, %ef.11, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.93 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1169, %61), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.94 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %64, %77), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1172 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %80, %82), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1173 : Tensor[] = prim::ListConstruct(%1172, %1135), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1174 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1173), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1175 : Tensor[] = prim::ListConstruct(%1174, %1121), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %input.95 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1175), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.95, %80, %82), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.11, %1114, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1179 : Tensor = prim::GetAttr[name="bias"](%1120)
  %1180 : Tensor = prim::GetAttr[name="weight"](%1120)
  %1181 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.96, %1181, %1180, %1179, %57, %58), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1183 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1184 : Float(13:121856, 119:1024, 1024:1), %1185 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1183)
  %1186 : __torch__.torch.nn.modules.normalization.___torch_mangle_40336.LayerNorm = prim::GetAttr[name="layer_norm"](%1118)
  %1187 : __torch__.torch.nn.modules.linear.___torch_mangle_40338.Linear = prim::GetAttr[name="layer_2"](%1118)
  %1188 : __torch__.torch.nn.modules.linear.___torch_mangle_40337.Linear = prim::GetAttr[name="layer_1"](%1118)
  %1189 : Tensor = prim::GetAttr[name="bias"](%1188)
  %1190 : Tensor = prim::GetAttr[name="weight"](%1188)
  %1191 : Float(1024:1, 4096:1024) = aten::t(%1190), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1184, %1191), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.31, %1189, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.98, %80, %82), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1196 : Tensor = prim::GetAttr[name="bias"](%1187)
  %1197 : Tensor = prim::GetAttr[name="weight"](%1187)
  %1198 : Float(4096:1, 1024:4096) = aten::t(%1197), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.99, %1198), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.32, %1196, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.100, %80, %82), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.33, %1184, %84), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1203 : Tensor = prim::GetAttr[name="bias"](%1186)
  %1204 : Tensor = prim::GetAttr[name="weight"](%1186)
  %1205 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.101, %1205, %1204, %1203, %57, %58), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1207 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1185)
  %1208 : Float(13:121856, 119:1024, 1024:1), %1209 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1207)
  %new_mem.12 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1208, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1211 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1212 : __torch__.transformers.modeling_xlnet.___torch_mangle_40350.XLNetFeedForward = prim::GetAttr[name="ff"](%111)
  %1213 : __torch__.transformers.modeling_xlnet.___torch_mangle_40345.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%111)
  %1214 : __torch__.torch.nn.modules.normalization.___torch_mangle_40343.LayerNorm = prim::GetAttr[name="layer_norm"](%1213)
  %1215 : Tensor = prim::GetAttr[name="o"](%1213)
  %1216 : Tensor = prim::GetAttr[name="seg_embed"](%1213)
  %1217 : Tensor = prim::GetAttr[name="r_s_bias"](%1213)
  %1218 : Tensor = prim::GetAttr[name="r_r_bias"](%1213)
  %1219 : Tensor = prim::GetAttr[name="r_w_bias"](%1213)
  %1220 : Tensor = prim::GetAttr[name="r"](%1213)
  %1221 : Tensor = prim::GetAttr[name="v"](%1213)
  %1222 : Tensor = prim::GetAttr[name="k"](%1213)
  %1223 : Tensor = prim::GetAttr[name="q"](%1213)
  %1224 : Tensor[] = prim::ListConstruct(%1208, %1223), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %q_head.12 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1224), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1226 : Tensor[] = prim::ListConstruct(%1208, %1222), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1227 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1226), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1228 : Tensor[] = prim::ListConstruct(%1208, %1221), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1229 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1228), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 119:0, 1024:1) = aten::to(%1209, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1231 : Tensor[] = prim::ListConstruct(%r.13, %1220), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1232 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1231), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1233 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %1219, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1234 : Tensor[] = prim::ListConstruct(%1233, %1227), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ac.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1234), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1236 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %1218, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1237 : Tensor[] = prim::ListConstruct(%1236, %1232), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.45 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1237), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1239 : int = aten::size(%ac.12, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1240 : int = aten::size(%x.45, %85), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1241 : int = aten::size(%x.45, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1242 : int = aten::size(%x.45, %76), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1243 : int = aten::size(%x.45, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1244 : Long() = prim::NumToTensor(%1243), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1245 : int[] = prim::ListConstruct(%1240, %1241, %1243, %1242), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.46 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1245), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1247 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1248 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1247, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1249 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1248, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1249, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1251 : Long() = aten::sub(%1244, %69, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1252 : int = aten::Int(%1251), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1253 : int[] = prim::ListConstruct(%1240, %1241, %1242, %1252), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.48 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1253), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1255 : Long(13:1) = aten::arange(%1239, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %64, %1255), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1257 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.12, %1217, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:294:0
  %1258 : Tensor[] = prim::ListConstruct(%1257, %1216), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1259 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1258), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1260 : Tensor[] = prim::ListConstruct(%154, %1259), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ef.12 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1260), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1262 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1263 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1262, %ef.12, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.102 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1263, %61), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.103 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %64, %77), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1266 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %80, %82), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1267 : Tensor[] = prim::ListConstruct(%1266, %1229), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1268 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1267), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1269 : Tensor[] = prim::ListConstruct(%1268, %1215), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %input.104 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1269), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.104, %80, %82), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.12, %1208, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1273 : Tensor = prim::GetAttr[name="bias"](%1214)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1214)
  %1275 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.105, %1275, %1274, %1273, %57, %58), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1277 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1278 : Float(13:121856, 119:1024, 1024:1), %1279 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1277)
  %1280 : __torch__.torch.nn.modules.normalization.___torch_mangle_40346.LayerNorm = prim::GetAttr[name="layer_norm"](%1212)
  %1281 : __torch__.torch.nn.modules.linear.___torch_mangle_40348.Linear = prim::GetAttr[name="layer_2"](%1212)
  %1282 : __torch__.torch.nn.modules.linear.___torch_mangle_40347.Linear = prim::GetAttr[name="layer_1"](%1212)
  %1283 : Tensor = prim::GetAttr[name="bias"](%1282)
  %1284 : Tensor = prim::GetAttr[name="weight"](%1282)
  %1285 : Float(1024:1, 4096:1024) = aten::t(%1284), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1278, %1285), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.34, %1283, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.107, %80, %82), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1290 : Tensor = prim::GetAttr[name="bias"](%1281)
  %1291 : Tensor = prim::GetAttr[name="weight"](%1281)
  %1292 : Float(4096:1, 1024:4096) = aten::t(%1291), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.108, %1292), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.35, %1290, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.109, %80, %82), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.36, %1278, %84), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1297 : Tensor = prim::GetAttr[name="bias"](%1280)
  %1298 : Tensor = prim::GetAttr[name="weight"](%1280)
  %1299 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.110, %1299, %1298, %1297, %57, %58), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1301 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1279)
  %1302 : Float(13:121856, 119:1024, 1024:1), %1303 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1301)
  %new_mem.13 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1302, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1305 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1306 : __torch__.transformers.modeling_xlnet.___torch_mangle_40360.XLNetFeedForward = prim::GetAttr[name="ff"](%109)
  %1307 : __torch__.transformers.modeling_xlnet.___torch_mangle_40355.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%109)
  %1308 : __torch__.torch.nn.modules.normalization.___torch_mangle_40353.LayerNorm = prim::GetAttr[name="layer_norm"](%1307)
  %1309 : Tensor = prim::GetAttr[name="o"](%1307)
  %1310 : Tensor = prim::GetAttr[name="seg_embed"](%1307)
  %1311 : Tensor = prim::GetAttr[name="r_s_bias"](%1307)
  %1312 : Tensor = prim::GetAttr[name="r_r_bias"](%1307)
  %1313 : Tensor = prim::GetAttr[name="r_w_bias"](%1307)
  %1314 : Tensor = prim::GetAttr[name="r"](%1307)
  %1315 : Tensor = prim::GetAttr[name="v"](%1307)
  %1316 : Tensor = prim::GetAttr[name="k"](%1307)
  %1317 : Tensor = prim::GetAttr[name="q"](%1307)
  %1318 : Tensor[] = prim::ListConstruct(%1302, %1317), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %q_head.13 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1318), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1320 : Tensor[] = prim::ListConstruct(%1302, %1316), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1321 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1320), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1322 : Tensor[] = prim::ListConstruct(%1302, %1315), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1323 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1322), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 119:0, 1024:1) = aten::to(%1303, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1325 : Tensor[] = prim::ListConstruct(%r.14, %1314), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1326 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1325), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1327 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %1313, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1328 : Tensor[] = prim::ListConstruct(%1327, %1321), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ac.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1328), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1330 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %1312, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1331 : Tensor[] = prim::ListConstruct(%1330, %1326), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.49 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1331), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1333 : int = aten::size(%ac.13, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1334 : int = aten::size(%x.49, %85), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1335 : int = aten::size(%x.49, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1336 : int = aten::size(%x.49, %76), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1337 : int = aten::size(%x.49, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1338 : Long() = prim::NumToTensor(%1337), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1339 : int[] = prim::ListConstruct(%1334, %1335, %1337, %1336), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.50 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1339), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1341 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1342 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1341, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1343 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1342, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1343, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1345 : Long() = aten::sub(%1338, %69, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1346 : int = aten::Int(%1345), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1347 : int[] = prim::ListConstruct(%1334, %1335, %1336, %1346), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.52 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1347), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1349 : Long(13:1) = aten::arange(%1333, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %64, %1349), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1351 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.13, %1311, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:294:0
  %1352 : Tensor[] = prim::ListConstruct(%1351, %1310), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1353 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1352), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1354 : Tensor[] = prim::ListConstruct(%154, %1353), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ef.13 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1354), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1356 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1357 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1356, %ef.13, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.111 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1357, %61), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.112 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %64, %77), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1360 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %80, %82), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1361 : Tensor[] = prim::ListConstruct(%1360, %1323), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1362 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1361), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1363 : Tensor[] = prim::ListConstruct(%1362, %1309), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %input.113 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1363), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.113, %80, %82), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.13, %1302, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1367 : Tensor = prim::GetAttr[name="bias"](%1308)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1308)
  %1369 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.114, %1369, %1368, %1367, %57, %58), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1371 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1372 : Float(13:121856, 119:1024, 1024:1), %1373 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1371)
  %1374 : __torch__.torch.nn.modules.normalization.___torch_mangle_40356.LayerNorm = prim::GetAttr[name="layer_norm"](%1306)
  %1375 : __torch__.torch.nn.modules.linear.___torch_mangle_40358.Linear = prim::GetAttr[name="layer_2"](%1306)
  %1376 : __torch__.torch.nn.modules.linear.___torch_mangle_40357.Linear = prim::GetAttr[name="layer_1"](%1306)
  %1377 : Tensor = prim::GetAttr[name="bias"](%1376)
  %1378 : Tensor = prim::GetAttr[name="weight"](%1376)
  %1379 : Float(1024:1, 4096:1024) = aten::t(%1378), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1372, %1379), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.37, %1377, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.115), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.116, %80, %82), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1384 : Tensor = prim::GetAttr[name="bias"](%1375)
  %1385 : Tensor = prim::GetAttr[name="weight"](%1375)
  %1386 : Float(4096:1, 1024:4096) = aten::t(%1385), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.117, %1386), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.38, %1384, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.118, %80, %82), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.39, %1372, %84), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1391 : Tensor = prim::GetAttr[name="bias"](%1374)
  %1392 : Tensor = prim::GetAttr[name="weight"](%1374)
  %1393 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.119, %1393, %1392, %1391, %57, %58), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1395 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1373)
  %1396 : Float(13:121856, 119:1024, 1024:1), %1397 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1395)
  %new_mem.14 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1396, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1399 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1400 : __torch__.transformers.modeling_xlnet.___torch_mangle_40370.XLNetFeedForward = prim::GetAttr[name="ff"](%107)
  %1401 : __torch__.transformers.modeling_xlnet.___torch_mangle_40365.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%107)
  %1402 : __torch__.torch.nn.modules.normalization.___torch_mangle_40363.LayerNorm = prim::GetAttr[name="layer_norm"](%1401)
  %1403 : Tensor = prim::GetAttr[name="o"](%1401)
  %1404 : Tensor = prim::GetAttr[name="seg_embed"](%1401)
  %1405 : Tensor = prim::GetAttr[name="r_s_bias"](%1401)
  %1406 : Tensor = prim::GetAttr[name="r_r_bias"](%1401)
  %1407 : Tensor = prim::GetAttr[name="r_w_bias"](%1401)
  %1408 : Tensor = prim::GetAttr[name="r"](%1401)
  %1409 : Tensor = prim::GetAttr[name="v"](%1401)
  %1410 : Tensor = prim::GetAttr[name="k"](%1401)
  %1411 : Tensor = prim::GetAttr[name="q"](%1401)
  %1412 : Tensor[] = prim::ListConstruct(%1396, %1411), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %q_head.14 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1412), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1414 : Tensor[] = prim::ListConstruct(%1396, %1410), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1415 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1414), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1416 : Tensor[] = prim::ListConstruct(%1396, %1409), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1417 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1416), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 119:0, 1024:1) = aten::to(%1397, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1419 : Tensor[] = prim::ListConstruct(%r.15, %1408), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1420 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1419), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1421 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %1407, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1422 : Tensor[] = prim::ListConstruct(%1421, %1415), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ac.14 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1422), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1424 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %1406, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1425 : Tensor[] = prim::ListConstruct(%1424, %1420), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.53 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1425), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1427 : int = aten::size(%ac.14, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1428 : int = aten::size(%x.53, %85), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1429 : int = aten::size(%x.53, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1430 : int = aten::size(%x.53, %76), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1431 : int = aten::size(%x.53, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1432 : Long() = prim::NumToTensor(%1431), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1433 : int[] = prim::ListConstruct(%1428, %1429, %1431, %1430), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.54 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1433), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1435 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1436 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1435, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1437 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1436, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1437, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1439 : Long() = aten::sub(%1432, %69, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1440 : int = aten::Int(%1439), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1441 : int[] = prim::ListConstruct(%1428, %1429, %1430, %1440), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.56 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1441), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1443 : Long(13:1) = aten::arange(%1427, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %64, %1443), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1445 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.14, %1405, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:294:0
  %1446 : Tensor[] = prim::ListConstruct(%1445, %1404), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1447 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1446), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1448 : Tensor[] = prim::ListConstruct(%154, %1447), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ef.14 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1448), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1450 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1451 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1450, %ef.14, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.120 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1451, %61), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.121 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %64, %77), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1454 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %80, %82), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1455 : Tensor[] = prim::ListConstruct(%1454, %1417), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1456 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1455), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1457 : Tensor[] = prim::ListConstruct(%1456, %1403), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %input.122 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1457), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.122, %80, %82), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.14, %1396, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1461 : Tensor = prim::GetAttr[name="bias"](%1402)
  %1462 : Tensor = prim::GetAttr[name="weight"](%1402)
  %1463 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.123, %1463, %1462, %1461, %57, %58), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1465 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1466 : Float(13:121856, 119:1024, 1024:1), %1467 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1465)
  %1468 : __torch__.torch.nn.modules.normalization.___torch_mangle_40366.LayerNorm = prim::GetAttr[name="layer_norm"](%1400)
  %1469 : __torch__.torch.nn.modules.linear.___torch_mangle_40368.Linear = prim::GetAttr[name="layer_2"](%1400)
  %1470 : __torch__.torch.nn.modules.linear.___torch_mangle_40367.Linear = prim::GetAttr[name="layer_1"](%1400)
  %1471 : Tensor = prim::GetAttr[name="bias"](%1470)
  %1472 : Tensor = prim::GetAttr[name="weight"](%1470)
  %1473 : Float(1024:1, 4096:1024) = aten::t(%1472), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1466, %1473), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.40, %1471, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.125, %80, %82), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1478 : Tensor = prim::GetAttr[name="bias"](%1469)
  %1479 : Tensor = prim::GetAttr[name="weight"](%1469)
  %1480 : Float(4096:1, 1024:4096) = aten::t(%1479), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.126, %1480), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.41, %1478, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.127, %80, %82), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.42, %1466, %84), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1485 : Tensor = prim::GetAttr[name="bias"](%1468)
  %1486 : Tensor = prim::GetAttr[name="weight"](%1468)
  %1487 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.128, %1487, %1486, %1485, %57, %58), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1489 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1467)
  %1490 : Float(13:121856, 119:1024, 1024:1), %1491 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1489)
  %new_mem.15 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1490, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1493 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1494 : __torch__.transformers.modeling_xlnet.___torch_mangle_40380.XLNetFeedForward = prim::GetAttr[name="ff"](%105)
  %1495 : __torch__.transformers.modeling_xlnet.___torch_mangle_40375.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%105)
  %1496 : __torch__.torch.nn.modules.normalization.___torch_mangle_40373.LayerNorm = prim::GetAttr[name="layer_norm"](%1495)
  %1497 : Tensor = prim::GetAttr[name="o"](%1495)
  %1498 : Tensor = prim::GetAttr[name="seg_embed"](%1495)
  %1499 : Tensor = prim::GetAttr[name="r_s_bias"](%1495)
  %1500 : Tensor = prim::GetAttr[name="r_r_bias"](%1495)
  %1501 : Tensor = prim::GetAttr[name="r_w_bias"](%1495)
  %1502 : Tensor = prim::GetAttr[name="r"](%1495)
  %1503 : Tensor = prim::GetAttr[name="v"](%1495)
  %1504 : Tensor = prim::GetAttr[name="k"](%1495)
  %1505 : Tensor = prim::GetAttr[name="q"](%1495)
  %1506 : Tensor[] = prim::ListConstruct(%1490, %1505), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %q_head.15 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1506), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1508 : Tensor[] = prim::ListConstruct(%1490, %1504), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1509 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1508), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1510 : Tensor[] = prim::ListConstruct(%1490, %1503), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1511 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1510), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 119:0, 1024:1) = aten::to(%1491, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1513 : Tensor[] = prim::ListConstruct(%r.16, %1502), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1514 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1513), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1515 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %1501, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1516 : Tensor[] = prim::ListConstruct(%1515, %1509), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ac.15 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1516), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1518 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %1500, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %1519 : Tensor[] = prim::ListConstruct(%1518, %1514), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.57 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1519), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1521 : int = aten::size(%ac.15, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1522 : int = aten::size(%x.57, %85), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1523 : int = aten::size(%x.57, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1524 : int = aten::size(%x.57, %76), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1525 : int = aten::size(%x.57, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1526 : Long() = prim::NumToTensor(%1525), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1527 : int[] = prim::ListConstruct(%1522, %1523, %1525, %1524), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.58 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %1527), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %1529 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1530 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1529, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1531 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1530, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1531, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1533 : Long() = aten::sub(%1526, %69, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1534 : int = aten::Int(%1533), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1535 : int[] = prim::ListConstruct(%1522, %1523, %1524, %1534), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.60 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %1535), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1537 : Long(13:1) = aten::arange(%1521, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %64, %1537), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1539 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.15, %1499, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:294:0
  %1540 : Tensor[] = prim::ListConstruct(%1539, %1498), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1541 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1540), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1542 : Tensor[] = prim::ListConstruct(%154, %1541), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ef.15 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1542), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1544 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1545 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1544, %ef.15, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.129 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1545, %61), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.130 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %64, %77), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %1548 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %80, %82), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1549 : Tensor[] = prim::ListConstruct(%1548, %1511), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1550 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1549), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1551 : Tensor[] = prim::ListConstruct(%1550, %1497), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %input.131 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1551), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.131, %80, %82), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.15, %1490, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %1555 : Tensor = prim::GetAttr[name="bias"](%1496)
  %1556 : Tensor = prim::GetAttr[name="weight"](%1496)
  %1557 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.132, %1557, %1556, %1555, %57, %58), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1559 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %1560 : Float(13:121856, 119:1024, 1024:1), %1561 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1559)
  %1562 : __torch__.torch.nn.modules.normalization.___torch_mangle_40376.LayerNorm = prim::GetAttr[name="layer_norm"](%1494)
  %1563 : __torch__.torch.nn.modules.linear.___torch_mangle_40378.Linear = prim::GetAttr[name="layer_2"](%1494)
  %1564 : __torch__.torch.nn.modules.linear.___torch_mangle_40377.Linear = prim::GetAttr[name="layer_1"](%1494)
  %1565 : Tensor = prim::GetAttr[name="bias"](%1564)
  %1566 : Tensor = prim::GetAttr[name="weight"](%1564)
  %1567 : Float(1024:1, 4096:1024) = aten::t(%1566), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1560, %1567), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.43, %1565, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.134, %80, %82), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %1572 : Tensor = prim::GetAttr[name="bias"](%1563)
  %1573 : Tensor = prim::GetAttr[name="weight"](%1563)
  %1574 : Float(4096:1, 1024:4096) = aten::t(%1573), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.135, %1574), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.44, %1572, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.136, %80, %82), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.45, %1560, %84), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %1579 : Tensor = prim::GetAttr[name="bias"](%1562)
  %1580 : Tensor = prim::GetAttr[name="weight"](%1562)
  %1581 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.137, %1581, %1580, %1579, %57, %58), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %1583 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %1561)
  %1584 : Float(13:121856, 119:1024, 1024:1), %1585 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1583)
  %new_mem.16 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1584, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1587 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1588 : __torch__.transformers.modeling_xlnet.___torch_mangle_40390.XLNetFeedForward = prim::GetAttr[name="ff"](%103)
  %1589 : __torch__.transformers.modeling_xlnet.___torch_mangle_40385.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%103)
  %1590 : __torch__.torch.nn.modules.normalization.___torch_mangle_40383.LayerNorm = prim::GetAttr[name="layer_norm"](%1589)
  %1591 : Tensor = prim::GetAttr[name="o"](%1589)
  %1592 : Tensor = prim::GetAttr[name="seg_embed"](%1589)
  %1593 : Tensor = prim::GetAttr[name="r_s_bias"](%1589)
  %1594 : Tensor = prim::GetAttr[name="r_r_bias"](%1589)
  %1595 : Tensor = prim::GetAttr[name="r_w_bias"](%1589)
  %1596 : Tensor = prim::GetAttr[name="r"](%1589)
  %1597 : Tensor = prim::GetAttr[name="v"](%1589)
  %1598 : Tensor = prim::GetAttr[name="k"](%1589)
  %1599 : Tensor = prim::GetAttr[name="q"](%1589)
  %1600 : Tensor[] = prim::ListConstruct(%1584, %1599), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %q_head.16 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1600), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1602 : Tensor[] = prim::ListConstruct(%1584, %1598), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1603 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1602), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1604 : Tensor[] = prim::ListConstruct(%1584, %1597), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1605 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1604), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 119:0, 1024:1) = aten::to(%1585, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %1607 : Tensor[] = prim::ListConstruct(%r.17, %1596), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1608 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1607), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1609 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %1595, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %1610 : Tensor[] = prim::ListConstruct(%1609, %1603), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ac.16 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1610), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1612 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %1594, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %1613 : Tensor[] = prim::ListConstruct(%1612, %1608), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.61 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1613), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1615 : int = aten::size(%ac.16, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %1616 : int = aten::size(%x.61, %85), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1617 : int = aten::size(%x.61, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1618 : int = aten::size(%x.61, %76), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1619 : int = aten::size(%x.61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1620 : Long() = prim::NumToTensor(%1619), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1621 : int[] = prim::ListConstruct(%1616, %1617, %1619, %1618), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.62 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %1621), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %1623 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1624 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1623, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1625 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1624, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1625, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1627 : Long() = aten::sub(%1620, %69, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1628 : int = aten::Int(%1627), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1629 : int[] = prim::ListConstruct(%1616, %1617, %1618, %1628), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.64 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %1629), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1631 : Long(13:1) = aten::arange(%1615, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %64, %1631), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %1633 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.16, %1593, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:294:0
  %1634 : Tensor[] = prim::ListConstruct(%1633, %1592), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1635 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1634), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1636 : Tensor[] = prim::ListConstruct(%154, %1635), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ef.16 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1636), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1638 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1639 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1638, %ef.16, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.138 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1639, %61), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.139 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %64, %77), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %1642 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %80, %82), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %1643 : Tensor[] = prim::ListConstruct(%1642, %1605), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1644 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1643), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1645 : Tensor[] = prim::ListConstruct(%1644, %1591), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %input.140 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1645), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.140, %80, %82), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.16, %1584, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %1649 : Tensor = prim::GetAttr[name="bias"](%1590)
  %1650 : Tensor = prim::GetAttr[name="weight"](%1590)
  %1651 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.141, %1651, %1650, %1649, %57, %58), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1653 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %1654 : Float(13:121856, 119:1024, 1024:1), %1655 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1653)
  %1656 : __torch__.torch.nn.modules.normalization.___torch_mangle_40386.LayerNorm = prim::GetAttr[name="layer_norm"](%1588)
  %1657 : __torch__.torch.nn.modules.linear.___torch_mangle_40388.Linear = prim::GetAttr[name="layer_2"](%1588)
  %1658 : __torch__.torch.nn.modules.linear.___torch_mangle_40387.Linear = prim::GetAttr[name="layer_1"](%1588)
  %1659 : Tensor = prim::GetAttr[name="bias"](%1658)
  %1660 : Tensor = prim::GetAttr[name="weight"](%1658)
  %1661 : Float(1024:1, 4096:1024) = aten::t(%1660), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1654, %1661), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.46, %1659, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.142), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.143, %80, %82), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %1666 : Tensor = prim::GetAttr[name="bias"](%1657)
  %1667 : Tensor = prim::GetAttr[name="weight"](%1657)
  %1668 : Float(4096:1, 1024:4096) = aten::t(%1667), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.144, %1668), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.47, %1666, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.145, %80, %82), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.48, %1654, %84), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %1673 : Tensor = prim::GetAttr[name="bias"](%1656)
  %1674 : Tensor = prim::GetAttr[name="weight"](%1656)
  %1675 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.146, %1675, %1674, %1673, %57, %58), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %1677 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %1655)
  %1678 : Float(13:121856, 119:1024, 1024:1), %1679 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1677)
  %new_mem.17 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1678, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1681 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1682 : __torch__.transformers.modeling_xlnet.___torch_mangle_40400.XLNetFeedForward = prim::GetAttr[name="ff"](%101)
  %1683 : __torch__.transformers.modeling_xlnet.___torch_mangle_40395.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%101)
  %1684 : __torch__.torch.nn.modules.normalization.___torch_mangle_40393.LayerNorm = prim::GetAttr[name="layer_norm"](%1683)
  %1685 : Tensor = prim::GetAttr[name="o"](%1683)
  %1686 : Tensor = prim::GetAttr[name="seg_embed"](%1683)
  %1687 : Tensor = prim::GetAttr[name="r_s_bias"](%1683)
  %1688 : Tensor = prim::GetAttr[name="r_r_bias"](%1683)
  %1689 : Tensor = prim::GetAttr[name="r_w_bias"](%1683)
  %1690 : Tensor = prim::GetAttr[name="r"](%1683)
  %1691 : Tensor = prim::GetAttr[name="v"](%1683)
  %1692 : Tensor = prim::GetAttr[name="k"](%1683)
  %1693 : Tensor = prim::GetAttr[name="q"](%1683)
  %1694 : Tensor[] = prim::ListConstruct(%1678, %1693), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %q_head.17 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1694), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1696 : Tensor[] = prim::ListConstruct(%1678, %1692), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1697 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1696), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1698 : Tensor[] = prim::ListConstruct(%1678, %1691), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1699 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1698), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 119:0, 1024:1) = aten::to(%1679, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %1701 : Tensor[] = prim::ListConstruct(%r.18, %1690), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1702 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1701), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1703 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %1689, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %1704 : Tensor[] = prim::ListConstruct(%1703, %1697), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ac.17 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1704), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1706 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %1688, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %1707 : Tensor[] = prim::ListConstruct(%1706, %1702), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.65 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1707), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1709 : int = aten::size(%ac.17, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %1710 : int = aten::size(%x.65, %85), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1711 : int = aten::size(%x.65, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1712 : int = aten::size(%x.65, %76), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1713 : int = aten::size(%x.65, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1714 : Long() = prim::NumToTensor(%1713), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1715 : int[] = prim::ListConstruct(%1710, %1711, %1713, %1712), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.66 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %1715), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %1717 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1718 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1717, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1719 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1718, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1719, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1721 : Long() = aten::sub(%1714, %69, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1722 : int = aten::Int(%1721), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1723 : int[] = prim::ListConstruct(%1710, %1711, %1712, %1722), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.68 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %1723), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1725 : Long(13:1) = aten::arange(%1709, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %64, %1725), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %1727 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.17, %1687, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:294:0
  %1728 : Tensor[] = prim::ListConstruct(%1727, %1686), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1729 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1728), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1730 : Tensor[] = prim::ListConstruct(%154, %1729), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ef.17 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1730), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1732 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1733 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1732, %ef.17, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.147 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1733, %61), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.148 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %64, %77), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %1736 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %80, %82), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %1737 : Tensor[] = prim::ListConstruct(%1736, %1699), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1738 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1737), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1739 : Tensor[] = prim::ListConstruct(%1738, %1685), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %input.149 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1739), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.149, %80, %82), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.17, %1678, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %1743 : Tensor = prim::GetAttr[name="bias"](%1684)
  %1744 : Tensor = prim::GetAttr[name="weight"](%1684)
  %1745 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.150, %1745, %1744, %1743, %57, %58), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1747 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %1748 : Float(13:121856, 119:1024, 1024:1), %1749 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1747)
  %1750 : __torch__.torch.nn.modules.normalization.___torch_mangle_40396.LayerNorm = prim::GetAttr[name="layer_norm"](%1682)
  %1751 : __torch__.torch.nn.modules.linear.___torch_mangle_40398.Linear = prim::GetAttr[name="layer_2"](%1682)
  %1752 : __torch__.torch.nn.modules.linear.___torch_mangle_40397.Linear = prim::GetAttr[name="layer_1"](%1682)
  %1753 : Tensor = prim::GetAttr[name="bias"](%1752)
  %1754 : Tensor = prim::GetAttr[name="weight"](%1752)
  %1755 : Float(1024:1, 4096:1024) = aten::t(%1754), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1748, %1755), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.49, %1753, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.151), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.152, %80, %82), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %1760 : Tensor = prim::GetAttr[name="bias"](%1751)
  %1761 : Tensor = prim::GetAttr[name="weight"](%1751)
  %1762 : Float(4096:1, 1024:4096) = aten::t(%1761), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.153, %1762), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.50, %1760, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.154, %80, %82), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.51, %1748, %84), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %1767 : Tensor = prim::GetAttr[name="bias"](%1750)
  %1768 : Tensor = prim::GetAttr[name="weight"](%1750)
  %1769 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.155, %1769, %1768, %1767, %57, %58), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %1771 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %1749)
  %1772 : Float(13:121856, 119:1024, 1024:1), %1773 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1771)
  %new_mem.18 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1772, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1775 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1776 : __torch__.transformers.modeling_xlnet.___torch_mangle_40410.XLNetFeedForward = prim::GetAttr[name="ff"](%99)
  %1777 : __torch__.transformers.modeling_xlnet.___torch_mangle_40405.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%99)
  %1778 : __torch__.torch.nn.modules.normalization.___torch_mangle_40403.LayerNorm = prim::GetAttr[name="layer_norm"](%1777)
  %1779 : Tensor = prim::GetAttr[name="o"](%1777)
  %1780 : Tensor = prim::GetAttr[name="seg_embed"](%1777)
  %1781 : Tensor = prim::GetAttr[name="r_s_bias"](%1777)
  %1782 : Tensor = prim::GetAttr[name="r_r_bias"](%1777)
  %1783 : Tensor = prim::GetAttr[name="r_w_bias"](%1777)
  %1784 : Tensor = prim::GetAttr[name="r"](%1777)
  %1785 : Tensor = prim::GetAttr[name="v"](%1777)
  %1786 : Tensor = prim::GetAttr[name="k"](%1777)
  %1787 : Tensor = prim::GetAttr[name="q"](%1777)
  %1788 : Tensor[] = prim::ListConstruct(%1772, %1787), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %q_head.18 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1788), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1790 : Tensor[] = prim::ListConstruct(%1772, %1786), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1791 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1790), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1792 : Tensor[] = prim::ListConstruct(%1772, %1785), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1793 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1792), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 119:0, 1024:1) = aten::to(%1773, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %1795 : Tensor[] = prim::ListConstruct(%r.19, %1784), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1796 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1795), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1797 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %1783, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %1798 : Tensor[] = prim::ListConstruct(%1797, %1791), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ac.18 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1798), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1800 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %1782, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %1801 : Tensor[] = prim::ListConstruct(%1800, %1796), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.69 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1801), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1803 : int = aten::size(%ac.18, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %1804 : int = aten::size(%x.69, %85), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1805 : int = aten::size(%x.69, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1806 : int = aten::size(%x.69, %76), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1807 : int = aten::size(%x.69, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1808 : Long() = prim::NumToTensor(%1807), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1809 : int[] = prim::ListConstruct(%1804, %1805, %1807, %1806), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.70 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %1809), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %1811 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1812 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1811, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1813 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1812, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1813, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1815 : Long() = aten::sub(%1808, %69, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1816 : int = aten::Int(%1815), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1817 : int[] = prim::ListConstruct(%1804, %1805, %1806, %1816), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.72 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %1817), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1819 : Long(13:1) = aten::arange(%1803, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %64, %1819), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %1821 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.18, %1781, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:294:0
  %1822 : Tensor[] = prim::ListConstruct(%1821, %1780), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1823 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1822), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1824 : Tensor[] = prim::ListConstruct(%154, %1823), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ef.18 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1824), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1826 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1827 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1826, %ef.18, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.156 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1827, %61), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.157 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %64, %77), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %1830 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %80, %82), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %1831 : Tensor[] = prim::ListConstruct(%1830, %1793), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1832 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1831), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1833 : Tensor[] = prim::ListConstruct(%1832, %1779), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %input.158 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1833), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.158, %80, %82), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.18, %1772, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %1837 : Tensor = prim::GetAttr[name="bias"](%1778)
  %1838 : Tensor = prim::GetAttr[name="weight"](%1778)
  %1839 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.159, %1839, %1838, %1837, %57, %58), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1841 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %1842 : Float(13:121856, 119:1024, 1024:1), %1843 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1841)
  %1844 : __torch__.torch.nn.modules.normalization.___torch_mangle_40406.LayerNorm = prim::GetAttr[name="layer_norm"](%1776)
  %1845 : __torch__.torch.nn.modules.linear.___torch_mangle_40408.Linear = prim::GetAttr[name="layer_2"](%1776)
  %1846 : __torch__.torch.nn.modules.linear.___torch_mangle_40407.Linear = prim::GetAttr[name="layer_1"](%1776)
  %1847 : Tensor = prim::GetAttr[name="bias"](%1846)
  %1848 : Tensor = prim::GetAttr[name="weight"](%1846)
  %1849 : Float(1024:1, 4096:1024) = aten::t(%1848), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1842, %1849), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.52, %1847, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.160), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.161, %80, %82), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %1854 : Tensor = prim::GetAttr[name="bias"](%1845)
  %1855 : Tensor = prim::GetAttr[name="weight"](%1845)
  %1856 : Float(4096:1, 1024:4096) = aten::t(%1855), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.162, %1856), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.53, %1854, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.163, %80, %82), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.54, %1842, %84), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %1861 : Tensor = prim::GetAttr[name="bias"](%1844)
  %1862 : Tensor = prim::GetAttr[name="weight"](%1844)
  %1863 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.164, %1863, %1862, %1861, %57, %58), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %1865 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %1843)
  %1866 : Float(13:121856, 119:1024, 1024:1), %1867 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1865)
  %new_mem.19 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1866, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1869 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1870 : __torch__.transformers.modeling_xlnet.___torch_mangle_40420.XLNetFeedForward = prim::GetAttr[name="ff"](%97)
  %1871 : __torch__.transformers.modeling_xlnet.___torch_mangle_40415.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%97)
  %1872 : __torch__.torch.nn.modules.normalization.___torch_mangle_40413.LayerNorm = prim::GetAttr[name="layer_norm"](%1871)
  %1873 : Tensor = prim::GetAttr[name="o"](%1871)
  %1874 : Tensor = prim::GetAttr[name="seg_embed"](%1871)
  %1875 : Tensor = prim::GetAttr[name="r_s_bias"](%1871)
  %1876 : Tensor = prim::GetAttr[name="r_r_bias"](%1871)
  %1877 : Tensor = prim::GetAttr[name="r_w_bias"](%1871)
  %1878 : Tensor = prim::GetAttr[name="r"](%1871)
  %1879 : Tensor = prim::GetAttr[name="v"](%1871)
  %1880 : Tensor = prim::GetAttr[name="k"](%1871)
  %1881 : Tensor = prim::GetAttr[name="q"](%1871)
  %1882 : Tensor[] = prim::ListConstruct(%1866, %1881), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %q_head.19 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1882), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1884 : Tensor[] = prim::ListConstruct(%1866, %1880), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1885 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1884), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1886 : Tensor[] = prim::ListConstruct(%1866, %1879), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1887 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1886), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 119:0, 1024:1) = aten::to(%1867, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %1889 : Tensor[] = prim::ListConstruct(%r.20, %1878), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1890 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1889), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1891 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %1877, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %1892 : Tensor[] = prim::ListConstruct(%1891, %1885), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ac.19 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1892), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1894 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %1876, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %1895 : Tensor[] = prim::ListConstruct(%1894, %1890), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.73 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1895), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1897 : int = aten::size(%ac.19, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %1898 : int = aten::size(%x.73, %85), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1899 : int = aten::size(%x.73, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1900 : int = aten::size(%x.73, %76), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1901 : int = aten::size(%x.73, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1902 : Long() = prim::NumToTensor(%1901), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1903 : int[] = prim::ListConstruct(%1898, %1899, %1901, %1900), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.74 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %1903), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %1905 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1906 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1905, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1907 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1906, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%1907, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1909 : Long() = aten::sub(%1902, %69, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1910 : int = aten::Int(%1909), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1911 : int[] = prim::ListConstruct(%1898, %1899, %1900, %1910), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.76 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %1911), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1913 : Long(13:1) = aten::arange(%1897, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %64, %1913), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %1915 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.19, %1875, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:294:0
  %1916 : Tensor[] = prim::ListConstruct(%1915, %1874), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1917 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %1916), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1918 : Tensor[] = prim::ListConstruct(%154, %1917), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ef.19 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %1918), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1920 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1921 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%1920, %ef.19, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.165 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%1921, %61), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.166 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %64, %77), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %1924 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %80, %82), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %1925 : Tensor[] = prim::ListConstruct(%1924, %1887), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1926 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %1925), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1927 : Tensor[] = prim::ListConstruct(%1926, %1873), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %input.167 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %1927), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.167, %80, %82), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.19, %1866, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %1931 : Tensor = prim::GetAttr[name="bias"](%1872)
  %1932 : Tensor = prim::GetAttr[name="weight"](%1872)
  %1933 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.168, %1933, %1932, %1931, %57, %58), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1935 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %1936 : Float(13:121856, 119:1024, 1024:1), %1937 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1935)
  %1938 : __torch__.torch.nn.modules.normalization.___torch_mangle_40416.LayerNorm = prim::GetAttr[name="layer_norm"](%1870)
  %1939 : __torch__.torch.nn.modules.linear.___torch_mangle_40418.Linear = prim::GetAttr[name="layer_2"](%1870)
  %1940 : __torch__.torch.nn.modules.linear.___torch_mangle_40417.Linear = prim::GetAttr[name="layer_1"](%1870)
  %1941 : Tensor = prim::GetAttr[name="bias"](%1940)
  %1942 : Tensor = prim::GetAttr[name="weight"](%1940)
  %1943 : Float(1024:1, 4096:1024) = aten::t(%1942), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%1936, %1943), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.55, %1941, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.169), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.170, %80, %82), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %1948 : Tensor = prim::GetAttr[name="bias"](%1939)
  %1949 : Tensor = prim::GetAttr[name="weight"](%1939)
  %1950 : Float(4096:1, 1024:4096) = aten::t(%1949), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.171, %1950), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.56, %1948, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.172, %80, %82), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.57, %1936, %84), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %1955 : Tensor = prim::GetAttr[name="bias"](%1938)
  %1956 : Tensor = prim::GetAttr[name="weight"](%1938)
  %1957 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.173, %1957, %1956, %1955, %57, %58), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %1959 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %1937)
  %1960 : Float(13:121856, 119:1024, 1024:1), %1961 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%1959)
  %new_mem.20 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%1960, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1963 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1964 : __torch__.transformers.modeling_xlnet.___torch_mangle_40430.XLNetFeedForward = prim::GetAttr[name="ff"](%95)
  %1965 : __torch__.transformers.modeling_xlnet.___torch_mangle_40425.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%95)
  %1966 : __torch__.torch.nn.modules.normalization.___torch_mangle_40423.LayerNorm = prim::GetAttr[name="layer_norm"](%1965)
  %1967 : Tensor = prim::GetAttr[name="o"](%1965)
  %1968 : Tensor = prim::GetAttr[name="seg_embed"](%1965)
  %1969 : Tensor = prim::GetAttr[name="r_s_bias"](%1965)
  %1970 : Tensor = prim::GetAttr[name="r_r_bias"](%1965)
  %1971 : Tensor = prim::GetAttr[name="r_w_bias"](%1965)
  %1972 : Tensor = prim::GetAttr[name="r"](%1965)
  %1973 : Tensor = prim::GetAttr[name="v"](%1965)
  %1974 : Tensor = prim::GetAttr[name="k"](%1965)
  %1975 : Tensor = prim::GetAttr[name="q"](%1965)
  %1976 : Tensor[] = prim::ListConstruct(%1960, %1975), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %q_head.20 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1976), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1978 : Tensor[] = prim::ListConstruct(%1960, %1974), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1979 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1978), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1980 : Tensor[] = prim::ListConstruct(%1960, %1973), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1981 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1980), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 119:0, 1024:1) = aten::to(%1961, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %1983 : Tensor[] = prim::ListConstruct(%r.21, %1972), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1984 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %1983), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1985 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %1971, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %1986 : Tensor[] = prim::ListConstruct(%1985, %1979), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ac.20 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %1986), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1988 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %1970, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %1989 : Tensor[] = prim::ListConstruct(%1988, %1984), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.77 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %1989), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1991 : int = aten::size(%ac.20, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %1992 : int = aten::size(%x.77, %85), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1993 : int = aten::size(%x.77, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1994 : int = aten::size(%x.77, %76), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1995 : int = aten::size(%x.77, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1996 : Long() = prim::NumToTensor(%1995), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1997 : int[] = prim::ListConstruct(%1992, %1993, %1995, %1994), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.78 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %1997), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %1999 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2000 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%1999, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2001 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2000, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2001, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2003 : Long() = aten::sub(%1996, %69, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %2004 : int = aten::Int(%2003), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %2005 : int[] = prim::ListConstruct(%1992, %1993, %1994, %2004), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.80 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %2005), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %2007 : Long(13:1) = aten::arange(%1991, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %64, %2007), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %2009 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.20, %1969, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:294:0
  %2010 : Tensor[] = prim::ListConstruct(%2009, %1968), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %2011 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %2010), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %2012 : Tensor[] = prim::ListConstruct(%154, %2011), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ef.20 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %2012), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %2014 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2015 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%2014, %ef.20, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.174 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%2015, %61), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.175 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %64, %77), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %2018 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %80, %82), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %2019 : Tensor[] = prim::ListConstruct(%2018, %1981), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %2020 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %2019), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %2021 : Tensor[] = prim::ListConstruct(%2020, %1967), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %input.176 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %2021), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.176, %80, %82), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.20, %1960, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %2025 : Tensor = prim::GetAttr[name="bias"](%1966)
  %2026 : Tensor = prim::GetAttr[name="weight"](%1966)
  %2027 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.177, %2027, %2026, %2025, %57, %58), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2029 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %2030 : Float(13:121856, 119:1024, 1024:1), %2031 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2029)
  %2032 : __torch__.torch.nn.modules.normalization.___torch_mangle_40426.LayerNorm = prim::GetAttr[name="layer_norm"](%1964)
  %2033 : __torch__.torch.nn.modules.linear.___torch_mangle_40428.Linear = prim::GetAttr[name="layer_2"](%1964)
  %2034 : __torch__.torch.nn.modules.linear.___torch_mangle_40427.Linear = prim::GetAttr[name="layer_1"](%1964)
  %2035 : Tensor = prim::GetAttr[name="bias"](%2034)
  %2036 : Tensor = prim::GetAttr[name="weight"](%2034)
  %2037 : Float(1024:1, 4096:1024) = aten::t(%2036), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%2030, %2037), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.58, %2035, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.178), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.179, %80, %82), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %2042 : Tensor = prim::GetAttr[name="bias"](%2033)
  %2043 : Tensor = prim::GetAttr[name="weight"](%2033)
  %2044 : Float(4096:1, 1024:4096) = aten::t(%2043), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.180, %2044), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.59, %2042, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.181, %80, %82), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.60, %2030, %84), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %2049 : Tensor = prim::GetAttr[name="bias"](%2032)
  %2050 : Tensor = prim::GetAttr[name="weight"](%2032)
  %2051 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.182, %2051, %2050, %2049, %57, %58), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %2053 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %2031)
  %2054 : Float(13:121856, 119:1024, 1024:1), %2055 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2053)
  %new_mem.21 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%2054, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2057 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2058 : __torch__.transformers.modeling_xlnet.___torch_mangle_40440.XLNetFeedForward = prim::GetAttr[name="ff"](%93)
  %2059 : __torch__.transformers.modeling_xlnet.___torch_mangle_40435.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%93)
  %2060 : __torch__.torch.nn.modules.normalization.___torch_mangle_40433.LayerNorm = prim::GetAttr[name="layer_norm"](%2059)
  %2061 : Tensor = prim::GetAttr[name="o"](%2059)
  %2062 : Tensor = prim::GetAttr[name="seg_embed"](%2059)
  %2063 : Tensor = prim::GetAttr[name="r_s_bias"](%2059)
  %2064 : Tensor = prim::GetAttr[name="r_r_bias"](%2059)
  %2065 : Tensor = prim::GetAttr[name="r_w_bias"](%2059)
  %2066 : Tensor = prim::GetAttr[name="r"](%2059)
  %2067 : Tensor = prim::GetAttr[name="v"](%2059)
  %2068 : Tensor = prim::GetAttr[name="k"](%2059)
  %2069 : Tensor = prim::GetAttr[name="q"](%2059)
  %2070 : Tensor[] = prim::ListConstruct(%2054, %2069), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %q_head.21 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2070), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2072 : Tensor[] = prim::ListConstruct(%2054, %2068), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2073 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2072), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2074 : Tensor[] = prim::ListConstruct(%2054, %2067), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2075 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2074), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 119:0, 1024:1) = aten::to(%2055, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2077 : Tensor[] = prim::ListConstruct(%r.22, %2066), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2078 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2077), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2079 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %2065, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2080 : Tensor[] = prim::ListConstruct(%2079, %2073), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ac.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %2080), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2082 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %2064, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2083 : Tensor[] = prim::ListConstruct(%2082, %2078), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.81 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %2083), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2085 : int = aten::size(%ac.21, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2086 : int = aten::size(%x.81, %85), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2087 : int = aten::size(%x.81, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2088 : int = aten::size(%x.81, %76), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2089 : int = aten::size(%x.81, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2090 : Long() = prim::NumToTensor(%2089), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2091 : int[] = prim::ListConstruct(%2086, %2087, %2089, %2088), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.82 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2091), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2093 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2094 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%2093, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2095 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2094, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2095, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2097 : Long() = aten::sub(%2090, %69, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2098 : int = aten::Int(%2097), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2099 : int[] = prim::ListConstruct(%2086, %2087, %2088, %2098), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.84 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2099), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2101 : Long(13:1) = aten::arange(%2085, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %64, %2101), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2103 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.21, %2063, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:294:0
  %2104 : Tensor[] = prim::ListConstruct(%2103, %2062), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2105 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %2104), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2106 : Tensor[] = prim::ListConstruct(%154, %2105), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ef.21 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %2106), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2108 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2109 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%2108, %ef.21, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.183 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%2109, %61), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.184 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %64, %77), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2112 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %80, %82), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2113 : Tensor[] = prim::ListConstruct(%2112, %2075), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2114 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %2113), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2115 : Tensor[] = prim::ListConstruct(%2114, %2061), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %input.185 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %2115), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.185, %80, %82), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.21, %2054, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2119 : Tensor = prim::GetAttr[name="bias"](%2060)
  %2120 : Tensor = prim::GetAttr[name="weight"](%2060)
  %2121 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.186, %2121, %2120, %2119, %57, %58), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2123 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2124 : Float(13:121856, 119:1024, 1024:1), %2125 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2123)
  %2126 : __torch__.torch.nn.modules.normalization.___torch_mangle_40436.LayerNorm = prim::GetAttr[name="layer_norm"](%2058)
  %2127 : __torch__.torch.nn.modules.linear.___torch_mangle_40438.Linear = prim::GetAttr[name="layer_2"](%2058)
  %2128 : __torch__.torch.nn.modules.linear.___torch_mangle_40437.Linear = prim::GetAttr[name="layer_1"](%2058)
  %2129 : Tensor = prim::GetAttr[name="bias"](%2128)
  %2130 : Tensor = prim::GetAttr[name="weight"](%2128)
  %2131 : Float(1024:1, 4096:1024) = aten::t(%2130), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%2124, %2131), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.61, %2129, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.187), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.188, %80, %82), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2136 : Tensor = prim::GetAttr[name="bias"](%2127)
  %2137 : Tensor = prim::GetAttr[name="weight"](%2127)
  %2138 : Float(4096:1, 1024:4096) = aten::t(%2137), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.189, %2138), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.62, %2136, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.190, %80, %82), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.63, %2124, %84), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2143 : Tensor = prim::GetAttr[name="bias"](%2126)
  %2144 : Tensor = prim::GetAttr[name="weight"](%2126)
  %2145 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.191, %2145, %2144, %2143, %57, %58), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2147 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2125)
  %2148 : Float(13:121856, 119:1024, 1024:1), %2149 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2147)
  %new_mem.22 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%2148, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2151 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2152 : __torch__.transformers.modeling_xlnet.___torch_mangle_40450.XLNetFeedForward = prim::GetAttr[name="ff"](%91)
  %2153 : __torch__.transformers.modeling_xlnet.___torch_mangle_40445.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%91)
  %2154 : __torch__.torch.nn.modules.normalization.___torch_mangle_40443.LayerNorm = prim::GetAttr[name="layer_norm"](%2153)
  %2155 : Tensor = prim::GetAttr[name="o"](%2153)
  %2156 : Tensor = prim::GetAttr[name="seg_embed"](%2153)
  %2157 : Tensor = prim::GetAttr[name="r_s_bias"](%2153)
  %2158 : Tensor = prim::GetAttr[name="r_r_bias"](%2153)
  %2159 : Tensor = prim::GetAttr[name="r_w_bias"](%2153)
  %2160 : Tensor = prim::GetAttr[name="r"](%2153)
  %2161 : Tensor = prim::GetAttr[name="v"](%2153)
  %2162 : Tensor = prim::GetAttr[name="k"](%2153)
  %2163 : Tensor = prim::GetAttr[name="q"](%2153)
  %2164 : Tensor[] = prim::ListConstruct(%2148, %2163), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %q_head.22 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2164), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2166 : Tensor[] = prim::ListConstruct(%2148, %2162), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2167 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2166), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2168 : Tensor[] = prim::ListConstruct(%2148, %2161), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2169 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2168), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 119:0, 1024:1) = aten::to(%2149, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2171 : Tensor[] = prim::ListConstruct(%r.23, %2160), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2172 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2171), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2173 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %2159, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2174 : Tensor[] = prim::ListConstruct(%2173, %2167), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ac.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %2174), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2176 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %2158, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2177 : Tensor[] = prim::ListConstruct(%2176, %2172), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.85 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %2177), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2179 : int = aten::size(%ac.22, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2180 : int = aten::size(%x.85, %85), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2181 : int = aten::size(%x.85, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2182 : int = aten::size(%x.85, %76), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2183 : int = aten::size(%x.85, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2184 : Long() = prim::NumToTensor(%2183), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2185 : int[] = prim::ListConstruct(%2180, %2181, %2183, %2182), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.86 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2185), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2187 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2188 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%2187, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2189 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2188, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2189, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2191 : Long() = aten::sub(%2184, %69, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2192 : int = aten::Int(%2191), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2193 : int[] = prim::ListConstruct(%2180, %2181, %2182, %2192), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.88 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2193), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2195 : Long(13:1) = aten::arange(%2179, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %64, %2195), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2197 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.22, %2157, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:294:0
  %2198 : Tensor[] = prim::ListConstruct(%2197, %2156), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2199 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %2198), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2200 : Tensor[] = prim::ListConstruct(%154, %2199), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ef.22 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %2200), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2202 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2203 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%2202, %ef.22, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.192 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%2203, %61), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.193 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %64, %77), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2206 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %80, %82), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2207 : Tensor[] = prim::ListConstruct(%2206, %2169), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2208 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %2207), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2209 : Tensor[] = prim::ListConstruct(%2208, %2155), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %input.194 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %2209), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.194, %80, %82), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.22, %2148, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2213 : Tensor = prim::GetAttr[name="bias"](%2154)
  %2214 : Tensor = prim::GetAttr[name="weight"](%2154)
  %2215 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.195, %2215, %2214, %2213, %57, %58), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2217 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2218 : Float(13:121856, 119:1024, 1024:1), %2219 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2217)
  %2220 : __torch__.torch.nn.modules.normalization.___torch_mangle_40446.LayerNorm = prim::GetAttr[name="layer_norm"](%2152)
  %2221 : __torch__.torch.nn.modules.linear.___torch_mangle_40448.Linear = prim::GetAttr[name="layer_2"](%2152)
  %2222 : __torch__.torch.nn.modules.linear.___torch_mangle_40447.Linear = prim::GetAttr[name="layer_1"](%2152)
  %2223 : Tensor = prim::GetAttr[name="bias"](%2222)
  %2224 : Tensor = prim::GetAttr[name="weight"](%2222)
  %2225 : Float(1024:1, 4096:1024) = aten::t(%2224), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%2218, %2225), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.64, %2223, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.196), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.197, %80, %82), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2230 : Tensor = prim::GetAttr[name="bias"](%2221)
  %2231 : Tensor = prim::GetAttr[name="weight"](%2221)
  %2232 : Float(4096:1, 1024:4096) = aten::t(%2231), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.198, %2232), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.65, %2230, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.199, %80, %82), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.66, %2218, %84), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2237 : Tensor = prim::GetAttr[name="bias"](%2220)
  %2238 : Tensor = prim::GetAttr[name="weight"](%2220)
  %2239 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.200, %2239, %2238, %2237, %57, %58), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2241 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2219)
  %2242 : Float(13:121856, 119:1024, 1024:1), %2243 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2241)
  %new_mem.23 : Float(13:121856, 119:1024, 1024:1) = aten::slice(%2242, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2245 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2246 : __torch__.transformers.modeling_xlnet.___torch_mangle_40460.XLNetFeedForward = prim::GetAttr[name="ff"](%89)
  %2247 : __torch__.transformers.modeling_xlnet.___torch_mangle_40455.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%89)
  %2248 : __torch__.torch.nn.modules.normalization.___torch_mangle_40453.LayerNorm = prim::GetAttr[name="layer_norm"](%2247)
  %2249 : Tensor = prim::GetAttr[name="o"](%2247)
  %2250 : Tensor = prim::GetAttr[name="seg_embed"](%2247)
  %2251 : Tensor = prim::GetAttr[name="r_s_bias"](%2247)
  %2252 : Tensor = prim::GetAttr[name="r_r_bias"](%2247)
  %2253 : Tensor = prim::GetAttr[name="r_w_bias"](%2247)
  %2254 : Tensor = prim::GetAttr[name="r"](%2247)
  %2255 : Tensor = prim::GetAttr[name="v"](%2247)
  %2256 : Tensor = prim::GetAttr[name="k"](%2247)
  %2257 : Tensor = prim::GetAttr[name="q"](%2247)
  %2258 : Tensor[] = prim::ListConstruct(%2242, %2257), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %q_head.23 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2258), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2260 : Tensor[] = prim::ListConstruct(%2242, %2256), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2261 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2260), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2262 : Tensor[] = prim::ListConstruct(%2242, %2255), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2263 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2262), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 119:0, 1024:1) = aten::to(%2243, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2265 : Tensor[] = prim::ListConstruct(%r, %2254), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2266 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2265), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2267 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %2253, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2268 : Tensor[] = prim::ListConstruct(%2267, %2261), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ac.23 : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %2268), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2270 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %2252, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2271 : Tensor[] = prim::ListConstruct(%2270, %2266), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.89 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %2271), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2273 : int = aten::size(%ac.23, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2274 : int = aten::size(%x.89, %85), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2275 : int = aten::size(%x.89, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2276 : int = aten::size(%x.89, %76), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2277 : int = aten::size(%x.89, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2278 : Long() = prim::NumToTensor(%2277), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2279 : int[] = prim::ListConstruct(%2274, %2275, %2277, %2276), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.90 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2279), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2281 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2282 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%2281, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2283 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2282, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2283, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2285 : Long() = aten::sub(%2278, %69, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2286 : int = aten::Int(%2285), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2287 : int[] = prim::ListConstruct(%2274, %2275, %2276, %2286), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.92 : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2287), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2289 : Long(13:1) = aten::arange(%2273, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %64, %2289), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2291 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head.23, %2251, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:294:0
  %2292 : Tensor[] = prim::ListConstruct(%2291, %2250), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2293 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %2292), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2294 : Tensor[] = prim::ListConstruct(%154, %2293), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ef.23 : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %2294), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2296 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2297 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%2296, %ef.23, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.201 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%2297, %61), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.202 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %64, %77), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2300 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %80, %82), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2301 : Tensor[] = prim::ListConstruct(%2300, %2263), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2302 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %2301), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2303 : Tensor[] = prim::ListConstruct(%2302, %2249), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %input.203 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %2303), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.203, %80, %82), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out.23, %2242, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2307 : Tensor = prim::GetAttr[name="bias"](%2248)
  %2308 : Tensor = prim::GetAttr[name="weight"](%2248)
  %2309 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.204, %2309, %2308, %2307, %57, %58), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2311 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2312 : Float(13:121856, 119:1024, 1024:1), %2313 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2311)
  %2314 : __torch__.torch.nn.modules.normalization.___torch_mangle_40456.LayerNorm = prim::GetAttr[name="layer_norm"](%2246)
  %2315 : __torch__.torch.nn.modules.linear.___torch_mangle_40458.Linear = prim::GetAttr[name="layer_2"](%2246)
  %2316 : __torch__.torch.nn.modules.linear.___torch_mangle_40457.Linear = prim::GetAttr[name="layer_1"](%2246)
  %2317 : Tensor = prim::GetAttr[name="bias"](%2316)
  %2318 : Tensor = prim::GetAttr[name="weight"](%2316)
  %2319 : Float(1024:1, 4096:1024) = aten::t(%2318), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%2312, %2319), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.67, %2317, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.206, %80, %82), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2324 : Tensor = prim::GetAttr[name="bias"](%2315)
  %2325 : Tensor = prim::GetAttr[name="weight"](%2315)
  %2326 : Float(4096:1, 1024:4096) = aten::t(%2325), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.207, %2326), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.68, %2324, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.208, %80, %82), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.69, %2312, %84), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2331 : Tensor = prim::GetAttr[name="bias"](%2314)
  %2332 : Tensor = prim::GetAttr[name="weight"](%2314)
  %2333 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
  %curr_out : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.209, %2333, %2332, %2331, %57, %58), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2335 : (Float(13:121856, 119:1024, 1024:1), Float(26:1024, 119:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2313)
  %2336 : Float(13:121856, 119:1024, 1024:1), %2337 : Float(26:1024, 119:0, 1024:1) = prim::TupleUnpack(%2335)
  %new_mem : Float(13:121856, 119:1024, 1024:1) = aten::slice(%2336, %85, %85, %79, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2339 : Float(13:121856, 119:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2340 : __torch__.transformers.modeling_xlnet.___torch_mangle_40470.XLNetFeedForward = prim::GetAttr[name="ff"](%87)
  %2341 : __torch__.transformers.modeling_xlnet.___torch_mangle_40465.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%87)
  %2342 : __torch__.torch.nn.modules.normalization.___torch_mangle_40463.LayerNorm = prim::GetAttr[name="layer_norm"](%2341)
  %2343 : Tensor = prim::GetAttr[name="o"](%2341)
  %2344 : Tensor = prim::GetAttr[name="seg_embed"](%2341)
  %2345 : Tensor = prim::GetAttr[name="r_s_bias"](%2341)
  %2346 : Tensor = prim::GetAttr[name="r_r_bias"](%2341)
  %2347 : Tensor = prim::GetAttr[name="r_w_bias"](%2341)
  %2348 : Tensor = prim::GetAttr[name="r"](%2341)
  %2349 : Tensor = prim::GetAttr[name="v"](%2341)
  %2350 : Tensor = prim::GetAttr[name="k"](%2341)
  %2351 : Tensor = prim::GetAttr[name="q"](%2341)
  %2352 : Tensor[] = prim::ListConstruct(%2336, %2351), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %q_head : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2352), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2354 : Tensor[] = prim::ListConstruct(%2336, %2350), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2355 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2354), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2356 : Tensor[] = prim::ListConstruct(%2336, %2349), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2357 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2356), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2358 : Float(26:1024, 119:0, 1024:1) = aten::to(%2337, %72, %75, %82, %82, %77), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2359 : Tensor[] = prim::ListConstruct(%2358, %2348), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2360 : Float(26:121856, 119:1024, 16:64, 64:1) = aten::einsum(%66, %2359), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2361 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %2347, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2362 : Tensor[] = prim::ListConstruct(%2361, %2355), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ac : Float(119:2704, 16:169, 13:13, 13:1) = aten::einsum(%65, %2362), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2364 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %2346, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %2365 : Tensor[] = prim::ListConstruct(%2364, %2360), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.93 : Float(119:5408, 16:338, 13:26, 26:1) = aten::einsum(%65, %2365), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2367 : int = aten::size(%ac, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2368 : int = aten::size(%x.93, %85), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2369 : int = aten::size(%x.93, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2370 : int = aten::size(%x.93, %76), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2371 : int = aten::size(%x.93, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2372 : Long() = prim::NumToTensor(%2371), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2373 : int[] = prim::ListConstruct(%2368, %2369, %2371, %2370), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.94 : Float(119:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %2373), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %2375 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %85, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2376 : Float(119:5408, 16:338, 26:13, 13:1) = aten::slice(%2375, %84, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2377 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2376, %76, %84, %79, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(119:5408, 16:338, 25:13, 13:1) = aten::slice(%2377, %64, %85, %79, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2379 : Long() = aten::sub(%2372, %69, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2380 : int = aten::Int(%2379), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2381 : int[] = prim::ListConstruct(%2368, %2369, %2370, %2380), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x : Float(119:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %2381), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2383 : Long(13:1) = aten::arange(%2367, %78, %85, %72, %82), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(119:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %64, %2383), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2385 : Float(13:121856, 119:1024, 16:64, 64:1) = aten::add(%q_head, %2345, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:294:0
  %2386 : Tensor[] = prim::ListConstruct(%2385, %2344), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2387 : Float(13:238, 119:2, 16:3094, 2:1) = aten::einsum(%63, %2386), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2388 : Tensor[] = prim::ListConstruct(%154, %2387), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ef : Float(119:2704, 16:1, 13:208, 13:16) = aten::einsum(%62, %2388), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2390 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2391 : Float(119:2704, 16:169, 13:13, 13:1) = aten::add(%2390, %ef, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.210 : Float(119:2704, 16:169, 13:13, 13:1) = aten::mul(%2391, %61), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %input.211 : Float(119:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %64, %77), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %2394 : Float(119:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %80, %82), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2395 : Tensor[] = prim::ListConstruct(%2394, %2357), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2396 : Float(13:64, 119:13312, 16:832, 64:1) = aten::einsum(%60, %2395), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2397 : Tensor[] = prim::ListConstruct(%2396, %2343), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %input.212 : Float(13:121856, 119:1024, 1024:1) = aten::einsum(%59, %2397), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.212, %80, %82), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:121856, 119:1024, 1024:1) = aten::add(%attn_out, %2336, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %2401 : Tensor = prim::GetAttr[name="bias"](%2342)
  %2402 : Tensor = prim::GetAttr[name="weight"](%2342)
  %2403 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.213, %2403, %2402, %2401, %57, %58), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2405 : __torch__.torch.nn.modules.normalization.___torch_mangle_40466.LayerNorm = prim::GetAttr[name="layer_norm"](%2340)
  %2406 : __torch__.torch.nn.modules.linear.___torch_mangle_40468.Linear = prim::GetAttr[name="layer_2"](%2340)
  %2407 : __torch__.torch.nn.modules.linear.___torch_mangle_40467.Linear = prim::GetAttr[name="layer_1"](%2340)
  %2408 : Tensor = prim::GetAttr[name="bias"](%2407)
  %2409 : Tensor = prim::GetAttr[name="weight"](%2407)
  %2410 : Float(1024:1, 4096:1024) = aten::t(%2409), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:487424, 119:4096, 4096:1) = aten::matmul(%input_tensor, %2410), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:487424, 119:4096, 4096:1) = aten::add_(%output.70, %2408, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:487424, 119:4096, 4096:1) = aten::gelu(%input.214), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:487424, 119:4096, 4096:1) = aten::dropout(%input.215, %80, %82), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %2415 : Tensor = prim::GetAttr[name="bias"](%2406)
  %2416 : Tensor = prim::GetAttr[name="weight"](%2406)
  %2417 : Float(4096:1, 1024:4096) = aten::t(%2416), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:121856, 119:1024, 1024:1) = aten::matmul(%input.216, %2417), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:121856, 119:1024, 1024:1) = aten::add_(%output.71, %2415, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.72 : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.217, %80, %82), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:121856, 119:1024, 1024:1) = aten::add(%output.72, %input_tensor, %84), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %2422 : Tensor = prim::GetAttr[name="bias"](%2405)
  %2423 : Tensor = prim::GetAttr[name="weight"](%2405)
  %2424 : int[] = prim::ListConstruct(%74), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
  %input.219 : Float(13:121856, 119:1024, 1024:1) = aten::layer_norm(%input.218, %2424, %2423, %2422, %57, %58), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %output_h : Float(13:121856, 119:1024, 1024:1) = aten::dropout(%input.219, %80, %82), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %2427 : int[] = prim::ListConstruct(%84, %85, %76), scope: __module.transformer
  %2428 : Float(119:1024, 13:121856, 1024:1) = aten::permute(%output_h, %2427), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %hidden_states : Float(119:13312, 13:1024, 1024:1) = aten::contiguous(%2428, %85), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %2430 : (Float(119:13312, 13:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1)) = prim::TupleConstruct(%hidden_states, %177, %271, %365, %459, %553, %647, %741, %835, %929, %1023, %1117, %1211, %1305, %1399, %1493, %1587, %1681, %1775, %1869, %1963, %2057, %2151, %2245, %2339)
  %25 : Float(119:13312, 13:1024, 1024:1), %26 : Float(13:121856, 119:1024, 1024:1), %27 : Float(13:121856, 119:1024, 1024:1), %28 : Float(13:121856, 119:1024, 1024:1), %29 : Float(13:121856, 119:1024, 1024:1), %30 : Float(13:121856, 119:1024, 1024:1), %31 : Float(13:121856, 119:1024, 1024:1), %32 : Float(13:121856, 119:1024, 1024:1), %33 : Float(13:121856, 119:1024, 1024:1), %34 : Float(13:121856, 119:1024, 1024:1), %35 : Float(13:121856, 119:1024, 1024:1), %36 : Float(13:121856, 119:1024, 1024:1), %37 : Float(13:121856, 119:1024, 1024:1), %38 : Float(13:121856, 119:1024, 1024:1), %39 : Float(13:121856, 119:1024, 1024:1), %40 : Float(13:121856, 119:1024, 1024:1), %41 : Float(13:121856, 119:1024, 1024:1), %42 : Float(13:121856, 119:1024, 1024:1), %43 : Float(13:121856, 119:1024, 1024:1), %44 : Float(13:121856, 119:1024, 1024:1), %45 : Float(13:121856, 119:1024, 1024:1), %46 : Float(13:121856, 119:1024, 1024:1), %47 : Float(13:121856, 119:1024, 1024:1), %48 : Float(13:121856, 119:1024, 1024:1), %49 : Float(13:121856, 119:1024, 1024:1) = prim::TupleUnpack(%2430)
  %2431 : float = prim::Constant[value=0.10000000000000001](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2432 : bool = prim::Constant[value=0](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2433 : int = prim::Constant[value=-1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2434 : int = prim::Constant[value=1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2435 : int = prim::Constant[value=9223372036854775807](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2436 : int = prim::Constant[value=0](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2437 : __torch__.torch.nn.modules.linear.___torch_mangle_40475.Linear = prim::GetAttr[name="summary"](%4)
  %2438 : Float(119:13312, 13:1024, 1024:1) = aten::slice(%25, %2436, %2436, %2435, %2434), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %input.220 : Float(119:13312, 1024:1) = aten::select(%2438, %2434, %2433), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2440 : Tensor = prim::GetAttr[name="bias"](%2437)
  %2441 : Tensor = prim::GetAttr[name="weight"](%2437)
  %2442 : Float(1024:1, 1024:1024) = aten::t(%2441), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %output : Float(119:1024, 1024:1) = aten::addmm(%2440, %input.220, %2442, %2434, %2434), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %input.221 : Float(119:1024, 1024:1) = aten::tanh(%output), scope: __module.sequence_summary # transformers/modeling_utils.py:1509:0
  %input : Float(119:1024, 1024:1) = aten::dropout(%input.221, %2431, %2432), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2446 : int = prim::Constant[value=1](), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %2447 : Tensor = prim::GetAttr[name="bias"](%3)
  %2448 : Tensor = prim::GetAttr[name="weight"](%3)
  %2449 : Float(1024:1, 1:1024) = aten::t(%2448), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %logits : Float(119:1, 1:1) = aten::addmm(%2447, %input, %2449, %2446, %2446), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %52 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1716:0
  %53 : int[] = prim::ListConstruct(%52, %9)
  %54 : Float(17:7, 7:1) = aten::view(%logits, %53) # transformers/modeling_xlnet.py:1716:0
  %55 : (Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1)) = prim::TupleConstruct(%26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49)
  %56 : (Float(17:7, 7:1), (Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1), Float(13:121856, 119:1024, 1024:1))) = prim::TupleConstruct(%54, %55)
  return (%56)
