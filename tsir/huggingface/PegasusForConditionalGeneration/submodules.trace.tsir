PegasusForConditionalGeneration(
  (model): BartModel(
    (shared): Embedding(50265, 1024, padding_idx=1)
    (encoder): BartEncoder(
      (embed_tokens): Embedding(50265, 1024, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (1): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (2): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (3): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (5): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (6): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (8): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (9): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (10): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (11): EncoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): BartDecoder(
      (embed_tokens): Embedding(50265, 1024, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
      (layers): ModuleList(
        (0): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (1): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (2): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (3): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (5): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (6): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (8): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (9): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (10): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (11): DecoderLayer(
          (self_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): Attention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)

PegasusForConditionalGeneration._actual_script_module
PegasusForConditionalGeneration.forward
  graph(%self.1 : __torch__.transformers.modeling_pegasus.PegasusForConditionalGeneration,
        %input_ids : Long(17:13, 13:1),
        %attention_mask : Long(17:13, 13:1)):
    %9416 : Tensor = prim::GetAttr[name="final_logits_bias"](%self.1)
    %9415 : __torch__.transformers.modeling_bart.BartModel = prim::GetAttr[name="model"](%self.1)
    %8550 : __torch__.transformers.modeling_bart.BartModel = prim::GetAttr[name="model"](%self.1)
    %8551 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="shared"](%8550)
    %8552 : Tensor = prim::GetAttr[name="weight"](%8551)
    %9743 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%9415, %input_ids, %attention_mask)
    %9741 : Float(17:1024, 13:17408, 1024:1), %9742 : Float(17:1024, 13:17408, 1024:1) = prim::TupleUnpack(%9743)
    %8224 : Float(1024:1, 50265:1024) = aten::t(%8552) # torch/nn/functional.py:1676:0
    %output : Float(17:653445, 13:50265, 50265:1) = aten::matmul(%9741, %8224) # torch/nn/functional.py:1676:0
    %8226 : int = prim::Constant[value=1]() # torch/nn/functional.py:1678:0
    %8227 : Float(17:653445, 13:50265, 50265:1) = aten::add_(%output, %9416, %8226) # torch/nn/functional.py:1678:0
    %8228 : (Float(17:653445, 13:50265, 50265:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%8227, %9742)
    return (%8228)

PegasusForConditionalGeneration.model
BartModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_bart.BartModel,
        %input_ids : Long(17:13, 13:1),
        %attention_mask : Long(17:13, 13:1)):
    %1 : __torch__.transformers.modeling_bart.BartDecoder = prim::GetAttr[name="decoder"](%self.2)
    %2 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="shared"](%self.2)
    %3 : Tensor = prim::GetAttr[name="weight"](%2)
    %4 : __torch__.transformers.modeling_bart.BartDecoder = prim::GetAttr[name="decoder"](%self.2)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%4)
    %6 : __torch__.transformers.modeling_bart.BartEncoder = prim::GetAttr[name="encoder"](%self.2)
    %7 : None = prim::Constant(), scope: __module.model
    %prev_output_tokens : Long(17:13, 13:1) = aten::clone(%input_ids, %7), scope: __module.model # transformers/modeling_bart.py:207:0
    %10 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:208:0
    %11 : Bool(17:13, 13:1) = aten::ne(%input_ids, %10), scope: __module.model # transformers/modeling_bart.py:208:0
    %12 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:208:0
    %13 : int[] = prim::ListConstruct(%12), scope: __module.model
    %14 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:208:0
    %15 : None = prim::Constant(), scope: __module.model
    %16 : Long(17:1) = aten::sum(%11, %13, %14, %15), scope: __module.model # transformers/modeling_bart.py:208:0
    %17 : Long() = prim::Constant[value={1}](), scope: __module.model # transformers/modeling_bart.py:208:0
    %18 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:208:0
    %19 : Long(17:1) = aten::sub(%16, %17, %18), scope: __module.model # transformers/modeling_bart.py:208:0
    %20 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:208:0
    %index_of_eos : Long(17:1, 1:1) = aten::unsqueeze(%19, %20), scope: __module.model # transformers/modeling_bart.py:208:0
    %22 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:209:0
    %23 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
    %24 : Long(17:1, 1:1) = aten::gather(%input_ids, %22, %index_of_eos, %23), scope: __module.model # transformers/modeling_bart.py:209:0
    %25 : Long(17:1) = aten::squeeze(%24), scope: __module.model # transformers/modeling_bart.py:209:0
    %26 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
    %27 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
    %28 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:209:0
    %29 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:209:0
    %30 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %26, %27, %28, %29), scope: __module.model # transformers/modeling_bart.py:209:0
    %31 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:209:0
    %32 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
    %33 : Long(17:13) = aten::select(%30, %31, %32), scope: __module.model # transformers/modeling_bart.py:209:0
    %34 : int = prim::Constant[value=17](), scope: __module.model # transformers/modeling_bart.py:209:0
    %35 : int[] = prim::ListConstruct(%34), scope: __module.model
    %36 : Long(17:1) = aten::view(%25, %35), scope: __module.model # transformers/modeling_bart.py:209:0
    %37 : bool = prim::Constant[value=0](), scope: __module.model
    %38 : Long(17:13) = aten::copy_(%33, %36, %37), scope: __module.model # transformers/modeling_bart.py:209:0
    %39 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:210:0
    %40 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:210:0
    %41 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:210:0
    %42 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %43 : Long(17:13, 13:1) = aten::slice(%input_ids, %39, %40, %41, %42), scope: __module.model # transformers/modeling_bart.py:210:0
    %44 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %45 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:210:0
    %46 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %47 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %48 : Long(17:13, 12:1) = aten::slice(%43, %44, %45, %46, %47), scope: __module.model # transformers/modeling_bart.py:210:0
    %49 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:210:0
    %50 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:210:0
    %51 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:210:0
    %52 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %53 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %49, %50, %51, %52), scope: __module.model # transformers/modeling_bart.py:210:0
    %54 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %55 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %56 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:210:0
    %57 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:210:0
    %58 : Long(17:13, 12:1) = aten::slice(%53, %54, %55, %56, %57), scope: __module.model # transformers/modeling_bart.py:210:0
    %59 : int = prim::Constant[value=17](), scope: __module.model # transformers/modeling_bart.py:210:0
    %60 : int = prim::Constant[value=12](), scope: __module.model # transformers/modeling_bart.py:210:0
    %61 : int[] = prim::ListConstruct(%59, %60), scope: __module.model
    %62 : Long(17:13, 12:1) = aten::view(%48, %61), scope: __module.model # transformers/modeling_bart.py:210:0
    %63 : bool = prim::Constant[value=0](), scope: __module.model
    %64 : Long(17:13, 12:1) = aten::copy_(%58, %62, %63), scope: __module.model # transformers/modeling_bart.py:210:0
    %68 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:149:0
    %69 : int = aten::size(%prev_output_tokens, %68), scope: __module.model # transformers/modeling_bart.py:149:0
    %tgt_len : Long() = prim::NumToTensor(%69), scope: __module.model
    %71 : int = aten::Int(%tgt_len), scope: __module.model
    %72 : int = aten::Int(%tgt_len), scope: __module.model
    %76 : int[] = prim::ListConstruct(%72, %71), scope: __module.model
    %77 : int = prim::Constant[value=6](), scope: __module.model # transformers/modeling_bart.py:157:0
    %78 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:157:0
    %79 : Device = prim::Constant[value="cpu"](), scope: __module.model # transformers/modeling_bart.py:157:0
    %80 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:157:0
    %t.1 : Float(13:13, 13:1) = aten::zeros(%76, %77, %78, %79, %80), scope: __module.model # transformers/modeling_bart.py:157:0
    %82 : int = prim::Constant[value=6](), scope: __module.model # transformers/modeling_bart.py:838:0
    %83 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:838:0
    %84 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:838:0
    %85 : None = prim::Constant(), scope: __module.model
    %t.2 : Float(13:13, 13:1) = aten::to(%t.1, %82, %83, %84, %85), scope: __module.model # transformers/modeling_bart.py:838:0
    %87 : float = prim::Constant[value=-inf](), scope: __module.model # transformers/modeling_bart.py:838:0
    %t : Float(13:13, 13:1) = aten::fill_(%t.2, %87), scope: __module.model # transformers/modeling_bart.py:838:0
    %tmp.1 : Float(13:13, 13:1) = aten::type_as(%t, %t), scope: __module.model # transformers/modeling_bart.py:838:0
    %90 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:158:0
    %91 : int = aten::size(%tmp.1, %90), scope: __module.model # transformers/modeling_bart.py:158:0
    %92 : Long() = prim::NumToTensor(%91), scope: __module.model
    %93 : Scalar = aten::ScalarImplicit(%92), scope: __module.model
    %94 : None = prim::Constant(), scope: __module.model
    %95 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:158:0
    %96 : Device = prim::Constant[value="cpu"](), scope: __module.model # transformers/modeling_bart.py:158:0
    %97 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:158:0
    %mask : Long(13:1) = aten::arange(%93, %94, %95, %96, %97), scope: __module.model # transformers/modeling_bart.py:158:0
    %99 : Long() = prim::Constant[value={1}](), scope: __module.model # transformers/modeling_bart.py:159:0
    %100 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:159:0
    %101 : Long(13:1) = aten::add(%mask, %99, %100), scope: __module.model # transformers/modeling_bart.py:159:0
    %102 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:159:0
    %103 : int = aten::size(%tmp.1, %102), scope: __module.model # transformers/modeling_bart.py:159:0
    %104 : Long() = prim::NumToTensor(%103), scope: __module.model
    %105 : int = aten::Int(%104), scope: __module.model
    %106 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:159:0
    %107 : int[] = prim::ListConstruct(%105, %106), scope: __module.model
    %108 : Long(13:1, 1:1) = aten::view(%101, %107), scope: __module.model # transformers/modeling_bart.py:159:0
    %109 : Bool(13:13, 13:1) = aten::lt(%mask, %108), scope: __module.model # torch/tensor.py:22:0
    %110 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:159:0
    %tmp : Float(13:13, 13:1) = aten::masked_fill_(%tmp.1, %109, %110), scope: __module.model # transformers/modeling_bart.py:159:0
    %112 : Device = prim::Constant[value="cpu"](), scope: __module.model # transformers/modeling_bart.py:160:0
    %113 : int = prim::Constant[value=6](), scope: __module.model # transformers/modeling_bart.py:160:0
    %114 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:160:0
    %115 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:160:0
    %116 : None = prim::Constant(), scope: __module.model
    %attn_mask : Float(13:13, 13:1) = aten::to(%tmp, %112, %113, %114, %115, %116), scope: __module.model # transformers/modeling_bart.py:160:0
    %122 : Tensor = prim::CallMethod[name="forward"](%6, %5, %3, %attention_mask, %input_ids)
    %123 : Tensor = prim::CallMethod[name="forward"](%1, %3, %attention_mask, %prev_output_tokens, %122, %attn_mask)
    %121 : (Float(17:1024, 13:17408, 1024:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%123, %122)
    return (%121)

BartModel.decoder
BartDecoder._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_bart.BartDecoder,
        %weight.259 : Float(50265:1024, 1024:1),
        %attention_mask : Long(17:13, 13:1),
        %prev_output_tokens : Long(17:13, 13:1),
        %4 : Float(17:1024, 13:17408, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %6 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %7 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="11"](%6)
    %8 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %9 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="10"](%8)
    %10 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %11 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="9"](%10)
    %12 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %13 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="8"](%12)
    %14 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %15 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="7"](%14)
    %16 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %17 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="6"](%16)
    %18 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %19 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="5"](%18)
    %20 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %21 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="4"](%20)
    %22 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %23 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="3"](%22)
    %24 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %25 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="2"](%24)
    %26 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %27 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="1"](%26)
    %28 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.127)
    %29 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="0"](%28)
    %30 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%self.127)
    %31 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%self.127)
    %32 : __torch__.transformers.modeling_bart.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%self.127)
    %33 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:136:0
    %key_padding_mask : Bool(17:13, 13:1) = aten::eq(%attention_mask, %33), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:136:0
    %69 : Tensor = prim::CallMethod[name="forward"](%32, %prev_output_tokens)
    %70 : Tensor = prim::CallMethod[name="forward1"](%31, %weight.259, %prev_output_tokens)
    %37 : Double() = prim::Constant[value={1}](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:556:0
    %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::mul(%70, %37), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:556:0
    %39 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:557:0
    %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%x.27, %69, %39), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:557:0
    %71 : Tensor = prim::CallMethod[name="forward"](%30, %input.137)
    %42 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder # torch/nn/functional.py:973:0
    %43 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder # torch/nn/functional.py:973:0
    %x.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%71, %42, %43), scope: __module.model/__module.model.decoder # torch/nn/functional.py:973:0
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:562:0
    %46 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:562:0
    %query.13 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.28, %45, %46), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:562:0
    %48 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:563:0
    %49 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:563:0
    %input.144 : Float(13:17408, 17:1024, 1024:1) = aten::transpose(%4, %48, %49), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:563:0
    %72 : Tensor = prim::CallMethod[name="forward"](%29, %query.13, %attn_mask, %input.144, %key_padding_mask)
    %73 : Tensor = prim::CallMethod[name="forward"](%27, %72, %attn_mask, %input.144, %key_padding_mask)
    %74 : Tensor = prim::CallMethod[name="forward"](%25, %73, %attn_mask, %input.144, %key_padding_mask)
    %75 : Tensor = prim::CallMethod[name="forward"](%23, %74, %attn_mask, %input.144, %key_padding_mask)
    %76 : Tensor = prim::CallMethod[name="forward"](%21, %75, %attn_mask, %input.144, %key_padding_mask)
    %77 : Tensor = prim::CallMethod[name="forward"](%19, %76, %attn_mask, %input.144, %key_padding_mask)
    %78 : Tensor = prim::CallMethod[name="forward"](%17, %77, %attn_mask, %input.144, %key_padding_mask)
    %79 : Tensor = prim::CallMethod[name="forward"](%15, %78, %attn_mask, %input.144, %key_padding_mask)
    %80 : Tensor = prim::CallMethod[name="forward"](%13, %79, %attn_mask, %input.144, %key_padding_mask)
    %81 : Tensor = prim::CallMethod[name="forward"](%11, %80, %attn_mask, %input.144, %key_padding_mask)
    %82 : Tensor = prim::CallMethod[name="forward"](%9, %81, %attn_mask, %input.144, %key_padding_mask)
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %82, %attn_mask, %input.144, %key_padding_mask)
    %63 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:601:0
    %64 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:601:0
    %input : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%83, %63, %64), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:601:0
    return (%input)

BartModel.encoder
BartEncoder._actual_script_module
  graph(%self.3 : __torch__.transformers.modeling_bart.BartEncoder,
        %1 : __torch__.torch.nn.modules.sparse.Embedding,
        %weight.257 : Float(50265:1024, 1024:1),
        %attention_mask : Long(17:13, 13:1),
        %input_ids : Long(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %6 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="11"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %8 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="10"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %10 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="9"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %12 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="8"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %14 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="7"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %16 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="6"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %18 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="5"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %20 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="4"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %22 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="3"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %24 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="2"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %26 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="1"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.3)
    %28 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="0"](%27)
    %29 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%self.3)
    %30 : __torch__.transformers.modeling_bart.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%self.3)
    %31 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:136:0
    %key_padding_mask.1 : Bool(17:13, 13:1) = aten::eq(%attention_mask, %31), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:136:0
    %61 : Tensor = prim::CallMethod[name="forward"](%1, %weight.257, %input_ids)
    %34 : Double() = prim::Constant[value={1}](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
    %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::mul(%61, %34), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
    %62 : Tensor = prim::CallMethod[name="forward"](%30, %input_ids)
    %37 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:336:0
    %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %62, %37), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:336:0
    %63 : Tensor = prim::CallMethod[name="forward"](%29, %input.2)
    %40 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
    %41 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
    %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%63, %40, %41), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
    %43 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:341:0
    %44 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:341:0
    %query.1 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.1, %43, %44), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:341:0
    %64 : Tensor = prim::CallMethod[name="forward"](%28, %query.1, %key_padding_mask.1)
    %65 : Tensor = prim::CallMethod[name="forward"](%26, %64, %key_padding_mask.1)
    %66 : Tensor = prim::CallMethod[name="forward"](%24, %65, %key_padding_mask.1)
    %67 : Tensor = prim::CallMethod[name="forward"](%22, %66, %key_padding_mask.1)
    %68 : Tensor = prim::CallMethod[name="forward"](%20, %67, %key_padding_mask.1)
    %69 : Tensor = prim::CallMethod[name="forward"](%18, %68, %key_padding_mask.1)
    %70 : Tensor = prim::CallMethod[name="forward"](%16, %69, %key_padding_mask.1)
    %71 : Tensor = prim::CallMethod[name="forward"](%14, %70, %key_padding_mask.1)
    %72 : Tensor = prim::CallMethod[name="forward"](%12, %71, %key_padding_mask.1)
    %73 : Tensor = prim::CallMethod[name="forward"](%10, %72, %key_padding_mask.1)
    %74 : Tensor = prim::CallMethod[name="forward"](%8, %73, %key_padding_mask.1)
    %75 : Tensor = prim::CallMethod[name="forward"](%6, %74, %key_padding_mask.1)
    %58 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:366:0
    %59 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:366:0
    %encoder_hidden_states : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%75, %58, %59), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:366:0
    return (%encoder_hidden_states)

Embedding.*
ModuleList.*
  module had no methods with graph attrs.

BartEncoder.embed_positions
LearnedPositionalEmbedding._actual_script_module
  graph(%self.5 : __torch__.transformers.modeling_bart.LearnedPositionalEmbedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:816:0
    %7 : int = aten::size(%input_ids, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:816:0
    %seq_len.1 : Long() = prim::NumToTensor(%7), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions
    %9 : Scalar = aten::ScalarImplicit(%seq_len.1), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions
    %10 : int = prim::Constant[value=4](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
    %11 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
    %12 : Device = prim::Constant[value="cpu"](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
    %13 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
    %positions.1 : Long(13:1) = aten::arange(%9, %10, %11, %12, %13), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
    %15 : Long() = prim::Constant[value={2}](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
    %input.1 : Long(13:1) = aten::add(%positions.1, %15, %16), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
    %24 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
    %25 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
    %embed_pos : Float(13:1024, 1024:1) = aten::embedding(%2, %input.1, %23, %24, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
    return (%embed_pos)

BartEncoder.layernorm_embedding
LayerNorm._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
    return (%input.3)

EncoderLayer._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_bart.EncoderLayer,
        %query.1 : Float(13:1024, 17:13312, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.7)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.7)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.7)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.7)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.7)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %query.1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %x.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:258:0
    %input.8 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.1, %x.2, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.8)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.11 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %input.12 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.11, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.12)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %x.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:269:0
    %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.3, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.14)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
    %output.5 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1678:0
    %input.10 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.5, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1678:0
    return (%input.10)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.linear.Linear,
        %input.12 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.15)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.15)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
    %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.12, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1678:0
    %input.13 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.6, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1678:0
    return (%input.13)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.14 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %query.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.14, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.2)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.8 : __torch__.transformers.modeling_bart.Attention,
        %query.1 : Float(13:1024, 17:13312, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.8)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.8)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.8)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.8)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%query.1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.2 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %10 : int = aten::Int(%seq_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %11 : int = aten::Int(%seq_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %12 : int = aten::Int(%seq_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %13 : int = aten::Int(%seq_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%query.1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %bsz.1 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %17 : int = aten::Int(%bsz.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %18 : int = aten::Int(%bsz.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%query.1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.1 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %24 : int = aten::Int(%embed_dim.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %query.1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
    %tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %query.1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %query.1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.1, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.1, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.2, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %q.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.1, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.4, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %k.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.1, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.6, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %v.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.1, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
    %src_len.1 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %92 : int = aten::Int(%src_len.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %93 : int = aten::Int(%src_len.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.1, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.1 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.1, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %attn_weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.1, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.1 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.2, %reshaped.1, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.1, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %input.4 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.3, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %input.5 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.4, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %attn_weights.4 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.5, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %attn_output.1 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.4, %v.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.1, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
    %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.6)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.8 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.8, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.9)

Attention.k_proj
Linear._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.linear.Linear,
        %query.1 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.3)

Attention.out_proj
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %input.6 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.4 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.6, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.4, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.7)

Attention.q_proj
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %query.1 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.1 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.1, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.11 : __torch__.torch.nn.modules.linear.Linear,
        %query.1 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.11)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.11)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.3, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.5)

EncoderLayer._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.17)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.17)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.17)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.17)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.17)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %x.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:258:0
    %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.4, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.19)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.22 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %input.23 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.22, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.23)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %x.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:269:0
    %input.25 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.5, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.25)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
    %output.11 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1678:0
    %input.21 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.11, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1678:0
    return (%input.21)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %input.23 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
    %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.23, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1678:0
    %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.12, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1678:0
    return (%input.24)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.25 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %query.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.25, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.3)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.18 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.18)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.18)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.18)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.18)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.3 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %10 : int = aten::Int(%seq_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %11 : int = aten::Int(%seq_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %12 : int = aten::Int(%seq_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %13 : int = aten::Int(%seq_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %bsz.2 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %17 : int = aten::Int(%bsz.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %18 : int = aten::Int(%bsz.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.2 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %24 : int = aten::Int(%embed_dim.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
    %tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.7, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.2, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.8, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %q.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.2, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.10, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %k.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.2, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.12, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %v.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.2, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
    %src_len.2 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %92 : int = aten::Int(%src_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %93 : int = aten::Int(%src_len.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.2, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.5 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.2, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %attn_weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.5, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.2 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.6, %reshaped.2, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.2, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %input.15 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.7, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %input.16 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.15, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %attn_weights.8 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.16, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %attn_output.2 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.8, %v.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.2, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
    %input.17 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.17)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.19, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.20)

Attention.k_proj
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.9)

Attention.out_proj
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.10 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.17, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.18 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.10, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.18)

Attention.q_proj
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.7 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.7, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.9, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.11)

EncoderLayer._actual_script_module
  graph(%self.27 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.27)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.27)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.27)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.27)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.27)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %x.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:258:0
    %input.30 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.6, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.30)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.33 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.33, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.34)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %x.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:269:0
    %input.36 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.7, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.36)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
    %output.17 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1678:0
    %input.32 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.17, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1678:0
    return (%input.32)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.35)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.35)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
    %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.34, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1678:0
    %input.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.18, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1678:0
    return (%input.35)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.36 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %query.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.36, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.4)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.28)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.28)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.28)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.28)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.4 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %10 : int = aten::Int(%seq_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %11 : int = aten::Int(%seq_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %12 : int = aten::Int(%seq_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %13 : int = aten::Int(%seq_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %bsz.3 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %17 : int = aten::Int(%bsz.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %18 : int = aten::Int(%bsz.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.3 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %24 : int = aten::Int(%embed_dim.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
    %tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.13, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.3, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.14, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %q.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.3, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.16, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %k.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.3, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.18, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %v.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.3, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
    %src_len.3 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %92 : int = aten::Int(%src_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %93 : int = aten::Int(%src_len.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.3, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.9 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.3, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %attn_weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.9, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.3 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.10, %reshaped.3, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.3, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %input.26 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.11, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %input.27 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.26, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %attn_weights.12 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.27, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %attn_output.3 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.12, %v.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.3, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
    %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.28)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.30 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.30, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.31)

Attention.k_proj
Linear._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.15)

Attention.out_proj
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %input.28 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.16 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.28, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.16, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.29)

Attention.q_proj
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.13 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.13, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.15, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.17)

EncoderLayer._actual_script_module
  graph(%self.37 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.37)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.37)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.37)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.37)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.37)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %x.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:258:0
    %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.8, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.41)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.45)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %x.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:269:0
    %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.9, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.47)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
    %output.23 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1678:0
    %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.23, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1678:0
    return (%input.43)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %input.45 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
    %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1678:0
    %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.24, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1678:0
    return (%input.46)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.47 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %query.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.5)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.38 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.38)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.38)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.38)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.38)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.5 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %10 : int = aten::Int(%seq_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %11 : int = aten::Int(%seq_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %12 : int = aten::Int(%seq_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %13 : int = aten::Int(%seq_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %bsz.4 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %17 : int = aten::Int(%bsz.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %18 : int = aten::Int(%bsz.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.4 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %24 : int = aten::Int(%embed_dim.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
    %tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.19, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.4, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.20, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %q.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.4, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.22, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %k.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.24 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.4, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.24, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %v.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.4, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
    %src_len.4 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %92 : int = aten::Int(%src_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %93 : int = aten::Int(%src_len.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.4, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.13 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.4, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %attn_weights.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.13, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.4 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.14, %reshaped.4, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.4, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %input.37 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.15, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %input.38 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.37, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %attn_weights.16 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.38, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %attn_output.4 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.16, %v.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.4, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
    %input.39 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.39)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.41 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.41, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.42)

Attention.k_proj
Linear._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.21)

Attention.out_proj
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %input.39 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.22 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.39, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.40 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.22, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.40)

Attention.q_proj
Linear._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.19 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.19, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.41)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.41)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.21, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.23)

EncoderLayer._actual_script_module
  graph(%self.47 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.47)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.47)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.47)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.47)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.47)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %x.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:258:0
    %input.52 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.10, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.52)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.55 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %input.56 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.55, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.56)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %x.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:269:0
    %input.58 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.11, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.58)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
    %output.29 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1678:0
    %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.29, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1678:0
    return (%input.54)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.linear.Linear,
        %input.56 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
    %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.56, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1678:0
    %input.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.30, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1678:0
    return (%input.57)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.58 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %query.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.58, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.6)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.48 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.48)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.48)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.48)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.48)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.6 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %10 : int = aten::Int(%seq_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %11 : int = aten::Int(%seq_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %12 : int = aten::Int(%seq_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %13 : int = aten::Int(%seq_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %bsz.5 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %17 : int = aten::Int(%bsz.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %18 : int = aten::Int(%bsz.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.5 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %24 : int = aten::Int(%embed_dim.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
    %tensor.25 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.26 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.25, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.5, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.26, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %q.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.28 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.5, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.28, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %k.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.30 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.5, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.30, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %v.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.5, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
    %src_len.5 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %92 : int = aten::Int(%src_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %93 : int = aten::Int(%src_len.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.5, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.17 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.5, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %attn_weights.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.17, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.5 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.18, %reshaped.5, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.5, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %input.48 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.19, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %input.49 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.48, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %attn_weights.20 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.49, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %attn_output.5 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.20, %v.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.5, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
    %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.50)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.52 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.53 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.52, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.53)

Attention.k_proj
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.27 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.27)

Attention.out_proj
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %input.50 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.28 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.50, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.28, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.51)

Attention.q_proj
Linear._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.25 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.25, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.51)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.51)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.27, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.29)

EncoderLayer._actual_script_module
  graph(%self.57 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.57)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.57)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.57)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.57)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.57)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %x.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:258:0
    %input.63 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.12, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.63)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.66 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %input.67 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.66, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.67)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %x.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:269:0
    %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.13, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.69)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
    %output.35 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1678:0
    %input.65 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.35, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1678:0
    return (%input.65)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
    %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.67, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1678:0
    %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.36, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1678:0
    return (%input.68)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %query.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.7)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.58 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.58)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.58)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.58)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.58)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.7 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %10 : int = aten::Int(%seq_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %11 : int = aten::Int(%seq_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %12 : int = aten::Int(%seq_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %13 : int = aten::Int(%seq_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %bsz.6 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %17 : int = aten::Int(%bsz.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %18 : int = aten::Int(%bsz.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.6 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %24 : int = aten::Int(%embed_dim.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
    %tensor.31 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.32 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.31, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.6, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.32, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %q.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.34 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.6, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.34, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %k.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.36 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.6, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.36, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %v.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.6, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
    %src_len.6 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %92 : int = aten::Int(%src_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %93 : int = aten::Int(%src_len.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.6, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.21 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.6, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %attn_weights.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.21, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.6 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.22, %reshaped.6, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.6, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %input.59 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.23, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %input.60 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.59, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %attn_weights.24 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.60, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %attn_output.6 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.24, %v.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.6, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
    %input.61 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.61)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.63 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.63, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.64)

Attention.k_proj
Linear._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.33 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.33)

Attention.out_proj
Linear._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.linear.Linear,
        %input.61 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.62)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.62)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.34 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.61, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.62 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.34, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.62)

Attention.q_proj
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.31 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.31, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.33, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.35)

EncoderLayer._actual_script_module
  graph(%self.67 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.67)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.67)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.67)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.67)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.67)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %x.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:258:0
    %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.14, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.74)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.77 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %input.78 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.77, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.78)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %x.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:269:0
    %input.80 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.15, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.80)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
    %output.41 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1678:0
    %input.76 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.41, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1678:0
    return (%input.76)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.linear.Linear,
        %input.78 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.75)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.75)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
    %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.78, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1678:0
    %input.79 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.42, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1678:0
    return (%input.79)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.80 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %query.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.80, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.8)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.68 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.68)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.68)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.68)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.68)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.8 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %10 : int = aten::Int(%seq_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %11 : int = aten::Int(%seq_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %12 : int = aten::Int(%seq_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %13 : int = aten::Int(%seq_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %bsz.7 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %17 : int = aten::Int(%bsz.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %18 : int = aten::Int(%bsz.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.7 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %24 : int = aten::Int(%embed_dim.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
    %tensor.37 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.38 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.37, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.7, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.38, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %q.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.40 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.7, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.40, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %k.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.42 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.7, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.42, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %v.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.7, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
    %src_len.7 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %92 : int = aten::Int(%src_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %93 : int = aten::Int(%src_len.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.7, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.25 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.7, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %attn_weights.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.25, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.7 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.26, %reshaped.7, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.7, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %input.70 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.27, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %input.71 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.70, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %attn_weights.28 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.71, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %attn_output.7 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.28, %v.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.7, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
    %input.72 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.72)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.74 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.75 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.75)

Attention.k_proj
Linear._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.39 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.39)

Attention.out_proj
Linear._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.linear.Linear,
        %input.72 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.40 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.40, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.73)

Attention.q_proj
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.37 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.37, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.71)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.71)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.41 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.39, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.41)

EncoderLayer._actual_script_module
  graph(%self.77 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.77)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.77)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.77)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.77)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.77)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %x.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:258:0
    %input.85 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.16, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.85)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.88, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.89)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %x.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:269:0
    %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.91)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
    %output.47 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1678:0
    %input.87 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.47, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1678:0
    return (%input.87)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.linear.Linear,
        %input.89 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.85)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.85)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
    %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.89, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1678:0
    %input.90 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.48, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1678:0
    return (%input.90)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.91 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %query.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.91, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.9)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.78 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.78)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.78)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.78)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.78)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.9 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %10 : int = aten::Int(%seq_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %11 : int = aten::Int(%seq_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %12 : int = aten::Int(%seq_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %13 : int = aten::Int(%seq_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %bsz.8 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %17 : int = aten::Int(%bsz.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %18 : int = aten::Int(%bsz.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.8 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %24 : int = aten::Int(%embed_dim.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
    %tensor.43 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.44 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.43, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.8, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.44, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %q.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.46 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.8, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.46, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %k.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.48 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.8, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.48, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %v.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.8, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
    %src_len.8 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %92 : int = aten::Int(%src_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %93 : int = aten::Int(%src_len.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.8, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.29 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.8, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %attn_weights.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.29, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.8 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.30, %reshaped.8, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.8, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %input.81 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.31, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %input.82 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.81, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %attn_weights.32 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.82, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %attn_output.8 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.32, %v.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.8, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
    %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.83)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.85 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.85, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.86)

Attention.k_proj
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.45 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.45)

Attention.out_proj
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %input.83 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.46 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.83, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.84 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.46, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.84)

Attention.q_proj
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.43 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.43, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.47 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.45, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.47)

EncoderLayer._actual_script_module
  graph(%self.87 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.87)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.87)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.87)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.87)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.87)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %x.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:258:0
    %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.18, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.96)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %input.100 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.99, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.100)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %x.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:269:0
    %input.102 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.19, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.102)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.94)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.94)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
    %output.53 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1678:0
    %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.53, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1678:0
    return (%input.98)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.linear.Linear,
        %input.100 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.95)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.95)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
    %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.100, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1678:0
    %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.54, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1678:0
    return (%input.101)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.102 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.96)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.96)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %query.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.102, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.10)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.88)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.88)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.88)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.88)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.10 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %10 : int = aten::Int(%seq_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %11 : int = aten::Int(%seq_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %12 : int = aten::Int(%seq_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %13 : int = aten::Int(%seq_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %bsz.9 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %17 : int = aten::Int(%bsz.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %18 : int = aten::Int(%bsz.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.9 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %24 : int = aten::Int(%embed_dim.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
    %tensor.49 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.50 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.49, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.9, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.50, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %q.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.52 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.9, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.52, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %k.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.54 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.9, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.54, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %v.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.9, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
    %src_len.9 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %92 : int = aten::Int(%src_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %93 : int = aten::Int(%src_len.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.9, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.33 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.9, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %attn_weights.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.33, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.9 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.34, %reshaped.9, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.9, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %input.92 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.35, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %input.93 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.92, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %attn_weights.36 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.93, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %attn_output.9 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.36, %v.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.9, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
    %input.94 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.94)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.96 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.97 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.97)

Attention.k_proj
Linear._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.51)

Attention.out_proj
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.52 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.94, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.52, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.95)

Attention.q_proj
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.49 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.49, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.53 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.51, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.53)

EncoderLayer._actual_script_module
  graph(%self.97 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.97)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.97)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.97)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.97)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.97)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %x.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:258:0
    %input.107 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.20, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.107)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.110 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %input.111 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.110, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.111)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %x.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:269:0
    %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.21, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.113)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
    %output.59 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1678:0
    %input.109 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.59, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1678:0
    return (%input.109)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.105 : __torch__.torch.nn.modules.linear.Linear,
        %input.111 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.105)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.105)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
    %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.111, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1678:0
    %input.112 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.60, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1678:0
    return (%input.112)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.113 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %query.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.113, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.11)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.98 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.98)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.98)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.98)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.98)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.11 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %10 : int = aten::Int(%seq_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %11 : int = aten::Int(%seq_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %12 : int = aten::Int(%seq_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %13 : int = aten::Int(%seq_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %bsz.10 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %17 : int = aten::Int(%bsz.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %18 : int = aten::Int(%bsz.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.10 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %24 : int = aten::Int(%embed_dim.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
    %tensor.55 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.56 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.55, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.10, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.56, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %q.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.58 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.10, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.58, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %k.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.60 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.10, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.60, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %v.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.10, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
    %src_len.10 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %92 : int = aten::Int(%src_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %93 : int = aten::Int(%src_len.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.10, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.37 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.10, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %attn_weights.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.37, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.10 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.38, %reshaped.10, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.10, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %input.103 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.39, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %input.104 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.103, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %attn_weights.40 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.104, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %attn_output.10 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.40, %v.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.10, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
    %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.105)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.107 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.108 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.107, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.108)

Attention.k_proj
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.57)

Attention.out_proj
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %input.105 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.58 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.105, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.106 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.58, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.106)

Attention.q_proj
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.55 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.55, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.101)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.101)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.59 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.57, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.59)

EncoderLayer._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.107)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.107)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.107)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.107)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.107)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %x.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:258:0
    %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.22, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.118)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.121 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %input.122 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.121, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.122)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %x.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:269:0
    %input.124 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.23, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.124)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
    %output.65 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1678:0
    %input.120 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.65, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1678:0
    return (%input.120)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.linear.Linear,
        %input.122 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.115)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
    %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.122, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1678:0
    %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.66, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1678:0
    return (%input.123)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.124 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %query.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.124, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.12)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.108 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.108)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.108)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.108)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.108)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.12 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %10 : int = aten::Int(%seq_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %11 : int = aten::Int(%seq_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %12 : int = aten::Int(%seq_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %13 : int = aten::Int(%seq_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %bsz.11 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %17 : int = aten::Int(%bsz.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %18 : int = aten::Int(%bsz.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.11 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %24 : int = aten::Int(%embed_dim.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
    %tensor.61 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.62 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.61, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.11, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.62, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %q.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.64 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.11, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.64, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %k.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.66 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.11, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.66, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %v.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.11, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
    %src_len.11 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %92 : int = aten::Int(%src_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %93 : int = aten::Int(%src_len.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.11, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.41 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.11, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %attn_weights.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.41, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.11 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.42, %reshaped.11, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.11, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %input.114 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.43, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %input.115 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.114, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %attn_weights.44 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.115, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %attn_output.11 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.44, %v.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.11, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
    %input.116 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.116)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.118 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.118, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.119)

Attention.k_proj
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.63 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.63)

Attention.out_proj
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %input.116 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.64 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.116, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.64, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.117)

Attention.q_proj
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.61 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.61, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.111)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.111)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.65 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.63, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.65)

EncoderLayer._actual_script_module
  graph(%self.117 : __torch__.transformers.modeling_bart.EncoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.117)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.117)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.117)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.117)
    %7 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.117)
    %27 : Tensor = prim::CallMethod[name="forward"](%7, %1, %key_padding_mask.1)
    %9 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %x.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%27, %9, %10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %12 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:258:0
    %input.129 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.24, %12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:258:0
    %28 : Tensor = prim::CallMethod[name="forward"](%6, %input.129)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %28)
    %input.132 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:1369:0
    %17 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %18 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.132, %17, %18), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.133)
    %21 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %22 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %x.25 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%30, %21, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
    %24 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:269:0
    %input.135 : Float(13:17408, 17:1024, 1024:1) = aten::add(%28, %x.25, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:269:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.135)
    return (%31)

EncoderLayer.fc1
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.124)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
    %output.71 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1678:0
    %input.131 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.71, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1678:0
    return (%input.131)

EncoderLayer.fc2
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %input.133 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
    %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.133, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1678:0
    %input.134 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.72, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1678:0
    return (%input.134)

EncoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.135 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.126)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.126)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %x.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.135, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    return (%x.26)

EncoderLayer.self_attn
Attention._actual_script_module
  graph(%self.118 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask.1 : Bool(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.118)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.118)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.118)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.118)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.13 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %10 : int = aten::Int(%seq_len.13), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %11 : int = aten::Int(%seq_len.13), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %12 : int = aten::Int(%seq_len.13), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %13 : int = aten::Int(%seq_len.13), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %bsz.12 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %17 : int = aten::Int(%bsz.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %18 : int = aten::Int(%bsz.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.12 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %24 : int = aten::Int(%embed_dim.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %164 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
    %tensor.67 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%164, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
    %165 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %166 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.68 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.67, %44), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.12, %46), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.68, %50), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %q.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.70 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%165, %55), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.12, %57), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.70, %62), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %k.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.72 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %67), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.12, %69), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.72, %74), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %v.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.12, %89), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
    %src_len.12 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %92 : int = aten::Int(%src_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %93 : int = aten::Int(%src_len.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.12, %94, %95), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.45 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.12, %96), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %120 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:718:0
    %121 : int[] = prim::ListConstruct(%18, %120, %12, %93), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %attn_weights.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.45, %121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:718:0
    %123 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
    %124 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
    %125 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
    %reshaped.12 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%124, %125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
    %127 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:720:0
    %attn_weights.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.46, %reshaped.12, %127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:720:0
    %129 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
    %130 : Long() = aten::mul(%bsz.12, %129), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
    %131 : int = aten::Int(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %132 : int[] = prim::ListConstruct(%131, %11, %92), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %input.125 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.47, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
    %134 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:1498:0
    %135 : None = prim::Constant(), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %input.126 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.125, %134, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:1498:0
    %137 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %138 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %attn_weights.48 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.126, %137, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %attn_output.12 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.48, %v.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
    %156 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %157 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %158 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.12, %156, %157), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %159 : int = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %160 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%158, %159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %161 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
    %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::view(%160, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %167 : Tensor = prim::CallMethod[name="forward"](%3, %input.127)
    return (%167)

EncoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.129 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.130 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.129, %5, %3, %2, %6, %7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.130)

Attention.k_proj
Linear._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.69 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.69)

Attention.out_proj
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %input.127 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.122)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.70 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.127, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.70, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.128)

Attention.q_proj
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.67 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.67, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.71 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.69, %2, %6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.71)

BartDecoder.embed_positions
LearnedPositionalEmbedding._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_bart.LearnedPositionalEmbedding,
        %prev_output_tokens : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.128)
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:816:0
    %7 : int = aten::size(%prev_output_tokens, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:816:0
    %seq_len.14 : Long() = prim::NumToTensor(%7), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions
    %9 : Scalar = aten::ScalarImplicit(%seq_len.14), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions
    %10 : int = prim::Constant[value=4](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
    %11 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
    %12 : Device = prim::Constant[value="cpu"](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
    %13 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
    %positions.2 : Long(13:1) = aten::arange(%9, %10, %11, %12, %13), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
    %15 : Long() = prim::Constant[value={2}](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:822:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:822:0
    %input.136 : Long(13:1) = aten::add(%positions.2, %15, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:822:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
    %24 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
    %25 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
    %positions : Float(13:1024, 1024:1) = aten::embedding(%2, %input.136, %23, %24, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
    return (%positions)

BartDecoder.embed_tokens
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %weight.1 : Float(50265:1024, 1024:1),
        %input_ids : Long(17:13, 13:1)):
    %8 : int = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %10 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %11 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%weight.1, %input_ids, %8, %9, %10), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
    return (%11)

BartDecoder.layernorm_embedding
LayerNorm._actual_script_module
  graph(%self.130 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.137 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.130)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.130)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
    %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.137, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
    return (%input.138)

DecoderLayer._actual_script_module
  graph(%self.131 : __torch__.transformers.modeling_bart.DecoderLayer,
        %query.13 : Float(13:1024, 17:13312, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.131)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.131)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.131)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.131)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.131)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.131)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.131)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %query.13, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %x.29 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:427:0
    %input.143 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.13, %x.29, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.143)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %x.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:443:0
    %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.30, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.149)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.153)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %x.31 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:455:0
    %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.31, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.155)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.138)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.138)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.138)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.138)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.16 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %11 : int = aten::Int(%seq_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %12 : int = aten::Int(%seq_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %13 : int = aten::Int(%seq_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %14 : int = aten::Int(%seq_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.14 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %18 : int = aten::Int(%bsz.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %19 : int = aten::Int(%bsz.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.14 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %25 : int = aten::Int(%embed_dim.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.79 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.80 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.79, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.14, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.80, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %q.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.82 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.14, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.82, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %k.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.84 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.14, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.84, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %v.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.14, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.14 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %93 : int = aten::Int(%src_len.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %94 : int = aten::Int(%src_len.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.14, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.52 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.14, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %attn_weights.53 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.52, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.13 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.54 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.53, %reshaped.13, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.14, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %input.145 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.54, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %input.146 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.145, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.55 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.146, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.14 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.55, %v.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.14, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
    %input.147 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.147)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.143 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.149 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.143)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.143)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.149, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.150)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
    %output.81 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1678:0
    %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.81, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1678:0
    return (%input.151)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.linear.Linear,
        %input.153 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.145)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.145)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
    %output.82 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1678:0
    %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.82, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1678:0
    return (%input.154)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.155 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    %query.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.15)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.132 : __torch__.transformers.modeling_bart.Attention,
        %query.13 : Float(13:1024, 17:13312, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.132)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.132)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.132)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.132)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%query.13, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.15 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %10 : int = aten::Int(%seq_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %11 : int = aten::Int(%seq_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %12 : int = aten::Int(%seq_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %13 : int = aten::Int(%seq_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%query.13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %bsz.13 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %17 : int = aten::Int(%bsz.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %18 : int = aten::Int(%bsz.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%query.13, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.13 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %24 : int = aten::Int(%embed_dim.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %query.13)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
    %tensor.73 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %query.13)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %query.13)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.74 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.73, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.13, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.74, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %q.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.76 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.13, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.76, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %k.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %tensor.78 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.13, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.78, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %v.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.13, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
    %src_len.13 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %92 : int = aten::Int(%src_len.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %93 : int = aten::Int(%src_len.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.13, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.49 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.13, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.49, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.50 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.13, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %input.139 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.50, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %input.140 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.139, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %attn_weights.51 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.140, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
    %attn_output.13 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.51, %v.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.13, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
    %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.141)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.143 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.137)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.137)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.143, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.14)

Attention.k_proj
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %query.13 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.74 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.75 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.74, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.75)

Attention.out_proj
Linear._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.linear.Linear,
        %input.141 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.76 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.141, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.142 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.76, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.142)

Attention.q_proj
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %query.13 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.73 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.73, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.linear.Linear,
        %query.13 : Float(13:1024, 17:13312, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.135)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.135)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.75 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.77 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.75, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.77)

Attention.k_proj
Linear._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.140)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.78 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.81 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.78, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.81)

Attention.out_proj
Linear._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.linear.Linear,
        %input.147 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.80 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.147, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.148 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.80, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.148)

Attention.q_proj
Linear._actual_script_module
  graph(%self.139 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.139)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.139)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.77 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.77, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.141 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.141)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.141)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.79 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.83 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.79, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.83)

DecoderLayer._actual_script_module
  graph(%self.147 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.147)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.147)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.147)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.147)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.147)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.147)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.147)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %x.32 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:427:0
    %input.160 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.32, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.160)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %x.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:443:0
    %input.165 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.33, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.165)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.168 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.168, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.169)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %x.34 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:455:0
    %input.171 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.34, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.171)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.154)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.154)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.154)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.154)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.18 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %11 : int = aten::Int(%seq_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %12 : int = aten::Int(%seq_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %13 : int = aten::Int(%seq_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %14 : int = aten::Int(%seq_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.16 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %18 : int = aten::Int(%bsz.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %19 : int = aten::Int(%bsz.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.16 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %25 : int = aten::Int(%embed_dim.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.91 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.92 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.91, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.16, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.92, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %q.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.94 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.16, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.94, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %k.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.96 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.16, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.96, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %v.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.16, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.16 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %93 : int = aten::Int(%src_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %94 : int = aten::Int(%src_len.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.16, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.59 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.16, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %attn_weights.60 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.59, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.14 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.61 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.60, %reshaped.14, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.16, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %input.161 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.61, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %input.162 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.161, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.62 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.162, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.16 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.62, %v.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.16, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
    %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.163)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.165 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.166 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.165, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.166)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.160 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.160)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.160)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
    %output.91 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1678:0
    %input.167 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.91, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1678:0
    return (%input.167)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.linear.Linear,
        %input.169 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.161)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.161)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
    %output.92 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.169, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1678:0
    %input.170 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.92, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1678:0
    return (%input.170)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.171 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.162)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.162)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    %query.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.171, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.17)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.148 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.148)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.148)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.148)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.148)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.17 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %10 : int = aten::Int(%seq_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %11 : int = aten::Int(%seq_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %12 : int = aten::Int(%seq_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %13 : int = aten::Int(%seq_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %bsz.15 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %17 : int = aten::Int(%bsz.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %18 : int = aten::Int(%bsz.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.15 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %24 : int = aten::Int(%embed_dim.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
    %tensor.85 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.86 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.85, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.15, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.86, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %q.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.88 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.15, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.88, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %k.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %tensor.90 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.15, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.90, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %v.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.15, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
    %src_len.15 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %92 : int = aten::Int(%src_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %93 : int = aten::Int(%src_len.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.15, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.56 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.15, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.56, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.15, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %input.156 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.57, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %input.157 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.156, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %attn_weights.58 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.157, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
    %attn_output.15 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.58, %v.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.15, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
    %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.158)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.160 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.160, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.16)

Attention.k_proj
Linear._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.150)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.150)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.84 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.87 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.84, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.87)

Attention.out_proj
Linear._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.linear.Linear,
        %input.158 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.152)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.152)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.86 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.158, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.86, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.159)

Attention.q_proj
Linear._actual_script_module
  graph(%self.149 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.149)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.149)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.83 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.83, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.151)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.151)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.85 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.89 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.85, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.89)

Attention.k_proj
Linear._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.156)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.156)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.88 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.93 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.88, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.93)

Attention.out_proj
Linear._actual_script_module
  graph(%self.158 : __torch__.torch.nn.modules.linear.Linear,
        %input.163 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.158)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.158)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.90 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.163, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.90, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.164)

Attention.q_proj
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.87 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.87, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.89 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.89, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.95)

DecoderLayer._actual_script_module
  graph(%self.163 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.163)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.163)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.163)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.163)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.163)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.163)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.163)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %x.35 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:427:0
    %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.35, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.176)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %x.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:443:0
    %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.36, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.181)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.184 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %input.185 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.184, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.185)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %x.37 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:455:0
    %input.187 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.37, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.187)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.170 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.170)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.170)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.170)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.170)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.20 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %11 : int = aten::Int(%seq_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %12 : int = aten::Int(%seq_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %13 : int = aten::Int(%seq_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %14 : int = aten::Int(%seq_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.18 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %18 : int = aten::Int(%bsz.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %19 : int = aten::Int(%bsz.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.18 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %25 : int = aten::Int(%embed_dim.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.103 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.104 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.103, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.18, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.104, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %q.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.106 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.18, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.106, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %k.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.108 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.18, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.108, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %v.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.18, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.18 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %93 : int = aten::Int(%src_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %94 : int = aten::Int(%src_len.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.18, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.66 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.18, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %attn_weights.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.66, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.15 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.68 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.67, %reshaped.15, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.18, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %input.177 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.68, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %input.178 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.177, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.69 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.178, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.18 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.69, %v.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.18, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
    %input.179 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.179)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.175 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.181 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.175)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.175)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.181, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.182)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.176 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
    %output.101 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1678:0
    %input.183 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.101, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1678:0
    return (%input.183)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.linear.Linear,
        %input.185 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.177)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.177)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
    %output.102 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.185, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1678:0
    %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.102, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1678:0
    return (%input.186)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.187 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    %query.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.187, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.19)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.164 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.164)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.164)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.164)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.164)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.19 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %10 : int = aten::Int(%seq_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %11 : int = aten::Int(%seq_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %12 : int = aten::Int(%seq_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %13 : int = aten::Int(%seq_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %bsz.17 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %17 : int = aten::Int(%bsz.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %18 : int = aten::Int(%bsz.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.17 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %24 : int = aten::Int(%embed_dim.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
    %tensor.97 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.98 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.97, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.17, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.98, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %q.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.100 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.17, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.100, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %k.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %tensor.102 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.17, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.102, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %v.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.17, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
    %src_len.17 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %92 : int = aten::Int(%src_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %93 : int = aten::Int(%src_len.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.17, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.63 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.17, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.63, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.64 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.17, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %input.172 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.64, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %input.173 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.172, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %attn_weights.65 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.173, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
    %attn_output.17 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.65, %v.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.17, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
    %input.174 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.174)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.169 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.176 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.169)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.169)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.176, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.18)

Attention.k_proj
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.94 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.99 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.94, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.99)

Attention.out_proj
Linear._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.linear.Linear,
        %input.174 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.96 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.174, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.175 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.96, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.175)

Attention.q_proj
Linear._actual_script_module
  graph(%self.165 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.165)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.165)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.93 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.93, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.167)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.95 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.95, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.101)

Attention.k_proj
Linear._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.172)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.98 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.105 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.98, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.105)

Attention.out_proj
Linear._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.linear.Linear,
        %input.179 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.100 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.179, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.180 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.100, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.180)

Attention.q_proj
Linear._actual_script_module
  graph(%self.171 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.171)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.171)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.97 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.97, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.173)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.173)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.99 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.107 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.99, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.107)

DecoderLayer._actual_script_module
  graph(%self.179 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.179)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.179)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.179)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.179)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.179)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.179)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.179)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %x.38 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:427:0
    %input.192 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.38, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.192)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %x.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:443:0
    %input.197 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.197)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.200 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %input.201 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.200, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.201)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %x.40 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:455:0
    %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.40, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.203)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.186 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.186)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.186)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.186)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.186)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.22 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %11 : int = aten::Int(%seq_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %12 : int = aten::Int(%seq_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %13 : int = aten::Int(%seq_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %14 : int = aten::Int(%seq_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.20 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %18 : int = aten::Int(%bsz.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %19 : int = aten::Int(%bsz.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.20 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %25 : int = aten::Int(%embed_dim.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.115 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.116 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.115, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.20, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.116, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %q.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.118 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.20, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.118, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %k.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.120 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.20, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.120, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %v.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.20, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.20 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %93 : int = aten::Int(%src_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %94 : int = aten::Int(%src_len.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.20, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.73 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.20, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %attn_weights.74 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.73, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.16 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.74, %reshaped.16, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.20, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %input.193 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.75, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %input.194 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.193, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.76 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.194, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.20 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.76, %v.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.20, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
    %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.195)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.191 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.197 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.191)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.191)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.198 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.197, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.198)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.192)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.192)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
    %output.111 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1678:0
    %input.199 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.111, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1678:0
    return (%input.199)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.193 : __torch__.torch.nn.modules.linear.Linear,
        %input.201 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.193)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.193)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
    %output.112 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.201, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1678:0
    %input.202 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.112, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1678:0
    return (%input.202)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.194 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.203 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.194)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.194)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    %query.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.203, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.21)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.180 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.180)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.180)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.180)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.180)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.21 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %10 : int = aten::Int(%seq_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %11 : int = aten::Int(%seq_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %12 : int = aten::Int(%seq_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %13 : int = aten::Int(%seq_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %bsz.19 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %17 : int = aten::Int(%bsz.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %18 : int = aten::Int(%bsz.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.19 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %24 : int = aten::Int(%embed_dim.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
    %tensor.109 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.110 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.109, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.19, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.110, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %q.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.112 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.19, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.112, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %k.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %tensor.114 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.19, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.114, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %v.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.19, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
    %src_len.19 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %92 : int = aten::Int(%src_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %93 : int = aten::Int(%src_len.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.19, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.70 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.19, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.70, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.19, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %input.188 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.71, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %input.189 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.188, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %attn_weights.72 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.189, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
    %attn_output.19 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.72, %v.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.19, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
    %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.190)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.192 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.185)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.192, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.20)

Attention.k_proj
Linear._actual_script_module
  graph(%self.182 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.182)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.182)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.104 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.111 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.104, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.111)

Attention.out_proj
Linear._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.linear.Linear,
        %input.190 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.184)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.184)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.106 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.190, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.106, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.191)

Attention.q_proj
Linear._actual_script_module
  graph(%self.181 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.181)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.181)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.103 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.103, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.183)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.183)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.105 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.113 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.105, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.113)

Attention.k_proj
Linear._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.188)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.188)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.108 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.108, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.117)

Attention.out_proj
Linear._actual_script_module
  graph(%self.190 : __torch__.torch.nn.modules.linear.Linear,
        %input.195 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.190)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.190)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.110 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.195, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.196 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.110, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.196)

Attention.q_proj
Linear._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.187)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.187)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.107 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.107, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.109 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.119 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.109, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.119)

DecoderLayer._actual_script_module
  graph(%self.195 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.195)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.195)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.195)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.195)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.195)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.195)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.195)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %x.41 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:427:0
    %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.41, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.208)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %x.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:443:0
    %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.42, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.213)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %input.217 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.216, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.217)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %x.43 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:455:0
    %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.43, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.219)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.202 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.202)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.202)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.202)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.202)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.24 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %11 : int = aten::Int(%seq_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %12 : int = aten::Int(%seq_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %13 : int = aten::Int(%seq_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %14 : int = aten::Int(%seq_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.22 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %18 : int = aten::Int(%bsz.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %19 : int = aten::Int(%bsz.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.22 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %25 : int = aten::Int(%embed_dim.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.127 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.128 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.127, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.22, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.128, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %q.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.130 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.22, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.130, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %k.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.132 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.22, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.132, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %v.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.22, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.22 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %93 : int = aten::Int(%src_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %94 : int = aten::Int(%src_len.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.22, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.80 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.22, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %attn_weights.81 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.80, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.17 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.82 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.81, %reshaped.17, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.22, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %input.209 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.82, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %input.210 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.209, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.83 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.210, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.22 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.83, %v.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.22, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
    %input.211 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.211)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.207 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.213 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.207)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.207)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.214 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.214)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.208 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
    %output.121 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1678:0
    %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.121, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1678:0
    return (%input.215)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.209 : __torch__.torch.nn.modules.linear.Linear,
        %input.217 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.209)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.209)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
    %output.122 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.217, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1678:0
    %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.122, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1678:0
    return (%input.218)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.219 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.210)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    %query.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.219, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.23)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.196 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.196)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.196)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.196)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.196)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.23 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %10 : int = aten::Int(%seq_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %11 : int = aten::Int(%seq_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %12 : int = aten::Int(%seq_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %13 : int = aten::Int(%seq_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %bsz.21 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %17 : int = aten::Int(%bsz.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %18 : int = aten::Int(%bsz.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.21 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %24 : int = aten::Int(%embed_dim.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
    %tensor.121 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.122 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.121, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.21, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.122, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %q.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.124 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.21, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.124, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %k.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %tensor.126 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.21, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.126, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %v.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.21, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
    %src_len.21 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %92 : int = aten::Int(%src_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %93 : int = aten::Int(%src_len.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.21, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.77 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.21, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.77, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.78 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.21, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %input.204 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.78, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %input.205 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.204, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %attn_weights.79 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.205, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
    %attn_output.21 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.79, %v.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.21, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
    %input.206 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.206)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.208 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.208, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.22)

Attention.k_proj
Linear._actual_script_module
  graph(%self.198 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.198)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.198)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.114 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.114, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.123)

Attention.out_proj
Linear._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.linear.Linear,
        %input.206 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.200)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.200)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.116 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.206, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.116, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.207)

Attention.q_proj
Linear._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.113 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.113, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.199)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.115 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.125 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.115, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.125)

Attention.k_proj
Linear._actual_script_module
  graph(%self.204 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.204)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.204)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.118 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.129 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.118, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.129)

Attention.out_proj
Linear._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.linear.Linear,
        %input.211 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.206)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.206)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.120 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.211, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.120, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.212)

Attention.q_proj
Linear._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.203)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.117 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.117, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.205 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.205)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.205)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.119 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.131 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.119, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.131)

DecoderLayer._actual_script_module
  graph(%self.211 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.211)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.211)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.211)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.211)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.211)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.211)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.211)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %x.44 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:427:0
    %input.224 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.44, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.224)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %x.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:443:0
    %input.229 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.45, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.229)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.232 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %input.233 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.232, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.233)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %x.46 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:455:0
    %input.235 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.46, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.235)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.218 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.218)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.218)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.218)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.218)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.26 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %11 : int = aten::Int(%seq_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %12 : int = aten::Int(%seq_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %13 : int = aten::Int(%seq_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %14 : int = aten::Int(%seq_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.24 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %18 : int = aten::Int(%bsz.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %19 : int = aten::Int(%bsz.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.24 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %25 : int = aten::Int(%embed_dim.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.139 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.140 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.139, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.24, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.140, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %q.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.142 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.24, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.142, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %k.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.144 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.24, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.144, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %v.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.24, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.24 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %93 : int = aten::Int(%src_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %94 : int = aten::Int(%src_len.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.24, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.87 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.24, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %attn_weights.88 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.87, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.18 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.89 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.88, %reshaped.18, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.24, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %input.225 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.89, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %input.226 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.225, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.90 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.226, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.24 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.90, %v.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.24, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
    %input.227 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.227)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.223 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.229 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.223)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.223)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.230 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.229, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.230)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.224 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.224)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.224)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
    %output.131 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1678:0
    %input.231 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.131, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1678:0
    return (%input.231)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.225 : __torch__.torch.nn.modules.linear.Linear,
        %input.233 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.225)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
    %output.132 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.233, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1678:0
    %input.234 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.132, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1678:0
    return (%input.234)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.226 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.235 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.226)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.226)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    %query.25 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.235, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.25)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.212 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.212)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.212)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.212)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.212)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.25 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %10 : int = aten::Int(%seq_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %11 : int = aten::Int(%seq_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %12 : int = aten::Int(%seq_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %13 : int = aten::Int(%seq_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %bsz.23 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %17 : int = aten::Int(%bsz.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %18 : int = aten::Int(%bsz.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.23 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %24 : int = aten::Int(%embed_dim.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
    %tensor.133 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.134 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.133, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.23, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.134, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %q.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.136 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.23, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.136, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %k.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %tensor.138 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.23, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.138, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %v.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.23, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
    %src_len.23 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %92 : int = aten::Int(%src_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %93 : int = aten::Int(%src_len.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.23, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.84 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.23, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.84, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.23, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %input.220 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.85, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %input.221 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.220, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %attn_weights.86 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.221, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
    %attn_output.23 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.86, %v.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.23, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
    %input.222 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.222)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.224 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.217)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.217)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.24 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.224, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.24)

Attention.k_proj
Linear._actual_script_module
  graph(%self.214 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.124 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.135 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.124, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.135)

Attention.out_proj
Linear._actual_script_module
  graph(%self.216 : __torch__.torch.nn.modules.linear.Linear,
        %input.222 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.216)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.216)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.126 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.222, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.223 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.126, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.223)

Attention.q_proj
Linear._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.213)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.213)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.123 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.123, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.215 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.215)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.215)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.125 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.125, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.137)

Attention.k_proj
Linear._actual_script_module
  graph(%self.220 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.220)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.220)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.128 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.141 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.128, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.141)

Attention.out_proj
Linear._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.linear.Linear,
        %input.227 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.222)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.222)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.130 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.227, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.228 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.130, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.228)

Attention.q_proj
Linear._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.219)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.219)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.127 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.127, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.221)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.221)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.129 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.143 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.129, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.143)

DecoderLayer._actual_script_module
  graph(%self.227 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.227)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.227)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.227)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.227)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.227)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.227)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.227)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %x.47 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:427:0
    %input.240 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.47, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.240)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %x.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:443:0
    %input.245 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.48, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.245)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.248 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %input.249 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.248, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.249)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %x.49 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:455:0
    %input.251 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.49, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.251)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.234 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.234)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.234)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.234)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.234)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.28 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %11 : int = aten::Int(%seq_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %12 : int = aten::Int(%seq_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %13 : int = aten::Int(%seq_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %14 : int = aten::Int(%seq_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.26 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %18 : int = aten::Int(%bsz.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %19 : int = aten::Int(%bsz.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.26 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %25 : int = aten::Int(%embed_dim.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.151 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.152 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.151, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.26, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.152, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %q.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.154 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.26, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.154, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %k.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.156 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.26, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.156, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %v.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.26, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.26 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %93 : int = aten::Int(%src_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %94 : int = aten::Int(%src_len.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.26, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.94 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.26, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %attn_weights.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.94, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.19 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.95, %reshaped.19, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.26, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %input.241 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.96, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %input.242 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.241, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.97 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.242, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.26 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.97, %v.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.26, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
    %input.243 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.243)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.239 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.245 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.239)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.239)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.246 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.245, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.246)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.240 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.240)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.240)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
    %output.141 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1678:0
    %input.247 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.141, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1678:0
    return (%input.247)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.241 : __torch__.torch.nn.modules.linear.Linear,
        %input.249 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.241)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.241)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
    %output.142 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.249, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1678:0
    %input.250 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.142, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1678:0
    return (%input.250)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.242 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.251 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.242)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.242)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    %query.27 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.251, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.27)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.228 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.228)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.228)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.228)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.228)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.27 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %10 : int = aten::Int(%seq_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %11 : int = aten::Int(%seq_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %12 : int = aten::Int(%seq_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %13 : int = aten::Int(%seq_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %bsz.25 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %17 : int = aten::Int(%bsz.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %18 : int = aten::Int(%bsz.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.25 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %24 : int = aten::Int(%embed_dim.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
    %tensor.145 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.146 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.145, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.25, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.146, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %q.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.148 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.25, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.148, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %k.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %tensor.150 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.25, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.150, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %v.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.25, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
    %src_len.25 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %92 : int = aten::Int(%src_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %93 : int = aten::Int(%src_len.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.25, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.91 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.25, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.91, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.92 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.25, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %input.236 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.92, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %input.237 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.236, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %attn_weights.93 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.237, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
    %attn_output.25 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.93, %v.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.25, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
    %input.238 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.238)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.240 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.233)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.233)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.240, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.26)

Attention.k_proj
Linear._actual_script_module
  graph(%self.230 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.230)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.230)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.134 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.147 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.134, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.147)

Attention.out_proj
Linear._actual_script_module
  graph(%self.232 : __torch__.torch.nn.modules.linear.Linear,
        %input.238 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.232)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.232)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.136 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.238, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.239 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.136, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.239)

Attention.q_proj
Linear._actual_script_module
  graph(%self.229 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.229)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.229)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.133 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.133, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.231 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.231)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.231)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.135 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.149 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.135, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.149)

Attention.k_proj
Linear._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.236)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.236)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.138 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.153 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.138, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.153)

Attention.out_proj
Linear._actual_script_module
  graph(%self.238 : __torch__.torch.nn.modules.linear.Linear,
        %input.243 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.238)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.238)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.140 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.243, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.244 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.140, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.244)

Attention.q_proj
Linear._actual_script_module
  graph(%self.235 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.235)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.235)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.137 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.137, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.237 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.237)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.237)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.139 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.155 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.139, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.155)

DecoderLayer._actual_script_module
  graph(%self.243 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.243)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.243)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.243)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.243)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.243)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.243)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.243)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %x.50 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:427:0
    %input.256 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.50, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.256)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %x.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:443:0
    %input.261 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.51, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.261)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.264 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %input.265 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.264, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.265)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %x.52 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:455:0
    %input.267 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.52, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.267)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.250 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.250)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.250)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.250)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.250)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.30 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %11 : int = aten::Int(%seq_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %12 : int = aten::Int(%seq_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %13 : int = aten::Int(%seq_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %14 : int = aten::Int(%seq_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.28 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %18 : int = aten::Int(%bsz.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %19 : int = aten::Int(%bsz.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.28 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %25 : int = aten::Int(%embed_dim.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.163 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.164 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.163, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.28, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.164, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %q.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.166 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.28, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.166, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %k.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.168 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.28, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.168, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %v.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.28, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.28 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %93 : int = aten::Int(%src_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %94 : int = aten::Int(%src_len.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.28, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.101 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.28, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %attn_weights.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.101, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.20 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.102, %reshaped.20, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.28, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %input.257 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.103, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %input.258 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.257, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.104 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.258, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.28 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.104, %v.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.28, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
    %input.259 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.259)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.261 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.255)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.255)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.262 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.261, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.262)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.256 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.256)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.256)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
    %output.151 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1678:0
    %input.263 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.151, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1678:0
    return (%input.263)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.257 : __torch__.torch.nn.modules.linear.Linear,
        %input.265 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.257)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.257)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
    %output.152 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.265, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1678:0
    %input.266 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.152, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1678:0
    return (%input.266)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.258 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.267 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.258)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.258)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    %query.29 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.267, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.29)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.244 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.244)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.244)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.244)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.244)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.29 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %10 : int = aten::Int(%seq_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %11 : int = aten::Int(%seq_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %12 : int = aten::Int(%seq_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %13 : int = aten::Int(%seq_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %bsz.27 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %17 : int = aten::Int(%bsz.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %18 : int = aten::Int(%bsz.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.27 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %24 : int = aten::Int(%embed_dim.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
    %tensor.157 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.158 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.157, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.27, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.158, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %q.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.160 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.27, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.160, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %k.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %tensor.162 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.27, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.162, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %v.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.27, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
    %src_len.27 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %92 : int = aten::Int(%src_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %93 : int = aten::Int(%src_len.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.27, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.98 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.27, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.98, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.99 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.27, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %input.252 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.99, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %input.253 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.252, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %attn_weights.100 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.253, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
    %attn_output.27 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.100, %v.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.27, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
    %input.254 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.254)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.249 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.256 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.249)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.249)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.28 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.256, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.28)

Attention.k_proj
Linear._actual_script_module
  graph(%self.246 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.246)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.246)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.144 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.144, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.159)

Attention.out_proj
Linear._actual_script_module
  graph(%self.248 : __torch__.torch.nn.modules.linear.Linear,
        %input.254 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.248)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.248)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.146 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.254, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.255 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.146, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.255)

Attention.q_proj
Linear._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.245)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.245)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.143 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.143, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.247 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.247)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.247)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.145 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.161 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.145, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.161)

Attention.k_proj
Linear._actual_script_module
  graph(%self.252 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.252)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.252)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.148 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.165 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.148, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.165)

Attention.out_proj
Linear._actual_script_module
  graph(%self.254 : __torch__.torch.nn.modules.linear.Linear,
        %input.259 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.254)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.254)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.150 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.259, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.260 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.150, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.260)

Attention.q_proj
Linear._actual_script_module
  graph(%self.251 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.251)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.251)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.147 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.147, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.253 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.253)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.253)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.149 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.167 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.149, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.167)

DecoderLayer._actual_script_module
  graph(%self.259 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.259)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.259)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.259)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.259)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.259)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.259)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.259)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %x.53 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:427:0
    %input.272 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.53, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.272)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %x.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:443:0
    %input.277 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.54, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.277)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.280 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %input.281 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.280, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.281)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %x.55 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:455:0
    %input.283 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.55, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.283)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.266 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.266)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.266)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.266)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.266)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.32 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %11 : int = aten::Int(%seq_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %12 : int = aten::Int(%seq_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %13 : int = aten::Int(%seq_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %14 : int = aten::Int(%seq_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.30 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %18 : int = aten::Int(%bsz.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %19 : int = aten::Int(%bsz.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.30 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %25 : int = aten::Int(%embed_dim.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.175 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.176 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.175, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.30, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.176, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %q.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.178 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.30, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.178, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %k.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.180 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.30, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.180, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %v.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.30, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.30 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %93 : int = aten::Int(%src_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %94 : int = aten::Int(%src_len.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.30, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.108 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.30, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %attn_weights.109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.108, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.21 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.110 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.109, %reshaped.21, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.30, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %input.273 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.110, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %input.274 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.273, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.111 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.274, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.30 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.111, %v.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.30, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
    %input.275 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.275)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.271 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.277 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.271)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.271)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.278 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.277, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.278)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.272 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.272)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.272)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
    %output.161 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1678:0
    %input.279 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.161, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1678:0
    return (%input.279)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.273 : __torch__.torch.nn.modules.linear.Linear,
        %input.281 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.273)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.273)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
    %output.162 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.281, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1678:0
    %input.282 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.162, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1678:0
    return (%input.282)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.274 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.283 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.274)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.274)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    %query.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.283, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.31)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.260 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.260)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.260)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.260)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.260)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.31 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %10 : int = aten::Int(%seq_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %11 : int = aten::Int(%seq_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %12 : int = aten::Int(%seq_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %13 : int = aten::Int(%seq_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %bsz.29 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %17 : int = aten::Int(%bsz.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %18 : int = aten::Int(%bsz.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.29 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %24 : int = aten::Int(%embed_dim.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
    %tensor.169 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.170 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.169, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.29, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.170, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %q.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.172 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.29, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.172, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %k.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %tensor.174 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.29, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.174, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %v.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.29, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
    %src_len.29 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %92 : int = aten::Int(%src_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %93 : int = aten::Int(%src_len.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.29, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.105 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.29, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.105, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.29, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %input.268 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.106, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %input.269 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.268, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %attn_weights.107 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.269, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
    %attn_output.29 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.107, %v.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.29, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
    %input.270 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.270)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.265 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.272 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.265)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.265)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.30 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.272, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.30)

Attention.k_proj
Linear._actual_script_module
  graph(%self.262 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.262)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.262)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.154 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.171 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.154, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.171)

Attention.out_proj
Linear._actual_script_module
  graph(%self.264 : __torch__.torch.nn.modules.linear.Linear,
        %input.270 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.264)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.264)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.156 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.270, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.271 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.156, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.271)

Attention.q_proj
Linear._actual_script_module
  graph(%self.261 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.261)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.261)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.153 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.153, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.263 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.263)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.263)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.155 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.173 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.155, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.173)

Attention.k_proj
Linear._actual_script_module
  graph(%self.268 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.268)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.268)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.158 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.177 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.158, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.177)

Attention.out_proj
Linear._actual_script_module
  graph(%self.270 : __torch__.torch.nn.modules.linear.Linear,
        %input.275 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.270)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.270)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.160 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.275, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.276 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.160, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.276)

Attention.q_proj
Linear._actual_script_module
  graph(%self.267 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.267)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.267)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.157 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.157, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.269 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.269)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.269)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.159 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.179 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.159, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.179)

DecoderLayer._actual_script_module
  graph(%self.275 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.275)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.275)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.275)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.275)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.275)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.275)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.275)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %x.56 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:427:0
    %input.288 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.56, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.288)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %x.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:443:0
    %input.293 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.57, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.293)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.296 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %input.297 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.296, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.297)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %x.58 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:455:0
    %input.299 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.58, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.299)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.282 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.282)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.282)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.282)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.282)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.34 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %11 : int = aten::Int(%seq_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %12 : int = aten::Int(%seq_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %13 : int = aten::Int(%seq_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %14 : int = aten::Int(%seq_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.32 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %18 : int = aten::Int(%bsz.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %19 : int = aten::Int(%bsz.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.32 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %25 : int = aten::Int(%embed_dim.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.187 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.188 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.187, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.32, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.188, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %q.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.190 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.32, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.190, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %k.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.192 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.32, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.192, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %v.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.32, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.32 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %93 : int = aten::Int(%src_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %94 : int = aten::Int(%src_len.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.32, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.115 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.32, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %attn_weights.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.115, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.22 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.117 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.116, %reshaped.22, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.32, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %input.289 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.117, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %input.290 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.289, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.118 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.290, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.32 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.118, %v.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.32, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
    %input.291 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.291)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.287 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.293 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.287)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.287)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.294 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.293, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.294)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.288 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.288)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.288)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
    %output.171 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1678:0
    %input.295 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.171, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1678:0
    return (%input.295)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.289 : __torch__.torch.nn.modules.linear.Linear,
        %input.297 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.289)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.289)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
    %output.172 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.297, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1678:0
    %input.298 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.172, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1678:0
    return (%input.298)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.290 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.299 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.290)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.290)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    %query.33 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.299, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.33)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.276 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.276)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.276)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.276)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.276)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.33 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %10 : int = aten::Int(%seq_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %11 : int = aten::Int(%seq_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %12 : int = aten::Int(%seq_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %13 : int = aten::Int(%seq_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %bsz.31 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %17 : int = aten::Int(%bsz.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %18 : int = aten::Int(%bsz.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.31 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %24 : int = aten::Int(%embed_dim.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
    %tensor.181 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.182 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.181, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.31, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.182, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %q.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.184 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.31, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.184, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %k.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %tensor.186 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.31, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.186, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %v.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.31, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
    %src_len.31 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %92 : int = aten::Int(%src_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %93 : int = aten::Int(%src_len.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.31, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.112 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.31, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.112, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.113 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.31, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %input.284 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.113, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %input.285 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.284, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %attn_weights.114 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.285, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
    %attn_output.31 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.114, %v.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.31, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
    %input.286 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.286)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.281 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.288 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.281)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.281)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.32 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.288, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.32)

Attention.k_proj
Linear._actual_script_module
  graph(%self.278 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.278)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.278)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.164 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.183 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.164, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.183)

Attention.out_proj
Linear._actual_script_module
  graph(%self.280 : __torch__.torch.nn.modules.linear.Linear,
        %input.286 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.280)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.280)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.166 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.286, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.287 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.166, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.287)

Attention.q_proj
Linear._actual_script_module
  graph(%self.277 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.277)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.277)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.163 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.163, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.279 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.279)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.279)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.165 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.185 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.165, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.185)

Attention.k_proj
Linear._actual_script_module
  graph(%self.284 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.284)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.284)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.168 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.189 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.168, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.189)

Attention.out_proj
Linear._actual_script_module
  graph(%self.286 : __torch__.torch.nn.modules.linear.Linear,
        %input.291 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.286)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.286)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.170 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.291, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.292 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.170, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.292)

Attention.q_proj
Linear._actual_script_module
  graph(%self.283 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.283)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.283)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.167 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.167, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.285 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.285)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.285)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.169 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.169, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.191)

DecoderLayer._actual_script_module
  graph(%self.291 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.291)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.291)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.291)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.291)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.291)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.291)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.291)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %x.59 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:427:0
    %input.304 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.59, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.304)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %x.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:443:0
    %input.309 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.60, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.309)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.312 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %input.313 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.312, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.313)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %x.61 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:455:0
    %input.315 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.61, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.315)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.298 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.298)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.298)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.298)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.298)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len.36 : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %11 : int = aten::Int(%seq_len.36), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %12 : int = aten::Int(%seq_len.36), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %13 : int = aten::Int(%seq_len.36), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %14 : int = aten::Int(%seq_len.36), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz.34 : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %18 : int = aten::Int(%bsz.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %19 : int = aten::Int(%bsz.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim.34 : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %25 : int = aten::Int(%embed_dim.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.199 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.200 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.199, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz.34, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.200, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %q.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.202 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz.34, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.202, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %k.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.204 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz.34, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.204, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %v.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k.34, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len.34 : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %93 : int = aten::Int(%src_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %94 : int = aten::Int(%src_len.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.34, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.122 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.34, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %attn_weights.123 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.122, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped.23 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.124 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.123, %reshaped.23, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz.34, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %input.305 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.124, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %input.306 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.305, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights.125 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.306, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
    %attn_output.34 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.125, %v.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.34, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
    %input.307 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.307)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.303 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.309 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.303)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.303)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.310 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.309, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.310)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.304 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.304)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.304)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
    %output.181 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1678:0
    %input.311 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.181, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1678:0
    return (%input.311)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.305 : __torch__.torch.nn.modules.linear.Linear,
        %input.313 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.305)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.305)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
    %output.182 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.313, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1678:0
    %input.314 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.182, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1678:0
    return (%input.314)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self.306 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.315 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.306)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.306)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    %query.35 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.315, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
    return (%query.35)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.292 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.292)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.292)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.292)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.292)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.35 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %10 : int = aten::Int(%seq_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %11 : int = aten::Int(%seq_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %12 : int = aten::Int(%seq_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %13 : int = aten::Int(%seq_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %bsz.33 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %17 : int = aten::Int(%bsz.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %18 : int = aten::Int(%bsz.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.33 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %24 : int = aten::Int(%embed_dim.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
    %tensor.193 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.194 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.193, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.33, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.194, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %q.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.196 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.33, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.196, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %k.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %tensor.198 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.33, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.198, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %v.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.33, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
    %src_len.33 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %92 : int = aten::Int(%src_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %93 : int = aten::Int(%src_len.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.33, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.119 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.33, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.119, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.33, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %input.300 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.120, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %input.301 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.300, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %attn_weights.121 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.301, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
    %attn_output.33 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.121, %v.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.33, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
    %input.302 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.302)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.297 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.304 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.297)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.297)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query.34 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.304, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query.34)

Attention.k_proj
Linear._actual_script_module
  graph(%self.294 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.294)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.294)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.174 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.195 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.174, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.195)

Attention.out_proj
Linear._actual_script_module
  graph(%self.296 : __torch__.torch.nn.modules.linear.Linear,
        %input.302 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.296)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.296)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.176 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.302, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.303 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.176, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.303)

Attention.q_proj
Linear._actual_script_module
  graph(%self.293 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.293)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.293)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.173 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.173, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.295 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.295)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.295)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.175 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.197 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.175, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.197)

Attention.k_proj
Linear._actual_script_module
  graph(%self.300 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.300)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.300)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.178 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.201 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.178, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.201)

Attention.out_proj
Linear._actual_script_module
  graph(%self.302 : __torch__.torch.nn.modules.linear.Linear,
        %input.307 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.302)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.302)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.180 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.307, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.308 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.180, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.308)

Attention.q_proj
Linear._actual_script_module
  graph(%self.299 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.299)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.299)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.177 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.177, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.301 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.301)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.301)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.179 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.203 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.179, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.203)

DecoderLayer._actual_script_module
  graph(%self.307 : __torch__.transformers.modeling_bart.DecoderLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.307)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc2"](%self.307)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="fc1"](%self.307)
    %8 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%self.307)
    %9 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="encoder_attn"](%self.307)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%self.307)
    %11 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%self.307)
    %38 : Tensor = prim::CallMethod[name="forward"](%11, %1, %attn_mask)
    %13 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %14 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %x.62 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%38, %13, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %16 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:427:0
    %input.320 : Float(13:17408, 17:1024, 1024:1) = aten::add(%1, %x.62, %16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:427:0
    %39 : Tensor = prim::CallMethod[name="forward"](%10, %input.320)
    %40 : Tensor = prim::CallMethod[name="forward"](%9, %39, %input.144, %key_padding_mask)
    %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %21 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %x.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%40, %20, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %23 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:443:0
    %input.325 : Float(13:17408, 17:1024, 1024:1) = aten::add(%39, %x.63, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:443:0
    %41 : Tensor = prim::CallMethod[name="forward"](%8, %input.325)
    %42 : Tensor = prim::CallMethod[name="forward"](%7, %41)
    %input.328 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:1369:0
    %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %input.329 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.328, %28, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %43 : Tensor = prim::CallMethod[name="forward"](%6, %input.329)
    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %33 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %x.64 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%43, %32, %33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
    %35 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:455:0
    %input.331 : Float(13:17408, 17:1024, 1024:1) = aten::add(%41, %x.64, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:455:0
    %44 : Tensor = prim::CallMethod[name="forward"](%5, %input.331)
    return (%44)

DecoderLayer.encoder_attn
Attention._actual_script_module
  graph(%self.314 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %input.144 : Float(13:17408, 17:1024, 1024:1),
        %key_padding_mask : Bool(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.314)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.314)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.314)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.314)
    %8 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %9 : int = aten::size(%1, %8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %seq_len : Long() = prim::NumToTensor(%9), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %11 : int = aten::Int(%seq_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %12 : int = aten::Int(%seq_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %13 : int = aten::Int(%seq_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %14 : int = aten::Int(%seq_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %15 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %16 : int = aten::size(%1, %15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %bsz : Long() = prim::NumToTensor(%16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %18 : int = aten::Int(%bsz), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %19 : int = aten::Int(%bsz), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %22 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %23 : int = aten::size(%1, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
    %embed_dim : Long() = prim::NumToTensor(%23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %25 : int = aten::Int(%embed_dim), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %165 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %41 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:673:0
    %tensor.211 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%165, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:673:0
    %166 : Tensor = prim::CallMethod[name="forward"](%6, %input.144)
    %167 : Tensor = prim::CallMethod[name="forward"](%5, %input.144)
    %45 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.212 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.211, %45), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %48 : Long() = aten::mul(%bsz, %47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %49 : int = aten::Int(%48), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %50 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %51 : int[] = prim::ListConstruct(%14, %49, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %52 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.212, %51), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %54 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %q : Float(272:64, 13:17408, 64:1) = aten::transpose(%52, %53, %54), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %56 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor.214 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%166, %56), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %59 : Long() = aten::mul(%bsz, %58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %60 : int = aten::Int(%59), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %61 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %62 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %63 : int[] = prim::ListConstruct(%61, %60, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %64 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.214, %63), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %66 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %k : Float(272:64, 13:17408, 64:1) = aten::transpose(%64, %65, %66), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %68 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %tensor : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%167, %68), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %71 : Long() = aten::mul(%bsz, %70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %72 : int = aten::Int(%71), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %73 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %74 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %75 : int[] = prim::ListConstruct(%73, %72, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %76 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor, %75), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %78 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %v : Float(272:64, 13:17408, 64:1) = aten::transpose(%76, %77, %78), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
    %90 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:701:0
    %91 : int = aten::size(%k, %90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:701:0
    %src_len : Long() = prim::NumToTensor(%91), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %93 : int = aten::Int(%src_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %94 : int = aten::Int(%src_len), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %95 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
    %96 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
    %97 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k, %95, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
    %attn_weights.129 : Float(272:169, 13:13, 13:1) = aten::bmm(%q, %97), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
    %121 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:718:0
    %122 : int[] = prim::ListConstruct(%19, %121, %13, %94), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %attn_weights.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.129, %122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:718:0
    %124 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
    %125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
    %126 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
    %reshaped : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
    %128 : float = prim::Constant[value=-inf](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:720:0
    %attn_weights.131 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.130, %reshaped, %128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:720:0
    %130 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
    %131 : Long() = aten::mul(%bsz, %130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
    %132 : int = aten::Int(%131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %133 : int[] = prim::ListConstruct(%132, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %input.321 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.131, %133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
    %135 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:1498:0
    %136 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %input.322 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.321, %135, %136), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:1498:0
    %138 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
    %139 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
    %attn_weights : Float(272:169, 13:13, 13:1) = aten::dropout(%input.322, %138, %139), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
    %attn_output : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights, %v), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:730:0
    %157 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %158 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %159 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output, %157, %158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %160 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %161 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%159, %160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %162 : int[] = prim::ListConstruct(%11, %18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
    %input.323 : Float(13:17408, 17:1024, 1024:1) = aten::view(%161, %162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
    %168 : Tensor = prim::CallMethod[name="forward"](%4, %input.323)
    return (%168)

DecoderLayer.encoder_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.319 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.325 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.319)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.319)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    %input.326 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.325, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%input.326)

DecoderLayer.fc1
Linear._actual_script_module
  graph(%self.320 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.320)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.320)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
    %output.191 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1678:0
    %input.327 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.191, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1678:0
    return (%input.327)

DecoderLayer.fc2
Linear._actual_script_module
  graph(%self.321 : __torch__.torch.nn.modules.linear.Linear,
        %input.329 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.321)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.321)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
    %output.192 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.329, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1678:0
    %input.330 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.192, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1678:0
    return (%input.330)

DecoderLayer.final_layer_norm
LayerNorm._actual_script_module
  graph(%self : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.331 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self)
    %3 : Tensor = prim::GetAttr[name="weight"](%self)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    %x : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.331, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
    return (%x)

DecoderLayer.self_attn
Attention._actual_script_module
  graph(%self.308 : __torch__.transformers.modeling_bart.Attention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %attn_mask : Float(13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.308)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_proj"](%self.308)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_proj"](%self.308)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self.308)
    %7 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %8 : int = aten::size(%1, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %seq_len.37 : Long() = prim::NumToTensor(%8), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %10 : int = aten::Int(%seq_len.37), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %11 : int = aten::Int(%seq_len.37), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %12 : int = aten::Int(%seq_len.37), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %13 : int = aten::Int(%seq_len.37), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %14 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %15 : int = aten::size(%1, %14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %bsz.35 : Long() = prim::NumToTensor(%15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %17 : int = aten::Int(%bsz.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %18 : int = aten::Int(%bsz.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %21 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %22 : int = aten::size(%1, %21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
    %embed_dim.35 : Long() = prim::NumToTensor(%22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %24 : int = aten::Int(%embed_dim.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %152 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %40 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
    %tensor.205 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%152, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
    %153 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %154 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %44 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.206 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.205, %44), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %46 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %47 : Long() = aten::mul(%bsz.35, %46), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %48 : int = aten::Int(%47), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %49 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %50 : int[] = prim::ListConstruct(%13, %48, %49), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %51 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.206, %50), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %52 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %53 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %q.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%51, %52, %53), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %55 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.208 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%153, %55), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %57 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %58 : Long() = aten::mul(%bsz.35, %57), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %59 : int = aten::Int(%58), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %60 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %61 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %62 : int[] = prim::ListConstruct(%60, %59, %61), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %63 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.208, %62), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %64 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %65 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %k.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%63, %64, %65), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %67 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %tensor.210 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%154, %67), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %69 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %70 : Long() = aten::mul(%bsz.35, %69), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %71 : int = aten::Int(%70), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %72 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %73 : int = prim::Constant[value=64](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %74 : int[] = prim::ListConstruct(%72, %71, %73), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %75 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.210, %74), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %76 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %77 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %v.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%75, %76, %77), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
    %89 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
    %90 : int = aten::size(%k.35, %89), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
    %src_len.35 : Long() = prim::NumToTensor(%90), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %92 : int = aten::Int(%src_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %93 : int = aten::Int(%src_len.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %94 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %95 : int = prim::Constant[value=2](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %96 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.35, %94, %95), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %attn_weights.126 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.35, %96), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
    %112 : int = prim::Constant[value=16](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
    %113 : int[] = prim::ListConstruct(%18, %112, %12, %93), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %114 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.126, %113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
    %115 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
    %attn_weights.127 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%114, %attn_mask, %115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
    %117 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
    %118 : Long() = aten::mul(%bsz.35, %117), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
    %119 : int = aten::Int(%118), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %120 : int[] = prim::ListConstruct(%119, %11, %92), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %input.316 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.127, %120), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
    %122 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %input.317 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.316, %122, %123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:1498:0
    %125 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %126 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %attn_weights.128 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.317, %125, %126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
    %attn_output.35 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.128, %v.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
    %144 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %145 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %146 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.35, %144, %145), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %147 : int = prim::Constant[value=0](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %148 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%146, %147), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %149 : int[] = prim::ListConstruct(%10, %17, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
    %input.318 : Float(13:17408, 17:1024, 1024:1) = aten::view(%148, %149), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
    %155 : Tensor = prim::CallMethod[name="forward"](%3, %input.318)
    return (%155)

DecoderLayer.self_attn_layer_norm
LayerNorm._actual_script_module
  graph(%self.313 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.320 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.313)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.313)
    %4 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    %query : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.320, %5, %3, %2, %6, %7), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
    return (%query)

Attention.k_proj
Linear._actual_script_module
  graph(%self.310 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.310)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.310)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
    %output.184 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.184, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.207)

Attention.out_proj
Linear._actual_script_module
  graph(%self.312 : __torch__.torch.nn.modules.linear.Linear,
        %input.318 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.312)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.312)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
    %output.186 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.318, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
    %input.319 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.186, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.319)

Attention.q_proj
Linear._actual_script_module
  graph(%self.309 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.309)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.309)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
    %output.183 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.183, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.311 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.311)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.311)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
    %output.185 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.209 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.185, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.209)

Attention.k_proj
Linear._actual_script_module
  graph(%self.316 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.316)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.316)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %output.188 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    %tensor.213 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.188, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1678:0
    return (%tensor.213)

Attention.out_proj
Linear._actual_script_module
  graph(%self.318 : __torch__.torch.nn.modules.linear.Linear,
        %input.323 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.318)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.318)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %output.190 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.323, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    %input.324 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.190, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1678:0
    return (%input.324)

Attention.q_proj
Linear._actual_script_module
  graph(%self.315 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.315)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.315)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %output.187 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    %7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.187, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1678:0
    return (%7)

Attention.v_proj
Linear._actual_script_module
  graph(%self.317 : __torch__.torch.nn.modules.linear.Linear,
        %input.144 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.317)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.317)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %output.189 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %4), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    %tensor.215 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.189, %2, %6), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1678:0
    return (%tensor.215)

