graph(%self.1 : __torch__.transformers.modeling_electra.ElectraForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_16491.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_electra.___torch_mangle_16490.ElectraModel = prim::GetAttr[name="electra"](%self.1)
  %17 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %18 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %19 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %20 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %21 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %22 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %23 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=128](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %25 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %26 : Double() = prim::Constant[value={-10000}](), scope: __module.electra # transformers/modeling_utils.py:258:0
  %27 : float = prim::Constant[value=1.](), scope: __module.electra # torch/tensor.py:396:0
  %28 : None = prim::Constant(), scope: __module.electra
  %29 : int = prim::Constant[value=6](), scope: __module.electra # transformers/modeling_utils.py:257:0
  %30 : int = prim::Constant[value=3](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %31 : int = prim::Constant[value=2](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %32 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %33 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %34 : Device = prim::Constant[value="cpu"](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %35 : int = prim::Constant[value=4](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %36 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %37 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %38 : __torch__.transformers.modeling_electra.___torch_mangle_16489.ElectraEncoder = prim::GetAttr[name="encoder"](%4)
  %39 : __torch__.torch.nn.modules.linear.___torch_mangle_16283.Linear = prim::GetAttr[name="embeddings_project"](%4)
  %40 : __torch__.transformers.modeling_electra.___torch_mangle_16282.ElectraEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %41 : int = aten::size(%input_ids, %37), scope: __module.electra # transformers/modeling_electra.py:724:0
  %42 : int = aten::size(%input_ids, %36), scope: __module.electra # transformers/modeling_electra.py:724:0
  %43 : int[] = prim::ListConstruct(%41, %42), scope: __module.electra
  %input.2 : Long(17:13, 13:1) = aten::zeros(%43, %35, %37, %34, %33), scope: __module.electra # transformers/modeling_electra.py:735:0
  %45 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %37, %37, %32, %36), scope: __module.electra # transformers/modeling_utils.py:244:0
  %46 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%45, %36), scope: __module.electra # transformers/modeling_utils.py:244:0
  %47 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%46, %31), scope: __module.electra # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%47, %30, %37, %32, %36), scope: __module.electra # transformers/modeling_utils.py:244:0
  %49 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %29, %33, %33, %28), scope: __module.electra # transformers/modeling_utils.py:257:0
  %50 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%49, %27, %36), scope: __module.electra # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%50, %26), scope: __module.electra # transformers/modeling_utils.py:258:0
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_16280.LayerNorm = prim::GetAttr[name="LayerNorm"](%40)
  %53 : __torch__.torch.nn.modules.sparse.___torch_mangle_16279.Embedding = prim::GetAttr[name="token_type_embeddings"](%40)
  %54 : __torch__.torch.nn.modules.sparse.___torch_mangle_16278.Embedding = prim::GetAttr[name="position_embeddings"](%40)
  %55 : __torch__.torch.nn.modules.sparse.___torch_mangle_16277.Embedding = prim::GetAttr[name="word_embeddings"](%40)
  %56 : Tensor = prim::GetAttr[name="position_ids"](%40)
  %57 : int = aten::size(%input_ids, %36), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:173:0
  %58 : Long(1:512, 512:1) = aten::slice(%56, %37, %37, %32, %36), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%58, %36, %37, %57, %36), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %60 : Tensor = prim::GetAttr[name="weight"](%55)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%60, %input_ids, %37, %33, %33), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %62 : Tensor = prim::GetAttr[name="weight"](%54)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%62, %input.1, %21, %33, %33), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %64 : Tensor = prim::GetAttr[name="weight"](%53)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%64, %input.2, %21, %33, %33), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %66 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %36), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%66, %token_type_embeddings, %36), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %68 : Tensor = prim::GetAttr[name="bias"](%52)
  %69 : Tensor = prim::GetAttr[name="weight"](%52)
  %70 : int[] = prim::ListConstruct(%24), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %70, %69, %68, %23, %22), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %25, %33), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %73 : Tensor = prim::GetAttr[name="bias"](%39)
  %74 : Tensor = prim::GetAttr[name="weight"](%39)
  %75 : Float(128:1, 256:128) = aten::t(%74), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %output.1 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.5, %75), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %input.6 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.1, %73, %36), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1678:0
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %79 : __torch__.transformers.modeling_electra.___torch_mangle_16487.ElectraLayer = prim::GetAttr[name="11"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %81 : __torch__.transformers.modeling_electra.___torch_mangle_16470.ElectraLayer = prim::GetAttr[name="10"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %83 : __torch__.transformers.modeling_electra.___torch_mangle_16453.ElectraLayer = prim::GetAttr[name="9"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %85 : __torch__.transformers.modeling_electra.___torch_mangle_16436.ElectraLayer = prim::GetAttr[name="8"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %87 : __torch__.transformers.modeling_electra.___torch_mangle_16419.ElectraLayer = prim::GetAttr[name="7"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %89 : __torch__.transformers.modeling_electra.___torch_mangle_16402.ElectraLayer = prim::GetAttr[name="6"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %91 : __torch__.transformers.modeling_electra.___torch_mangle_16385.ElectraLayer = prim::GetAttr[name="5"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %93 : __torch__.transformers.modeling_electra.___torch_mangle_16368.ElectraLayer = prim::GetAttr[name="4"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %95 : __torch__.transformers.modeling_electra.___torch_mangle_16351.ElectraLayer = prim::GetAttr[name="3"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %97 : __torch__.transformers.modeling_electra.___torch_mangle_16334.ElectraLayer = prim::GetAttr[name="2"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %99 : __torch__.transformers.modeling_electra.___torch_mangle_16317.ElectraLayer = prim::GetAttr[name="1"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_16488.ModuleList = prim::GetAttr[name="layer"](%38)
  %101 : __torch__.transformers.modeling_electra.___torch_mangle_16300.ElectraLayer = prim::GetAttr[name="0"](%100)
  %102 : __torch__.transformers.modeling_electra.___torch_mangle_16299.ElectraOutput = prim::GetAttr[name="output"](%101)
  %103 : __torch__.transformers.modeling_electra.___torch_mangle_16295.ElectraIntermediate = prim::GetAttr[name="intermediate"](%101)
  %104 : __torch__.transformers.modeling_electra.___torch_mangle_16293.ElectraAttention = prim::GetAttr[name="attention"](%101)
  %105 : __torch__.transformers.modeling_electra.___torch_mangle_16292.ElectraSelfOutput = prim::GetAttr[name="output"](%104)
  %106 : __torch__.transformers.modeling_electra.___torch_mangle_16288.ElectraSelfAttention = prim::GetAttr[name="self"](%104)
  %107 : __torch__.torch.nn.modules.linear.___torch_mangle_16286.Linear = prim::GetAttr[name="value"](%106)
  %108 : __torch__.torch.nn.modules.linear.___torch_mangle_16285.Linear = prim::GetAttr[name="key"](%106)
  %109 : __torch__.torch.nn.modules.linear.___torch_mangle_16284.Linear = prim::GetAttr[name="query"](%106)
  %110 : Tensor = prim::GetAttr[name="bias"](%109)
  %111 : Tensor = prim::GetAttr[name="weight"](%109)
  %112 : Float(256:1, 256:256) = aten::t(%111), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %112), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.2, %110, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %115 : Tensor = prim::GetAttr[name="bias"](%108)
  %116 : Tensor = prim::GetAttr[name="weight"](%108)
  %117 : Float(256:1, 256:256) = aten::t(%116), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %117), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.3, %115, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %120 : Tensor = prim::GetAttr[name="bias"](%107)
  %121 : Tensor = prim::GetAttr[name="weight"](%107)
  %122 : Float(256:1, 256:256) = aten::t(%121), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.4, %120, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %125 : int = aten::size(%x.1, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %126 : int = aten::size(%x.1, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %127 : int[] = prim::ListConstruct(%125, %126, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.1, %127), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %129 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.2, %129), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %131 : int = aten::size(%x.3, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %132 : int = aten::size(%x.3, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %133 : int[] = prim::ListConstruct(%131, %132, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.3, %133), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %135 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.4, %135), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %137 : int = aten::size(%x.5, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %138 : int = aten::size(%x.5, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %139 : int[] = prim::ListConstruct(%137, %138, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.5, %139), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %141 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.6, %141), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %143 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.1, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %143), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %input.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
  %input.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.7, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.8, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:265:0
  %150 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %151 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.1, %150), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%151, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %153 : int = aten::size(%context_layer.2, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %154 : int = aten::size(%context_layer.2, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %155 : int[] = prim::ListConstruct(%153, %154, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %input.9 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.2, %155), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %157 : __torch__.torch.nn.modules.normalization.___torch_mangle_16290.LayerNorm = prim::GetAttr[name="LayerNorm"](%105)
  %158 : __torch__.torch.nn.modules.linear.___torch_mangle_16289.Linear = prim::GetAttr[name="dense"](%105)
  %159 : Tensor = prim::GetAttr[name="bias"](%158)
  %160 : Tensor = prim::GetAttr[name="weight"](%158)
  %161 : Float(256:1, 256:256) = aten::t(%160), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.9, %161), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.5, %159, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.10, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.1, %input.6, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
  %166 : Tensor = prim::GetAttr[name="bias"](%157)
  %167 : Tensor = prim::GetAttr[name="weight"](%157)
  %168 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.11, %168, %167, %166, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %170 : __torch__.torch.nn.modules.linear.___torch_mangle_16294.Linear = prim::GetAttr[name="dense"](%103)
  %171 : Tensor = prim::GetAttr[name="bias"](%170)
  %172 : Tensor = prim::GetAttr[name="weight"](%170)
  %173 : Float(256:1, 1024:256) = aten::t(%172), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.1, %173), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %171, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %177 : __torch__.torch.nn.modules.normalization.___torch_mangle_16297.LayerNorm = prim::GetAttr[name="LayerNorm"](%102)
  %178 : __torch__.torch.nn.modules.linear.___torch_mangle_16296.Linear = prim::GetAttr[name="dense"](%102)
  %179 : Tensor = prim::GetAttr[name="bias"](%178)
  %180 : Tensor = prim::GetAttr[name="weight"](%178)
  %181 : Float(1024:1, 256:1024) = aten::t(%180), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.7 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.13, %181), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.14 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.7, %179, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.14, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.2, %input_tensor.1, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output # transformers/modeling_electra.py:365:0
  %186 : Tensor = prim::GetAttr[name="bias"](%177)
  %187 : Tensor = prim::GetAttr[name="weight"](%177)
  %188 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm
  %input.16 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.15, %188, %187, %186, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %190 : __torch__.transformers.modeling_electra.___torch_mangle_16316.ElectraOutput = prim::GetAttr[name="output"](%99)
  %191 : __torch__.transformers.modeling_electra.___torch_mangle_16312.ElectraIntermediate = prim::GetAttr[name="intermediate"](%99)
  %192 : __torch__.transformers.modeling_electra.___torch_mangle_16310.ElectraAttention = prim::GetAttr[name="attention"](%99)
  %193 : __torch__.transformers.modeling_electra.___torch_mangle_16309.ElectraSelfOutput = prim::GetAttr[name="output"](%192)
  %194 : __torch__.transformers.modeling_electra.___torch_mangle_16305.ElectraSelfAttention = prim::GetAttr[name="self"](%192)
  %195 : __torch__.torch.nn.modules.linear.___torch_mangle_16303.Linear = prim::GetAttr[name="value"](%194)
  %196 : __torch__.torch.nn.modules.linear.___torch_mangle_16302.Linear = prim::GetAttr[name="key"](%194)
  %197 : __torch__.torch.nn.modules.linear.___torch_mangle_16301.Linear = prim::GetAttr[name="query"](%194)
  %198 : Tensor = prim::GetAttr[name="bias"](%197)
  %199 : Tensor = prim::GetAttr[name="weight"](%197)
  %200 : Float(256:1, 256:256) = aten::t(%199), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.8 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %200), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.8, %198, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %203 : Tensor = prim::GetAttr[name="bias"](%196)
  %204 : Tensor = prim::GetAttr[name="weight"](%196)
  %205 : Float(256:1, 256:256) = aten::t(%204), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.9 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %205), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.9, %203, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %208 : Tensor = prim::GetAttr[name="bias"](%195)
  %209 : Tensor = prim::GetAttr[name="weight"](%195)
  %210 : Float(256:1, 256:256) = aten::t(%209), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.10 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %210), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.10, %208, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %213 : int = aten::size(%x.7, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %214 : int = aten::size(%x.7, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %215 : int[] = prim::ListConstruct(%213, %214, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.7, %215), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %217 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.8, %217), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %219 : int = aten::size(%x.9, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %220 : int = aten::size(%x.9, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %221 : int[] = prim::ListConstruct(%219, %220, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.9, %221), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %223 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.10, %223), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %225 : int = aten::size(%x.11, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %226 : int = aten::size(%x.11, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %227 : int[] = prim::ListConstruct(%225, %226, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.11, %227), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %229 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.12, %229), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %231 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.2, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %231), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
  %input.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
  %input.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.17, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.18, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:265:0
  %238 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %239 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.3, %238), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%239, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %241 : int = aten::size(%context_layer.4, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %242 : int = aten::size(%context_layer.4, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %243 : int[] = prim::ListConstruct(%241, %242, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %input.19 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.4, %243), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
  %245 : __torch__.torch.nn.modules.normalization.___torch_mangle_16307.LayerNorm = prim::GetAttr[name="LayerNorm"](%193)
  %246 : __torch__.torch.nn.modules.linear.___torch_mangle_16306.Linear = prim::GetAttr[name="dense"](%193)
  %247 : Tensor = prim::GetAttr[name="bias"](%246)
  %248 : Tensor = prim::GetAttr[name="weight"](%246)
  %249 : Float(256:1, 256:256) = aten::t(%248), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.19, %249), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.11, %247, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.20, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.21 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.3, %input.16, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
  %254 : Tensor = prim::GetAttr[name="bias"](%245)
  %255 : Tensor = prim::GetAttr[name="weight"](%245)
  %256 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.21, %256, %255, %254, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %258 : __torch__.torch.nn.modules.linear.___torch_mangle_16311.Linear = prim::GetAttr[name="dense"](%191)
  %259 : Tensor = prim::GetAttr[name="bias"](%258)
  %260 : Tensor = prim::GetAttr[name="weight"](%258)
  %261 : Float(256:1, 1024:256) = aten::t(%260), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.2, %261), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %259, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %265 : __torch__.torch.nn.modules.normalization.___torch_mangle_16314.LayerNorm = prim::GetAttr[name="LayerNorm"](%190)
  %266 : __torch__.torch.nn.modules.linear.___torch_mangle_16313.Linear = prim::GetAttr[name="dense"](%190)
  %267 : Tensor = prim::GetAttr[name="bias"](%266)
  %268 : Tensor = prim::GetAttr[name="weight"](%266)
  %269 : Float(1024:1, 256:1024) = aten::t(%268), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.13 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.23, %269), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.24 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.13, %267, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.24, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.25 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.4, %input_tensor.2, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output # transformers/modeling_electra.py:365:0
  %274 : Tensor = prim::GetAttr[name="bias"](%265)
  %275 : Tensor = prim::GetAttr[name="weight"](%265)
  %276 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm
  %input.26 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.25, %276, %275, %274, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %278 : __torch__.transformers.modeling_electra.___torch_mangle_16333.ElectraOutput = prim::GetAttr[name="output"](%97)
  %279 : __torch__.transformers.modeling_electra.___torch_mangle_16329.ElectraIntermediate = prim::GetAttr[name="intermediate"](%97)
  %280 : __torch__.transformers.modeling_electra.___torch_mangle_16327.ElectraAttention = prim::GetAttr[name="attention"](%97)
  %281 : __torch__.transformers.modeling_electra.___torch_mangle_16326.ElectraSelfOutput = prim::GetAttr[name="output"](%280)
  %282 : __torch__.transformers.modeling_electra.___torch_mangle_16322.ElectraSelfAttention = prim::GetAttr[name="self"](%280)
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_16320.Linear = prim::GetAttr[name="value"](%282)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_16319.Linear = prim::GetAttr[name="key"](%282)
  %285 : __torch__.torch.nn.modules.linear.___torch_mangle_16318.Linear = prim::GetAttr[name="query"](%282)
  %286 : Tensor = prim::GetAttr[name="bias"](%285)
  %287 : Tensor = prim::GetAttr[name="weight"](%285)
  %288 : Float(256:1, 256:256) = aten::t(%287), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.14 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %288), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.14, %286, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %291 : Tensor = prim::GetAttr[name="bias"](%284)
  %292 : Tensor = prim::GetAttr[name="weight"](%284)
  %293 : Float(256:1, 256:256) = aten::t(%292), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.15 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %293), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.15, %291, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %296 : Tensor = prim::GetAttr[name="bias"](%283)
  %297 : Tensor = prim::GetAttr[name="weight"](%283)
  %298 : Float(256:1, 256:256) = aten::t(%297), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.16 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %298), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.16, %296, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %301 : int = aten::size(%x.13, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %302 : int = aten::size(%x.13, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %303 : int[] = prim::ListConstruct(%301, %302, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.13, %303), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %305 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.14, %305), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %307 : int = aten::size(%x.15, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %308 : int = aten::size(%x.15, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %309 : int[] = prim::ListConstruct(%307, %308, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.15, %309), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %311 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.16, %311), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %313 : int = aten::size(%x.17, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %314 : int = aten::size(%x.17, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %315 : int[] = prim::ListConstruct(%313, %314, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.17, %315), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %317 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.18, %317), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %319 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.3, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %319), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
  %input.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
  %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.27, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.28, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:265:0
  %326 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %327 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.5, %326), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%327, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %329 : int = aten::size(%context_layer.6, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %330 : int = aten::size(%context_layer.6, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %331 : int[] = prim::ListConstruct(%329, %330, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %input.29 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.6, %331), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
  %333 : __torch__.torch.nn.modules.normalization.___torch_mangle_16324.LayerNorm = prim::GetAttr[name="LayerNorm"](%281)
  %334 : __torch__.torch.nn.modules.linear.___torch_mangle_16323.Linear = prim::GetAttr[name="dense"](%281)
  %335 : Tensor = prim::GetAttr[name="bias"](%334)
  %336 : Tensor = prim::GetAttr[name="weight"](%334)
  %337 : Float(256:1, 256:256) = aten::t(%336), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.29, %337), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.17, %335, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.30, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.5, %input.26, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
  %342 : Tensor = prim::GetAttr[name="bias"](%333)
  %343 : Tensor = prim::GetAttr[name="weight"](%333)
  %344 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.31, %344, %343, %342, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %346 : __torch__.torch.nn.modules.linear.___torch_mangle_16328.Linear = prim::GetAttr[name="dense"](%279)
  %347 : Tensor = prim::GetAttr[name="bias"](%346)
  %348 : Tensor = prim::GetAttr[name="weight"](%346)
  %349 : Float(256:1, 1024:256) = aten::t(%348), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.3, %349), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %347, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %353 : __torch__.torch.nn.modules.normalization.___torch_mangle_16331.LayerNorm = prim::GetAttr[name="LayerNorm"](%278)
  %354 : __torch__.torch.nn.modules.linear.___torch_mangle_16330.Linear = prim::GetAttr[name="dense"](%278)
  %355 : Tensor = prim::GetAttr[name="bias"](%354)
  %356 : Tensor = prim::GetAttr[name="weight"](%354)
  %357 : Float(1024:1, 256:1024) = aten::t(%356), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.19 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.33, %357), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.34 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.19, %355, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.34, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.35 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.6, %input_tensor.3, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output # transformers/modeling_electra.py:365:0
  %362 : Tensor = prim::GetAttr[name="bias"](%353)
  %363 : Tensor = prim::GetAttr[name="weight"](%353)
  %364 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm
  %input.36 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.35, %364, %363, %362, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %366 : __torch__.transformers.modeling_electra.___torch_mangle_16350.ElectraOutput = prim::GetAttr[name="output"](%95)
  %367 : __torch__.transformers.modeling_electra.___torch_mangle_16346.ElectraIntermediate = prim::GetAttr[name="intermediate"](%95)
  %368 : __torch__.transformers.modeling_electra.___torch_mangle_16344.ElectraAttention = prim::GetAttr[name="attention"](%95)
  %369 : __torch__.transformers.modeling_electra.___torch_mangle_16343.ElectraSelfOutput = prim::GetAttr[name="output"](%368)
  %370 : __torch__.transformers.modeling_electra.___torch_mangle_16339.ElectraSelfAttention = prim::GetAttr[name="self"](%368)
  %371 : __torch__.torch.nn.modules.linear.___torch_mangle_16337.Linear = prim::GetAttr[name="value"](%370)
  %372 : __torch__.torch.nn.modules.linear.___torch_mangle_16336.Linear = prim::GetAttr[name="key"](%370)
  %373 : __torch__.torch.nn.modules.linear.___torch_mangle_16335.Linear = prim::GetAttr[name="query"](%370)
  %374 : Tensor = prim::GetAttr[name="bias"](%373)
  %375 : Tensor = prim::GetAttr[name="weight"](%373)
  %376 : Float(256:1, 256:256) = aten::t(%375), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.20 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %376), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.20, %374, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %379 : Tensor = prim::GetAttr[name="bias"](%372)
  %380 : Tensor = prim::GetAttr[name="weight"](%372)
  %381 : Float(256:1, 256:256) = aten::t(%380), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.21 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %381), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.21, %379, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %384 : Tensor = prim::GetAttr[name="bias"](%371)
  %385 : Tensor = prim::GetAttr[name="weight"](%371)
  %386 : Float(256:1, 256:256) = aten::t(%385), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.22 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %386), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.22, %384, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %389 : int = aten::size(%x.19, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %390 : int = aten::size(%x.19, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %391 : int[] = prim::ListConstruct(%389, %390, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.19, %391), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %393 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.20, %393), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %395 : int = aten::size(%x.21, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %396 : int = aten::size(%x.21, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %397 : int[] = prim::ListConstruct(%395, %396, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.21, %397), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %399 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.22, %399), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %401 : int = aten::size(%x.23, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %402 : int = aten::size(%x.23, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %403 : int[] = prim::ListConstruct(%401, %402, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.24 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.23, %403), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %405 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.24, %405), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %407 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.4, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %407), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
  %input.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
  %input.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.37, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.38, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:265:0
  %414 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %415 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.7, %414), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%415, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %417 : int = aten::size(%context_layer.8, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %418 : int = aten::size(%context_layer.8, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %419 : int[] = prim::ListConstruct(%417, %418, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %input.39 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.8, %419), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
  %421 : __torch__.torch.nn.modules.normalization.___torch_mangle_16341.LayerNorm = prim::GetAttr[name="LayerNorm"](%369)
  %422 : __torch__.torch.nn.modules.linear.___torch_mangle_16340.Linear = prim::GetAttr[name="dense"](%369)
  %423 : Tensor = prim::GetAttr[name="bias"](%422)
  %424 : Tensor = prim::GetAttr[name="weight"](%422)
  %425 : Float(256:1, 256:256) = aten::t(%424), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.39, %425), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.23, %423, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.40, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.7, %input.36, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
  %430 : Tensor = prim::GetAttr[name="bias"](%421)
  %431 : Tensor = prim::GetAttr[name="weight"](%421)
  %432 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.41, %432, %431, %430, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %434 : __torch__.torch.nn.modules.linear.___torch_mangle_16345.Linear = prim::GetAttr[name="dense"](%367)
  %435 : Tensor = prim::GetAttr[name="bias"](%434)
  %436 : Tensor = prim::GetAttr[name="weight"](%434)
  %437 : Float(256:1, 1024:256) = aten::t(%436), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.4, %437), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %435, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.42), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %441 : __torch__.torch.nn.modules.normalization.___torch_mangle_16348.LayerNorm = prim::GetAttr[name="LayerNorm"](%366)
  %442 : __torch__.torch.nn.modules.linear.___torch_mangle_16347.Linear = prim::GetAttr[name="dense"](%366)
  %443 : Tensor = prim::GetAttr[name="bias"](%442)
  %444 : Tensor = prim::GetAttr[name="weight"](%442)
  %445 : Float(1024:1, 256:1024) = aten::t(%444), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.25 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.43, %445), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.44 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.25, %443, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.44, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.8, %input_tensor.4, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output # transformers/modeling_electra.py:365:0
  %450 : Tensor = prim::GetAttr[name="bias"](%441)
  %451 : Tensor = prim::GetAttr[name="weight"](%441)
  %452 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm
  %input.46 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.45, %452, %451, %450, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %454 : __torch__.transformers.modeling_electra.___torch_mangle_16367.ElectraOutput = prim::GetAttr[name="output"](%93)
  %455 : __torch__.transformers.modeling_electra.___torch_mangle_16363.ElectraIntermediate = prim::GetAttr[name="intermediate"](%93)
  %456 : __torch__.transformers.modeling_electra.___torch_mangle_16361.ElectraAttention = prim::GetAttr[name="attention"](%93)
  %457 : __torch__.transformers.modeling_electra.___torch_mangle_16360.ElectraSelfOutput = prim::GetAttr[name="output"](%456)
  %458 : __torch__.transformers.modeling_electra.___torch_mangle_16356.ElectraSelfAttention = prim::GetAttr[name="self"](%456)
  %459 : __torch__.torch.nn.modules.linear.___torch_mangle_16354.Linear = prim::GetAttr[name="value"](%458)
  %460 : __torch__.torch.nn.modules.linear.___torch_mangle_16353.Linear = prim::GetAttr[name="key"](%458)
  %461 : __torch__.torch.nn.modules.linear.___torch_mangle_16352.Linear = prim::GetAttr[name="query"](%458)
  %462 : Tensor = prim::GetAttr[name="bias"](%461)
  %463 : Tensor = prim::GetAttr[name="weight"](%461)
  %464 : Float(256:1, 256:256) = aten::t(%463), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.26 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %464), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.26, %462, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %467 : Tensor = prim::GetAttr[name="bias"](%460)
  %468 : Tensor = prim::GetAttr[name="weight"](%460)
  %469 : Float(256:1, 256:256) = aten::t(%468), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.27 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %469), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.27, %467, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %472 : Tensor = prim::GetAttr[name="bias"](%459)
  %473 : Tensor = prim::GetAttr[name="weight"](%459)
  %474 : Float(256:1, 256:256) = aten::t(%473), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.28 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %474), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.28, %472, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %477 : int = aten::size(%x.25, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %478 : int = aten::size(%x.25, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %479 : int[] = prim::ListConstruct(%477, %478, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.26 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.25, %479), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %481 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.26, %481), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %483 : int = aten::size(%x.27, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %484 : int = aten::size(%x.27, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %485 : int[] = prim::ListConstruct(%483, %484, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.28 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.27, %485), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %487 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.28, %487), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %489 : int = aten::size(%x.29, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %490 : int = aten::size(%x.29, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %491 : int[] = prim::ListConstruct(%489, %490, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.30 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.29, %491), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %493 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.30, %493), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %495 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.5, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %495), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
  %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
  %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:265:0
  %502 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %503 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.9, %502), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%503, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %505 : int = aten::size(%context_layer.10, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %506 : int = aten::size(%context_layer.10, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %507 : int[] = prim::ListConstruct(%505, %506, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %input.49 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.10, %507), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
  %509 : __torch__.torch.nn.modules.normalization.___torch_mangle_16358.LayerNorm = prim::GetAttr[name="LayerNorm"](%457)
  %510 : __torch__.torch.nn.modules.linear.___torch_mangle_16357.Linear = prim::GetAttr[name="dense"](%457)
  %511 : Tensor = prim::GetAttr[name="bias"](%510)
  %512 : Tensor = prim::GetAttr[name="weight"](%510)
  %513 : Float(256:1, 256:256) = aten::t(%512), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.49, %513), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.29, %511, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.50, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.9, %input.46, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
  %518 : Tensor = prim::GetAttr[name="bias"](%509)
  %519 : Tensor = prim::GetAttr[name="weight"](%509)
  %520 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.51, %520, %519, %518, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %522 : __torch__.torch.nn.modules.linear.___torch_mangle_16362.Linear = prim::GetAttr[name="dense"](%455)
  %523 : Tensor = prim::GetAttr[name="bias"](%522)
  %524 : Tensor = prim::GetAttr[name="weight"](%522)
  %525 : Float(256:1, 1024:256) = aten::t(%524), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.5, %525), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %523, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %529 : __torch__.torch.nn.modules.normalization.___torch_mangle_16365.LayerNorm = prim::GetAttr[name="LayerNorm"](%454)
  %530 : __torch__.torch.nn.modules.linear.___torch_mangle_16364.Linear = prim::GetAttr[name="dense"](%454)
  %531 : Tensor = prim::GetAttr[name="bias"](%530)
  %532 : Tensor = prim::GetAttr[name="weight"](%530)
  %533 : Float(1024:1, 256:1024) = aten::t(%532), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.31 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.53, %533), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.54 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.31, %531, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.54, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.10, %input_tensor.5, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output # transformers/modeling_electra.py:365:0
  %538 : Tensor = prim::GetAttr[name="bias"](%529)
  %539 : Tensor = prim::GetAttr[name="weight"](%529)
  %540 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm
  %input.56 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.55, %540, %539, %538, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %542 : __torch__.transformers.modeling_electra.___torch_mangle_16384.ElectraOutput = prim::GetAttr[name="output"](%91)
  %543 : __torch__.transformers.modeling_electra.___torch_mangle_16380.ElectraIntermediate = prim::GetAttr[name="intermediate"](%91)
  %544 : __torch__.transformers.modeling_electra.___torch_mangle_16378.ElectraAttention = prim::GetAttr[name="attention"](%91)
  %545 : __torch__.transformers.modeling_electra.___torch_mangle_16377.ElectraSelfOutput = prim::GetAttr[name="output"](%544)
  %546 : __torch__.transformers.modeling_electra.___torch_mangle_16373.ElectraSelfAttention = prim::GetAttr[name="self"](%544)
  %547 : __torch__.torch.nn.modules.linear.___torch_mangle_16371.Linear = prim::GetAttr[name="value"](%546)
  %548 : __torch__.torch.nn.modules.linear.___torch_mangle_16370.Linear = prim::GetAttr[name="key"](%546)
  %549 : __torch__.torch.nn.modules.linear.___torch_mangle_16369.Linear = prim::GetAttr[name="query"](%546)
  %550 : Tensor = prim::GetAttr[name="bias"](%549)
  %551 : Tensor = prim::GetAttr[name="weight"](%549)
  %552 : Float(256:1, 256:256) = aten::t(%551), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %552), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.32, %550, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %555 : Tensor = prim::GetAttr[name="bias"](%548)
  %556 : Tensor = prim::GetAttr[name="weight"](%548)
  %557 : Float(256:1, 256:256) = aten::t(%556), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %557), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.33, %555, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %560 : Tensor = prim::GetAttr[name="bias"](%547)
  %561 : Tensor = prim::GetAttr[name="weight"](%547)
  %562 : Float(256:1, 256:256) = aten::t(%561), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %562), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.34, %560, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %565 : int = aten::size(%x.31, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %566 : int = aten::size(%x.31, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %567 : int[] = prim::ListConstruct(%565, %566, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.32 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.31, %567), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %569 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.32, %569), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %571 : int = aten::size(%x.33, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %572 : int = aten::size(%x.33, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %573 : int[] = prim::ListConstruct(%571, %572, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.34 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.33, %573), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %575 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.34, %575), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %577 : int = aten::size(%x.35, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %578 : int = aten::size(%x.35, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %579 : int[] = prim::ListConstruct(%577, %578, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.36 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.35, %579), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %581 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.36, %581), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %583 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.6, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %583), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
  %input.57 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
  %input.58 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.57, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.58, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:265:0
  %590 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %591 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.11, %590), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%591, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %593 : int = aten::size(%context_layer.12, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %594 : int = aten::size(%context_layer.12, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %595 : int[] = prim::ListConstruct(%593, %594, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %input.59 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.12, %595), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
  %597 : __torch__.torch.nn.modules.normalization.___torch_mangle_16375.LayerNorm = prim::GetAttr[name="LayerNorm"](%545)
  %598 : __torch__.torch.nn.modules.linear.___torch_mangle_16374.Linear = prim::GetAttr[name="dense"](%545)
  %599 : Tensor = prim::GetAttr[name="bias"](%598)
  %600 : Tensor = prim::GetAttr[name="weight"](%598)
  %601 : Float(256:1, 256:256) = aten::t(%600), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.59, %601), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.35, %599, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.60, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.61 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.11, %input.56, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
  %606 : Tensor = prim::GetAttr[name="bias"](%597)
  %607 : Tensor = prim::GetAttr[name="weight"](%597)
  %608 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.61, %608, %607, %606, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %610 : __torch__.torch.nn.modules.linear.___torch_mangle_16379.Linear = prim::GetAttr[name="dense"](%543)
  %611 : Tensor = prim::GetAttr[name="bias"](%610)
  %612 : Tensor = prim::GetAttr[name="weight"](%610)
  %613 : Float(256:1, 1024:256) = aten::t(%612), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.6, %613), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %611, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.62), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %617 : __torch__.torch.nn.modules.normalization.___torch_mangle_16382.LayerNorm = prim::GetAttr[name="LayerNorm"](%542)
  %618 : __torch__.torch.nn.modules.linear.___torch_mangle_16381.Linear = prim::GetAttr[name="dense"](%542)
  %619 : Tensor = prim::GetAttr[name="bias"](%618)
  %620 : Tensor = prim::GetAttr[name="weight"](%618)
  %621 : Float(1024:1, 256:1024) = aten::t(%620), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.37 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.63, %621), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.64 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.37, %619, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.64, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.12, %input_tensor.6, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output # transformers/modeling_electra.py:365:0
  %626 : Tensor = prim::GetAttr[name="bias"](%617)
  %627 : Tensor = prim::GetAttr[name="weight"](%617)
  %628 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm
  %input.66 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.65, %628, %627, %626, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %630 : __torch__.transformers.modeling_electra.___torch_mangle_16401.ElectraOutput = prim::GetAttr[name="output"](%89)
  %631 : __torch__.transformers.modeling_electra.___torch_mangle_16397.ElectraIntermediate = prim::GetAttr[name="intermediate"](%89)
  %632 : __torch__.transformers.modeling_electra.___torch_mangle_16395.ElectraAttention = prim::GetAttr[name="attention"](%89)
  %633 : __torch__.transformers.modeling_electra.___torch_mangle_16394.ElectraSelfOutput = prim::GetAttr[name="output"](%632)
  %634 : __torch__.transformers.modeling_electra.___torch_mangle_16390.ElectraSelfAttention = prim::GetAttr[name="self"](%632)
  %635 : __torch__.torch.nn.modules.linear.___torch_mangle_16388.Linear = prim::GetAttr[name="value"](%634)
  %636 : __torch__.torch.nn.modules.linear.___torch_mangle_16387.Linear = prim::GetAttr[name="key"](%634)
  %637 : __torch__.torch.nn.modules.linear.___torch_mangle_16386.Linear = prim::GetAttr[name="query"](%634)
  %638 : Tensor = prim::GetAttr[name="bias"](%637)
  %639 : Tensor = prim::GetAttr[name="weight"](%637)
  %640 : Float(256:1, 256:256) = aten::t(%639), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.38 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %640), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.38, %638, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %643 : Tensor = prim::GetAttr[name="bias"](%636)
  %644 : Tensor = prim::GetAttr[name="weight"](%636)
  %645 : Float(256:1, 256:256) = aten::t(%644), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.39 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %645), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.39, %643, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %648 : Tensor = prim::GetAttr[name="bias"](%635)
  %649 : Tensor = prim::GetAttr[name="weight"](%635)
  %650 : Float(256:1, 256:256) = aten::t(%649), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.40 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %650), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.40, %648, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %653 : int = aten::size(%x.37, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %654 : int = aten::size(%x.37, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %655 : int[] = prim::ListConstruct(%653, %654, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.38 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.37, %655), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %657 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.38, %657), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %659 : int = aten::size(%x.39, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %660 : int = aten::size(%x.39, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %661 : int[] = prim::ListConstruct(%659, %660, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.40 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.39, %661), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %663 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.40, %663), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %665 : int = aten::size(%x.41, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %666 : int = aten::size(%x.41, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %667 : int[] = prim::ListConstruct(%665, %666, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.42 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.41, %667), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %669 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.42, %669), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %671 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.7, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %671), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
  %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
  %input.68 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.67, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.68, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:265:0
  %678 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %679 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.13, %678), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%679, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %681 : int = aten::size(%context_layer.14, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %682 : int = aten::size(%context_layer.14, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %683 : int[] = prim::ListConstruct(%681, %682, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %input.69 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.14, %683), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
  %685 : __torch__.torch.nn.modules.normalization.___torch_mangle_16392.LayerNorm = prim::GetAttr[name="LayerNorm"](%633)
  %686 : __torch__.torch.nn.modules.linear.___torch_mangle_16391.Linear = prim::GetAttr[name="dense"](%633)
  %687 : Tensor = prim::GetAttr[name="bias"](%686)
  %688 : Tensor = prim::GetAttr[name="weight"](%686)
  %689 : Float(256:1, 256:256) = aten::t(%688), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.69, %689), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.41, %687, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.70, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.71 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.13, %input.66, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
  %694 : Tensor = prim::GetAttr[name="bias"](%685)
  %695 : Tensor = prim::GetAttr[name="weight"](%685)
  %696 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.71, %696, %695, %694, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_16396.Linear = prim::GetAttr[name="dense"](%631)
  %699 : Tensor = prim::GetAttr[name="bias"](%698)
  %700 : Tensor = prim::GetAttr[name="weight"](%698)
  %701 : Float(256:1, 1024:256) = aten::t(%700), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.7, %701), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %699, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.72), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %705 : __torch__.torch.nn.modules.normalization.___torch_mangle_16399.LayerNorm = prim::GetAttr[name="LayerNorm"](%630)
  %706 : __torch__.torch.nn.modules.linear.___torch_mangle_16398.Linear = prim::GetAttr[name="dense"](%630)
  %707 : Tensor = prim::GetAttr[name="bias"](%706)
  %708 : Tensor = prim::GetAttr[name="weight"](%706)
  %709 : Float(1024:1, 256:1024) = aten::t(%708), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.73, %709), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.74 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.43, %707, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.74, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.75 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.14, %input_tensor.7, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output # transformers/modeling_electra.py:365:0
  %714 : Tensor = prim::GetAttr[name="bias"](%705)
  %715 : Tensor = prim::GetAttr[name="weight"](%705)
  %716 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm
  %input.76 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.75, %716, %715, %714, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %718 : __torch__.transformers.modeling_electra.___torch_mangle_16418.ElectraOutput = prim::GetAttr[name="output"](%87)
  %719 : __torch__.transformers.modeling_electra.___torch_mangle_16414.ElectraIntermediate = prim::GetAttr[name="intermediate"](%87)
  %720 : __torch__.transformers.modeling_electra.___torch_mangle_16412.ElectraAttention = prim::GetAttr[name="attention"](%87)
  %721 : __torch__.transformers.modeling_electra.___torch_mangle_16411.ElectraSelfOutput = prim::GetAttr[name="output"](%720)
  %722 : __torch__.transformers.modeling_electra.___torch_mangle_16407.ElectraSelfAttention = prim::GetAttr[name="self"](%720)
  %723 : __torch__.torch.nn.modules.linear.___torch_mangle_16405.Linear = prim::GetAttr[name="value"](%722)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_16404.Linear = prim::GetAttr[name="key"](%722)
  %725 : __torch__.torch.nn.modules.linear.___torch_mangle_16403.Linear = prim::GetAttr[name="query"](%722)
  %726 : Tensor = prim::GetAttr[name="bias"](%725)
  %727 : Tensor = prim::GetAttr[name="weight"](%725)
  %728 : Float(256:1, 256:256) = aten::t(%727), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.44 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %728), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.44, %726, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %731 : Tensor = prim::GetAttr[name="bias"](%724)
  %732 : Tensor = prim::GetAttr[name="weight"](%724)
  %733 : Float(256:1, 256:256) = aten::t(%732), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %733), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.45, %731, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %736 : Tensor = prim::GetAttr[name="bias"](%723)
  %737 : Tensor = prim::GetAttr[name="weight"](%723)
  %738 : Float(256:1, 256:256) = aten::t(%737), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %738), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.46, %736, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %741 : int = aten::size(%x.43, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %742 : int = aten::size(%x.43, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %743 : int[] = prim::ListConstruct(%741, %742, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.44 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.43, %743), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %745 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.44, %745), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %747 : int = aten::size(%x.45, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %748 : int = aten::size(%x.45, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %749 : int[] = prim::ListConstruct(%747, %748, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.46 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.45, %749), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %751 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.46, %751), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %753 : int = aten::size(%x.47, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %754 : int = aten::size(%x.47, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %755 : int[] = prim::ListConstruct(%753, %754, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.48 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.47, %755), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %757 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.48, %757), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %759 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.8, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %759), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
  %input.77 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
  %input.78 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.77, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.78, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:265:0
  %766 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %767 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.15, %766), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%767, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %769 : int = aten::size(%context_layer.16, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %770 : int = aten::size(%context_layer.16, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %771 : int[] = prim::ListConstruct(%769, %770, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %input.79 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.16, %771), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
  %773 : __torch__.torch.nn.modules.normalization.___torch_mangle_16409.LayerNorm = prim::GetAttr[name="LayerNorm"](%721)
  %774 : __torch__.torch.nn.modules.linear.___torch_mangle_16408.Linear = prim::GetAttr[name="dense"](%721)
  %775 : Tensor = prim::GetAttr[name="bias"](%774)
  %776 : Tensor = prim::GetAttr[name="weight"](%774)
  %777 : Float(256:1, 256:256) = aten::t(%776), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.79, %777), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.47, %775, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.80, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.81 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.15, %input.76, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
  %782 : Tensor = prim::GetAttr[name="bias"](%773)
  %783 : Tensor = prim::GetAttr[name="weight"](%773)
  %784 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.81, %784, %783, %782, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %786 : __torch__.torch.nn.modules.linear.___torch_mangle_16413.Linear = prim::GetAttr[name="dense"](%719)
  %787 : Tensor = prim::GetAttr[name="bias"](%786)
  %788 : Tensor = prim::GetAttr[name="weight"](%786)
  %789 : Float(256:1, 1024:256) = aten::t(%788), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.8, %789), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %787, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %793 : __torch__.torch.nn.modules.normalization.___torch_mangle_16416.LayerNorm = prim::GetAttr[name="LayerNorm"](%718)
  %794 : __torch__.torch.nn.modules.linear.___torch_mangle_16415.Linear = prim::GetAttr[name="dense"](%718)
  %795 : Tensor = prim::GetAttr[name="bias"](%794)
  %796 : Tensor = prim::GetAttr[name="weight"](%794)
  %797 : Float(1024:1, 256:1024) = aten::t(%796), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.49 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.83, %797), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.84 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.49, %795, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.84, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.85 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.16, %input_tensor.8, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output # transformers/modeling_electra.py:365:0
  %802 : Tensor = prim::GetAttr[name="bias"](%793)
  %803 : Tensor = prim::GetAttr[name="weight"](%793)
  %804 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm
  %input.86 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.85, %804, %803, %802, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %806 : __torch__.transformers.modeling_electra.___torch_mangle_16435.ElectraOutput = prim::GetAttr[name="output"](%85)
  %807 : __torch__.transformers.modeling_electra.___torch_mangle_16431.ElectraIntermediate = prim::GetAttr[name="intermediate"](%85)
  %808 : __torch__.transformers.modeling_electra.___torch_mangle_16429.ElectraAttention = prim::GetAttr[name="attention"](%85)
  %809 : __torch__.transformers.modeling_electra.___torch_mangle_16428.ElectraSelfOutput = prim::GetAttr[name="output"](%808)
  %810 : __torch__.transformers.modeling_electra.___torch_mangle_16424.ElectraSelfAttention = prim::GetAttr[name="self"](%808)
  %811 : __torch__.torch.nn.modules.linear.___torch_mangle_16422.Linear = prim::GetAttr[name="value"](%810)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_16421.Linear = prim::GetAttr[name="key"](%810)
  %813 : __torch__.torch.nn.modules.linear.___torch_mangle_16420.Linear = prim::GetAttr[name="query"](%810)
  %814 : Tensor = prim::GetAttr[name="bias"](%813)
  %815 : Tensor = prim::GetAttr[name="weight"](%813)
  %816 : Float(256:1, 256:256) = aten::t(%815), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %816), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.50, %814, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %819 : Tensor = prim::GetAttr[name="bias"](%812)
  %820 : Tensor = prim::GetAttr[name="weight"](%812)
  %821 : Float(256:1, 256:256) = aten::t(%820), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %821), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.51, %819, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %824 : Tensor = prim::GetAttr[name="bias"](%811)
  %825 : Tensor = prim::GetAttr[name="weight"](%811)
  %826 : Float(256:1, 256:256) = aten::t(%825), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %826), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.52, %824, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %829 : int = aten::size(%x.49, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %830 : int = aten::size(%x.49, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %831 : int[] = prim::ListConstruct(%829, %830, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.50 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.49, %831), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %833 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.50, %833), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %835 : int = aten::size(%x.51, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %836 : int = aten::size(%x.51, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %837 : int[] = prim::ListConstruct(%835, %836, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.52 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.51, %837), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %839 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.52, %839), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %841 : int = aten::size(%x.53, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %842 : int = aten::size(%x.53, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %843 : int[] = prim::ListConstruct(%841, %842, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.54 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.53, %843), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %845 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.54, %845), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %847 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.9, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %847), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
  %input.87 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
  %input.88 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.87, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.88, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:265:0
  %854 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %855 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.17, %854), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%855, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %857 : int = aten::size(%context_layer.18, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %858 : int = aten::size(%context_layer.18, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %859 : int[] = prim::ListConstruct(%857, %858, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %input.89 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.18, %859), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
  %861 : __torch__.torch.nn.modules.normalization.___torch_mangle_16426.LayerNorm = prim::GetAttr[name="LayerNorm"](%809)
  %862 : __torch__.torch.nn.modules.linear.___torch_mangle_16425.Linear = prim::GetAttr[name="dense"](%809)
  %863 : Tensor = prim::GetAttr[name="bias"](%862)
  %864 : Tensor = prim::GetAttr[name="weight"](%862)
  %865 : Float(256:1, 256:256) = aten::t(%864), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.89, %865), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.53, %863, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.90, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.91 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.17, %input.86, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
  %870 : Tensor = prim::GetAttr[name="bias"](%861)
  %871 : Tensor = prim::GetAttr[name="weight"](%861)
  %872 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.91, %872, %871, %870, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %874 : __torch__.torch.nn.modules.linear.___torch_mangle_16430.Linear = prim::GetAttr[name="dense"](%807)
  %875 : Tensor = prim::GetAttr[name="bias"](%874)
  %876 : Tensor = prim::GetAttr[name="weight"](%874)
  %877 : Float(256:1, 1024:256) = aten::t(%876), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.9, %877), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %875, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %881 : __torch__.torch.nn.modules.normalization.___torch_mangle_16433.LayerNorm = prim::GetAttr[name="LayerNorm"](%806)
  %882 : __torch__.torch.nn.modules.linear.___torch_mangle_16432.Linear = prim::GetAttr[name="dense"](%806)
  %883 : Tensor = prim::GetAttr[name="bias"](%882)
  %884 : Tensor = prim::GetAttr[name="weight"](%882)
  %885 : Float(1024:1, 256:1024) = aten::t(%884), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.93, %885), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.94 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.55, %883, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.94, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.95 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.18, %input_tensor.9, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output # transformers/modeling_electra.py:365:0
  %890 : Tensor = prim::GetAttr[name="bias"](%881)
  %891 : Tensor = prim::GetAttr[name="weight"](%881)
  %892 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm
  %input.96 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.95, %892, %891, %890, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %894 : __torch__.transformers.modeling_electra.___torch_mangle_16452.ElectraOutput = prim::GetAttr[name="output"](%83)
  %895 : __torch__.transformers.modeling_electra.___torch_mangle_16448.ElectraIntermediate = prim::GetAttr[name="intermediate"](%83)
  %896 : __torch__.transformers.modeling_electra.___torch_mangle_16446.ElectraAttention = prim::GetAttr[name="attention"](%83)
  %897 : __torch__.transformers.modeling_electra.___torch_mangle_16445.ElectraSelfOutput = prim::GetAttr[name="output"](%896)
  %898 : __torch__.transformers.modeling_electra.___torch_mangle_16441.ElectraSelfAttention = prim::GetAttr[name="self"](%896)
  %899 : __torch__.torch.nn.modules.linear.___torch_mangle_16439.Linear = prim::GetAttr[name="value"](%898)
  %900 : __torch__.torch.nn.modules.linear.___torch_mangle_16438.Linear = prim::GetAttr[name="key"](%898)
  %901 : __torch__.torch.nn.modules.linear.___torch_mangle_16437.Linear = prim::GetAttr[name="query"](%898)
  %902 : Tensor = prim::GetAttr[name="bias"](%901)
  %903 : Tensor = prim::GetAttr[name="weight"](%901)
  %904 : Float(256:1, 256:256) = aten::t(%903), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %904), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.56, %902, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %907 : Tensor = prim::GetAttr[name="bias"](%900)
  %908 : Tensor = prim::GetAttr[name="weight"](%900)
  %909 : Float(256:1, 256:256) = aten::t(%908), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %909), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.57, %907, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %912 : Tensor = prim::GetAttr[name="bias"](%899)
  %913 : Tensor = prim::GetAttr[name="weight"](%899)
  %914 : Float(256:1, 256:256) = aten::t(%913), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %914), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.58, %912, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %917 : int = aten::size(%x.55, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %918 : int = aten::size(%x.55, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %919 : int[] = prim::ListConstruct(%917, %918, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.56 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.55, %919), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %921 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.56, %921), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %923 : int = aten::size(%x.57, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %924 : int = aten::size(%x.57, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %925 : int[] = prim::ListConstruct(%923, %924, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.58 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.57, %925), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %927 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.58, %927), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %929 : int = aten::size(%x.59, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %930 : int = aten::size(%x.59, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %931 : int[] = prim::ListConstruct(%929, %930, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.60 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.59, %931), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %933 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.60, %933), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %935 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.10, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %935), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
  %input.97 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
  %input.98 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.97, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.98, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:265:0
  %942 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %943 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.19, %942), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%943, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %945 : int = aten::size(%context_layer.20, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %946 : int = aten::size(%context_layer.20, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %947 : int[] = prim::ListConstruct(%945, %946, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %input.99 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.20, %947), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
  %949 : __torch__.torch.nn.modules.normalization.___torch_mangle_16443.LayerNorm = prim::GetAttr[name="LayerNorm"](%897)
  %950 : __torch__.torch.nn.modules.linear.___torch_mangle_16442.Linear = prim::GetAttr[name="dense"](%897)
  %951 : Tensor = prim::GetAttr[name="bias"](%950)
  %952 : Tensor = prim::GetAttr[name="weight"](%950)
  %953 : Float(256:1, 256:256) = aten::t(%952), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.99, %953), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.59, %951, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.100, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.19, %input.96, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
  %958 : Tensor = prim::GetAttr[name="bias"](%949)
  %959 : Tensor = prim::GetAttr[name="weight"](%949)
  %960 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.101, %960, %959, %958, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %962 : __torch__.torch.nn.modules.linear.___torch_mangle_16447.Linear = prim::GetAttr[name="dense"](%895)
  %963 : Tensor = prim::GetAttr[name="bias"](%962)
  %964 : Tensor = prim::GetAttr[name="weight"](%962)
  %965 : Float(256:1, 1024:256) = aten::t(%964), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.10, %965), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %963, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %969 : __torch__.torch.nn.modules.normalization.___torch_mangle_16450.LayerNorm = prim::GetAttr[name="LayerNorm"](%894)
  %970 : __torch__.torch.nn.modules.linear.___torch_mangle_16449.Linear = prim::GetAttr[name="dense"](%894)
  %971 : Tensor = prim::GetAttr[name="bias"](%970)
  %972 : Tensor = prim::GetAttr[name="weight"](%970)
  %973 : Float(1024:1, 256:1024) = aten::t(%972), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.61 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.103, %973), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.104 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.61, %971, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.104, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.20, %input_tensor.10, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output # transformers/modeling_electra.py:365:0
  %978 : Tensor = prim::GetAttr[name="bias"](%969)
  %979 : Tensor = prim::GetAttr[name="weight"](%969)
  %980 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm
  %input.106 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.105, %980, %979, %978, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %982 : __torch__.transformers.modeling_electra.___torch_mangle_16469.ElectraOutput = prim::GetAttr[name="output"](%81)
  %983 : __torch__.transformers.modeling_electra.___torch_mangle_16465.ElectraIntermediate = prim::GetAttr[name="intermediate"](%81)
  %984 : __torch__.transformers.modeling_electra.___torch_mangle_16463.ElectraAttention = prim::GetAttr[name="attention"](%81)
  %985 : __torch__.transformers.modeling_electra.___torch_mangle_16462.ElectraSelfOutput = prim::GetAttr[name="output"](%984)
  %986 : __torch__.transformers.modeling_electra.___torch_mangle_16458.ElectraSelfAttention = prim::GetAttr[name="self"](%984)
  %987 : __torch__.torch.nn.modules.linear.___torch_mangle_16456.Linear = prim::GetAttr[name="value"](%986)
  %988 : __torch__.torch.nn.modules.linear.___torch_mangle_16455.Linear = prim::GetAttr[name="key"](%986)
  %989 : __torch__.torch.nn.modules.linear.___torch_mangle_16454.Linear = prim::GetAttr[name="query"](%986)
  %990 : Tensor = prim::GetAttr[name="bias"](%989)
  %991 : Tensor = prim::GetAttr[name="weight"](%989)
  %992 : Float(256:1, 256:256) = aten::t(%991), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.62 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %992), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.62, %990, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %995 : Tensor = prim::GetAttr[name="bias"](%988)
  %996 : Tensor = prim::GetAttr[name="weight"](%988)
  %997 : Float(256:1, 256:256) = aten::t(%996), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.63 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %997), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.63, %995, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1000 : Tensor = prim::GetAttr[name="bias"](%987)
  %1001 : Tensor = prim::GetAttr[name="weight"](%987)
  %1002 : Float(256:1, 256:256) = aten::t(%1001), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.64 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %1002), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.64, %1000, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1005 : int = aten::size(%x.61, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1006 : int = aten::size(%x.61, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1007 : int[] = prim::ListConstruct(%1005, %1006, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.62 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.61, %1007), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1009 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.62, %1009), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1011 : int = aten::size(%x.63, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1012 : int = aten::size(%x.63, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1013 : int[] = prim::ListConstruct(%1011, %1012, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.64 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.63, %1013), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1015 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.64, %1015), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1017 : int = aten::size(%x.65, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1018 : int = aten::size(%x.65, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1019 : int[] = prim::ListConstruct(%1017, %1018, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.66 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.65, %1019), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1021 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.66, %1021), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1023 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.11, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1023), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
  %input.107 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
  %input.108 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.107, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.108, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:265:0
  %1030 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %1031 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.21, %1030), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1031, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %1033 : int = aten::size(%context_layer.22, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1034 : int = aten::size(%context_layer.22, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1035 : int[] = prim::ListConstruct(%1033, %1034, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %input.109 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.22, %1035), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
  %1037 : __torch__.torch.nn.modules.normalization.___torch_mangle_16460.LayerNorm = prim::GetAttr[name="LayerNorm"](%985)
  %1038 : __torch__.torch.nn.modules.linear.___torch_mangle_16459.Linear = prim::GetAttr[name="dense"](%985)
  %1039 : Tensor = prim::GetAttr[name="bias"](%1038)
  %1040 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1041 : Float(256:1, 256:256) = aten::t(%1040), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.109, %1041), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.65, %1039, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.110, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.111 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.21, %input.106, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
  %1046 : Tensor = prim::GetAttr[name="bias"](%1037)
  %1047 : Tensor = prim::GetAttr[name="weight"](%1037)
  %1048 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.111, %1048, %1047, %1046, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1050 : __torch__.torch.nn.modules.linear.___torch_mangle_16464.Linear = prim::GetAttr[name="dense"](%983)
  %1051 : Tensor = prim::GetAttr[name="bias"](%1050)
  %1052 : Tensor = prim::GetAttr[name="weight"](%1050)
  %1053 : Float(256:1, 1024:256) = aten::t(%1052), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.11, %1053), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1051, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.112), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1057 : __torch__.torch.nn.modules.normalization.___torch_mangle_16467.LayerNorm = prim::GetAttr[name="LayerNorm"](%982)
  %1058 : __torch__.torch.nn.modules.linear.___torch_mangle_16466.Linear = prim::GetAttr[name="dense"](%982)
  %1059 : Tensor = prim::GetAttr[name="bias"](%1058)
  %1060 : Tensor = prim::GetAttr[name="weight"](%1058)
  %1061 : Float(1024:1, 256:1024) = aten::t(%1060), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.67 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.113, %1061), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.67, %1059, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.114, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.22, %input_tensor.11, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output # transformers/modeling_electra.py:365:0
  %1066 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1067 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1068 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm
  %input.116 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.115, %1068, %1067, %1066, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1070 : __torch__.transformers.modeling_electra.___torch_mangle_16486.ElectraOutput = prim::GetAttr[name="output"](%79)
  %1071 : __torch__.transformers.modeling_electra.___torch_mangle_16482.ElectraIntermediate = prim::GetAttr[name="intermediate"](%79)
  %1072 : __torch__.transformers.modeling_electra.___torch_mangle_16480.ElectraAttention = prim::GetAttr[name="attention"](%79)
  %1073 : __torch__.transformers.modeling_electra.___torch_mangle_16479.ElectraSelfOutput = prim::GetAttr[name="output"](%1072)
  %1074 : __torch__.transformers.modeling_electra.___torch_mangle_16475.ElectraSelfAttention = prim::GetAttr[name="self"](%1072)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_16473.Linear = prim::GetAttr[name="value"](%1074)
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_16472.Linear = prim::GetAttr[name="key"](%1074)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_16471.Linear = prim::GetAttr[name="query"](%1074)
  %1078 : Tensor = prim::GetAttr[name="bias"](%1077)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1080 : Float(256:1, 256:256) = aten::t(%1079), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.68 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1080), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.68, %1078, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1083 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1084 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1085 : Float(256:1, 256:256) = aten::t(%1084), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.69 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1085), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.69, %1083, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1089 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1090 : Float(256:1, 256:256) = aten::t(%1089), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.70 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1090), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.70, %1088, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1093 : int = aten::size(%x.67, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1094 : int = aten::size(%x.67, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1095 : int[] = prim::ListConstruct(%1093, %1094, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.68 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.67, %1095), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1097 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %query_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.68, %1097), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1099 : int = aten::size(%x.69, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1100 : int = aten::size(%x.69, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1101 : int[] = prim::ListConstruct(%1099, %1100, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.70 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.69, %1101), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1103 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %key_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.70, %1103), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1105 : int = aten::size(%x.71, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1106 : int = aten::size(%x.71, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1107 : int[] = prim::ListConstruct(%1105, %1106, %35, %20), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.71, %1107), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1109 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %value_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x, %1109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1111 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer, %21, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %1111), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %18), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
  %input.117 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
  %input.118 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.117, %21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.118, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:265:0
  %1118 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %1119 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.23, %1118), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %context_layer : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1119, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %1121 : int = aten::size(%context_layer, %37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1122 : int = aten::size(%context_layer, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1123 : int[] = prim::ListConstruct(%1121, %1122, %17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %input.119 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer, %1123), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
  %1125 : __torch__.torch.nn.modules.normalization.___torch_mangle_16477.LayerNorm = prim::GetAttr[name="LayerNorm"](%1073)
  %1126 : __torch__.torch.nn.modules.linear.___torch_mangle_16476.Linear = prim::GetAttr[name="dense"](%1073)
  %1127 : Tensor = prim::GetAttr[name="bias"](%1126)
  %1128 : Tensor = prim::GetAttr[name="weight"](%1126)
  %1129 : Float(256:1, 256:256) = aten::t(%1128), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.119, %1129), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.71, %1127, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.120, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.121 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.23, %input.116, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
  %1134 : Tensor = prim::GetAttr[name="bias"](%1125)
  %1135 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1136 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.121, %1136, %1135, %1134, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1138 : __torch__.torch.nn.modules.linear.___torch_mangle_16481.Linear = prim::GetAttr[name="dense"](%1071)
  %1139 : Tensor = prim::GetAttr[name="bias"](%1138)
  %1140 : Tensor = prim::GetAttr[name="weight"](%1138)
  %1141 : Float(256:1, 1024:256) = aten::t(%1140), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor, %1141), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1139, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1145 : __torch__.torch.nn.modules.normalization.___torch_mangle_16484.LayerNorm = prim::GetAttr[name="LayerNorm"](%1070)
  %1146 : __torch__.torch.nn.modules.linear.___torch_mangle_16483.Linear = prim::GetAttr[name="dense"](%1070)
  %1147 : Tensor = prim::GetAttr[name="bias"](%1146)
  %1148 : Tensor = prim::GetAttr[name="weight"](%1146)
  %1149 : Float(1024:1, 256:1024) = aten::t(%1148), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.123, %1149), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.124 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.73, %1147, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.124, %25, %33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.125 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states, %input_tensor, %36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output # transformers/modeling_electra.py:365:0
  %1154 : Tensor = prim::GetAttr[name="bias"](%1145)
  %1155 : Tensor = prim::GetAttr[name="weight"](%1145)
  %1156 : int[] = prim::ListConstruct(%17), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm
  %input : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.125, %1156, %1155, %1154, %23, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1158 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %1159 : Tensor = prim::GetAttr[name="bias"](%3)
  %1160 : Tensor = prim::GetAttr[name="weight"](%3)
  %1161 : Float(256:1, 2:256) = aten::t(%1160), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1161), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %1163 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1159, %1158), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%1163, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_electra.py:1194:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_electra.py:1194:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_electra.py:1195:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_electra.py:1195:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
