graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForSequenceClassification,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_41980.Linear = prim::GetAttr[name="logits_proj"](%self.1)
  %4 : __torch__.transformers.modeling_utils.___torch_mangle_41979.SequenceSummary = prim::GetAttr[name="sequence_summary"](%self.1)
  %5 : __torch__.transformers.modeling_xlnet.___torch_mangle_41975.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
  %36 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %37 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %38 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %39 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %40 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %41 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %42 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %43 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %44 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %45 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %46 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
  %47 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %48 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
  %49 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %50 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %51 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %52 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %53 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %54 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %55 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %56 : None = prim::Constant(), scope: __module.transformer
  %57 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %58 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %59 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %60 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %62 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
  %63 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %64 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %65 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %66 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %67 : __torch__.transformers.modeling_xlnet.___torch_mangle_41972.XLNetLayer = prim::GetAttr[name="23"](%66)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %69 : __torch__.transformers.modeling_xlnet.___torch_mangle_41962.XLNetLayer = prim::GetAttr[name="22"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %71 : __torch__.transformers.modeling_xlnet.___torch_mangle_41952.XLNetLayer = prim::GetAttr[name="21"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %73 : __torch__.transformers.modeling_xlnet.___torch_mangle_41942.XLNetLayer = prim::GetAttr[name="20"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %75 : __torch__.transformers.modeling_xlnet.___torch_mangle_41932.XLNetLayer = prim::GetAttr[name="19"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %77 : __torch__.transformers.modeling_xlnet.___torch_mangle_41922.XLNetLayer = prim::GetAttr[name="18"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %79 : __torch__.transformers.modeling_xlnet.___torch_mangle_41912.XLNetLayer = prim::GetAttr[name="17"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %81 : __torch__.transformers.modeling_xlnet.___torch_mangle_41902.XLNetLayer = prim::GetAttr[name="16"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %83 : __torch__.transformers.modeling_xlnet.___torch_mangle_41892.XLNetLayer = prim::GetAttr[name="15"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %85 : __torch__.transformers.modeling_xlnet.___torch_mangle_41882.XLNetLayer = prim::GetAttr[name="14"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %87 : __torch__.transformers.modeling_xlnet.___torch_mangle_41872.XLNetLayer = prim::GetAttr[name="13"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %89 : __torch__.transformers.modeling_xlnet.___torch_mangle_41862.XLNetLayer = prim::GetAttr[name="12"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %91 : __torch__.transformers.modeling_xlnet.___torch_mangle_41852.XLNetLayer = prim::GetAttr[name="11"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %93 : __torch__.transformers.modeling_xlnet.___torch_mangle_41842.XLNetLayer = prim::GetAttr[name="10"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %95 : __torch__.transformers.modeling_xlnet.___torch_mangle_41832.XLNetLayer = prim::GetAttr[name="9"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %97 : __torch__.transformers.modeling_xlnet.___torch_mangle_41822.XLNetLayer = prim::GetAttr[name="8"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %99 : __torch__.transformers.modeling_xlnet.___torch_mangle_41812.XLNetLayer = prim::GetAttr[name="7"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %101 : __torch__.transformers.modeling_xlnet.___torch_mangle_41802.XLNetLayer = prim::GetAttr[name="6"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %103 : __torch__.transformers.modeling_xlnet.___torch_mangle_41792.XLNetLayer = prim::GetAttr[name="5"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %105 : __torch__.transformers.modeling_xlnet.___torch_mangle_41782.XLNetLayer = prim::GetAttr[name="4"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %107 : __torch__.transformers.modeling_xlnet.___torch_mangle_41772.XLNetLayer = prim::GetAttr[name="3"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %109 : __torch__.transformers.modeling_xlnet.___torch_mangle_41762.XLNetLayer = prim::GetAttr[name="2"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %111 : __torch__.transformers.modeling_xlnet.___torch_mangle_41752.XLNetLayer = prim::GetAttr[name="1"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_41973.ModuleList = prim::GetAttr[name="layer"](%5)
  %113 : __torch__.transformers.modeling_xlnet.___torch_mangle_41742.XLNetLayer = prim::GetAttr[name="0"](%112)
  %114 : __torch__.torch.nn.modules.sparse.___torch_mangle_41732.Embedding = prim::GetAttr[name="word_embedding"](%5)
  %115 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %65, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:17, 17:1) = aten::contiguous(%115, %65), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %117 : int = aten::size(%input_ids, %65), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%117), scope: __module.transformer
  %119 : int = aten::size(%input_ids, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %120 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %65, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %121 : Long(13:17, 17:1) = aten::contiguous(%120, %65), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %klen.1 : Long() = aten::add(%qlen, %63, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %123 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
  %input_mask : Float(13:17, 17:1) = aten::rsub(%121, %62, %64), scope: __module.transformer # torch/tensor.py:396:0
  %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %65), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
  %126 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %127 : Float(1:221, 13:17, 17:1) = aten::slice(%126, %64, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %128 : Float(1:221, 13:17, 17:1) = aten::slice(%127, %60, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %129 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%128, %59), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %130 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%129, %65), scope: __module.transformer # torch/tensor.py:22:0
  %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%130, %58, %57, %57, %56), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %132 : Float(13:13, 13:1) = aten::eye(%117, %58, %65, %55, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %133 : Float(13:13, 13:1) = aten::to(%132, %55, %58, %57, %57, %56), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%133), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %135 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %136 : Float(13:13, 13:1) = aten::slice(%135, %64, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %137 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%136, %60), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %138 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%137, %59), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %139 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %138, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %140 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%139, %65), scope: __module.transformer # torch/tensor.py:22:0
  %141 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%140, %55, %58, %57, %57, %56), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %142 : Tensor = prim::GetAttr[name="weight"](%114)
  %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%142, %input_ids, %54, %57, %57), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.1, %53, %57), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %freq_seq : Float(512:1) = aten::arange(%65, %52, %51, %58, %65, %55, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %146 : Float(512:1) = aten::div(%freq_seq, %50), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %147 : Float(512:1) = aten::pow(%49, %146), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %148 : Float(512:1) = aten::reciprocal(%147), scope: __module.transformer # torch/tensor.py:400:0
  %149 : Float(512:1) = aten::mul(%148, %48), scope: __module.transformer # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
  %151 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
  %152 : Float(26:1) = aten::arange(%123, %151, %47, %56, %65, %55, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %153 : Tensor[] = prim::ListConstruct(%152, %149), scope: __module.transformer
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%46, %153), scope: __module.transformer # torch/functional.py:327:0
  %155 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %156 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %157 : Tensor[] = prim::ListConstruct(%155, %156), scope: __module.transformer
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%157, %54), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %159 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %160 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%159, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%160, %60, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %162 : int[] = prim::ListConstruct(%54, %119, %54), scope: __module.transformer
  %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %162, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
  %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %58, %65, %55, %57, %57, %57, %56), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
  %r.1 : Float(26:1024, 17:0, 1024:1) = aten::dropout(%input.2, %53, %57), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%curr_out.1, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %167 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %168 : __torch__.transformers.modeling_xlnet.___torch_mangle_41740.XLNetFeedForward = prim::GetAttr[name="ff"](%113)
  %169 : __torch__.transformers.modeling_xlnet.___torch_mangle_41735.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%113)
  %170 : __torch__.torch.nn.modules.normalization.___torch_mangle_41733.LayerNorm = prim::GetAttr[name="layer_norm"](%169)
  %171 : Tensor = prim::GetAttr[name="o"](%169)
  %172 : Tensor = prim::GetAttr[name="r_r_bias"](%169)
  %173 : Tensor = prim::GetAttr[name="r_w_bias"](%169)
  %174 : Tensor = prim::GetAttr[name="r"](%169)
  %175 : Tensor = prim::GetAttr[name="v"](%169)
  %176 : Tensor = prim::GetAttr[name="k"](%169)
  %177 : Tensor = prim::GetAttr[name="q"](%169)
  %178 : Tensor[] = prim::ListConstruct(%curr_out.1, %177), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %178), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %180 : Tensor[] = prim::ListConstruct(%curr_out.1, %176), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %181 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %180), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %182 : Tensor[] = prim::ListConstruct(%curr_out.1, %175), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %183 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %182), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%r.1, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %185 : Tensor[] = prim::ListConstruct(%r.2, %174), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %186 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %185), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %187 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %173, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %188 : Tensor[] = prim::ListConstruct(%187, %181), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %188), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %190 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %172, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %191 : Tensor[] = prim::ListConstruct(%190, %186), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %191), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %193 : int = aten::size(%ac.1, %59), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %194 : int = aten::size(%x.1, %65), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %195 : int = aten::size(%x.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %196 : int = aten::size(%x.1, %60), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %197 : int = aten::size(%x.1, %59), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %198 : Long() = prim::NumToTensor(%197), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %199 : int[] = prim::ListConstruct(%194, %195, %197, %196), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %199), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %201 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %202 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%201, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %203 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%202, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%203, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %205 : Long() = aten::sub(%198, %48, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %206 : int = aten::Int(%205), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %207 : int[] = prim::ListConstruct(%194, %195, %196, %206), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %207), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %209 : Long(13:1) = aten::arange(%193, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %59, %209), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %212 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%211, %63, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%212, %42), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %214 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %215 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %214), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %216 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%215, %40), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %216, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %59, %56), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %219 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %53, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %220 : Tensor[] = prim::ListConstruct(%219, %183), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %221 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %220), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %222 : Tensor[] = prim::ListConstruct(%221, %171), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %222), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.5, %53, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %226 : Tensor = prim::GetAttr[name="bias"](%170)
  %227 : Tensor = prim::GetAttr[name="weight"](%170)
  %228 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %228, %227, %226, %36, %37), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %230 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %231 : Float(13:17408, 17:1024, 1024:1), %232 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%230)
  %233 : __torch__.torch.nn.modules.normalization.___torch_mangle_41736.LayerNorm = prim::GetAttr[name="layer_norm"](%168)
  %234 : __torch__.torch.nn.modules.linear.___torch_mangle_41738.Linear = prim::GetAttr[name="layer_2"](%168)
  %235 : __torch__.torch.nn.modules.linear.___torch_mangle_41737.Linear = prim::GetAttr[name="layer_1"](%168)
  %236 : Tensor = prim::GetAttr[name="bias"](%235)
  %237 : Tensor = prim::GetAttr[name="weight"](%235)
  %238 : Float(1024:1, 4096:1024) = aten::t(%237), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%231, %238), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %236, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %53, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %243 : Tensor = prim::GetAttr[name="bias"](%234)
  %244 : Tensor = prim::GetAttr[name="weight"](%234)
  %245 : Float(4096:1, 1024:4096) = aten::t(%244), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.9, %245), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %243, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.10, %53, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.3, %231, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %250 : Tensor = prim::GetAttr[name="bias"](%233)
  %251 : Tensor = prim::GetAttr[name="weight"](%233)
  %252 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %252, %251, %250, %36, %37), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %254 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %232)
  %255 : Float(13:17408, 17:1024, 1024:1), %256 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%254)
  %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%255, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %258 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %259 : __torch__.transformers.modeling_xlnet.___torch_mangle_41750.XLNetFeedForward = prim::GetAttr[name="ff"](%111)
  %260 : __torch__.transformers.modeling_xlnet.___torch_mangle_41745.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%111)
  %261 : __torch__.torch.nn.modules.normalization.___torch_mangle_41743.LayerNorm = prim::GetAttr[name="layer_norm"](%260)
  %262 : Tensor = prim::GetAttr[name="o"](%260)
  %263 : Tensor = prim::GetAttr[name="r_r_bias"](%260)
  %264 : Tensor = prim::GetAttr[name="r_w_bias"](%260)
  %265 : Tensor = prim::GetAttr[name="r"](%260)
  %266 : Tensor = prim::GetAttr[name="v"](%260)
  %267 : Tensor = prim::GetAttr[name="k"](%260)
  %268 : Tensor = prim::GetAttr[name="q"](%260)
  %269 : Tensor[] = prim::ListConstruct(%255, %268), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %269), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %271 : Tensor[] = prim::ListConstruct(%255, %267), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %272 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %271), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %273 : Tensor[] = prim::ListConstruct(%255, %266), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %274 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %273), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%256, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %276 : Tensor[] = prim::ListConstruct(%r.3, %265), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %277 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %276), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %278 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %264, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %279 : Tensor[] = prim::ListConstruct(%278, %272), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %279), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %281 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %263, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %282 : Tensor[] = prim::ListConstruct(%281, %277), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %282), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %284 : int = aten::size(%ac.2, %59), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %285 : int = aten::size(%x.5, %65), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %286 : int = aten::size(%x.5, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %287 : int = aten::size(%x.5, %60), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %288 : int = aten::size(%x.5, %59), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %289 : Long() = prim::NumToTensor(%288), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %290 : int[] = prim::ListConstruct(%285, %286, %288, %287), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %290), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %292 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %293 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%292, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %294 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%293, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%294, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %296 : Long() = aten::sub(%289, %48, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %297 : int = aten::Int(%296), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %298 : int[] = prim::ListConstruct(%285, %286, %287, %297), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %298), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %300 : Long(13:1) = aten::arange(%284, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %59, %300), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %302 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %303 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%302, %63, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%303, %42), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %305 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %306 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %305), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %307 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%306, %40), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %307, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %59, %56), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %310 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %53, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %311 : Tensor[] = prim::ListConstruct(%310, %274), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %312 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %311), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %313 : Tensor[] = prim::ListConstruct(%312, %262), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %313), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.14, %53, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.2, %255, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %317 : Tensor = prim::GetAttr[name="bias"](%261)
  %318 : Tensor = prim::GetAttr[name="weight"](%261)
  %319 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %319, %318, %317, %36, %37), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %321 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %322 : Float(13:17408, 17:1024, 1024:1), %323 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%321)
  %324 : __torch__.torch.nn.modules.normalization.___torch_mangle_41746.LayerNorm = prim::GetAttr[name="layer_norm"](%259)
  %325 : __torch__.torch.nn.modules.linear.___torch_mangle_41748.Linear = prim::GetAttr[name="layer_2"](%259)
  %326 : __torch__.torch.nn.modules.linear.___torch_mangle_41747.Linear = prim::GetAttr[name="layer_1"](%259)
  %327 : Tensor = prim::GetAttr[name="bias"](%326)
  %328 : Tensor = prim::GetAttr[name="weight"](%326)
  %329 : Float(1024:1, 4096:1024) = aten::t(%328), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%322, %329), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %327, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %53, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %334 : Tensor = prim::GetAttr[name="bias"](%325)
  %335 : Tensor = prim::GetAttr[name="weight"](%325)
  %336 : Float(4096:1, 1024:4096) = aten::t(%335), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.18, %336), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %334, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.19, %53, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.6, %322, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %341 : Tensor = prim::GetAttr[name="bias"](%324)
  %342 : Tensor = prim::GetAttr[name="weight"](%324)
  %343 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %343, %342, %341, %36, %37), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %345 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %323)
  %346 : Float(13:17408, 17:1024, 1024:1), %347 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%345)
  %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%346, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %349 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %350 : __torch__.transformers.modeling_xlnet.___torch_mangle_41760.XLNetFeedForward = prim::GetAttr[name="ff"](%109)
  %351 : __torch__.transformers.modeling_xlnet.___torch_mangle_41755.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%109)
  %352 : __torch__.torch.nn.modules.normalization.___torch_mangle_41753.LayerNorm = prim::GetAttr[name="layer_norm"](%351)
  %353 : Tensor = prim::GetAttr[name="o"](%351)
  %354 : Tensor = prim::GetAttr[name="r_r_bias"](%351)
  %355 : Tensor = prim::GetAttr[name="r_w_bias"](%351)
  %356 : Tensor = prim::GetAttr[name="r"](%351)
  %357 : Tensor = prim::GetAttr[name="v"](%351)
  %358 : Tensor = prim::GetAttr[name="k"](%351)
  %359 : Tensor = prim::GetAttr[name="q"](%351)
  %360 : Tensor[] = prim::ListConstruct(%346, %359), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %360), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %362 : Tensor[] = prim::ListConstruct(%346, %358), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %363 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %362), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %364 : Tensor[] = prim::ListConstruct(%346, %357), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %365 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %364), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%347, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %367 : Tensor[] = prim::ListConstruct(%r.4, %356), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %368 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %367), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %369 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %355, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %370 : Tensor[] = prim::ListConstruct(%369, %363), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %370), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %372 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %354, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %373 : Tensor[] = prim::ListConstruct(%372, %368), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %373), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %375 : int = aten::size(%ac.3, %59), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %376 : int = aten::size(%x.9, %65), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %377 : int = aten::size(%x.9, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %378 : int = aten::size(%x.9, %60), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %379 : int = aten::size(%x.9, %59), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %380 : Long() = prim::NumToTensor(%379), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %381 : int[] = prim::ListConstruct(%376, %377, %379, %378), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %381), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %383 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %384 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%383, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %385 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%384, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%385, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %387 : Long() = aten::sub(%380, %48, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %388 : int = aten::Int(%387), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %389 : int[] = prim::ListConstruct(%376, %377, %378, %388), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %389), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %391 : Long(13:1) = aten::arange(%375, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %59, %391), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %393 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %394 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%393, %63, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%394, %42), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %396 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %397 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %396), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %398 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%397, %40), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %398, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %59, %56), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %401 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %53, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %402 : Tensor[] = prim::ListConstruct(%401, %365), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %403 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %402), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %404 : Tensor[] = prim::ListConstruct(%403, %353), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %404), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.23, %53, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.3, %346, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %408 : Tensor = prim::GetAttr[name="bias"](%352)
  %409 : Tensor = prim::GetAttr[name="weight"](%352)
  %410 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %410, %409, %408, %36, %37), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %412 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %413 : Float(13:17408, 17:1024, 1024:1), %414 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%412)
  %415 : __torch__.torch.nn.modules.normalization.___torch_mangle_41756.LayerNorm = prim::GetAttr[name="layer_norm"](%350)
  %416 : __torch__.torch.nn.modules.linear.___torch_mangle_41758.Linear = prim::GetAttr[name="layer_2"](%350)
  %417 : __torch__.torch.nn.modules.linear.___torch_mangle_41757.Linear = prim::GetAttr[name="layer_1"](%350)
  %418 : Tensor = prim::GetAttr[name="bias"](%417)
  %419 : Tensor = prim::GetAttr[name="weight"](%417)
  %420 : Float(1024:1, 4096:1024) = aten::t(%419), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%413, %420), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %418, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.25), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %53, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %425 : Tensor = prim::GetAttr[name="bias"](%416)
  %426 : Tensor = prim::GetAttr[name="weight"](%416)
  %427 : Float(4096:1, 1024:4096) = aten::t(%426), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.27, %427), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %425, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.28, %53, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.9, %413, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %432 : Tensor = prim::GetAttr[name="bias"](%415)
  %433 : Tensor = prim::GetAttr[name="weight"](%415)
  %434 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %434, %433, %432, %36, %37), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %436 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %414)
  %437 : Float(13:17408, 17:1024, 1024:1), %438 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%436)
  %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%437, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %440 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %441 : __torch__.transformers.modeling_xlnet.___torch_mangle_41770.XLNetFeedForward = prim::GetAttr[name="ff"](%107)
  %442 : __torch__.transformers.modeling_xlnet.___torch_mangle_41765.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%107)
  %443 : __torch__.torch.nn.modules.normalization.___torch_mangle_41763.LayerNorm = prim::GetAttr[name="layer_norm"](%442)
  %444 : Tensor = prim::GetAttr[name="o"](%442)
  %445 : Tensor = prim::GetAttr[name="r_r_bias"](%442)
  %446 : Tensor = prim::GetAttr[name="r_w_bias"](%442)
  %447 : Tensor = prim::GetAttr[name="r"](%442)
  %448 : Tensor = prim::GetAttr[name="v"](%442)
  %449 : Tensor = prim::GetAttr[name="k"](%442)
  %450 : Tensor = prim::GetAttr[name="q"](%442)
  %451 : Tensor[] = prim::ListConstruct(%437, %450), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %451), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %453 : Tensor[] = prim::ListConstruct(%437, %449), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %454 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %453), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %455 : Tensor[] = prim::ListConstruct(%437, %448), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %456 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %455), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%438, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %458 : Tensor[] = prim::ListConstruct(%r.5, %447), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %459 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %458), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %460 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %446, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %461 : Tensor[] = prim::ListConstruct(%460, %454), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %461), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %463 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %445, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %464 : Tensor[] = prim::ListConstruct(%463, %459), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %464), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %466 : int = aten::size(%ac.4, %59), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %467 : int = aten::size(%x.13, %65), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %468 : int = aten::size(%x.13, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %469 : int = aten::size(%x.13, %60), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %470 : int = aten::size(%x.13, %59), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %471 : Long() = prim::NumToTensor(%470), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %472 : int[] = prim::ListConstruct(%467, %468, %470, %469), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %472), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %474 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %475 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%474, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %476 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%475, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%476, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %478 : Long() = aten::sub(%471, %48, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %479 : int = aten::Int(%478), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %480 : int[] = prim::ListConstruct(%467, %468, %469, %479), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %480), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %482 : Long(13:1) = aten::arange(%466, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %59, %482), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %484 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %485 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%484, %63, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%485, %42), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %487 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %488 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %487), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %489 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%488, %40), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %489, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %59, %56), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %492 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %53, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %493 : Tensor[] = prim::ListConstruct(%492, %456), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %494 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %493), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %495 : Tensor[] = prim::ListConstruct(%494, %444), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %495), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.32, %53, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.4, %437, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %499 : Tensor = prim::GetAttr[name="bias"](%443)
  %500 : Tensor = prim::GetAttr[name="weight"](%443)
  %501 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %501, %500, %499, %36, %37), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %503 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %504 : Float(13:17408, 17:1024, 1024:1), %505 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%503)
  %506 : __torch__.torch.nn.modules.normalization.___torch_mangle_41766.LayerNorm = prim::GetAttr[name="layer_norm"](%441)
  %507 : __torch__.torch.nn.modules.linear.___torch_mangle_41768.Linear = prim::GetAttr[name="layer_2"](%441)
  %508 : __torch__.torch.nn.modules.linear.___torch_mangle_41767.Linear = prim::GetAttr[name="layer_1"](%441)
  %509 : Tensor = prim::GetAttr[name="bias"](%508)
  %510 : Tensor = prim::GetAttr[name="weight"](%508)
  %511 : Float(1024:1, 4096:1024) = aten::t(%510), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%504, %511), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %509, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %53, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %516 : Tensor = prim::GetAttr[name="bias"](%507)
  %517 : Tensor = prim::GetAttr[name="weight"](%507)
  %518 : Float(4096:1, 1024:4096) = aten::t(%517), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.36, %518), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %516, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.37, %53, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.12, %504, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %523 : Tensor = prim::GetAttr[name="bias"](%506)
  %524 : Tensor = prim::GetAttr[name="weight"](%506)
  %525 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %525, %524, %523, %36, %37), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %527 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %505)
  %528 : Float(13:17408, 17:1024, 1024:1), %529 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%527)
  %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%528, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %531 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %532 : __torch__.transformers.modeling_xlnet.___torch_mangle_41780.XLNetFeedForward = prim::GetAttr[name="ff"](%105)
  %533 : __torch__.transformers.modeling_xlnet.___torch_mangle_41775.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%105)
  %534 : __torch__.torch.nn.modules.normalization.___torch_mangle_41773.LayerNorm = prim::GetAttr[name="layer_norm"](%533)
  %535 : Tensor = prim::GetAttr[name="o"](%533)
  %536 : Tensor = prim::GetAttr[name="r_r_bias"](%533)
  %537 : Tensor = prim::GetAttr[name="r_w_bias"](%533)
  %538 : Tensor = prim::GetAttr[name="r"](%533)
  %539 : Tensor = prim::GetAttr[name="v"](%533)
  %540 : Tensor = prim::GetAttr[name="k"](%533)
  %541 : Tensor = prim::GetAttr[name="q"](%533)
  %542 : Tensor[] = prim::ListConstruct(%528, %541), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %542), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %544 : Tensor[] = prim::ListConstruct(%528, %540), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %545 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %544), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %546 : Tensor[] = prim::ListConstruct(%528, %539), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %547 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %546), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%529, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %549 : Tensor[] = prim::ListConstruct(%r.6, %538), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %550 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %549), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %551 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %537, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %552 : Tensor[] = prim::ListConstruct(%551, %545), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %552), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %554 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %536, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %555 : Tensor[] = prim::ListConstruct(%554, %550), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %555), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %557 : int = aten::size(%ac.5, %59), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %558 : int = aten::size(%x.17, %65), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %559 : int = aten::size(%x.17, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %560 : int = aten::size(%x.17, %60), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %561 : int = aten::size(%x.17, %59), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %562 : Long() = prim::NumToTensor(%561), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %563 : int[] = prim::ListConstruct(%558, %559, %561, %560), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %563), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %565 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %566 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%565, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %567 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%566, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%567, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %569 : Long() = aten::sub(%562, %48, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %570 : int = aten::Int(%569), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %571 : int[] = prim::ListConstruct(%558, %559, %560, %570), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %571), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %573 : Long(13:1) = aten::arange(%557, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %59, %573), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %575 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %576 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%575, %63, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%576, %42), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %578 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %579 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %578), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %580 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%579, %40), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %580, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %59, %56), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %583 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %53, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %584 : Tensor[] = prim::ListConstruct(%583, %547), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %585 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %584), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %586 : Tensor[] = prim::ListConstruct(%585, %535), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %586), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.41, %53, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.5, %528, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %590 : Tensor = prim::GetAttr[name="bias"](%534)
  %591 : Tensor = prim::GetAttr[name="weight"](%534)
  %592 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %592, %591, %590, %36, %37), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %594 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %595 : Float(13:17408, 17:1024, 1024:1), %596 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%594)
  %597 : __torch__.torch.nn.modules.normalization.___torch_mangle_41776.LayerNorm = prim::GetAttr[name="layer_norm"](%532)
  %598 : __torch__.torch.nn.modules.linear.___torch_mangle_41778.Linear = prim::GetAttr[name="layer_2"](%532)
  %599 : __torch__.torch.nn.modules.linear.___torch_mangle_41777.Linear = prim::GetAttr[name="layer_1"](%532)
  %600 : Tensor = prim::GetAttr[name="bias"](%599)
  %601 : Tensor = prim::GetAttr[name="weight"](%599)
  %602 : Float(1024:1, 4096:1024) = aten::t(%601), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%595, %602), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %600, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %53, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %607 : Tensor = prim::GetAttr[name="bias"](%598)
  %608 : Tensor = prim::GetAttr[name="weight"](%598)
  %609 : Float(4096:1, 1024:4096) = aten::t(%608), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %609), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %607, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %53, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.15, %595, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %614 : Tensor = prim::GetAttr[name="bias"](%597)
  %615 : Tensor = prim::GetAttr[name="weight"](%597)
  %616 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %616, %615, %614, %36, %37), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %618 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %596)
  %619 : Float(13:17408, 17:1024, 1024:1), %620 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%618)
  %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%619, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %622 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %623 : __torch__.transformers.modeling_xlnet.___torch_mangle_41790.XLNetFeedForward = prim::GetAttr[name="ff"](%103)
  %624 : __torch__.transformers.modeling_xlnet.___torch_mangle_41785.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%103)
  %625 : __torch__.torch.nn.modules.normalization.___torch_mangle_41783.LayerNorm = prim::GetAttr[name="layer_norm"](%624)
  %626 : Tensor = prim::GetAttr[name="o"](%624)
  %627 : Tensor = prim::GetAttr[name="r_r_bias"](%624)
  %628 : Tensor = prim::GetAttr[name="r_w_bias"](%624)
  %629 : Tensor = prim::GetAttr[name="r"](%624)
  %630 : Tensor = prim::GetAttr[name="v"](%624)
  %631 : Tensor = prim::GetAttr[name="k"](%624)
  %632 : Tensor = prim::GetAttr[name="q"](%624)
  %633 : Tensor[] = prim::ListConstruct(%619, %632), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %633), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %635 : Tensor[] = prim::ListConstruct(%619, %631), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %636 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %635), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %637 : Tensor[] = prim::ListConstruct(%619, %630), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %638 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %637), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%620, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %640 : Tensor[] = prim::ListConstruct(%r.7, %629), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %641 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %640), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %642 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %628, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %643 : Tensor[] = prim::ListConstruct(%642, %636), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %643), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %645 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %627, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %646 : Tensor[] = prim::ListConstruct(%645, %641), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %646), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %648 : int = aten::size(%ac.6, %59), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %649 : int = aten::size(%x.21, %65), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %650 : int = aten::size(%x.21, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %651 : int = aten::size(%x.21, %60), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %652 : int = aten::size(%x.21, %59), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %653 : Long() = prim::NumToTensor(%652), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %654 : int[] = prim::ListConstruct(%649, %650, %652, %651), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %654), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %656 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %657 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%656, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %658 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%657, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%658, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %660 : Long() = aten::sub(%653, %48, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %661 : int = aten::Int(%660), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %662 : int[] = prim::ListConstruct(%649, %650, %651, %661), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %662), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %664 : Long(13:1) = aten::arange(%648, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %59, %664), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %666 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %667 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%666, %63, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%667, %42), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %669 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %670 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %669), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %671 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%670, %40), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %671, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %59, %56), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %674 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %53, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %675 : Tensor[] = prim::ListConstruct(%674, %638), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %676 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %675), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %677 : Tensor[] = prim::ListConstruct(%676, %626), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %677), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.50, %53, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.6, %619, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %681 : Tensor = prim::GetAttr[name="bias"](%625)
  %682 : Tensor = prim::GetAttr[name="weight"](%625)
  %683 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %683, %682, %681, %36, %37), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %685 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %686 : Float(13:17408, 17:1024, 1024:1), %687 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%685)
  %688 : __torch__.torch.nn.modules.normalization.___torch_mangle_41786.LayerNorm = prim::GetAttr[name="layer_norm"](%623)
  %689 : __torch__.torch.nn.modules.linear.___torch_mangle_41788.Linear = prim::GetAttr[name="layer_2"](%623)
  %690 : __torch__.torch.nn.modules.linear.___torch_mangle_41787.Linear = prim::GetAttr[name="layer_1"](%623)
  %691 : Tensor = prim::GetAttr[name="bias"](%690)
  %692 : Tensor = prim::GetAttr[name="weight"](%690)
  %693 : Float(1024:1, 4096:1024) = aten::t(%692), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%686, %693), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %691, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %53, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %698 : Tensor = prim::GetAttr[name="bias"](%689)
  %699 : Tensor = prim::GetAttr[name="weight"](%689)
  %700 : Float(4096:1, 1024:4096) = aten::t(%699), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.54, %700), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %698, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.55, %53, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.18, %686, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %705 : Tensor = prim::GetAttr[name="bias"](%688)
  %706 : Tensor = prim::GetAttr[name="weight"](%688)
  %707 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %707, %706, %705, %36, %37), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %709 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %687)
  %710 : Float(13:17408, 17:1024, 1024:1), %711 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%709)
  %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%710, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %713 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %714 : __torch__.transformers.modeling_xlnet.___torch_mangle_41800.XLNetFeedForward = prim::GetAttr[name="ff"](%101)
  %715 : __torch__.transformers.modeling_xlnet.___torch_mangle_41795.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%101)
  %716 : __torch__.torch.nn.modules.normalization.___torch_mangle_41793.LayerNorm = prim::GetAttr[name="layer_norm"](%715)
  %717 : Tensor = prim::GetAttr[name="o"](%715)
  %718 : Tensor = prim::GetAttr[name="r_r_bias"](%715)
  %719 : Tensor = prim::GetAttr[name="r_w_bias"](%715)
  %720 : Tensor = prim::GetAttr[name="r"](%715)
  %721 : Tensor = prim::GetAttr[name="v"](%715)
  %722 : Tensor = prim::GetAttr[name="k"](%715)
  %723 : Tensor = prim::GetAttr[name="q"](%715)
  %724 : Tensor[] = prim::ListConstruct(%710, %723), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %724), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %726 : Tensor[] = prim::ListConstruct(%710, %722), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %727 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %726), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %728 : Tensor[] = prim::ListConstruct(%710, %721), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %729 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %728), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%711, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %731 : Tensor[] = prim::ListConstruct(%r.8, %720), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %732 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %731), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %733 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %719, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %734 : Tensor[] = prim::ListConstruct(%733, %727), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %734), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %736 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %718, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %737 : Tensor[] = prim::ListConstruct(%736, %732), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %737), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %739 : int = aten::size(%ac.7, %59), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %740 : int = aten::size(%x.25, %65), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %741 : int = aten::size(%x.25, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %742 : int = aten::size(%x.25, %60), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %743 : int = aten::size(%x.25, %59), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %744 : Long() = prim::NumToTensor(%743), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %745 : int[] = prim::ListConstruct(%740, %741, %743, %742), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %745), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %747 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %748 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%747, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %749 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%748, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%749, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %751 : Long() = aten::sub(%744, %48, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %752 : int = aten::Int(%751), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %753 : int[] = prim::ListConstruct(%740, %741, %742, %752), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %753), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %755 : Long(13:1) = aten::arange(%739, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %59, %755), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %757 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %758 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%757, %63, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%758, %42), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %760 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %761 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %760), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %762 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%761, %40), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %762, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %59, %56), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %765 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %53, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %766 : Tensor[] = prim::ListConstruct(%765, %729), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %767 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %766), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %768 : Tensor[] = prim::ListConstruct(%767, %717), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %768), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.59, %53, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.7, %710, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %772 : Tensor = prim::GetAttr[name="bias"](%716)
  %773 : Tensor = prim::GetAttr[name="weight"](%716)
  %774 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %774, %773, %772, %36, %37), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %776 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %777 : Float(13:17408, 17:1024, 1024:1), %778 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%776)
  %779 : __torch__.torch.nn.modules.normalization.___torch_mangle_41796.LayerNorm = prim::GetAttr[name="layer_norm"](%714)
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_41798.Linear = prim::GetAttr[name="layer_2"](%714)
  %781 : __torch__.torch.nn.modules.linear.___torch_mangle_41797.Linear = prim::GetAttr[name="layer_1"](%714)
  %782 : Tensor = prim::GetAttr[name="bias"](%781)
  %783 : Tensor = prim::GetAttr[name="weight"](%781)
  %784 : Float(1024:1, 4096:1024) = aten::t(%783), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%777, %784), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %782, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %53, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %789 : Tensor = prim::GetAttr[name="bias"](%780)
  %790 : Tensor = prim::GetAttr[name="weight"](%780)
  %791 : Float(4096:1, 1024:4096) = aten::t(%790), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.63, %791), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %789, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.64, %53, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.21, %777, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %796 : Tensor = prim::GetAttr[name="bias"](%779)
  %797 : Tensor = prim::GetAttr[name="weight"](%779)
  %798 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %798, %797, %796, %36, %37), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %800 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %778)
  %801 : Float(13:17408, 17:1024, 1024:1), %802 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%800)
  %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%801, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %804 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %805 : __torch__.transformers.modeling_xlnet.___torch_mangle_41810.XLNetFeedForward = prim::GetAttr[name="ff"](%99)
  %806 : __torch__.transformers.modeling_xlnet.___torch_mangle_41805.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%99)
  %807 : __torch__.torch.nn.modules.normalization.___torch_mangle_41803.LayerNorm = prim::GetAttr[name="layer_norm"](%806)
  %808 : Tensor = prim::GetAttr[name="o"](%806)
  %809 : Tensor = prim::GetAttr[name="r_r_bias"](%806)
  %810 : Tensor = prim::GetAttr[name="r_w_bias"](%806)
  %811 : Tensor = prim::GetAttr[name="r"](%806)
  %812 : Tensor = prim::GetAttr[name="v"](%806)
  %813 : Tensor = prim::GetAttr[name="k"](%806)
  %814 : Tensor = prim::GetAttr[name="q"](%806)
  %815 : Tensor[] = prim::ListConstruct(%801, %814), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %815), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %817 : Tensor[] = prim::ListConstruct(%801, %813), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %818 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %817), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %819 : Tensor[] = prim::ListConstruct(%801, %812), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %820 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %819), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%802, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %822 : Tensor[] = prim::ListConstruct(%r.9, %811), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %823 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %822), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %824 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %810, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %825 : Tensor[] = prim::ListConstruct(%824, %818), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %825), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %827 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %809, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %828 : Tensor[] = prim::ListConstruct(%827, %823), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %828), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %830 : int = aten::size(%ac.8, %59), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %831 : int = aten::size(%x.29, %65), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %832 : int = aten::size(%x.29, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %833 : int = aten::size(%x.29, %60), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %834 : int = aten::size(%x.29, %59), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %835 : Long() = prim::NumToTensor(%834), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %836 : int[] = prim::ListConstruct(%831, %832, %834, %833), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %836), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %838 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %839 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%838, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %840 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%839, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%840, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %842 : Long() = aten::sub(%835, %48, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %843 : int = aten::Int(%842), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %844 : int[] = prim::ListConstruct(%831, %832, %833, %843), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %844), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %846 : Long(13:1) = aten::arange(%830, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %59, %846), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %848 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %849 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%848, %63, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%849, %42), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %851 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %852 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %851), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %853 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%852, %40), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %853, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %59, %56), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %856 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %53, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %857 : Tensor[] = prim::ListConstruct(%856, %820), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %858 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %857), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %859 : Tensor[] = prim::ListConstruct(%858, %808), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %859), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %53, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.8, %801, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %863 : Tensor = prim::GetAttr[name="bias"](%807)
  %864 : Tensor = prim::GetAttr[name="weight"](%807)
  %865 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %865, %864, %863, %36, %37), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %867 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %868 : Float(13:17408, 17:1024, 1024:1), %869 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%867)
  %870 : __torch__.torch.nn.modules.normalization.___torch_mangle_41806.LayerNorm = prim::GetAttr[name="layer_norm"](%805)
  %871 : __torch__.torch.nn.modules.linear.___torch_mangle_41808.Linear = prim::GetAttr[name="layer_2"](%805)
  %872 : __torch__.torch.nn.modules.linear.___torch_mangle_41807.Linear = prim::GetAttr[name="layer_1"](%805)
  %873 : Tensor = prim::GetAttr[name="bias"](%872)
  %874 : Tensor = prim::GetAttr[name="weight"](%872)
  %875 : Float(1024:1, 4096:1024) = aten::t(%874), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%868, %875), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %873, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %53, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %880 : Tensor = prim::GetAttr[name="bias"](%871)
  %881 : Tensor = prim::GetAttr[name="weight"](%871)
  %882 : Float(4096:1, 1024:4096) = aten::t(%881), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %882), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %880, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %53, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.24, %868, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %887 : Tensor = prim::GetAttr[name="bias"](%870)
  %888 : Tensor = prim::GetAttr[name="weight"](%870)
  %889 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %889, %888, %887, %36, %37), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %891 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %869)
  %892 : Float(13:17408, 17:1024, 1024:1), %893 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%891)
  %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%892, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %895 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %896 : __torch__.transformers.modeling_xlnet.___torch_mangle_41820.XLNetFeedForward = prim::GetAttr[name="ff"](%97)
  %897 : __torch__.transformers.modeling_xlnet.___torch_mangle_41815.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%97)
  %898 : __torch__.torch.nn.modules.normalization.___torch_mangle_41813.LayerNorm = prim::GetAttr[name="layer_norm"](%897)
  %899 : Tensor = prim::GetAttr[name="o"](%897)
  %900 : Tensor = prim::GetAttr[name="r_r_bias"](%897)
  %901 : Tensor = prim::GetAttr[name="r_w_bias"](%897)
  %902 : Tensor = prim::GetAttr[name="r"](%897)
  %903 : Tensor = prim::GetAttr[name="v"](%897)
  %904 : Tensor = prim::GetAttr[name="k"](%897)
  %905 : Tensor = prim::GetAttr[name="q"](%897)
  %906 : Tensor[] = prim::ListConstruct(%892, %905), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %906), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %908 : Tensor[] = prim::ListConstruct(%892, %904), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %909 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %908), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %910 : Tensor[] = prim::ListConstruct(%892, %903), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %911 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %910), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%893, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %913 : Tensor[] = prim::ListConstruct(%r.10, %902), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %914 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %913), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %915 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %901, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %916 : Tensor[] = prim::ListConstruct(%915, %909), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %916), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %918 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %900, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %919 : Tensor[] = prim::ListConstruct(%918, %914), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %919), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %921 : int = aten::size(%ac.9, %59), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %922 : int = aten::size(%x.33, %65), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %923 : int = aten::size(%x.33, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %924 : int = aten::size(%x.33, %60), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %925 : int = aten::size(%x.33, %59), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %926 : Long() = prim::NumToTensor(%925), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %927 : int[] = prim::ListConstruct(%922, %923, %925, %924), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %927), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %929 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %930 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%929, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %931 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%930, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%931, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %933 : Long() = aten::sub(%926, %48, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %934 : int = aten::Int(%933), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %935 : int[] = prim::ListConstruct(%922, %923, %924, %934), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %935), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %937 : Long(13:1) = aten::arange(%921, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %59, %937), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %939 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %940 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%939, %63, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%940, %42), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %942 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %943 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %942), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %944 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%943, %40), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %944, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %59, %56), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %947 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %53, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %948 : Tensor[] = prim::ListConstruct(%947, %911), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %949 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %948), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %950 : Tensor[] = prim::ListConstruct(%949, %899), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %950), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.77, %53, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.9, %892, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %954 : Tensor = prim::GetAttr[name="bias"](%898)
  %955 : Tensor = prim::GetAttr[name="weight"](%898)
  %956 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %956, %955, %954, %36, %37), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %958 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %959 : Float(13:17408, 17:1024, 1024:1), %960 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%958)
  %961 : __torch__.torch.nn.modules.normalization.___torch_mangle_41816.LayerNorm = prim::GetAttr[name="layer_norm"](%896)
  %962 : __torch__.torch.nn.modules.linear.___torch_mangle_41818.Linear = prim::GetAttr[name="layer_2"](%896)
  %963 : __torch__.torch.nn.modules.linear.___torch_mangle_41817.Linear = prim::GetAttr[name="layer_1"](%896)
  %964 : Tensor = prim::GetAttr[name="bias"](%963)
  %965 : Tensor = prim::GetAttr[name="weight"](%963)
  %966 : Float(1024:1, 4096:1024) = aten::t(%965), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%959, %966), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %964, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.79), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %53, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %971 : Tensor = prim::GetAttr[name="bias"](%962)
  %972 : Tensor = prim::GetAttr[name="weight"](%962)
  %973 : Float(4096:1, 1024:4096) = aten::t(%972), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.81, %973), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %971, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.82, %53, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.27, %959, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %978 : Tensor = prim::GetAttr[name="bias"](%961)
  %979 : Tensor = prim::GetAttr[name="weight"](%961)
  %980 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %980, %979, %978, %36, %37), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %982 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %960)
  %983 : Float(13:17408, 17:1024, 1024:1), %984 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%982)
  %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%983, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %986 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %987 : __torch__.transformers.modeling_xlnet.___torch_mangle_41830.XLNetFeedForward = prim::GetAttr[name="ff"](%95)
  %988 : __torch__.transformers.modeling_xlnet.___torch_mangle_41825.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%95)
  %989 : __torch__.torch.nn.modules.normalization.___torch_mangle_41823.LayerNorm = prim::GetAttr[name="layer_norm"](%988)
  %990 : Tensor = prim::GetAttr[name="o"](%988)
  %991 : Tensor = prim::GetAttr[name="r_r_bias"](%988)
  %992 : Tensor = prim::GetAttr[name="r_w_bias"](%988)
  %993 : Tensor = prim::GetAttr[name="r"](%988)
  %994 : Tensor = prim::GetAttr[name="v"](%988)
  %995 : Tensor = prim::GetAttr[name="k"](%988)
  %996 : Tensor = prim::GetAttr[name="q"](%988)
  %997 : Tensor[] = prim::ListConstruct(%983, %996), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %997), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %999 : Tensor[] = prim::ListConstruct(%983, %995), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1000 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %999), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1001 : Tensor[] = prim::ListConstruct(%983, %994), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1002 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1001), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%984, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1004 : Tensor[] = prim::ListConstruct(%r.11, %993), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1005 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1004), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1006 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %992, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1007 : Tensor[] = prim::ListConstruct(%1006, %1000), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1007), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1009 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %991, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1010 : Tensor[] = prim::ListConstruct(%1009, %1005), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1010), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1012 : int = aten::size(%ac.10, %59), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1013 : int = aten::size(%x.37, %65), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1014 : int = aten::size(%x.37, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1015 : int = aten::size(%x.37, %60), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1016 : int = aten::size(%x.37, %59), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1017 : Long() = prim::NumToTensor(%1016), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1018 : int[] = prim::ListConstruct(%1013, %1014, %1016, %1015), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1018), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1020 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1021 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1020, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1022 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1021, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1022, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1024 : Long() = aten::sub(%1017, %48, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1025 : int = aten::Int(%1024), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1026 : int[] = prim::ListConstruct(%1013, %1014, %1015, %1025), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1026), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1028 : Long(13:1) = aten::arange(%1012, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %59, %1028), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1030 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1031 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1030, %63, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1031, %42), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1033 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1034 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1033), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1035 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1034, %40), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %1035, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %59, %56), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1038 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %53, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1039 : Tensor[] = prim::ListConstruct(%1038, %1002), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1040 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1039), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1041 : Tensor[] = prim::ListConstruct(%1040, %990), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1041), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.86, %53, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.10, %983, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1045 : Tensor = prim::GetAttr[name="bias"](%989)
  %1046 : Tensor = prim::GetAttr[name="weight"](%989)
  %1047 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %1047, %1046, %1045, %36, %37), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1049 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1050 : Float(13:17408, 17:1024, 1024:1), %1051 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1049)
  %1052 : __torch__.torch.nn.modules.normalization.___torch_mangle_41826.LayerNorm = prim::GetAttr[name="layer_norm"](%987)
  %1053 : __torch__.torch.nn.modules.linear.___torch_mangle_41828.Linear = prim::GetAttr[name="layer_2"](%987)
  %1054 : __torch__.torch.nn.modules.linear.___torch_mangle_41827.Linear = prim::GetAttr[name="layer_1"](%987)
  %1055 : Tensor = prim::GetAttr[name="bias"](%1054)
  %1056 : Tensor = prim::GetAttr[name="weight"](%1054)
  %1057 : Float(1024:1, 4096:1024) = aten::t(%1056), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1050, %1057), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %1055, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %53, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1062 : Tensor = prim::GetAttr[name="bias"](%1053)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1053)
  %1064 : Float(4096:1, 1024:4096) = aten::t(%1063), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.90, %1064), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %1062, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.91, %53, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.30, %1050, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%1052)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1052)
  %1071 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %1071, %1070, %1069, %36, %37), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1073 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1051)
  %1074 : Float(13:17408, 17:1024, 1024:1), %1075 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1073)
  %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1074, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1077 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1078 : __torch__.transformers.modeling_xlnet.___torch_mangle_41840.XLNetFeedForward = prim::GetAttr[name="ff"](%93)
  %1079 : __torch__.transformers.modeling_xlnet.___torch_mangle_41835.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%93)
  %1080 : __torch__.torch.nn.modules.normalization.___torch_mangle_41833.LayerNorm = prim::GetAttr[name="layer_norm"](%1079)
  %1081 : Tensor = prim::GetAttr[name="o"](%1079)
  %1082 : Tensor = prim::GetAttr[name="r_r_bias"](%1079)
  %1083 : Tensor = prim::GetAttr[name="r_w_bias"](%1079)
  %1084 : Tensor = prim::GetAttr[name="r"](%1079)
  %1085 : Tensor = prim::GetAttr[name="v"](%1079)
  %1086 : Tensor = prim::GetAttr[name="k"](%1079)
  %1087 : Tensor = prim::GetAttr[name="q"](%1079)
  %1088 : Tensor[] = prim::ListConstruct(%1074, %1087), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1088), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1090 : Tensor[] = prim::ListConstruct(%1074, %1086), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1091 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1090), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1092 : Tensor[] = prim::ListConstruct(%1074, %1085), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1093 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1092), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%1075, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1095 : Tensor[] = prim::ListConstruct(%r.12, %1084), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1096 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1095), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1097 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1083, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1098 : Tensor[] = prim::ListConstruct(%1097, %1091), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1098), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1100 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1082, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1101 : Tensor[] = prim::ListConstruct(%1100, %1096), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1101), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1103 : int = aten::size(%ac.11, %59), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1104 : int = aten::size(%x.41, %65), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1105 : int = aten::size(%x.41, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1106 : int = aten::size(%x.41, %60), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1107 : int = aten::size(%x.41, %59), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1108 : Long() = prim::NumToTensor(%1107), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1109 : int[] = prim::ListConstruct(%1104, %1105, %1107, %1106), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1109), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1111 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1112 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1111, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1113 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1112, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1113, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1115 : Long() = aten::sub(%1108, %48, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1116 : int = aten::Int(%1115), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1117 : int[] = prim::ListConstruct(%1104, %1105, %1106, %1116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1119 : Long(13:1) = aten::arange(%1103, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %59, %1119), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1122 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1121, %63, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1122, %42), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1124 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1125 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1126 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1125, %40), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %1126, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %59, %56), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %53, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1130 : Tensor[] = prim::ListConstruct(%1129, %1093), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1131 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1130), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1132 : Tensor[] = prim::ListConstruct(%1131, %1081), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1132), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %53, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.11, %1074, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1136 : Tensor = prim::GetAttr[name="bias"](%1080)
  %1137 : Tensor = prim::GetAttr[name="weight"](%1080)
  %1138 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1138, %1137, %1136, %36, %37), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1140 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1141 : Float(13:17408, 17:1024, 1024:1), %1142 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1140)
  %1143 : __torch__.torch.nn.modules.normalization.___torch_mangle_41836.LayerNorm = prim::GetAttr[name="layer_norm"](%1078)
  %1144 : __torch__.torch.nn.modules.linear.___torch_mangle_41838.Linear = prim::GetAttr[name="layer_2"](%1078)
  %1145 : __torch__.torch.nn.modules.linear.___torch_mangle_41837.Linear = prim::GetAttr[name="layer_1"](%1078)
  %1146 : Tensor = prim::GetAttr[name="bias"](%1145)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1145)
  %1148 : Float(1024:1, 4096:1024) = aten::t(%1147), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1141, %1148), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %1146, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %53, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%1144)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1144)
  %1155 : Float(4096:1, 1024:4096) = aten::t(%1154), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.99, %1155), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %1153, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.100, %53, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.33, %1141, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1160 : Tensor = prim::GetAttr[name="bias"](%1143)
  %1161 : Tensor = prim::GetAttr[name="weight"](%1143)
  %1162 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %1162, %1161, %1160, %36, %37), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1164 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1142)
  %1165 : Float(13:17408, 17:1024, 1024:1), %1166 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1164)
  %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1165, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1168 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1169 : __torch__.transformers.modeling_xlnet.___torch_mangle_41850.XLNetFeedForward = prim::GetAttr[name="ff"](%91)
  %1170 : __torch__.transformers.modeling_xlnet.___torch_mangle_41845.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%91)
  %1171 : __torch__.torch.nn.modules.normalization.___torch_mangle_41843.LayerNorm = prim::GetAttr[name="layer_norm"](%1170)
  %1172 : Tensor = prim::GetAttr[name="o"](%1170)
  %1173 : Tensor = prim::GetAttr[name="r_r_bias"](%1170)
  %1174 : Tensor = prim::GetAttr[name="r_w_bias"](%1170)
  %1175 : Tensor = prim::GetAttr[name="r"](%1170)
  %1176 : Tensor = prim::GetAttr[name="v"](%1170)
  %1177 : Tensor = prim::GetAttr[name="k"](%1170)
  %1178 : Tensor = prim::GetAttr[name="q"](%1170)
  %1179 : Tensor[] = prim::ListConstruct(%1165, %1178), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1179), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1181 : Tensor[] = prim::ListConstruct(%1165, %1177), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1182 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1181), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1183 : Tensor[] = prim::ListConstruct(%1165, %1176), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1184 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1183), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%1166, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1186 : Tensor[] = prim::ListConstruct(%r.13, %1175), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1187 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1186), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1188 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1174, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1189 : Tensor[] = prim::ListConstruct(%1188, %1182), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1189), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1191 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1173, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1192 : Tensor[] = prim::ListConstruct(%1191, %1187), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1192), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1194 : int = aten::size(%ac.12, %59), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1195 : int = aten::size(%x.45, %65), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1196 : int = aten::size(%x.45, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1197 : int = aten::size(%x.45, %60), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1198 : int = aten::size(%x.45, %59), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1199 : Long() = prim::NumToTensor(%1198), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1200 : int[] = prim::ListConstruct(%1195, %1196, %1198, %1197), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1200), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1202 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1203 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1202, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1204 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1203, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1204, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1206 : Long() = aten::sub(%1199, %48, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1207 : int = aten::Int(%1206), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1208 : int[] = prim::ListConstruct(%1195, %1196, %1197, %1207), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1208), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1210 : Long(13:1) = aten::arange(%1194, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %59, %1210), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1212 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1213 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1212, %63, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1213, %42), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1215 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1216 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1215), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1217 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1216, %40), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %1217, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %59, %56), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1220 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %53, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1221 : Tensor[] = prim::ListConstruct(%1220, %1184), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1222 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1221), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1223 : Tensor[] = prim::ListConstruct(%1222, %1172), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1223), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.104, %53, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.12, %1165, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1227 : Tensor = prim::GetAttr[name="bias"](%1171)
  %1228 : Tensor = prim::GetAttr[name="weight"](%1171)
  %1229 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %1229, %1228, %1227, %36, %37), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1231 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1232 : Float(13:17408, 17:1024, 1024:1), %1233 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1231)
  %1234 : __torch__.torch.nn.modules.normalization.___torch_mangle_41846.LayerNorm = prim::GetAttr[name="layer_norm"](%1169)
  %1235 : __torch__.torch.nn.modules.linear.___torch_mangle_41848.Linear = prim::GetAttr[name="layer_2"](%1169)
  %1236 : __torch__.torch.nn.modules.linear.___torch_mangle_41847.Linear = prim::GetAttr[name="layer_1"](%1169)
  %1237 : Tensor = prim::GetAttr[name="bias"](%1236)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1236)
  %1239 : Float(1024:1, 4096:1024) = aten::t(%1238), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1232, %1239), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %1237, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %53, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1244 : Tensor = prim::GetAttr[name="bias"](%1235)
  %1245 : Tensor = prim::GetAttr[name="weight"](%1235)
  %1246 : Float(4096:1, 1024:4096) = aten::t(%1245), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.108, %1246), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %1244, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.109, %53, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.36, %1232, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1251 : Tensor = prim::GetAttr[name="bias"](%1234)
  %1252 : Tensor = prim::GetAttr[name="weight"](%1234)
  %1253 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %1253, %1252, %1251, %36, %37), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1255 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1233)
  %1256 : Float(13:17408, 17:1024, 1024:1), %1257 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1255)
  %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1256, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1259 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1260 : __torch__.transformers.modeling_xlnet.___torch_mangle_41860.XLNetFeedForward = prim::GetAttr[name="ff"](%89)
  %1261 : __torch__.transformers.modeling_xlnet.___torch_mangle_41855.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%89)
  %1262 : __torch__.torch.nn.modules.normalization.___torch_mangle_41853.LayerNorm = prim::GetAttr[name="layer_norm"](%1261)
  %1263 : Tensor = prim::GetAttr[name="o"](%1261)
  %1264 : Tensor = prim::GetAttr[name="r_r_bias"](%1261)
  %1265 : Tensor = prim::GetAttr[name="r_w_bias"](%1261)
  %1266 : Tensor = prim::GetAttr[name="r"](%1261)
  %1267 : Tensor = prim::GetAttr[name="v"](%1261)
  %1268 : Tensor = prim::GetAttr[name="k"](%1261)
  %1269 : Tensor = prim::GetAttr[name="q"](%1261)
  %1270 : Tensor[] = prim::ListConstruct(%1256, %1269), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1270), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1272 : Tensor[] = prim::ListConstruct(%1256, %1268), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1273 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1272), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1274 : Tensor[] = prim::ListConstruct(%1256, %1267), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1275 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1274), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%1257, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1277 : Tensor[] = prim::ListConstruct(%r.14, %1266), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1278 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1277), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1279 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1265, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1280 : Tensor[] = prim::ListConstruct(%1279, %1273), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1280), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1282 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1264, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1283 : Tensor[] = prim::ListConstruct(%1282, %1278), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1283), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1285 : int = aten::size(%ac.13, %59), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1286 : int = aten::size(%x.49, %65), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1287 : int = aten::size(%x.49, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1288 : int = aten::size(%x.49, %60), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1289 : int = aten::size(%x.49, %59), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1290 : Long() = prim::NumToTensor(%1289), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1291 : int[] = prim::ListConstruct(%1286, %1287, %1289, %1288), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1291), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1293 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1294 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1293, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1295 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1294, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1295, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1297 : Long() = aten::sub(%1290, %48, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1298 : int = aten::Int(%1297), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1299 : int[] = prim::ListConstruct(%1286, %1287, %1288, %1298), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1299), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1301 : Long(13:1) = aten::arange(%1285, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %59, %1301), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1303 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1304 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1303, %63, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1304, %42), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1306 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1307 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1306), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1308 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1307, %40), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %1308, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %59, %56), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1311 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %53, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1312 : Tensor[] = prim::ListConstruct(%1311, %1275), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1313 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1312), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1314 : Tensor[] = prim::ListConstruct(%1313, %1263), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1314), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.113, %53, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.13, %1256, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1318 : Tensor = prim::GetAttr[name="bias"](%1262)
  %1319 : Tensor = prim::GetAttr[name="weight"](%1262)
  %1320 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %1320, %1319, %1318, %36, %37), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1322 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1323 : Float(13:17408, 17:1024, 1024:1), %1324 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1322)
  %1325 : __torch__.torch.nn.modules.normalization.___torch_mangle_41856.LayerNorm = prim::GetAttr[name="layer_norm"](%1260)
  %1326 : __torch__.torch.nn.modules.linear.___torch_mangle_41858.Linear = prim::GetAttr[name="layer_2"](%1260)
  %1327 : __torch__.torch.nn.modules.linear.___torch_mangle_41857.Linear = prim::GetAttr[name="layer_1"](%1260)
  %1328 : Tensor = prim::GetAttr[name="bias"](%1327)
  %1329 : Tensor = prim::GetAttr[name="weight"](%1327)
  %1330 : Float(1024:1, 4096:1024) = aten::t(%1329), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1323, %1330), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %1328, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.115), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %53, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1335 : Tensor = prim::GetAttr[name="bias"](%1326)
  %1336 : Tensor = prim::GetAttr[name="weight"](%1326)
  %1337 : Float(4096:1, 1024:4096) = aten::t(%1336), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.117, %1337), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %1335, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.118, %53, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.39, %1323, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1342 : Tensor = prim::GetAttr[name="bias"](%1325)
  %1343 : Tensor = prim::GetAttr[name="weight"](%1325)
  %1344 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %1344, %1343, %1342, %36, %37), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1346 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1324)
  %1347 : Float(13:17408, 17:1024, 1024:1), %1348 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1346)
  %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1347, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1350 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1351 : __torch__.transformers.modeling_xlnet.___torch_mangle_41870.XLNetFeedForward = prim::GetAttr[name="ff"](%87)
  %1352 : __torch__.transformers.modeling_xlnet.___torch_mangle_41865.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%87)
  %1353 : __torch__.torch.nn.modules.normalization.___torch_mangle_41863.LayerNorm = prim::GetAttr[name="layer_norm"](%1352)
  %1354 : Tensor = prim::GetAttr[name="o"](%1352)
  %1355 : Tensor = prim::GetAttr[name="r_r_bias"](%1352)
  %1356 : Tensor = prim::GetAttr[name="r_w_bias"](%1352)
  %1357 : Tensor = prim::GetAttr[name="r"](%1352)
  %1358 : Tensor = prim::GetAttr[name="v"](%1352)
  %1359 : Tensor = prim::GetAttr[name="k"](%1352)
  %1360 : Tensor = prim::GetAttr[name="q"](%1352)
  %1361 : Tensor[] = prim::ListConstruct(%1347, %1360), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1361), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1363 : Tensor[] = prim::ListConstruct(%1347, %1359), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1364 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1363), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1365 : Tensor[] = prim::ListConstruct(%1347, %1358), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1366 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1365), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%1348, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1368 : Tensor[] = prim::ListConstruct(%r.15, %1357), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1369 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1368), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1370 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1356, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1371 : Tensor[] = prim::ListConstruct(%1370, %1364), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1371), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1373 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1355, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1374 : Tensor[] = prim::ListConstruct(%1373, %1369), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1374), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1376 : int = aten::size(%ac.14, %59), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1377 : int = aten::size(%x.53, %65), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1378 : int = aten::size(%x.53, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1379 : int = aten::size(%x.53, %60), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1380 : int = aten::size(%x.53, %59), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1381 : Long() = prim::NumToTensor(%1380), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1382 : int[] = prim::ListConstruct(%1377, %1378, %1380, %1379), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1382), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1384 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1385 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1384, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1386 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1385, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1386, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1388 : Long() = aten::sub(%1381, %48, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1389 : int = aten::Int(%1388), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1390 : int[] = prim::ListConstruct(%1377, %1378, %1379, %1389), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1390), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1392 : Long(13:1) = aten::arange(%1376, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %59, %1392), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1394 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1395 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1394, %63, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1395, %42), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1397 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1398 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1397), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1399 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1398, %40), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %1399, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %59, %56), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1402 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %53, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1403 : Tensor[] = prim::ListConstruct(%1402, %1366), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1404 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1403), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1405 : Tensor[] = prim::ListConstruct(%1404, %1354), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1405), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.122, %53, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.14, %1347, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1409 : Tensor = prim::GetAttr[name="bias"](%1353)
  %1410 : Tensor = prim::GetAttr[name="weight"](%1353)
  %1411 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %1411, %1410, %1409, %36, %37), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1413 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1414 : Float(13:17408, 17:1024, 1024:1), %1415 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1413)
  %1416 : __torch__.torch.nn.modules.normalization.___torch_mangle_41866.LayerNorm = prim::GetAttr[name="layer_norm"](%1351)
  %1417 : __torch__.torch.nn.modules.linear.___torch_mangle_41868.Linear = prim::GetAttr[name="layer_2"](%1351)
  %1418 : __torch__.torch.nn.modules.linear.___torch_mangle_41867.Linear = prim::GetAttr[name="layer_1"](%1351)
  %1419 : Tensor = prim::GetAttr[name="bias"](%1418)
  %1420 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1421 : Float(1024:1, 4096:1024) = aten::t(%1420), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1414, %1421), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %1419, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %53, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1426 : Tensor = prim::GetAttr[name="bias"](%1417)
  %1427 : Tensor = prim::GetAttr[name="weight"](%1417)
  %1428 : Float(4096:1, 1024:4096) = aten::t(%1427), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.126, %1428), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %1426, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.127, %53, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.42, %1414, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1433 : Tensor = prim::GetAttr[name="bias"](%1416)
  %1434 : Tensor = prim::GetAttr[name="weight"](%1416)
  %1435 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %1435, %1434, %1433, %36, %37), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1437 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1415)
  %1438 : Float(13:17408, 17:1024, 1024:1), %1439 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1437)
  %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1438, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1441 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1442 : __torch__.transformers.modeling_xlnet.___torch_mangle_41880.XLNetFeedForward = prim::GetAttr[name="ff"](%85)
  %1443 : __torch__.transformers.modeling_xlnet.___torch_mangle_41875.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%85)
  %1444 : __torch__.torch.nn.modules.normalization.___torch_mangle_41873.LayerNorm = prim::GetAttr[name="layer_norm"](%1443)
  %1445 : Tensor = prim::GetAttr[name="o"](%1443)
  %1446 : Tensor = prim::GetAttr[name="r_r_bias"](%1443)
  %1447 : Tensor = prim::GetAttr[name="r_w_bias"](%1443)
  %1448 : Tensor = prim::GetAttr[name="r"](%1443)
  %1449 : Tensor = prim::GetAttr[name="v"](%1443)
  %1450 : Tensor = prim::GetAttr[name="k"](%1443)
  %1451 : Tensor = prim::GetAttr[name="q"](%1443)
  %1452 : Tensor[] = prim::ListConstruct(%1438, %1451), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1452), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1454 : Tensor[] = prim::ListConstruct(%1438, %1450), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1455 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1454), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1456 : Tensor[] = prim::ListConstruct(%1438, %1449), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1457 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1456), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%1439, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1459 : Tensor[] = prim::ListConstruct(%r.16, %1448), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1460 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1459), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1461 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1447, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1462 : Tensor[] = prim::ListConstruct(%1461, %1455), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1462), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1464 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1446, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %1465 : Tensor[] = prim::ListConstruct(%1464, %1460), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1465), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1467 : int = aten::size(%ac.15, %59), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1468 : int = aten::size(%x.57, %65), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1469 : int = aten::size(%x.57, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1470 : int = aten::size(%x.57, %60), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1471 : int = aten::size(%x.57, %59), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1472 : Long() = prim::NumToTensor(%1471), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1473 : int[] = prim::ListConstruct(%1468, %1469, %1471, %1470), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %1473), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %1475 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1476 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1475, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1477 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1476, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1477, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1479 : Long() = aten::sub(%1472, %48, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1480 : int = aten::Int(%1479), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1481 : int[] = prim::ListConstruct(%1468, %1469, %1470, %1480), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %1481), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1483 : Long(13:1) = aten::arange(%1467, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %59, %1483), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1485 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1486 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1485, %63, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1486, %42), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1488 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1489 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1488), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1490 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1489, %40), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %1490, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %59, %56), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %1493 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %53, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1494 : Tensor[] = prim::ListConstruct(%1493, %1457), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1495 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1494), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1496 : Tensor[] = prim::ListConstruct(%1495, %1445), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1496), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.131, %53, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.15, %1438, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %1500 : Tensor = prim::GetAttr[name="bias"](%1444)
  %1501 : Tensor = prim::GetAttr[name="weight"](%1444)
  %1502 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %1502, %1501, %1500, %36, %37), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1504 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %1505 : Float(13:17408, 17:1024, 1024:1), %1506 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1504)
  %1507 : __torch__.torch.nn.modules.normalization.___torch_mangle_41876.LayerNorm = prim::GetAttr[name="layer_norm"](%1442)
  %1508 : __torch__.torch.nn.modules.linear.___torch_mangle_41878.Linear = prim::GetAttr[name="layer_2"](%1442)
  %1509 : __torch__.torch.nn.modules.linear.___torch_mangle_41877.Linear = prim::GetAttr[name="layer_1"](%1442)
  %1510 : Tensor = prim::GetAttr[name="bias"](%1509)
  %1511 : Tensor = prim::GetAttr[name="weight"](%1509)
  %1512 : Float(1024:1, 4096:1024) = aten::t(%1511), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1505, %1512), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %1510, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %53, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %1517 : Tensor = prim::GetAttr[name="bias"](%1508)
  %1518 : Tensor = prim::GetAttr[name="weight"](%1508)
  %1519 : Float(4096:1, 1024:4096) = aten::t(%1518), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.135, %1519), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %1517, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.136, %53, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.45, %1505, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %1524 : Tensor = prim::GetAttr[name="bias"](%1507)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1507)
  %1526 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %1526, %1525, %1524, %36, %37), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %1528 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %1506)
  %1529 : Float(13:17408, 17:1024, 1024:1), %1530 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1528)
  %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1529, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1532 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1533 : __torch__.transformers.modeling_xlnet.___torch_mangle_41890.XLNetFeedForward = prim::GetAttr[name="ff"](%83)
  %1534 : __torch__.transformers.modeling_xlnet.___torch_mangle_41885.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%83)
  %1535 : __torch__.torch.nn.modules.normalization.___torch_mangle_41883.LayerNorm = prim::GetAttr[name="layer_norm"](%1534)
  %1536 : Tensor = prim::GetAttr[name="o"](%1534)
  %1537 : Tensor = prim::GetAttr[name="r_r_bias"](%1534)
  %1538 : Tensor = prim::GetAttr[name="r_w_bias"](%1534)
  %1539 : Tensor = prim::GetAttr[name="r"](%1534)
  %1540 : Tensor = prim::GetAttr[name="v"](%1534)
  %1541 : Tensor = prim::GetAttr[name="k"](%1534)
  %1542 : Tensor = prim::GetAttr[name="q"](%1534)
  %1543 : Tensor[] = prim::ListConstruct(%1529, %1542), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1543), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1545 : Tensor[] = prim::ListConstruct(%1529, %1541), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1546 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1545), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1547 : Tensor[] = prim::ListConstruct(%1529, %1540), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1548 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1547), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%1530, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %1550 : Tensor[] = prim::ListConstruct(%r.17, %1539), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1551 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1550), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1552 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1538, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %1553 : Tensor[] = prim::ListConstruct(%1552, %1546), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1553), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1555 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1537, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %1556 : Tensor[] = prim::ListConstruct(%1555, %1551), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1556), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1558 : int = aten::size(%ac.16, %59), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %1559 : int = aten::size(%x.61, %65), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1560 : int = aten::size(%x.61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1561 : int = aten::size(%x.61, %60), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1562 : int = aten::size(%x.61, %59), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1563 : Long() = prim::NumToTensor(%1562), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1564 : int[] = prim::ListConstruct(%1559, %1560, %1562, %1561), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %1564), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %1566 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1567 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1566, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1568 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1567, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1568, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1570 : Long() = aten::sub(%1563, %48, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1571 : int = aten::Int(%1570), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1572 : int[] = prim::ListConstruct(%1559, %1560, %1561, %1571), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %1572), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1574 : Long(13:1) = aten::arange(%1558, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %59, %1574), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %1576 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1577 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1576, %63, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1577, %42), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1579 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1580 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1579), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1581 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1580, %40), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %1581, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %59, %56), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %1584 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %53, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %1585 : Tensor[] = prim::ListConstruct(%1584, %1548), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1586 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1585), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1587 : Tensor[] = prim::ListConstruct(%1586, %1536), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1587), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.140, %53, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.16, %1529, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %1591 : Tensor = prim::GetAttr[name="bias"](%1535)
  %1592 : Tensor = prim::GetAttr[name="weight"](%1535)
  %1593 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %1593, %1592, %1591, %36, %37), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1595 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %1596 : Float(13:17408, 17:1024, 1024:1), %1597 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1595)
  %1598 : __torch__.torch.nn.modules.normalization.___torch_mangle_41886.LayerNorm = prim::GetAttr[name="layer_norm"](%1533)
  %1599 : __torch__.torch.nn.modules.linear.___torch_mangle_41888.Linear = prim::GetAttr[name="layer_2"](%1533)
  %1600 : __torch__.torch.nn.modules.linear.___torch_mangle_41887.Linear = prim::GetAttr[name="layer_1"](%1533)
  %1601 : Tensor = prim::GetAttr[name="bias"](%1600)
  %1602 : Tensor = prim::GetAttr[name="weight"](%1600)
  %1603 : Float(1024:1, 4096:1024) = aten::t(%1602), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1596, %1603), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %1601, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.142), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %53, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %1608 : Tensor = prim::GetAttr[name="bias"](%1599)
  %1609 : Tensor = prim::GetAttr[name="weight"](%1599)
  %1610 : Float(4096:1, 1024:4096) = aten::t(%1609), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1610), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %1608, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.145, %53, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.48, %1596, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %1615 : Tensor = prim::GetAttr[name="bias"](%1598)
  %1616 : Tensor = prim::GetAttr[name="weight"](%1598)
  %1617 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %1617, %1616, %1615, %36, %37), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %1619 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %1597)
  %1620 : Float(13:17408, 17:1024, 1024:1), %1621 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1619)
  %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1620, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1623 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1624 : __torch__.transformers.modeling_xlnet.___torch_mangle_41900.XLNetFeedForward = prim::GetAttr[name="ff"](%81)
  %1625 : __torch__.transformers.modeling_xlnet.___torch_mangle_41895.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%81)
  %1626 : __torch__.torch.nn.modules.normalization.___torch_mangle_41893.LayerNorm = prim::GetAttr[name="layer_norm"](%1625)
  %1627 : Tensor = prim::GetAttr[name="o"](%1625)
  %1628 : Tensor = prim::GetAttr[name="r_r_bias"](%1625)
  %1629 : Tensor = prim::GetAttr[name="r_w_bias"](%1625)
  %1630 : Tensor = prim::GetAttr[name="r"](%1625)
  %1631 : Tensor = prim::GetAttr[name="v"](%1625)
  %1632 : Tensor = prim::GetAttr[name="k"](%1625)
  %1633 : Tensor = prim::GetAttr[name="q"](%1625)
  %1634 : Tensor[] = prim::ListConstruct(%1620, %1633), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1634), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1636 : Tensor[] = prim::ListConstruct(%1620, %1632), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1637 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1636), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1638 : Tensor[] = prim::ListConstruct(%1620, %1631), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1639 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1638), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%1621, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %1641 : Tensor[] = prim::ListConstruct(%r.18, %1630), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1642 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1641), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1643 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1629, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %1644 : Tensor[] = prim::ListConstruct(%1643, %1637), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1644), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1646 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1628, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %1647 : Tensor[] = prim::ListConstruct(%1646, %1642), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1647), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1649 : int = aten::size(%ac.17, %59), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %1650 : int = aten::size(%x.65, %65), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1651 : int = aten::size(%x.65, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1652 : int = aten::size(%x.65, %60), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1653 : int = aten::size(%x.65, %59), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1654 : Long() = prim::NumToTensor(%1653), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1655 : int[] = prim::ListConstruct(%1650, %1651, %1653, %1652), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %1655), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %1657 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1658 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1657, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1659 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1658, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1659, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1661 : Long() = aten::sub(%1654, %48, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1662 : int = aten::Int(%1661), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1663 : int[] = prim::ListConstruct(%1650, %1651, %1652, %1662), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %1663), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1665 : Long(13:1) = aten::arange(%1649, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %59, %1665), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %1667 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1668 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1667, %63, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1668, %42), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1670 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1671 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1670), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1672 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1671, %40), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %1672, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %59, %56), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %1675 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %53, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %1676 : Tensor[] = prim::ListConstruct(%1675, %1639), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1677 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1676), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1678 : Tensor[] = prim::ListConstruct(%1677, %1627), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1678), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.149, %53, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.17, %1620, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %1682 : Tensor = prim::GetAttr[name="bias"](%1626)
  %1683 : Tensor = prim::GetAttr[name="weight"](%1626)
  %1684 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %1684, %1683, %1682, %36, %37), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1686 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %1687 : Float(13:17408, 17:1024, 1024:1), %1688 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1686)
  %1689 : __torch__.torch.nn.modules.normalization.___torch_mangle_41896.LayerNorm = prim::GetAttr[name="layer_norm"](%1624)
  %1690 : __torch__.torch.nn.modules.linear.___torch_mangle_41898.Linear = prim::GetAttr[name="layer_2"](%1624)
  %1691 : __torch__.torch.nn.modules.linear.___torch_mangle_41897.Linear = prim::GetAttr[name="layer_1"](%1624)
  %1692 : Tensor = prim::GetAttr[name="bias"](%1691)
  %1693 : Tensor = prim::GetAttr[name="weight"](%1691)
  %1694 : Float(1024:1, 4096:1024) = aten::t(%1693), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1687, %1694), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %1692, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %53, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %1699 : Tensor = prim::GetAttr[name="bias"](%1690)
  %1700 : Tensor = prim::GetAttr[name="weight"](%1690)
  %1701 : Float(4096:1, 1024:4096) = aten::t(%1700), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1701), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %1699, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %53, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.51, %1687, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %1706 : Tensor = prim::GetAttr[name="bias"](%1689)
  %1707 : Tensor = prim::GetAttr[name="weight"](%1689)
  %1708 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1708, %1707, %1706, %36, %37), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %1710 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %1688)
  %1711 : Float(13:17408, 17:1024, 1024:1), %1712 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1710)
  %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1711, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1714 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1715 : __torch__.transformers.modeling_xlnet.___torch_mangle_41910.XLNetFeedForward = prim::GetAttr[name="ff"](%79)
  %1716 : __torch__.transformers.modeling_xlnet.___torch_mangle_41905.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%79)
  %1717 : __torch__.torch.nn.modules.normalization.___torch_mangle_41903.LayerNorm = prim::GetAttr[name="layer_norm"](%1716)
  %1718 : Tensor = prim::GetAttr[name="o"](%1716)
  %1719 : Tensor = prim::GetAttr[name="r_r_bias"](%1716)
  %1720 : Tensor = prim::GetAttr[name="r_w_bias"](%1716)
  %1721 : Tensor = prim::GetAttr[name="r"](%1716)
  %1722 : Tensor = prim::GetAttr[name="v"](%1716)
  %1723 : Tensor = prim::GetAttr[name="k"](%1716)
  %1724 : Tensor = prim::GetAttr[name="q"](%1716)
  %1725 : Tensor[] = prim::ListConstruct(%1711, %1724), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1725), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1727 : Tensor[] = prim::ListConstruct(%1711, %1723), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1728 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1727), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1729 : Tensor[] = prim::ListConstruct(%1711, %1722), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1730 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1729), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%1712, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %1732 : Tensor[] = prim::ListConstruct(%r.19, %1721), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1733 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1732), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1734 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1720, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %1735 : Tensor[] = prim::ListConstruct(%1734, %1728), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1735), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1737 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1719, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %1738 : Tensor[] = prim::ListConstruct(%1737, %1733), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1738), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1740 : int = aten::size(%ac.18, %59), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %1741 : int = aten::size(%x.69, %65), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1742 : int = aten::size(%x.69, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1743 : int = aten::size(%x.69, %60), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1744 : int = aten::size(%x.69, %59), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1745 : Long() = prim::NumToTensor(%1744), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1746 : int[] = prim::ListConstruct(%1741, %1742, %1744, %1743), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %1746), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %1748 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1749 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1748, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1750 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1749, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1750, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1752 : Long() = aten::sub(%1745, %48, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1753 : int = aten::Int(%1752), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1754 : int[] = prim::ListConstruct(%1741, %1742, %1743, %1753), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %1754), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1756 : Long(13:1) = aten::arange(%1740, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %59, %1756), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %1758 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1759 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1758, %63, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1759, %42), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1761 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1762 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1761), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1763 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1762, %40), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %1763, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %59, %56), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %1766 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %53, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %1767 : Tensor[] = prim::ListConstruct(%1766, %1730), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1768 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1767), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1769 : Tensor[] = prim::ListConstruct(%1768, %1718), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1769), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.158, %53, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.18, %1711, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %1773 : Tensor = prim::GetAttr[name="bias"](%1717)
  %1774 : Tensor = prim::GetAttr[name="weight"](%1717)
  %1775 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %1775, %1774, %1773, %36, %37), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1777 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %1778 : Float(13:17408, 17:1024, 1024:1), %1779 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1777)
  %1780 : __torch__.torch.nn.modules.normalization.___torch_mangle_41906.LayerNorm = prim::GetAttr[name="layer_norm"](%1715)
  %1781 : __torch__.torch.nn.modules.linear.___torch_mangle_41908.Linear = prim::GetAttr[name="layer_2"](%1715)
  %1782 : __torch__.torch.nn.modules.linear.___torch_mangle_41907.Linear = prim::GetAttr[name="layer_1"](%1715)
  %1783 : Tensor = prim::GetAttr[name="bias"](%1782)
  %1784 : Tensor = prim::GetAttr[name="weight"](%1782)
  %1785 : Float(1024:1, 4096:1024) = aten::t(%1784), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1778, %1785), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %1783, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.160), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %53, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %1790 : Tensor = prim::GetAttr[name="bias"](%1781)
  %1791 : Tensor = prim::GetAttr[name="weight"](%1781)
  %1792 : Float(4096:1, 1024:4096) = aten::t(%1791), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.162, %1792), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %1790, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.163, %53, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.54, %1778, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %1797 : Tensor = prim::GetAttr[name="bias"](%1780)
  %1798 : Tensor = prim::GetAttr[name="weight"](%1780)
  %1799 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %1799, %1798, %1797, %36, %37), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %1801 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %1779)
  %1802 : Float(13:17408, 17:1024, 1024:1), %1803 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1801)
  %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1802, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1805 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1806 : __torch__.transformers.modeling_xlnet.___torch_mangle_41920.XLNetFeedForward = prim::GetAttr[name="ff"](%77)
  %1807 : __torch__.transformers.modeling_xlnet.___torch_mangle_41915.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%77)
  %1808 : __torch__.torch.nn.modules.normalization.___torch_mangle_41913.LayerNorm = prim::GetAttr[name="layer_norm"](%1807)
  %1809 : Tensor = prim::GetAttr[name="o"](%1807)
  %1810 : Tensor = prim::GetAttr[name="r_r_bias"](%1807)
  %1811 : Tensor = prim::GetAttr[name="r_w_bias"](%1807)
  %1812 : Tensor = prim::GetAttr[name="r"](%1807)
  %1813 : Tensor = prim::GetAttr[name="v"](%1807)
  %1814 : Tensor = prim::GetAttr[name="k"](%1807)
  %1815 : Tensor = prim::GetAttr[name="q"](%1807)
  %1816 : Tensor[] = prim::ListConstruct(%1802, %1815), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1816), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1818 : Tensor[] = prim::ListConstruct(%1802, %1814), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1819 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1818), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1820 : Tensor[] = prim::ListConstruct(%1802, %1813), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1821 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1820), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%1803, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %1823 : Tensor[] = prim::ListConstruct(%r.20, %1812), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1824 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1823), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1825 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1811, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %1826 : Tensor[] = prim::ListConstruct(%1825, %1819), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1826), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1828 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1810, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %1829 : Tensor[] = prim::ListConstruct(%1828, %1824), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1829), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1831 : int = aten::size(%ac.19, %59), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %1832 : int = aten::size(%x.73, %65), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1833 : int = aten::size(%x.73, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1834 : int = aten::size(%x.73, %60), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1835 : int = aten::size(%x.73, %59), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1836 : Long() = prim::NumToTensor(%1835), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1837 : int[] = prim::ListConstruct(%1832, %1833, %1835, %1834), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %1837), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %1839 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1840 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1839, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1841 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1840, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1841, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1843 : Long() = aten::sub(%1836, %48, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1844 : int = aten::Int(%1843), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1845 : int[] = prim::ListConstruct(%1832, %1833, %1834, %1844), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %1845), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1847 : Long(13:1) = aten::arange(%1831, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %59, %1847), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %1849 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1850 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1849, %63, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1850, %42), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1852 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1853 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1852), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1854 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1853, %40), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %1854, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %59, %56), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %1857 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %53, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %1858 : Tensor[] = prim::ListConstruct(%1857, %1821), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1859 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1858), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1860 : Tensor[] = prim::ListConstruct(%1859, %1809), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1860), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.167, %53, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.19, %1802, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %1864 : Tensor = prim::GetAttr[name="bias"](%1808)
  %1865 : Tensor = prim::GetAttr[name="weight"](%1808)
  %1866 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %1866, %1865, %1864, %36, %37), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1868 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %1869 : Float(13:17408, 17:1024, 1024:1), %1870 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1868)
  %1871 : __torch__.torch.nn.modules.normalization.___torch_mangle_41916.LayerNorm = prim::GetAttr[name="layer_norm"](%1806)
  %1872 : __torch__.torch.nn.modules.linear.___torch_mangle_41918.Linear = prim::GetAttr[name="layer_2"](%1806)
  %1873 : __torch__.torch.nn.modules.linear.___torch_mangle_41917.Linear = prim::GetAttr[name="layer_1"](%1806)
  %1874 : Tensor = prim::GetAttr[name="bias"](%1873)
  %1875 : Tensor = prim::GetAttr[name="weight"](%1873)
  %1876 : Float(1024:1, 4096:1024) = aten::t(%1875), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1869, %1876), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %1874, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.169), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %53, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %1881 : Tensor = prim::GetAttr[name="bias"](%1872)
  %1882 : Tensor = prim::GetAttr[name="weight"](%1872)
  %1883 : Float(4096:1, 1024:4096) = aten::t(%1882), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.171, %1883), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %1881, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.172, %53, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.57, %1869, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %1888 : Tensor = prim::GetAttr[name="bias"](%1871)
  %1889 : Tensor = prim::GetAttr[name="weight"](%1871)
  %1890 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %1890, %1889, %1888, %36, %37), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %1892 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %1870)
  %1893 : Float(13:17408, 17:1024, 1024:1), %1894 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1892)
  %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1893, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1896 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1897 : __torch__.transformers.modeling_xlnet.___torch_mangle_41930.XLNetFeedForward = prim::GetAttr[name="ff"](%75)
  %1898 : __torch__.transformers.modeling_xlnet.___torch_mangle_41925.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%75)
  %1899 : __torch__.torch.nn.modules.normalization.___torch_mangle_41923.LayerNorm = prim::GetAttr[name="layer_norm"](%1898)
  %1900 : Tensor = prim::GetAttr[name="o"](%1898)
  %1901 : Tensor = prim::GetAttr[name="r_r_bias"](%1898)
  %1902 : Tensor = prim::GetAttr[name="r_w_bias"](%1898)
  %1903 : Tensor = prim::GetAttr[name="r"](%1898)
  %1904 : Tensor = prim::GetAttr[name="v"](%1898)
  %1905 : Tensor = prim::GetAttr[name="k"](%1898)
  %1906 : Tensor = prim::GetAttr[name="q"](%1898)
  %1907 : Tensor[] = prim::ListConstruct(%1893, %1906), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1907), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1909 : Tensor[] = prim::ListConstruct(%1893, %1905), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1910 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1909), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1911 : Tensor[] = prim::ListConstruct(%1893, %1904), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1912 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1911), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%1894, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %1914 : Tensor[] = prim::ListConstruct(%r.21, %1903), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1915 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1914), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1916 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1902, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %1917 : Tensor[] = prim::ListConstruct(%1916, %1910), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %1917), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1919 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1901, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %1920 : Tensor[] = prim::ListConstruct(%1919, %1915), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %1920), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1922 : int = aten::size(%ac.20, %59), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %1923 : int = aten::size(%x.77, %65), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1924 : int = aten::size(%x.77, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1925 : int = aten::size(%x.77, %60), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1926 : int = aten::size(%x.77, %59), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1927 : Long() = prim::NumToTensor(%1926), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1928 : int[] = prim::ListConstruct(%1923, %1924, %1926, %1925), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %1928), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %1930 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1931 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1930, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1932 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1931, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1932, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1934 : Long() = aten::sub(%1927, %48, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1935 : int = aten::Int(%1934), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1936 : int[] = prim::ListConstruct(%1923, %1924, %1925, %1935), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %1936), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1938 : Long(13:1) = aten::arange(%1922, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %59, %1938), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %1940 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1941 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1940, %63, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1941, %42), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1943 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1944 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %1943), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1945 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1944, %40), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %1945, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %59, %56), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %1948 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %53, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %1949 : Tensor[] = prim::ListConstruct(%1948, %1912), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1950 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %1949), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1951 : Tensor[] = prim::ListConstruct(%1950, %1900), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %1951), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.176, %53, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.20, %1893, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %1955 : Tensor = prim::GetAttr[name="bias"](%1899)
  %1956 : Tensor = prim::GetAttr[name="weight"](%1899)
  %1957 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %1957, %1956, %1955, %36, %37), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1959 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %1960 : Float(13:17408, 17:1024, 1024:1), %1961 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1959)
  %1962 : __torch__.torch.nn.modules.normalization.___torch_mangle_41926.LayerNorm = prim::GetAttr[name="layer_norm"](%1897)
  %1963 : __torch__.torch.nn.modules.linear.___torch_mangle_41928.Linear = prim::GetAttr[name="layer_2"](%1897)
  %1964 : __torch__.torch.nn.modules.linear.___torch_mangle_41927.Linear = prim::GetAttr[name="layer_1"](%1897)
  %1965 : Tensor = prim::GetAttr[name="bias"](%1964)
  %1966 : Tensor = prim::GetAttr[name="weight"](%1964)
  %1967 : Float(1024:1, 4096:1024) = aten::t(%1966), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1960, %1967), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %1965, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.178), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %53, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %1972 : Tensor = prim::GetAttr[name="bias"](%1963)
  %1973 : Tensor = prim::GetAttr[name="weight"](%1963)
  %1974 : Float(4096:1, 1024:4096) = aten::t(%1973), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.180, %1974), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %1972, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.181, %53, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.60, %1960, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %1979 : Tensor = prim::GetAttr[name="bias"](%1962)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1962)
  %1981 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %1981, %1980, %1979, %36, %37), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %1983 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %1961)
  %1984 : Float(13:17408, 17:1024, 1024:1), %1985 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1983)
  %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1984, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1987 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1988 : __torch__.transformers.modeling_xlnet.___torch_mangle_41940.XLNetFeedForward = prim::GetAttr[name="ff"](%73)
  %1989 : __torch__.transformers.modeling_xlnet.___torch_mangle_41935.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%73)
  %1990 : __torch__.torch.nn.modules.normalization.___torch_mangle_41933.LayerNorm = prim::GetAttr[name="layer_norm"](%1989)
  %1991 : Tensor = prim::GetAttr[name="o"](%1989)
  %1992 : Tensor = prim::GetAttr[name="r_r_bias"](%1989)
  %1993 : Tensor = prim::GetAttr[name="r_w_bias"](%1989)
  %1994 : Tensor = prim::GetAttr[name="r"](%1989)
  %1995 : Tensor = prim::GetAttr[name="v"](%1989)
  %1996 : Tensor = prim::GetAttr[name="k"](%1989)
  %1997 : Tensor = prim::GetAttr[name="q"](%1989)
  %1998 : Tensor[] = prim::ListConstruct(%1984, %1997), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %1998), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2000 : Tensor[] = prim::ListConstruct(%1984, %1996), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2001 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2000), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2002 : Tensor[] = prim::ListConstruct(%1984, %1995), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2003 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2002), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%1985, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2005 : Tensor[] = prim::ListConstruct(%r.22, %1994), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2006 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2005), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2007 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %1993, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2008 : Tensor[] = prim::ListConstruct(%2007, %2001), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %2008), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2010 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %1992, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2011 : Tensor[] = prim::ListConstruct(%2010, %2006), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %2011), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2013 : int = aten::size(%ac.21, %59), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2014 : int = aten::size(%x.81, %65), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2015 : int = aten::size(%x.81, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2016 : int = aten::size(%x.81, %60), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2017 : int = aten::size(%x.81, %59), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2018 : Long() = prim::NumToTensor(%2017), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2019 : int[] = prim::ListConstruct(%2014, %2015, %2017, %2016), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2019), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2021 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2022 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2021, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2023 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2022, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2023, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2025 : Long() = aten::sub(%2018, %48, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2026 : int = aten::Int(%2025), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2027 : int[] = prim::ListConstruct(%2014, %2015, %2016, %2026), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2027), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2029 : Long(13:1) = aten::arange(%2013, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %59, %2029), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2031 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2032 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2031, %63, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2032, %42), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2034 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2035 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %2034), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2036 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2035, %40), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %2036, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %59, %56), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2039 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %53, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2040 : Tensor[] = prim::ListConstruct(%2039, %2003), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2041 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %2040), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2042 : Tensor[] = prim::ListConstruct(%2041, %1991), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %2042), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.185, %53, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.21, %1984, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2046 : Tensor = prim::GetAttr[name="bias"](%1990)
  %2047 : Tensor = prim::GetAttr[name="weight"](%1990)
  %2048 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %2048, %2047, %2046, %36, %37), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2050 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2051 : Float(13:17408, 17:1024, 1024:1), %2052 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2050)
  %2053 : __torch__.torch.nn.modules.normalization.___torch_mangle_41936.LayerNorm = prim::GetAttr[name="layer_norm"](%1988)
  %2054 : __torch__.torch.nn.modules.linear.___torch_mangle_41938.Linear = prim::GetAttr[name="layer_2"](%1988)
  %2055 : __torch__.torch.nn.modules.linear.___torch_mangle_41937.Linear = prim::GetAttr[name="layer_1"](%1988)
  %2056 : Tensor = prim::GetAttr[name="bias"](%2055)
  %2057 : Tensor = prim::GetAttr[name="weight"](%2055)
  %2058 : Float(1024:1, 4096:1024) = aten::t(%2057), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2051, %2058), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2056, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.187), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %53, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2063 : Tensor = prim::GetAttr[name="bias"](%2054)
  %2064 : Tensor = prim::GetAttr[name="weight"](%2054)
  %2065 : Float(4096:1, 1024:4096) = aten::t(%2064), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.189, %2065), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2063, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.190, %53, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.63, %2051, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2070 : Tensor = prim::GetAttr[name="bias"](%2053)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2053)
  %2072 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %2072, %2071, %2070, %36, %37), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2074 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2052)
  %2075 : Float(13:17408, 17:1024, 1024:1), %2076 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2074)
  %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2075, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2078 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2079 : __torch__.transformers.modeling_xlnet.___torch_mangle_41950.XLNetFeedForward = prim::GetAttr[name="ff"](%71)
  %2080 : __torch__.transformers.modeling_xlnet.___torch_mangle_41945.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%71)
  %2081 : __torch__.torch.nn.modules.normalization.___torch_mangle_41943.LayerNorm = prim::GetAttr[name="layer_norm"](%2080)
  %2082 : Tensor = prim::GetAttr[name="o"](%2080)
  %2083 : Tensor = prim::GetAttr[name="r_r_bias"](%2080)
  %2084 : Tensor = prim::GetAttr[name="r_w_bias"](%2080)
  %2085 : Tensor = prim::GetAttr[name="r"](%2080)
  %2086 : Tensor = prim::GetAttr[name="v"](%2080)
  %2087 : Tensor = prim::GetAttr[name="k"](%2080)
  %2088 : Tensor = prim::GetAttr[name="q"](%2080)
  %2089 : Tensor[] = prim::ListConstruct(%2075, %2088), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2089), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2091 : Tensor[] = prim::ListConstruct(%2075, %2087), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2092 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2091), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2093 : Tensor[] = prim::ListConstruct(%2075, %2086), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2094 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2093), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2076, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2096 : Tensor[] = prim::ListConstruct(%r.23, %2085), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2097 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2096), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2098 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2084, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2099 : Tensor[] = prim::ListConstruct(%2098, %2092), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %2099), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2101 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2083, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2102 : Tensor[] = prim::ListConstruct(%2101, %2097), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %2102), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2104 : int = aten::size(%ac.22, %59), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2105 : int = aten::size(%x.85, %65), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2106 : int = aten::size(%x.85, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2107 : int = aten::size(%x.85, %60), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2108 : int = aten::size(%x.85, %59), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2109 : Long() = prim::NumToTensor(%2108), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2110 : int[] = prim::ListConstruct(%2105, %2106, %2108, %2107), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2110), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2112 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2113 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2112, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2114 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2113, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2114, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2116 : Long() = aten::sub(%2109, %48, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2117 : int = aten::Int(%2116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2118 : int[] = prim::ListConstruct(%2105, %2106, %2107, %2117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2118), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2120 : Long(13:1) = aten::arange(%2104, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %59, %2120), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2122 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2123 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2122, %63, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2123, %42), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2125 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2126 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %2125), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2127 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2126, %40), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %2127, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %59, %56), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %53, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2131 : Tensor[] = prim::ListConstruct(%2130, %2094), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2132 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %2131), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2133 : Tensor[] = prim::ListConstruct(%2132, %2082), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %2133), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.194, %53, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.22, %2075, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2137 : Tensor = prim::GetAttr[name="bias"](%2081)
  %2138 : Tensor = prim::GetAttr[name="weight"](%2081)
  %2139 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %2139, %2138, %2137, %36, %37), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2141 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2142 : Float(13:17408, 17:1024, 1024:1), %2143 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2141)
  %2144 : __torch__.torch.nn.modules.normalization.___torch_mangle_41946.LayerNorm = prim::GetAttr[name="layer_norm"](%2079)
  %2145 : __torch__.torch.nn.modules.linear.___torch_mangle_41948.Linear = prim::GetAttr[name="layer_2"](%2079)
  %2146 : __torch__.torch.nn.modules.linear.___torch_mangle_41947.Linear = prim::GetAttr[name="layer_1"](%2079)
  %2147 : Tensor = prim::GetAttr[name="bias"](%2146)
  %2148 : Tensor = prim::GetAttr[name="weight"](%2146)
  %2149 : Float(1024:1, 4096:1024) = aten::t(%2148), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2142, %2149), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2147, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.196), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %53, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2154 : Tensor = prim::GetAttr[name="bias"](%2145)
  %2155 : Tensor = prim::GetAttr[name="weight"](%2145)
  %2156 : Float(4096:1, 1024:4096) = aten::t(%2155), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.198, %2156), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2154, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.199, %53, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.66, %2142, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2161 : Tensor = prim::GetAttr[name="bias"](%2144)
  %2162 : Tensor = prim::GetAttr[name="weight"](%2144)
  %2163 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %2163, %2162, %2161, %36, %37), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2165 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2143)
  %2166 : Float(13:17408, 17:1024, 1024:1), %2167 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2165)
  %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2166, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2169 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2170 : __torch__.transformers.modeling_xlnet.___torch_mangle_41960.XLNetFeedForward = prim::GetAttr[name="ff"](%69)
  %2171 : __torch__.transformers.modeling_xlnet.___torch_mangle_41955.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%69)
  %2172 : __torch__.torch.nn.modules.normalization.___torch_mangle_41953.LayerNorm = prim::GetAttr[name="layer_norm"](%2171)
  %2173 : Tensor = prim::GetAttr[name="o"](%2171)
  %2174 : Tensor = prim::GetAttr[name="r_r_bias"](%2171)
  %2175 : Tensor = prim::GetAttr[name="r_w_bias"](%2171)
  %2176 : Tensor = prim::GetAttr[name="r"](%2171)
  %2177 : Tensor = prim::GetAttr[name="v"](%2171)
  %2178 : Tensor = prim::GetAttr[name="k"](%2171)
  %2179 : Tensor = prim::GetAttr[name="q"](%2171)
  %2180 : Tensor[] = prim::ListConstruct(%2166, %2179), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2180), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2182 : Tensor[] = prim::ListConstruct(%2166, %2178), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2183 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2182), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2184 : Tensor[] = prim::ListConstruct(%2166, %2177), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2185 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2184), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2167, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2187 : Tensor[] = prim::ListConstruct(%r, %2176), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2188 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2187), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2189 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2175, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2190 : Tensor[] = prim::ListConstruct(%2189, %2183), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %2190), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2192 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2174, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2193 : Tensor[] = prim::ListConstruct(%2192, %2188), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %2193), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2195 : int = aten::size(%ac.23, %59), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2196 : int = aten::size(%x.89, %65), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2197 : int = aten::size(%x.89, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2198 : int = aten::size(%x.89, %60), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2199 : int = aten::size(%x.89, %59), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2200 : Long() = prim::NumToTensor(%2199), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2201 : int[] = prim::ListConstruct(%2196, %2197, %2199, %2198), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2201), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2203 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2204 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2203, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2205 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2204, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2205, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2207 : Long() = aten::sub(%2200, %48, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2208 : int = aten::Int(%2207), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2209 : int[] = prim::ListConstruct(%2196, %2197, %2198, %2208), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2209), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2211 : Long(13:1) = aten::arange(%2195, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %59, %2211), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2213 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2214 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2213, %63, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2214, %42), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2216 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2217 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %2216), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2218 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2217, %40), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %2218, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %59, %56), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2221 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %53, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2222 : Tensor[] = prim::ListConstruct(%2221, %2185), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2223 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %2222), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2224 : Tensor[] = prim::ListConstruct(%2223, %2173), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %2224), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.203, %53, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.23, %2166, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2228 : Tensor = prim::GetAttr[name="bias"](%2172)
  %2229 : Tensor = prim::GetAttr[name="weight"](%2172)
  %2230 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %2230, %2229, %2228, %36, %37), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2232 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2233 : Float(13:17408, 17:1024, 1024:1), %2234 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2232)
  %2235 : __torch__.torch.nn.modules.normalization.___torch_mangle_41956.LayerNorm = prim::GetAttr[name="layer_norm"](%2170)
  %2236 : __torch__.torch.nn.modules.linear.___torch_mangle_41958.Linear = prim::GetAttr[name="layer_2"](%2170)
  %2237 : __torch__.torch.nn.modules.linear.___torch_mangle_41957.Linear = prim::GetAttr[name="layer_1"](%2170)
  %2238 : Tensor = prim::GetAttr[name="bias"](%2237)
  %2239 : Tensor = prim::GetAttr[name="weight"](%2237)
  %2240 : Float(1024:1, 4096:1024) = aten::t(%2239), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2233, %2240), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2238, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %53, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2245 : Tensor = prim::GetAttr[name="bias"](%2236)
  %2246 : Tensor = prim::GetAttr[name="weight"](%2236)
  %2247 : Float(4096:1, 1024:4096) = aten::t(%2246), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.207, %2247), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2245, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.208, %53, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.69, %2233, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2252 : Tensor = prim::GetAttr[name="bias"](%2235)
  %2253 : Tensor = prim::GetAttr[name="weight"](%2235)
  %2254 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
  %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %2254, %2253, %2252, %36, %37), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2256 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2234)
  %2257 : Float(13:17408, 17:1024, 1024:1), %2258 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2256)
  %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2257, %65, %65, %61, %64), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2260 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2261 : __torch__.transformers.modeling_xlnet.___torch_mangle_41970.XLNetFeedForward = prim::GetAttr[name="ff"](%67)
  %2262 : __torch__.transformers.modeling_xlnet.___torch_mangle_41965.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%67)
  %2263 : __torch__.torch.nn.modules.normalization.___torch_mangle_41963.LayerNorm = prim::GetAttr[name="layer_norm"](%2262)
  %2264 : Tensor = prim::GetAttr[name="o"](%2262)
  %2265 : Tensor = prim::GetAttr[name="r_r_bias"](%2262)
  %2266 : Tensor = prim::GetAttr[name="r_w_bias"](%2262)
  %2267 : Tensor = prim::GetAttr[name="r"](%2262)
  %2268 : Tensor = prim::GetAttr[name="v"](%2262)
  %2269 : Tensor = prim::GetAttr[name="k"](%2262)
  %2270 : Tensor = prim::GetAttr[name="q"](%2262)
  %2271 : Tensor[] = prim::ListConstruct(%2257, %2270), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2271), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2273 : Tensor[] = prim::ListConstruct(%2257, %2269), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2274 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2273), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2275 : Tensor[] = prim::ListConstruct(%2257, %2268), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2276 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2275), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2277 : Float(26:1024, 17:0, 1024:1) = aten::to(%2258, %55, %58, %57, %57, %56), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2278 : Tensor[] = prim::ListConstruct(%2277, %2267), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2279 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%45, %2278), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2280 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2266, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2281 : Tensor[] = prim::ListConstruct(%2280, %2274), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%44, %2281), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2283 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2265, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %2284 : Tensor[] = prim::ListConstruct(%2283, %2279), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%44, %2284), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2286 : int = aten::size(%ac, %59), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2287 : int = aten::size(%x.93, %65), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2288 : int = aten::size(%x.93, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2289 : int = aten::size(%x.93, %60), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2290 : int = aten::size(%x.93, %59), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2291 : Long() = prim::NumToTensor(%2290), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2292 : int[] = prim::ListConstruct(%2287, %2288, %2290, %2289), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %2292), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %2294 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %65, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2295 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2294, %64, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2296 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2295, %60, %64, %61, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2296, %59, %65, %61, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2298 : Long() = aten::sub(%2291, %48, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2299 : int = aten::Int(%2298), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2300 : int[] = prim::ListConstruct(%2287, %2288, %2289, %2299), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %2300), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2302 : Long(13:1) = aten::arange(%2286, %43, %65, %55, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %59, %2302), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2304 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2305 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2304, %63, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2305, %42), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2307 : Tensor[] = prim::ListConstruct(%141), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2308 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%41, %2307), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2309 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2308, %40), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %2309, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %59, %56), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %2312 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %53, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2313 : Tensor[] = prim::ListConstruct(%2312, %2276), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2314 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%39, %2313), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2315 : Tensor[] = prim::ListConstruct(%2314, %2264), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%38, %2315), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %53, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out, %2257, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %2319 : Tensor = prim::GetAttr[name="bias"](%2263)
  %2320 : Tensor = prim::GetAttr[name="weight"](%2263)
  %2321 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2321, %2320, %2319, %36, %37), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2323 : __torch__.torch.nn.modules.normalization.___torch_mangle_41966.LayerNorm = prim::GetAttr[name="layer_norm"](%2261)
  %2324 : __torch__.torch.nn.modules.linear.___torch_mangle_41968.Linear = prim::GetAttr[name="layer_2"](%2261)
  %2325 : __torch__.torch.nn.modules.linear.___torch_mangle_41967.Linear = prim::GetAttr[name="layer_1"](%2261)
  %2326 : Tensor = prim::GetAttr[name="bias"](%2325)
  %2327 : Tensor = prim::GetAttr[name="weight"](%2325)
  %2328 : Float(1024:1, 4096:1024) = aten::t(%2327), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input_tensor, %2328), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2326, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.214), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %53, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %2333 : Tensor = prim::GetAttr[name="bias"](%2324)
  %2334 : Tensor = prim::GetAttr[name="weight"](%2324)
  %2335 : Float(4096:1, 1024:4096) = aten::t(%2334), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.216, %2335), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2333, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.217, %53, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.72, %input_tensor, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %2340 : Tensor = prim::GetAttr[name="bias"](%2323)
  %2341 : Tensor = prim::GetAttr[name="weight"](%2323)
  %2342 : int[] = prim::ListConstruct(%52), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %2342, %2341, %2340, %36, %37), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %output_h : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.219, %53, %57), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %2345 : int[] = prim::ListConstruct(%64, %65, %60), scope: __module.transformer
  %2346 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%output_h, %2345), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %hidden_states : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%2346, %65), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %2348 : (Float(17:13312, 13:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%hidden_states, %167, %258, %349, %440, %531, %622, %713, %804, %895, %986, %1077, %1168, %1259, %1350, %1441, %1532, %1623, %1714, %1805, %1896, %1987, %2078, %2169, %2260)
  %7 : Float(17:13312, 13:1024, 1024:1), %8 : Float(13:17408, 17:1024, 1024:1), %9 : Float(13:17408, 17:1024, 1024:1), %10 : Float(13:17408, 17:1024, 1024:1), %11 : Float(13:17408, 17:1024, 1024:1), %12 : Float(13:17408, 17:1024, 1024:1), %13 : Float(13:17408, 17:1024, 1024:1), %14 : Float(13:17408, 17:1024, 1024:1), %15 : Float(13:17408, 17:1024, 1024:1), %16 : Float(13:17408, 17:1024, 1024:1), %17 : Float(13:17408, 17:1024, 1024:1), %18 : Float(13:17408, 17:1024, 1024:1), %19 : Float(13:17408, 17:1024, 1024:1), %20 : Float(13:17408, 17:1024, 1024:1), %21 : Float(13:17408, 17:1024, 1024:1), %22 : Float(13:17408, 17:1024, 1024:1), %23 : Float(13:17408, 17:1024, 1024:1), %24 : Float(13:17408, 17:1024, 1024:1), %25 : Float(13:17408, 17:1024, 1024:1), %26 : Float(13:17408, 17:1024, 1024:1), %27 : Float(13:17408, 17:1024, 1024:1), %28 : Float(13:17408, 17:1024, 1024:1), %29 : Float(13:17408, 17:1024, 1024:1), %30 : Float(13:17408, 17:1024, 1024:1), %31 : Float(13:17408, 17:1024, 1024:1) = prim::TupleUnpack(%2348)
  %2349 : float = prim::Constant[value=0.10000000000000001](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2350 : bool = prim::Constant[value=0](), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2351 : int = prim::Constant[value=-1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2352 : int = prim::Constant[value=1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2353 : int = prim::Constant[value=9223372036854775807](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2354 : int = prim::Constant[value=0](), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2355 : __torch__.torch.nn.modules.linear.___torch_mangle_41976.Linear = prim::GetAttr[name="summary"](%4)
  %2356 : Float(17:13312, 13:1024, 1024:1) = aten::slice(%7, %2354, %2354, %2353, %2352), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %input.220 : Float(17:13312, 1024:1) = aten::select(%2356, %2352, %2351), scope: __module.sequence_summary # transformers/modeling_utils.py:1487:0
  %2358 : Tensor = prim::GetAttr[name="bias"](%2355)
  %2359 : Tensor = prim::GetAttr[name="weight"](%2355)
  %2360 : Float(1024:1, 1024:1024) = aten::t(%2359), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %output : Float(17:1024, 1024:1) = aten::addmm(%2358, %input.220, %2360, %2352, %2352), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %input.221 : Float(17:1024, 1024:1) = aten::tanh(%output), scope: __module.sequence_summary # transformers/modeling_utils.py:1509:0
  %input : Float(17:1024, 1024:1) = aten::dropout(%input.221, %2349, %2350), scope: __module.sequence_summary/__module.sequence_summary.last_dropout # torch/nn/functional.py:973:0
  %2364 : int = prim::Constant[value=1](), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %2365 : Tensor = prim::GetAttr[name="bias"](%3)
  %2366 : Tensor = prim::GetAttr[name="weight"](%3)
  %2367 : Float(1024:1, 2:1024) = aten::t(%2366), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %2368 : Float(17:2, 2:1) = aten::addmm(%2365, %input, %2367, %2364, %2364), scope: __module.logits_proj # torch/nn/functional.py:1674:0
  %34 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, %28, %29, %30, %31)
  %35 : (Float(17:2, 2:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%2368, %34)
  return (%35)
