graph(%self.1 : __torch__.transformers.modeling_electra.___torch_mangle_17790.ElectraModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_electra.___torch_mangle_17789.ElectraEncoder = prim::GetAttr[name="encoder"](%self.1)
  %4 : __torch__.torch.nn.modules.linear.___torch_mangle_17583.Linear = prim::GetAttr[name="embeddings_project"](%self.1)
  %5 : __torch__.transformers.modeling_electra.___torch_mangle_17582.ElectraEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %6 : int = prim::Constant[value=0]() # transformers/modeling_electra.py:724:0
  %7 : int = aten::size(%input_ids, %6) # transformers/modeling_electra.py:724:0
  %8 : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%8)
  %10 : int = prim::Constant[value=1]() # transformers/modeling_electra.py:724:0
  %11 : int = aten::size(%input_ids, %10) # transformers/modeling_electra.py:724:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int[] = prim::ListConstruct(%9, %13)
  %15 : int = prim::Constant[value=4]() # transformers/modeling_electra.py:735:0
  %16 : int = prim::Constant[value=0]() # transformers/modeling_electra.py:735:0
  %17 : Device = prim::Constant[value="cpu"]() # transformers/modeling_electra.py:735:0
  %18 : bool = prim::Constant[value=0]() # transformers/modeling_electra.py:735:0
  %input.2 : Long(17:13, 13:1) = aten::zeros(%14, %15, %16, %17, %18) # transformers/modeling_electra.py:735:0
  %20 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %21 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %22 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %24 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %20, %21, %22, %23) # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %26 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%24, %25) # transformers/modeling_utils.py:244:0
  %27 : int = prim::Constant[value=2]() # transformers/modeling_utils.py:244:0
  %28 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%26, %27) # transformers/modeling_utils.py:244:0
  %29 : int = prim::Constant[value=3]() # transformers/modeling_utils.py:244:0
  %30 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %31 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %32 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%28, %29, %30, %31, %32) # transformers/modeling_utils.py:244:0
  %34 : int = prim::Constant[value=6]() # transformers/modeling_utils.py:257:0
  %35 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %36 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %37 : None = prim::Constant()
  %38 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %34, %35, %36, %37) # transformers/modeling_utils.py:257:0
  %39 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
  %40 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
  %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%38, %39, %40) # torch/tensor.py:396:0
  %42 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_utils.py:258:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%41, %42) # transformers/modeling_utils.py:258:0
  %48 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %49 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %50 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %51 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %52 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %53 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %54 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_electra.py:180:0
  %55 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_electra.py:180:0
  %56 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_electra.py:173:0
  %57 : __torch__.torch.nn.modules.normalization.___torch_mangle_17580.LayerNorm = prim::GetAttr[name="LayerNorm"](%5)
  %58 : __torch__.torch.nn.modules.sparse.___torch_mangle_17579.Embedding = prim::GetAttr[name="token_type_embeddings"](%5)
  %59 : __torch__.torch.nn.modules.sparse.___torch_mangle_17578.Embedding = prim::GetAttr[name="position_embeddings"](%5)
  %60 : __torch__.torch.nn.modules.sparse.___torch_mangle_17577.Embedding = prim::GetAttr[name="word_embeddings"](%5)
  %61 : Tensor = prim::GetAttr[name="position_ids"](%5)
  %62 : int = aten::size(%input_ids, %56), scope: __module.embeddings # transformers/modeling_electra.py:173:0
  %63 : Long(1:512, 512:1) = aten::slice(%61, %55, %55, %54, %56), scope: __module.embeddings # transformers/modeling_electra.py:180:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%63, %56, %55, %62, %56), scope: __module.embeddings # transformers/modeling_electra.py:180:0
  %65 : Tensor = prim::GetAttr[name="weight"](%60)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%65, %input_ids, %55, %53, %53), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %67 : Tensor = prim::GetAttr[name="weight"](%59)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%67, %input.1, %52, %53, %53), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %69 : Tensor = prim::GetAttr[name="weight"](%58)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%69, %input.2, %52, %53, %53), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %71 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %56), scope: __module.embeddings # transformers/modeling_electra.py:190:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%71, %token_type_embeddings, %56), scope: __module.embeddings # transformers/modeling_electra.py:190:0
  %73 : Tensor = prim::GetAttr[name="bias"](%57)
  %74 : Tensor = prim::GetAttr[name="weight"](%57)
  %75 : int[] = prim::ListConstruct(%49), scope: __module.embeddings/__module.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %75, %74, %73, %50, %51), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %48, %53), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %78 : int = prim::Constant[value=1](), scope: __module.embeddings_project # torch/nn/functional.py:1678:0
  %79 : Tensor = prim::GetAttr[name="bias"](%4)
  %80 : Tensor = prim::GetAttr[name="weight"](%4)
  %81 : Float(128:1, 256:128) = aten::t(%80), scope: __module.embeddings_project # torch/nn/functional.py:1676:0
  %output.1 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.5, %81), scope: __module.embeddings_project # torch/nn/functional.py:1676:0
  %input.6 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.1, %79, %78), scope: __module.embeddings_project # torch/nn/functional.py:1678:0
  %84 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %85 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %86 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %87 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %88 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %89 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %90 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %91 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %92 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %93 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %94 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %95 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %96 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %97 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %98 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %100 : __torch__.transformers.modeling_electra.___torch_mangle_17787.ElectraLayer = prim::GetAttr[name="11"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %102 : __torch__.transformers.modeling_electra.___torch_mangle_17770.ElectraLayer = prim::GetAttr[name="10"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %104 : __torch__.transformers.modeling_electra.___torch_mangle_17753.ElectraLayer = prim::GetAttr[name="9"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %106 : __torch__.transformers.modeling_electra.___torch_mangle_17736.ElectraLayer = prim::GetAttr[name="8"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %108 : __torch__.transformers.modeling_electra.___torch_mangle_17719.ElectraLayer = prim::GetAttr[name="7"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %110 : __torch__.transformers.modeling_electra.___torch_mangle_17702.ElectraLayer = prim::GetAttr[name="6"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %112 : __torch__.transformers.modeling_electra.___torch_mangle_17685.ElectraLayer = prim::GetAttr[name="5"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %114 : __torch__.transformers.modeling_electra.___torch_mangle_17668.ElectraLayer = prim::GetAttr[name="4"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %116 : __torch__.transformers.modeling_electra.___torch_mangle_17651.ElectraLayer = prim::GetAttr[name="3"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %118 : __torch__.transformers.modeling_electra.___torch_mangle_17634.ElectraLayer = prim::GetAttr[name="2"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %120 : __torch__.transformers.modeling_electra.___torch_mangle_17617.ElectraLayer = prim::GetAttr[name="1"](%119)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_17788.ModuleList = prim::GetAttr[name="layer"](%3)
  %122 : __torch__.transformers.modeling_electra.___torch_mangle_17600.ElectraLayer = prim::GetAttr[name="0"](%121)
  %123 : __torch__.transformers.modeling_electra.___torch_mangle_17599.ElectraOutput = prim::GetAttr[name="output"](%122)
  %124 : __torch__.transformers.modeling_electra.___torch_mangle_17595.ElectraIntermediate = prim::GetAttr[name="intermediate"](%122)
  %125 : __torch__.transformers.modeling_electra.___torch_mangle_17593.ElectraAttention = prim::GetAttr[name="attention"](%122)
  %126 : __torch__.transformers.modeling_electra.___torch_mangle_17592.ElectraSelfOutput = prim::GetAttr[name="output"](%125)
  %127 : __torch__.transformers.modeling_electra.___torch_mangle_17588.ElectraSelfAttention = prim::GetAttr[name="self"](%125)
  %128 : __torch__.torch.nn.modules.linear.___torch_mangle_17586.Linear = prim::GetAttr[name="value"](%127)
  %129 : __torch__.torch.nn.modules.linear.___torch_mangle_17585.Linear = prim::GetAttr[name="key"](%127)
  %130 : __torch__.torch.nn.modules.linear.___torch_mangle_17584.Linear = prim::GetAttr[name="query"](%127)
  %131 : Tensor = prim::GetAttr[name="bias"](%130)
  %132 : Tensor = prim::GetAttr[name="weight"](%130)
  %133 : Float(256:1, 256:256) = aten::t(%132), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %133), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.2, %131, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %136 : Tensor = prim::GetAttr[name="bias"](%129)
  %137 : Tensor = prim::GetAttr[name="weight"](%129)
  %138 : Float(256:1, 256:256) = aten::t(%137), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %138), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.3, %136, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %141 : Tensor = prim::GetAttr[name="bias"](%128)
  %142 : Tensor = prim::GetAttr[name="weight"](%128)
  %143 : Float(256:1, 256:256) = aten::t(%142), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %143), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.4, %141, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %146 : int = aten::size(%x.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %147 : int = aten::size(%x.1, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %148 : int[] = prim::ListConstruct(%146, %147, %88, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.1, %148), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %150 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.2, %150), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %152 : int = aten::size(%x.3, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %153 : int = aten::size(%x.3, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %154 : int[] = prim::ListConstruct(%152, %153, %88, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.3, %154), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %156 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.4, %156), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %158 : int = aten::size(%x.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %159 : int = aten::size(%x.5, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %160 : int[] = prim::ListConstruct(%158, %159, %88, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.5, %160), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %162 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.6, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %164 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.1, %92, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %164), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %94), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %input.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
  %input.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.7, %92, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.8, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:265:0
  %171 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %172 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.1, %171), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%172, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %174 : int = aten::size(%context_layer.2, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %175 : int = aten::size(%context_layer.2, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %176 : int[] = prim::ListConstruct(%174, %175, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %input.9 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.2, %176), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %178 : __torch__.torch.nn.modules.normalization.___torch_mangle_17590.LayerNorm = prim::GetAttr[name="LayerNorm"](%126)
  %179 : __torch__.torch.nn.modules.linear.___torch_mangle_17589.Linear = prim::GetAttr[name="dense"](%126)
  %180 : Tensor = prim::GetAttr[name="bias"](%179)
  %181 : Tensor = prim::GetAttr[name="weight"](%179)
  %182 : Float(256:1, 256:256) = aten::t(%181), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.9, %182), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.5, %180, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.10, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.1, %input.6, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
  %187 : Tensor = prim::GetAttr[name="bias"](%178)
  %188 : Tensor = prim::GetAttr[name="weight"](%178)
  %189 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.11, %189, %188, %187, %85, %84), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_17594.Linear = prim::GetAttr[name="dense"](%124)
  %192 : Tensor = prim::GetAttr[name="bias"](%191)
  %193 : Tensor = prim::GetAttr[name="weight"](%191)
  %194 : Float(256:1, 1024:256) = aten::t(%193), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.1, %194), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %192, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.12), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %198 : __torch__.torch.nn.modules.normalization.___torch_mangle_17597.LayerNorm = prim::GetAttr[name="LayerNorm"](%123)
  %199 : __torch__.torch.nn.modules.linear.___torch_mangle_17596.Linear = prim::GetAttr[name="dense"](%123)
  %200 : Tensor = prim::GetAttr[name="bias"](%199)
  %201 : Tensor = prim::GetAttr[name="weight"](%199)
  %202 : Float(1024:1, 256:1024) = aten::t(%201), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.7 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.13, %202), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.14 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.7, %200, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.14, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.2, %input_tensor.1, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_electra.py:365:0
  %207 : Tensor = prim::GetAttr[name="bias"](%198)
  %208 : Tensor = prim::GetAttr[name="weight"](%198)
  %209 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm
  %input.16 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.15, %209, %208, %207, %85, %84), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %211 : __torch__.transformers.modeling_electra.___torch_mangle_17616.ElectraOutput = prim::GetAttr[name="output"](%120)
  %212 : __torch__.transformers.modeling_electra.___torch_mangle_17612.ElectraIntermediate = prim::GetAttr[name="intermediate"](%120)
  %213 : __torch__.transformers.modeling_electra.___torch_mangle_17610.ElectraAttention = prim::GetAttr[name="attention"](%120)
  %214 : __torch__.transformers.modeling_electra.___torch_mangle_17609.ElectraSelfOutput = prim::GetAttr[name="output"](%213)
  %215 : __torch__.transformers.modeling_electra.___torch_mangle_17605.ElectraSelfAttention = prim::GetAttr[name="self"](%213)
  %216 : __torch__.torch.nn.modules.linear.___torch_mangle_17603.Linear = prim::GetAttr[name="value"](%215)
  %217 : __torch__.torch.nn.modules.linear.___torch_mangle_17602.Linear = prim::GetAttr[name="key"](%215)
  %218 : __torch__.torch.nn.modules.linear.___torch_mangle_17601.Linear = prim::GetAttr[name="query"](%215)
  %219 : Tensor = prim::GetAttr[name="bias"](%218)
  %220 : Tensor = prim::GetAttr[name="weight"](%218)
  %221 : Float(256:1, 256:256) = aten::t(%220), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.8 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %221), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.8, %219, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %224 : Tensor = prim::GetAttr[name="bias"](%217)
  %225 : Tensor = prim::GetAttr[name="weight"](%217)
  %226 : Float(256:1, 256:256) = aten::t(%225), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.9 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %226), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.9, %224, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %229 : Tensor = prim::GetAttr[name="bias"](%216)
  %230 : Tensor = prim::GetAttr[name="weight"](%216)
  %231 : Float(256:1, 256:256) = aten::t(%230), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.10 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %231), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.10, %229, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %234 : int = aten::size(%x.7, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %235 : int = aten::size(%x.7, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %236 : int[] = prim::ListConstruct(%234, %235, %88, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.7, %236), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %238 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.8, %238), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %240 : int = aten::size(%x.9, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %241 : int = aten::size(%x.9, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %242 : int[] = prim::ListConstruct(%240, %241, %88, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.9, %242), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %244 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.10, %244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %246 : int = aten::size(%x.11, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %247 : int = aten::size(%x.11, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %248 : int[] = prim::ListConstruct(%246, %247, %88, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.11, %248), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %250 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.12, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %252 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.2, %92, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %252), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %94), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
  %input.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
  %input.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.17, %92, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.18, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:265:0
  %259 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %260 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.3, %259), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%260, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %262 : int = aten::size(%context_layer.4, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %263 : int = aten::size(%context_layer.4, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %264 : int[] = prim::ListConstruct(%262, %263, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %input.19 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.4, %264), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
  %266 : __torch__.torch.nn.modules.normalization.___torch_mangle_17607.LayerNorm = prim::GetAttr[name="LayerNorm"](%214)
  %267 : __torch__.torch.nn.modules.linear.___torch_mangle_17606.Linear = prim::GetAttr[name="dense"](%214)
  %268 : Tensor = prim::GetAttr[name="bias"](%267)
  %269 : Tensor = prim::GetAttr[name="weight"](%267)
  %270 : Float(256:1, 256:256) = aten::t(%269), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.19, %270), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.11, %268, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.20, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.21 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.3, %input.16, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
  %275 : Tensor = prim::GetAttr[name="bias"](%266)
  %276 : Tensor = prim::GetAttr[name="weight"](%266)
  %277 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.21, %277, %276, %275, %85, %84), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %279 : __torch__.torch.nn.modules.linear.___torch_mangle_17611.Linear = prim::GetAttr[name="dense"](%212)
  %280 : Tensor = prim::GetAttr[name="bias"](%279)
  %281 : Tensor = prim::GetAttr[name="weight"](%279)
  %282 : Float(256:1, 1024:256) = aten::t(%281), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.2, %282), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %280, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.22), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %286 : __torch__.torch.nn.modules.normalization.___torch_mangle_17614.LayerNorm = prim::GetAttr[name="LayerNorm"](%211)
  %287 : __torch__.torch.nn.modules.linear.___torch_mangle_17613.Linear = prim::GetAttr[name="dense"](%211)
  %288 : Tensor = prim::GetAttr[name="bias"](%287)
  %289 : Tensor = prim::GetAttr[name="weight"](%287)
  %290 : Float(1024:1, 256:1024) = aten::t(%289), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.13 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.23, %290), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.24 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.13, %288, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.24, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.25 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.4, %input_tensor.2, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_electra.py:365:0
  %295 : Tensor = prim::GetAttr[name="bias"](%286)
  %296 : Tensor = prim::GetAttr[name="weight"](%286)
  %297 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm
  %input.26 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.25, %297, %296, %295, %85, %84), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %299 : __torch__.transformers.modeling_electra.___torch_mangle_17633.ElectraOutput = prim::GetAttr[name="output"](%118)
  %300 : __torch__.transformers.modeling_electra.___torch_mangle_17629.ElectraIntermediate = prim::GetAttr[name="intermediate"](%118)
  %301 : __torch__.transformers.modeling_electra.___torch_mangle_17627.ElectraAttention = prim::GetAttr[name="attention"](%118)
  %302 : __torch__.transformers.modeling_electra.___torch_mangle_17626.ElectraSelfOutput = prim::GetAttr[name="output"](%301)
  %303 : __torch__.transformers.modeling_electra.___torch_mangle_17622.ElectraSelfAttention = prim::GetAttr[name="self"](%301)
  %304 : __torch__.torch.nn.modules.linear.___torch_mangle_17620.Linear = prim::GetAttr[name="value"](%303)
  %305 : __torch__.torch.nn.modules.linear.___torch_mangle_17619.Linear = prim::GetAttr[name="key"](%303)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_17618.Linear = prim::GetAttr[name="query"](%303)
  %307 : Tensor = prim::GetAttr[name="bias"](%306)
  %308 : Tensor = prim::GetAttr[name="weight"](%306)
  %309 : Float(256:1, 256:256) = aten::t(%308), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.14 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %309), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.14, %307, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %312 : Tensor = prim::GetAttr[name="bias"](%305)
  %313 : Tensor = prim::GetAttr[name="weight"](%305)
  %314 : Float(256:1, 256:256) = aten::t(%313), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.15 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %314), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.15, %312, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %317 : Tensor = prim::GetAttr[name="bias"](%304)
  %318 : Tensor = prim::GetAttr[name="weight"](%304)
  %319 : Float(256:1, 256:256) = aten::t(%318), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.16 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %319), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.16, %317, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %322 : int = aten::size(%x.13, %87), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %323 : int = aten::size(%x.13, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %324 : int[] = prim::ListConstruct(%322, %323, %88, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.13, %324), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %326 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.14, %326), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %328 : int = aten::size(%x.15, %87), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %329 : int = aten::size(%x.15, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %330 : int[] = prim::ListConstruct(%328, %329, %88, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.15, %330), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %332 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.16, %332), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %334 : int = aten::size(%x.17, %87), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %335 : int = aten::size(%x.17, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %336 : int[] = prim::ListConstruct(%334, %335, %88, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.17, %336), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %338 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.18, %338), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %340 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.3, %92, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %340), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %94), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
  %input.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
  %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.27, %92, %95), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.28, %97, %96), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:265:0
  %347 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %348 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.5, %347), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%348, %87), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %350 : int = aten::size(%context_layer.6, %87), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %351 : int = aten::size(%context_layer.6, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %352 : int[] = prim::ListConstruct(%350, %351, %98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %input.29 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.6, %352), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
  %354 : __torch__.torch.nn.modules.normalization.___torch_mangle_17624.LayerNorm = prim::GetAttr[name="LayerNorm"](%302)
  %355 : __torch__.torch.nn.modules.linear.___torch_mangle_17623.Linear = prim::GetAttr[name="dense"](%302)
  %356 : Tensor = prim::GetAttr[name="bias"](%355)
  %357 : Tensor = prim::GetAttr[name="weight"](%355)
  %358 : Float(256:1, 256:256) = aten::t(%357), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.29, %358), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.17, %356, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.30, %97, %96), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.5, %input.26, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
  %363 : Tensor = prim::GetAttr[name="bias"](%354)
  %364 : Tensor = prim::GetAttr[name="weight"](%354)
  %365 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.31, %365, %364, %363, %85, %84), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %367 : __torch__.torch.nn.modules.linear.___torch_mangle_17628.Linear = prim::GetAttr[name="dense"](%300)
  %368 : Tensor = prim::GetAttr[name="bias"](%367)
  %369 : Tensor = prim::GetAttr[name="weight"](%367)
  %370 : Float(256:1, 1024:256) = aten::t(%369), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.3, %370), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %368, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.32), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %374 : __torch__.torch.nn.modules.normalization.___torch_mangle_17631.LayerNorm = prim::GetAttr[name="LayerNorm"](%299)
  %375 : __torch__.torch.nn.modules.linear.___torch_mangle_17630.Linear = prim::GetAttr[name="dense"](%299)
  %376 : Tensor = prim::GetAttr[name="bias"](%375)
  %377 : Tensor = prim::GetAttr[name="weight"](%375)
  %378 : Float(1024:1, 256:1024) = aten::t(%377), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.19 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.33, %378), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.34 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.19, %376, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.34, %97, %96), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.35 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.6, %input_tensor.3, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_electra.py:365:0
  %383 : Tensor = prim::GetAttr[name="bias"](%374)
  %384 : Tensor = prim::GetAttr[name="weight"](%374)
  %385 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm
  %input.36 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.35, %385, %384, %383, %85, %84), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %387 : __torch__.transformers.modeling_electra.___torch_mangle_17650.ElectraOutput = prim::GetAttr[name="output"](%116)
  %388 : __torch__.transformers.modeling_electra.___torch_mangle_17646.ElectraIntermediate = prim::GetAttr[name="intermediate"](%116)
  %389 : __torch__.transformers.modeling_electra.___torch_mangle_17644.ElectraAttention = prim::GetAttr[name="attention"](%116)
  %390 : __torch__.transformers.modeling_electra.___torch_mangle_17643.ElectraSelfOutput = prim::GetAttr[name="output"](%389)
  %391 : __torch__.transformers.modeling_electra.___torch_mangle_17639.ElectraSelfAttention = prim::GetAttr[name="self"](%389)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_17637.Linear = prim::GetAttr[name="value"](%391)
  %393 : __torch__.torch.nn.modules.linear.___torch_mangle_17636.Linear = prim::GetAttr[name="key"](%391)
  %394 : __torch__.torch.nn.modules.linear.___torch_mangle_17635.Linear = prim::GetAttr[name="query"](%391)
  %395 : Tensor = prim::GetAttr[name="bias"](%394)
  %396 : Tensor = prim::GetAttr[name="weight"](%394)
  %397 : Float(256:1, 256:256) = aten::t(%396), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.20 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %397), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.20, %395, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %400 : Tensor = prim::GetAttr[name="bias"](%393)
  %401 : Tensor = prim::GetAttr[name="weight"](%393)
  %402 : Float(256:1, 256:256) = aten::t(%401), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.21 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %402), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.21, %400, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %405 : Tensor = prim::GetAttr[name="bias"](%392)
  %406 : Tensor = prim::GetAttr[name="weight"](%392)
  %407 : Float(256:1, 256:256) = aten::t(%406), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.22 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %407), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.22, %405, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %410 : int = aten::size(%x.19, %87), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %411 : int = aten::size(%x.19, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %412 : int[] = prim::ListConstruct(%410, %411, %88, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.19, %412), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %414 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.20, %414), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %416 : int = aten::size(%x.21, %87), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %417 : int = aten::size(%x.21, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %418 : int[] = prim::ListConstruct(%416, %417, %88, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.21, %418), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %420 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.22, %420), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %422 : int = aten::size(%x.23, %87), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %423 : int = aten::size(%x.23, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %424 : int[] = prim::ListConstruct(%422, %423, %88, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.24 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.23, %424), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %426 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.24, %426), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %428 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.4, %92, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %428), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %94), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
  %input.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
  %input.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.37, %92, %95), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.38, %97, %96), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:265:0
  %435 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %436 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.7, %435), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%436, %87), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %438 : int = aten::size(%context_layer.8, %87), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %439 : int = aten::size(%context_layer.8, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %440 : int[] = prim::ListConstruct(%438, %439, %98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %input.39 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.8, %440), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
  %442 : __torch__.torch.nn.modules.normalization.___torch_mangle_17641.LayerNorm = prim::GetAttr[name="LayerNorm"](%390)
  %443 : __torch__.torch.nn.modules.linear.___torch_mangle_17640.Linear = prim::GetAttr[name="dense"](%390)
  %444 : Tensor = prim::GetAttr[name="bias"](%443)
  %445 : Tensor = prim::GetAttr[name="weight"](%443)
  %446 : Float(256:1, 256:256) = aten::t(%445), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.39, %446), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.23, %444, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.40, %97, %96), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.7, %input.36, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
  %451 : Tensor = prim::GetAttr[name="bias"](%442)
  %452 : Tensor = prim::GetAttr[name="weight"](%442)
  %453 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.41, %453, %452, %451, %85, %84), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %455 : __torch__.torch.nn.modules.linear.___torch_mangle_17645.Linear = prim::GetAttr[name="dense"](%388)
  %456 : Tensor = prim::GetAttr[name="bias"](%455)
  %457 : Tensor = prim::GetAttr[name="weight"](%455)
  %458 : Float(256:1, 1024:256) = aten::t(%457), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.4, %458), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %456, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.42), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %462 : __torch__.torch.nn.modules.normalization.___torch_mangle_17648.LayerNorm = prim::GetAttr[name="LayerNorm"](%387)
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_17647.Linear = prim::GetAttr[name="dense"](%387)
  %464 : Tensor = prim::GetAttr[name="bias"](%463)
  %465 : Tensor = prim::GetAttr[name="weight"](%463)
  %466 : Float(1024:1, 256:1024) = aten::t(%465), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.25 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.43, %466), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.44 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.25, %464, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.44, %97, %96), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.8, %input_tensor.4, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_electra.py:365:0
  %471 : Tensor = prim::GetAttr[name="bias"](%462)
  %472 : Tensor = prim::GetAttr[name="weight"](%462)
  %473 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm
  %input.46 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.45, %473, %472, %471, %85, %84), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %475 : __torch__.transformers.modeling_electra.___torch_mangle_17667.ElectraOutput = prim::GetAttr[name="output"](%114)
  %476 : __torch__.transformers.modeling_electra.___torch_mangle_17663.ElectraIntermediate = prim::GetAttr[name="intermediate"](%114)
  %477 : __torch__.transformers.modeling_electra.___torch_mangle_17661.ElectraAttention = prim::GetAttr[name="attention"](%114)
  %478 : __torch__.transformers.modeling_electra.___torch_mangle_17660.ElectraSelfOutput = prim::GetAttr[name="output"](%477)
  %479 : __torch__.transformers.modeling_electra.___torch_mangle_17656.ElectraSelfAttention = prim::GetAttr[name="self"](%477)
  %480 : __torch__.torch.nn.modules.linear.___torch_mangle_17654.Linear = prim::GetAttr[name="value"](%479)
  %481 : __torch__.torch.nn.modules.linear.___torch_mangle_17653.Linear = prim::GetAttr[name="key"](%479)
  %482 : __torch__.torch.nn.modules.linear.___torch_mangle_17652.Linear = prim::GetAttr[name="query"](%479)
  %483 : Tensor = prim::GetAttr[name="bias"](%482)
  %484 : Tensor = prim::GetAttr[name="weight"](%482)
  %485 : Float(256:1, 256:256) = aten::t(%484), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.26 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %485), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.26, %483, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %488 : Tensor = prim::GetAttr[name="bias"](%481)
  %489 : Tensor = prim::GetAttr[name="weight"](%481)
  %490 : Float(256:1, 256:256) = aten::t(%489), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.27 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %490), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.27, %488, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %493 : Tensor = prim::GetAttr[name="bias"](%480)
  %494 : Tensor = prim::GetAttr[name="weight"](%480)
  %495 : Float(256:1, 256:256) = aten::t(%494), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.28 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %495), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.28, %493, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %498 : int = aten::size(%x.25, %87), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %499 : int = aten::size(%x.25, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %500 : int[] = prim::ListConstruct(%498, %499, %88, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.26 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.25, %500), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %502 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.26, %502), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %504 : int = aten::size(%x.27, %87), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %505 : int = aten::size(%x.27, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %506 : int[] = prim::ListConstruct(%504, %505, %88, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.28 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.27, %506), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %508 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.28, %508), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %510 : int = aten::size(%x.29, %87), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %511 : int = aten::size(%x.29, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %512 : int[] = prim::ListConstruct(%510, %511, %88, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.30 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.29, %512), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %514 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.30, %514), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %516 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.5, %92, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %516), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %94), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
  %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
  %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %92, %95), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %97, %96), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:265:0
  %523 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %524 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.9, %523), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%524, %87), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %526 : int = aten::size(%context_layer.10, %87), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %527 : int = aten::size(%context_layer.10, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %528 : int[] = prim::ListConstruct(%526, %527, %98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %input.49 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.10, %528), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
  %530 : __torch__.torch.nn.modules.normalization.___torch_mangle_17658.LayerNorm = prim::GetAttr[name="LayerNorm"](%478)
  %531 : __torch__.torch.nn.modules.linear.___torch_mangle_17657.Linear = prim::GetAttr[name="dense"](%478)
  %532 : Tensor = prim::GetAttr[name="bias"](%531)
  %533 : Tensor = prim::GetAttr[name="weight"](%531)
  %534 : Float(256:1, 256:256) = aten::t(%533), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.49, %534), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.29, %532, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.50, %97, %96), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.9, %input.46, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
  %539 : Tensor = prim::GetAttr[name="bias"](%530)
  %540 : Tensor = prim::GetAttr[name="weight"](%530)
  %541 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.51, %541, %540, %539, %85, %84), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %543 : __torch__.torch.nn.modules.linear.___torch_mangle_17662.Linear = prim::GetAttr[name="dense"](%476)
  %544 : Tensor = prim::GetAttr[name="bias"](%543)
  %545 : Tensor = prim::GetAttr[name="weight"](%543)
  %546 : Float(256:1, 1024:256) = aten::t(%545), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.5, %546), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %544, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.52), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %550 : __torch__.torch.nn.modules.normalization.___torch_mangle_17665.LayerNorm = prim::GetAttr[name="LayerNorm"](%475)
  %551 : __torch__.torch.nn.modules.linear.___torch_mangle_17664.Linear = prim::GetAttr[name="dense"](%475)
  %552 : Tensor = prim::GetAttr[name="bias"](%551)
  %553 : Tensor = prim::GetAttr[name="weight"](%551)
  %554 : Float(1024:1, 256:1024) = aten::t(%553), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.31 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.53, %554), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.54 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.31, %552, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.54, %97, %96), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.10, %input_tensor.5, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_electra.py:365:0
  %559 : Tensor = prim::GetAttr[name="bias"](%550)
  %560 : Tensor = prim::GetAttr[name="weight"](%550)
  %561 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm
  %input.56 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.55, %561, %560, %559, %85, %84), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %563 : __torch__.transformers.modeling_electra.___torch_mangle_17684.ElectraOutput = prim::GetAttr[name="output"](%112)
  %564 : __torch__.transformers.modeling_electra.___torch_mangle_17680.ElectraIntermediate = prim::GetAttr[name="intermediate"](%112)
  %565 : __torch__.transformers.modeling_electra.___torch_mangle_17678.ElectraAttention = prim::GetAttr[name="attention"](%112)
  %566 : __torch__.transformers.modeling_electra.___torch_mangle_17677.ElectraSelfOutput = prim::GetAttr[name="output"](%565)
  %567 : __torch__.transformers.modeling_electra.___torch_mangle_17673.ElectraSelfAttention = prim::GetAttr[name="self"](%565)
  %568 : __torch__.torch.nn.modules.linear.___torch_mangle_17671.Linear = prim::GetAttr[name="value"](%567)
  %569 : __torch__.torch.nn.modules.linear.___torch_mangle_17670.Linear = prim::GetAttr[name="key"](%567)
  %570 : __torch__.torch.nn.modules.linear.___torch_mangle_17669.Linear = prim::GetAttr[name="query"](%567)
  %571 : Tensor = prim::GetAttr[name="bias"](%570)
  %572 : Tensor = prim::GetAttr[name="weight"](%570)
  %573 : Float(256:1, 256:256) = aten::t(%572), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %573), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.32, %571, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %576 : Tensor = prim::GetAttr[name="bias"](%569)
  %577 : Tensor = prim::GetAttr[name="weight"](%569)
  %578 : Float(256:1, 256:256) = aten::t(%577), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %578), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.33, %576, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %581 : Tensor = prim::GetAttr[name="bias"](%568)
  %582 : Tensor = prim::GetAttr[name="weight"](%568)
  %583 : Float(256:1, 256:256) = aten::t(%582), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %583), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.34, %581, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %586 : int = aten::size(%x.31, %87), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %587 : int = aten::size(%x.31, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %588 : int[] = prim::ListConstruct(%586, %587, %88, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.32 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.31, %588), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %590 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.32, %590), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %592 : int = aten::size(%x.33, %87), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %593 : int = aten::size(%x.33, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %594 : int[] = prim::ListConstruct(%592, %593, %88, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.34 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.33, %594), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %596 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.34, %596), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %598 : int = aten::size(%x.35, %87), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %599 : int = aten::size(%x.35, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %600 : int[] = prim::ListConstruct(%598, %599, %88, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.36 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.35, %600), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %602 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.36, %602), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %604 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.6, %92, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %604), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %94), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
  %input.57 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
  %input.58 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.57, %92, %95), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.58, %97, %96), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:265:0
  %611 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %612 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.11, %611), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%612, %87), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %614 : int = aten::size(%context_layer.12, %87), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %615 : int = aten::size(%context_layer.12, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %616 : int[] = prim::ListConstruct(%614, %615, %98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %input.59 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.12, %616), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
  %618 : __torch__.torch.nn.modules.normalization.___torch_mangle_17675.LayerNorm = prim::GetAttr[name="LayerNorm"](%566)
  %619 : __torch__.torch.nn.modules.linear.___torch_mangle_17674.Linear = prim::GetAttr[name="dense"](%566)
  %620 : Tensor = prim::GetAttr[name="bias"](%619)
  %621 : Tensor = prim::GetAttr[name="weight"](%619)
  %622 : Float(256:1, 256:256) = aten::t(%621), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.59, %622), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.35, %620, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.60, %97, %96), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.61 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.11, %input.56, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
  %627 : Tensor = prim::GetAttr[name="bias"](%618)
  %628 : Tensor = prim::GetAttr[name="weight"](%618)
  %629 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.61, %629, %628, %627, %85, %84), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %631 : __torch__.torch.nn.modules.linear.___torch_mangle_17679.Linear = prim::GetAttr[name="dense"](%564)
  %632 : Tensor = prim::GetAttr[name="bias"](%631)
  %633 : Tensor = prim::GetAttr[name="weight"](%631)
  %634 : Float(256:1, 1024:256) = aten::t(%633), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.6, %634), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %632, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.62), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %638 : __torch__.torch.nn.modules.normalization.___torch_mangle_17682.LayerNorm = prim::GetAttr[name="LayerNorm"](%563)
  %639 : __torch__.torch.nn.modules.linear.___torch_mangle_17681.Linear = prim::GetAttr[name="dense"](%563)
  %640 : Tensor = prim::GetAttr[name="bias"](%639)
  %641 : Tensor = prim::GetAttr[name="weight"](%639)
  %642 : Float(1024:1, 256:1024) = aten::t(%641), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.37 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.63, %642), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.64 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.37, %640, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.64, %97, %96), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.12, %input_tensor.6, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_electra.py:365:0
  %647 : Tensor = prim::GetAttr[name="bias"](%638)
  %648 : Tensor = prim::GetAttr[name="weight"](%638)
  %649 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm
  %input.66 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.65, %649, %648, %647, %85, %84), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %651 : __torch__.transformers.modeling_electra.___torch_mangle_17701.ElectraOutput = prim::GetAttr[name="output"](%110)
  %652 : __torch__.transformers.modeling_electra.___torch_mangle_17697.ElectraIntermediate = prim::GetAttr[name="intermediate"](%110)
  %653 : __torch__.transformers.modeling_electra.___torch_mangle_17695.ElectraAttention = prim::GetAttr[name="attention"](%110)
  %654 : __torch__.transformers.modeling_electra.___torch_mangle_17694.ElectraSelfOutput = prim::GetAttr[name="output"](%653)
  %655 : __torch__.transformers.modeling_electra.___torch_mangle_17690.ElectraSelfAttention = prim::GetAttr[name="self"](%653)
  %656 : __torch__.torch.nn.modules.linear.___torch_mangle_17688.Linear = prim::GetAttr[name="value"](%655)
  %657 : __torch__.torch.nn.modules.linear.___torch_mangle_17687.Linear = prim::GetAttr[name="key"](%655)
  %658 : __torch__.torch.nn.modules.linear.___torch_mangle_17686.Linear = prim::GetAttr[name="query"](%655)
  %659 : Tensor = prim::GetAttr[name="bias"](%658)
  %660 : Tensor = prim::GetAttr[name="weight"](%658)
  %661 : Float(256:1, 256:256) = aten::t(%660), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.38 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %661), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.38, %659, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %664 : Tensor = prim::GetAttr[name="bias"](%657)
  %665 : Tensor = prim::GetAttr[name="weight"](%657)
  %666 : Float(256:1, 256:256) = aten::t(%665), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.39 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %666), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.39, %664, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %669 : Tensor = prim::GetAttr[name="bias"](%656)
  %670 : Tensor = prim::GetAttr[name="weight"](%656)
  %671 : Float(256:1, 256:256) = aten::t(%670), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.40 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %671), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.40, %669, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %674 : int = aten::size(%x.37, %87), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %675 : int = aten::size(%x.37, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %676 : int[] = prim::ListConstruct(%674, %675, %88, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.38 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.37, %676), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %678 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.38, %678), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %680 : int = aten::size(%x.39, %87), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %681 : int = aten::size(%x.39, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %682 : int[] = prim::ListConstruct(%680, %681, %88, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.40 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.39, %682), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %684 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.40, %684), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %686 : int = aten::size(%x.41, %87), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %687 : int = aten::size(%x.41, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %688 : int[] = prim::ListConstruct(%686, %687, %88, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.42 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.41, %688), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %690 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.42, %690), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %692 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.7, %92, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %692), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %94), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
  %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
  %input.68 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.67, %92, %95), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.68, %97, %96), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:265:0
  %699 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %700 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.13, %699), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%700, %87), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %702 : int = aten::size(%context_layer.14, %87), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %703 : int = aten::size(%context_layer.14, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %704 : int[] = prim::ListConstruct(%702, %703, %98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %input.69 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.14, %704), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
  %706 : __torch__.torch.nn.modules.normalization.___torch_mangle_17692.LayerNorm = prim::GetAttr[name="LayerNorm"](%654)
  %707 : __torch__.torch.nn.modules.linear.___torch_mangle_17691.Linear = prim::GetAttr[name="dense"](%654)
  %708 : Tensor = prim::GetAttr[name="bias"](%707)
  %709 : Tensor = prim::GetAttr[name="weight"](%707)
  %710 : Float(256:1, 256:256) = aten::t(%709), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.69, %710), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.41, %708, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.70, %97, %96), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.71 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.13, %input.66, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
  %715 : Tensor = prim::GetAttr[name="bias"](%706)
  %716 : Tensor = prim::GetAttr[name="weight"](%706)
  %717 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.71, %717, %716, %715, %85, %84), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %719 : __torch__.torch.nn.modules.linear.___torch_mangle_17696.Linear = prim::GetAttr[name="dense"](%652)
  %720 : Tensor = prim::GetAttr[name="bias"](%719)
  %721 : Tensor = prim::GetAttr[name="weight"](%719)
  %722 : Float(256:1, 1024:256) = aten::t(%721), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.7, %722), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %720, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.72), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %726 : __torch__.torch.nn.modules.normalization.___torch_mangle_17699.LayerNorm = prim::GetAttr[name="LayerNorm"](%651)
  %727 : __torch__.torch.nn.modules.linear.___torch_mangle_17698.Linear = prim::GetAttr[name="dense"](%651)
  %728 : Tensor = prim::GetAttr[name="bias"](%727)
  %729 : Tensor = prim::GetAttr[name="weight"](%727)
  %730 : Float(1024:1, 256:1024) = aten::t(%729), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.73, %730), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.74 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.43, %728, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.74, %97, %96), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.75 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.14, %input_tensor.7, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_electra.py:365:0
  %735 : Tensor = prim::GetAttr[name="bias"](%726)
  %736 : Tensor = prim::GetAttr[name="weight"](%726)
  %737 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm
  %input.76 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.75, %737, %736, %735, %85, %84), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %739 : __torch__.transformers.modeling_electra.___torch_mangle_17718.ElectraOutput = prim::GetAttr[name="output"](%108)
  %740 : __torch__.transformers.modeling_electra.___torch_mangle_17714.ElectraIntermediate = prim::GetAttr[name="intermediate"](%108)
  %741 : __torch__.transformers.modeling_electra.___torch_mangle_17712.ElectraAttention = prim::GetAttr[name="attention"](%108)
  %742 : __torch__.transformers.modeling_electra.___torch_mangle_17711.ElectraSelfOutput = prim::GetAttr[name="output"](%741)
  %743 : __torch__.transformers.modeling_electra.___torch_mangle_17707.ElectraSelfAttention = prim::GetAttr[name="self"](%741)
  %744 : __torch__.torch.nn.modules.linear.___torch_mangle_17705.Linear = prim::GetAttr[name="value"](%743)
  %745 : __torch__.torch.nn.modules.linear.___torch_mangle_17704.Linear = prim::GetAttr[name="key"](%743)
  %746 : __torch__.torch.nn.modules.linear.___torch_mangle_17703.Linear = prim::GetAttr[name="query"](%743)
  %747 : Tensor = prim::GetAttr[name="bias"](%746)
  %748 : Tensor = prim::GetAttr[name="weight"](%746)
  %749 : Float(256:1, 256:256) = aten::t(%748), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.44 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %749), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.44, %747, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %752 : Tensor = prim::GetAttr[name="bias"](%745)
  %753 : Tensor = prim::GetAttr[name="weight"](%745)
  %754 : Float(256:1, 256:256) = aten::t(%753), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %754), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.45, %752, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %757 : Tensor = prim::GetAttr[name="bias"](%744)
  %758 : Tensor = prim::GetAttr[name="weight"](%744)
  %759 : Float(256:1, 256:256) = aten::t(%758), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %759), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.46, %757, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %762 : int = aten::size(%x.43, %87), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %763 : int = aten::size(%x.43, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %764 : int[] = prim::ListConstruct(%762, %763, %88, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.44 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.43, %764), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %766 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.44, %766), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %768 : int = aten::size(%x.45, %87), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %769 : int = aten::size(%x.45, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %770 : int[] = prim::ListConstruct(%768, %769, %88, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.46 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.45, %770), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %772 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.46, %772), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %774 : int = aten::size(%x.47, %87), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %775 : int = aten::size(%x.47, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %776 : int[] = prim::ListConstruct(%774, %775, %88, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.48 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.47, %776), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %778 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.48, %778), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %780 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.8, %92, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %780), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %94), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
  %input.77 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
  %input.78 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.77, %92, %95), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.78, %97, %96), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:265:0
  %787 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %788 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.15, %787), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%788, %87), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %790 : int = aten::size(%context_layer.16, %87), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %791 : int = aten::size(%context_layer.16, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %792 : int[] = prim::ListConstruct(%790, %791, %98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %input.79 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.16, %792), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
  %794 : __torch__.torch.nn.modules.normalization.___torch_mangle_17709.LayerNorm = prim::GetAttr[name="LayerNorm"](%742)
  %795 : __torch__.torch.nn.modules.linear.___torch_mangle_17708.Linear = prim::GetAttr[name="dense"](%742)
  %796 : Tensor = prim::GetAttr[name="bias"](%795)
  %797 : Tensor = prim::GetAttr[name="weight"](%795)
  %798 : Float(256:1, 256:256) = aten::t(%797), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.79, %798), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.47, %796, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.80, %97, %96), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.81 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.15, %input.76, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
  %803 : Tensor = prim::GetAttr[name="bias"](%794)
  %804 : Tensor = prim::GetAttr[name="weight"](%794)
  %805 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.81, %805, %804, %803, %85, %84), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %807 : __torch__.torch.nn.modules.linear.___torch_mangle_17713.Linear = prim::GetAttr[name="dense"](%740)
  %808 : Tensor = prim::GetAttr[name="bias"](%807)
  %809 : Tensor = prim::GetAttr[name="weight"](%807)
  %810 : Float(256:1, 1024:256) = aten::t(%809), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.8, %810), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %808, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.82), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %814 : __torch__.torch.nn.modules.normalization.___torch_mangle_17716.LayerNorm = prim::GetAttr[name="LayerNorm"](%739)
  %815 : __torch__.torch.nn.modules.linear.___torch_mangle_17715.Linear = prim::GetAttr[name="dense"](%739)
  %816 : Tensor = prim::GetAttr[name="bias"](%815)
  %817 : Tensor = prim::GetAttr[name="weight"](%815)
  %818 : Float(1024:1, 256:1024) = aten::t(%817), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.49 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.83, %818), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.84 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.49, %816, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.84, %97, %96), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.85 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.16, %input_tensor.8, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_electra.py:365:0
  %823 : Tensor = prim::GetAttr[name="bias"](%814)
  %824 : Tensor = prim::GetAttr[name="weight"](%814)
  %825 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm
  %input.86 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.85, %825, %824, %823, %85, %84), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %827 : __torch__.transformers.modeling_electra.___torch_mangle_17735.ElectraOutput = prim::GetAttr[name="output"](%106)
  %828 : __torch__.transformers.modeling_electra.___torch_mangle_17731.ElectraIntermediate = prim::GetAttr[name="intermediate"](%106)
  %829 : __torch__.transformers.modeling_electra.___torch_mangle_17729.ElectraAttention = prim::GetAttr[name="attention"](%106)
  %830 : __torch__.transformers.modeling_electra.___torch_mangle_17728.ElectraSelfOutput = prim::GetAttr[name="output"](%829)
  %831 : __torch__.transformers.modeling_electra.___torch_mangle_17724.ElectraSelfAttention = prim::GetAttr[name="self"](%829)
  %832 : __torch__.torch.nn.modules.linear.___torch_mangle_17722.Linear = prim::GetAttr[name="value"](%831)
  %833 : __torch__.torch.nn.modules.linear.___torch_mangle_17721.Linear = prim::GetAttr[name="key"](%831)
  %834 : __torch__.torch.nn.modules.linear.___torch_mangle_17720.Linear = prim::GetAttr[name="query"](%831)
  %835 : Tensor = prim::GetAttr[name="bias"](%834)
  %836 : Tensor = prim::GetAttr[name="weight"](%834)
  %837 : Float(256:1, 256:256) = aten::t(%836), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %837), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.50, %835, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %840 : Tensor = prim::GetAttr[name="bias"](%833)
  %841 : Tensor = prim::GetAttr[name="weight"](%833)
  %842 : Float(256:1, 256:256) = aten::t(%841), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %842), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.51, %840, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %845 : Tensor = prim::GetAttr[name="bias"](%832)
  %846 : Tensor = prim::GetAttr[name="weight"](%832)
  %847 : Float(256:1, 256:256) = aten::t(%846), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %847), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.52, %845, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %850 : int = aten::size(%x.49, %87), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %851 : int = aten::size(%x.49, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %852 : int[] = prim::ListConstruct(%850, %851, %88, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.50 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.49, %852), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %854 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.50, %854), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %856 : int = aten::size(%x.51, %87), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %857 : int = aten::size(%x.51, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %858 : int[] = prim::ListConstruct(%856, %857, %88, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.52 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.51, %858), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %860 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.52, %860), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %862 : int = aten::size(%x.53, %87), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %863 : int = aten::size(%x.53, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %864 : int[] = prim::ListConstruct(%862, %863, %88, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.54 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.53, %864), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %866 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.54, %866), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %868 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.9, %92, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %868), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %94), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
  %input.87 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
  %input.88 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.87, %92, %95), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.88, %97, %96), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:265:0
  %875 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %876 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.17, %875), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%876, %87), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %878 : int = aten::size(%context_layer.18, %87), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %879 : int = aten::size(%context_layer.18, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %880 : int[] = prim::ListConstruct(%878, %879, %98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %input.89 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.18, %880), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
  %882 : __torch__.torch.nn.modules.normalization.___torch_mangle_17726.LayerNorm = prim::GetAttr[name="LayerNorm"](%830)
  %883 : __torch__.torch.nn.modules.linear.___torch_mangle_17725.Linear = prim::GetAttr[name="dense"](%830)
  %884 : Tensor = prim::GetAttr[name="bias"](%883)
  %885 : Tensor = prim::GetAttr[name="weight"](%883)
  %886 : Float(256:1, 256:256) = aten::t(%885), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.89, %886), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.53, %884, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.90, %97, %96), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.91 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.17, %input.86, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
  %891 : Tensor = prim::GetAttr[name="bias"](%882)
  %892 : Tensor = prim::GetAttr[name="weight"](%882)
  %893 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.91, %893, %892, %891, %85, %84), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %895 : __torch__.torch.nn.modules.linear.___torch_mangle_17730.Linear = prim::GetAttr[name="dense"](%828)
  %896 : Tensor = prim::GetAttr[name="bias"](%895)
  %897 : Tensor = prim::GetAttr[name="weight"](%895)
  %898 : Float(256:1, 1024:256) = aten::t(%897), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.9, %898), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %896, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.92), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %902 : __torch__.torch.nn.modules.normalization.___torch_mangle_17733.LayerNorm = prim::GetAttr[name="LayerNorm"](%827)
  %903 : __torch__.torch.nn.modules.linear.___torch_mangle_17732.Linear = prim::GetAttr[name="dense"](%827)
  %904 : Tensor = prim::GetAttr[name="bias"](%903)
  %905 : Tensor = prim::GetAttr[name="weight"](%903)
  %906 : Float(1024:1, 256:1024) = aten::t(%905), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.93, %906), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.94 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.55, %904, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.94, %97, %96), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.95 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.18, %input_tensor.9, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_electra.py:365:0
  %911 : Tensor = prim::GetAttr[name="bias"](%902)
  %912 : Tensor = prim::GetAttr[name="weight"](%902)
  %913 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm
  %input.96 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.95, %913, %912, %911, %85, %84), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %915 : __torch__.transformers.modeling_electra.___torch_mangle_17752.ElectraOutput = prim::GetAttr[name="output"](%104)
  %916 : __torch__.transformers.modeling_electra.___torch_mangle_17748.ElectraIntermediate = prim::GetAttr[name="intermediate"](%104)
  %917 : __torch__.transformers.modeling_electra.___torch_mangle_17746.ElectraAttention = prim::GetAttr[name="attention"](%104)
  %918 : __torch__.transformers.modeling_electra.___torch_mangle_17745.ElectraSelfOutput = prim::GetAttr[name="output"](%917)
  %919 : __torch__.transformers.modeling_electra.___torch_mangle_17741.ElectraSelfAttention = prim::GetAttr[name="self"](%917)
  %920 : __torch__.torch.nn.modules.linear.___torch_mangle_17739.Linear = prim::GetAttr[name="value"](%919)
  %921 : __torch__.torch.nn.modules.linear.___torch_mangle_17738.Linear = prim::GetAttr[name="key"](%919)
  %922 : __torch__.torch.nn.modules.linear.___torch_mangle_17737.Linear = prim::GetAttr[name="query"](%919)
  %923 : Tensor = prim::GetAttr[name="bias"](%922)
  %924 : Tensor = prim::GetAttr[name="weight"](%922)
  %925 : Float(256:1, 256:256) = aten::t(%924), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %925), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.56, %923, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %928 : Tensor = prim::GetAttr[name="bias"](%921)
  %929 : Tensor = prim::GetAttr[name="weight"](%921)
  %930 : Float(256:1, 256:256) = aten::t(%929), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %930), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.57, %928, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %933 : Tensor = prim::GetAttr[name="bias"](%920)
  %934 : Tensor = prim::GetAttr[name="weight"](%920)
  %935 : Float(256:1, 256:256) = aten::t(%934), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %935), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.58, %933, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %938 : int = aten::size(%x.55, %87), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %939 : int = aten::size(%x.55, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %940 : int[] = prim::ListConstruct(%938, %939, %88, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.56 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.55, %940), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %942 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.56, %942), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %944 : int = aten::size(%x.57, %87), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %945 : int = aten::size(%x.57, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %946 : int[] = prim::ListConstruct(%944, %945, %88, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.58 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.57, %946), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %948 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.58, %948), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %950 : int = aten::size(%x.59, %87), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %951 : int = aten::size(%x.59, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %952 : int[] = prim::ListConstruct(%950, %951, %88, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.60 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.59, %952), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %954 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.60, %954), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %956 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.10, %92, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %956), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %94), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
  %input.97 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
  %input.98 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.97, %92, %95), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.98, %97, %96), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:265:0
  %963 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %964 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.19, %963), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%964, %87), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %966 : int = aten::size(%context_layer.20, %87), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %967 : int = aten::size(%context_layer.20, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %968 : int[] = prim::ListConstruct(%966, %967, %98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %input.99 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.20, %968), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
  %970 : __torch__.torch.nn.modules.normalization.___torch_mangle_17743.LayerNorm = prim::GetAttr[name="LayerNorm"](%918)
  %971 : __torch__.torch.nn.modules.linear.___torch_mangle_17742.Linear = prim::GetAttr[name="dense"](%918)
  %972 : Tensor = prim::GetAttr[name="bias"](%971)
  %973 : Tensor = prim::GetAttr[name="weight"](%971)
  %974 : Float(256:1, 256:256) = aten::t(%973), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.99, %974), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.59, %972, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.100, %97, %96), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.19, %input.96, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
  %979 : Tensor = prim::GetAttr[name="bias"](%970)
  %980 : Tensor = prim::GetAttr[name="weight"](%970)
  %981 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.101, %981, %980, %979, %85, %84), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %983 : __torch__.torch.nn.modules.linear.___torch_mangle_17747.Linear = prim::GetAttr[name="dense"](%916)
  %984 : Tensor = prim::GetAttr[name="bias"](%983)
  %985 : Tensor = prim::GetAttr[name="weight"](%983)
  %986 : Float(256:1, 1024:256) = aten::t(%985), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.10, %986), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %984, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.102), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %990 : __torch__.torch.nn.modules.normalization.___torch_mangle_17750.LayerNorm = prim::GetAttr[name="LayerNorm"](%915)
  %991 : __torch__.torch.nn.modules.linear.___torch_mangle_17749.Linear = prim::GetAttr[name="dense"](%915)
  %992 : Tensor = prim::GetAttr[name="bias"](%991)
  %993 : Tensor = prim::GetAttr[name="weight"](%991)
  %994 : Float(1024:1, 256:1024) = aten::t(%993), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.61 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.103, %994), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.104 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.61, %992, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.104, %97, %96), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.20, %input_tensor.10, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_electra.py:365:0
  %999 : Tensor = prim::GetAttr[name="bias"](%990)
  %1000 : Tensor = prim::GetAttr[name="weight"](%990)
  %1001 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm
  %input.106 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.105, %1001, %1000, %999, %85, %84), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %1003 : __torch__.transformers.modeling_electra.___torch_mangle_17769.ElectraOutput = prim::GetAttr[name="output"](%102)
  %1004 : __torch__.transformers.modeling_electra.___torch_mangle_17765.ElectraIntermediate = prim::GetAttr[name="intermediate"](%102)
  %1005 : __torch__.transformers.modeling_electra.___torch_mangle_17763.ElectraAttention = prim::GetAttr[name="attention"](%102)
  %1006 : __torch__.transformers.modeling_electra.___torch_mangle_17762.ElectraSelfOutput = prim::GetAttr[name="output"](%1005)
  %1007 : __torch__.transformers.modeling_electra.___torch_mangle_17758.ElectraSelfAttention = prim::GetAttr[name="self"](%1005)
  %1008 : __torch__.torch.nn.modules.linear.___torch_mangle_17756.Linear = prim::GetAttr[name="value"](%1007)
  %1009 : __torch__.torch.nn.modules.linear.___torch_mangle_17755.Linear = prim::GetAttr[name="key"](%1007)
  %1010 : __torch__.torch.nn.modules.linear.___torch_mangle_17754.Linear = prim::GetAttr[name="query"](%1007)
  %1011 : Tensor = prim::GetAttr[name="bias"](%1010)
  %1012 : Tensor = prim::GetAttr[name="weight"](%1010)
  %1013 : Float(256:1, 256:256) = aten::t(%1012), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.62 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %1013), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.62, %1011, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %1016 : Tensor = prim::GetAttr[name="bias"](%1009)
  %1017 : Tensor = prim::GetAttr[name="weight"](%1009)
  %1018 : Float(256:1, 256:256) = aten::t(%1017), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.63 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %1018), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.63, %1016, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1021 : Tensor = prim::GetAttr[name="bias"](%1008)
  %1022 : Tensor = prim::GetAttr[name="weight"](%1008)
  %1023 : Float(256:1, 256:256) = aten::t(%1022), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.64 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %1023), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.64, %1021, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1026 : int = aten::size(%x.61, %87), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1027 : int = aten::size(%x.61, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1028 : int[] = prim::ListConstruct(%1026, %1027, %88, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.62 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.61, %1028), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1030 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.62, %1030), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1032 : int = aten::size(%x.63, %87), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1033 : int = aten::size(%x.63, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1034 : int[] = prim::ListConstruct(%1032, %1033, %88, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.64 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.63, %1034), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1036 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.64, %1036), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1038 : int = aten::size(%x.65, %87), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1039 : int = aten::size(%x.65, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1040 : int[] = prim::ListConstruct(%1038, %1039, %88, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.66 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.65, %1040), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1042 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.66, %1042), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1044 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.11, %92, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1044), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %94), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
  %input.107 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
  %input.108 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.107, %92, %95), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.108, %97, %96), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:265:0
  %1051 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %1052 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.21, %1051), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1052, %87), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %1054 : int = aten::size(%context_layer.22, %87), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1055 : int = aten::size(%context_layer.22, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1056 : int[] = prim::ListConstruct(%1054, %1055, %98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %input.109 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.22, %1056), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
  %1058 : __torch__.torch.nn.modules.normalization.___torch_mangle_17760.LayerNorm = prim::GetAttr[name="LayerNorm"](%1006)
  %1059 : __torch__.torch.nn.modules.linear.___torch_mangle_17759.Linear = prim::GetAttr[name="dense"](%1006)
  %1060 : Tensor = prim::GetAttr[name="bias"](%1059)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1062 : Float(256:1, 256:256) = aten::t(%1061), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.109, %1062), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.65, %1060, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.110, %97, %96), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.111 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.21, %input.106, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
  %1067 : Tensor = prim::GetAttr[name="bias"](%1058)
  %1068 : Tensor = prim::GetAttr[name="weight"](%1058)
  %1069 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.111, %1069, %1068, %1067, %85, %84), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_17764.Linear = prim::GetAttr[name="dense"](%1004)
  %1072 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1073 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1074 : Float(256:1, 1024:256) = aten::t(%1073), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.11, %1074), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1072, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.112), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1078 : __torch__.torch.nn.modules.normalization.___torch_mangle_17767.LayerNorm = prim::GetAttr[name="LayerNorm"](%1003)
  %1079 : __torch__.torch.nn.modules.linear.___torch_mangle_17766.Linear = prim::GetAttr[name="dense"](%1003)
  %1080 : Tensor = prim::GetAttr[name="bias"](%1079)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1079)
  %1082 : Float(1024:1, 256:1024) = aten::t(%1081), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.67 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.113, %1082), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.67, %1080, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.114, %97, %96), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.22, %input_tensor.11, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_electra.py:365:0
  %1087 : Tensor = prim::GetAttr[name="bias"](%1078)
  %1088 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1089 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm
  %input.116 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.115, %1089, %1088, %1087, %85, %84), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1091 : __torch__.transformers.modeling_electra.___torch_mangle_17786.ElectraOutput = prim::GetAttr[name="output"](%100)
  %1092 : __torch__.transformers.modeling_electra.___torch_mangle_17782.ElectraIntermediate = prim::GetAttr[name="intermediate"](%100)
  %1093 : __torch__.transformers.modeling_electra.___torch_mangle_17780.ElectraAttention = prim::GetAttr[name="attention"](%100)
  %1094 : __torch__.transformers.modeling_electra.___torch_mangle_17779.ElectraSelfOutput = prim::GetAttr[name="output"](%1093)
  %1095 : __torch__.transformers.modeling_electra.___torch_mangle_17775.ElectraSelfAttention = prim::GetAttr[name="self"](%1093)
  %1096 : __torch__.torch.nn.modules.linear.___torch_mangle_17773.Linear = prim::GetAttr[name="value"](%1095)
  %1097 : __torch__.torch.nn.modules.linear.___torch_mangle_17772.Linear = prim::GetAttr[name="key"](%1095)
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_17771.Linear = prim::GetAttr[name="query"](%1095)
  %1099 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1100 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1101 : Float(256:1, 256:256) = aten::t(%1100), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.68 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1101), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.68, %1099, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1104 : Tensor = prim::GetAttr[name="bias"](%1097)
  %1105 : Tensor = prim::GetAttr[name="weight"](%1097)
  %1106 : Float(256:1, 256:256) = aten::t(%1105), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.69 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1106), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.69, %1104, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1109 : Tensor = prim::GetAttr[name="bias"](%1096)
  %1110 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1111 : Float(256:1, 256:256) = aten::t(%1110), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.70 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1111), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.70, %1109, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1114 : int = aten::size(%x.67, %87), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1115 : int = aten::size(%x.67, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1116 : int[] = prim::ListConstruct(%1114, %1115, %88, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.68 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.67, %1116), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1118 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %query_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.68, %1118), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1120 : int = aten::size(%x.69, %87), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1121 : int = aten::size(%x.69, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1122 : int[] = prim::ListConstruct(%1120, %1121, %88, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.70 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.69, %1122), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1124 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %key_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.70, %1124), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1126 : int = aten::size(%x.71, %87), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1127 : int = aten::size(%x.71, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1128 : int[] = prim::ListConstruct(%1126, %1127, %88, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.71, %1128), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1130 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %value_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x, %1130), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1132 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer, %92, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %1132), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %94), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
  %input.117 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
  %input.118 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.117, %92, %95), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.118, %97, %96), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:265:0
  %1139 : int[] = prim::ListConstruct(%87, %90, %86, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %1140 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.23, %1139), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %context_layer : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1140, %87), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %1142 : int = aten::size(%context_layer, %87), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1143 : int = aten::size(%context_layer, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1144 : int[] = prim::ListConstruct(%1142, %1143, %98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %input.119 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer, %1144), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
  %1146 : __torch__.torch.nn.modules.normalization.___torch_mangle_17777.LayerNorm = prim::GetAttr[name="LayerNorm"](%1094)
  %1147 : __torch__.torch.nn.modules.linear.___torch_mangle_17776.Linear = prim::GetAttr[name="dense"](%1094)
  %1148 : Tensor = prim::GetAttr[name="bias"](%1147)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1147)
  %1150 : Float(256:1, 256:256) = aten::t(%1149), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.119, %1150), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.71, %1148, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.120, %97, %96), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.121 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.23, %input.116, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
  %1155 : Tensor = prim::GetAttr[name="bias"](%1146)
  %1156 : Tensor = prim::GetAttr[name="weight"](%1146)
  %1157 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.121, %1157, %1156, %1155, %85, %84), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1159 : __torch__.torch.nn.modules.linear.___torch_mangle_17781.Linear = prim::GetAttr[name="dense"](%1092)
  %1160 : Tensor = prim::GetAttr[name="bias"](%1159)
  %1161 : Tensor = prim::GetAttr[name="weight"](%1159)
  %1162 : Float(256:1, 1024:256) = aten::t(%1161), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor, %1162), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1160, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.122), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1166 : __torch__.torch.nn.modules.normalization.___torch_mangle_17784.LayerNorm = prim::GetAttr[name="LayerNorm"](%1091)
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_17783.Linear = prim::GetAttr[name="dense"](%1091)
  %1168 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1169 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1170 : Float(1024:1, 256:1024) = aten::t(%1169), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.123, %1170), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.124 : Float(17:3328, 13:256, 256:1) = aten::add_(%output, %1168, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.124, %97, %96), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states, %input_tensor, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_electra.py:365:0
  %1175 : Tensor = prim::GetAttr[name="bias"](%1166)
  %1176 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1177 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm
  %1178 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input, %1177, %1176, %1175, %85, %84), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %47 : (Float(17:3328, 13:256, 256:1)) = prim::TupleConstruct(%1178)
  return (%47)
