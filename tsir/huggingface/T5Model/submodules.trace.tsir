T5Model(
  (shared): Embedding(32128, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)

T5Model._actual_script_module
T5Model.forward
  graph(%self.1 : __torch__.transformers.modeling_t5.T5Model,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %4055 : __torch__.transformers.modeling_t5.T5Stack = prim::GetAttr[name="decoder"](%self.1)
    %3797 : __torch__.transformers.modeling_t5.T5Stack = prim::GetAttr[name="decoder"](%self.1)
    %3798 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%3797)
    %3796 : __torch__.transformers.modeling_t5.T5Stack = prim::GetAttr[name="encoder"](%self.1)
    %3624 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="shared"](%self.1)
    %3625 : Tensor = prim::GetAttr[name="weight"](%3624)
    %4771 : Tensor = prim::CallMethod[name="forward"](%3796, %3798, %3625, %input_ids, %attention_mask.1)
    %4772 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4055, %3625, %input_ids, %attention_mask.1, %4771)
    %4746 : Float(17:6656, 8:64, 13:512, 64:1), %4747 : Float(17:6656, 8:64, 13:512, 64:1), %4748 : Float(17:6656, 8:64, 13:512, 64:1), %4749 : Float(17:6656, 8:64, 13:512, 64:1), %4750 : Float(17:6656, 8:64, 13:512, 64:1), %4751 : Float(17:6656, 8:64, 13:512, 64:1), %4752 : Float(17:6656, 8:64, 13:512, 64:1), %4753 : Float(17:6656, 8:64, 13:512, 64:1), %4754 : Float(17:6656, 8:64, 13:512, 64:1), %4755 : Float(17:6656, 8:64, 13:512, 64:1), %4756 : Float(17:6656, 8:64, 13:512, 64:1), %4757 : Float(17:6656, 8:64, 13:512, 64:1), %4758 : Float(17:6656, 8:64, 13:512, 64:1), %4759 : Float(17:6656, 8:64, 13:512, 64:1), %4760 : Float(17:6656, 8:64, 13:512, 64:1), %4761 : Float(17:6656, 8:64, 13:512, 64:1), %4762 : Float(17:6656, 8:64, 13:512, 64:1), %4763 : Float(17:6656, 8:64, 13:512, 64:1), %4764 : Float(17:6656, 8:64, 13:512, 64:1), %4765 : Float(17:6656, 8:64, 13:512, 64:1), %4766 : Float(17:6656, 8:64, 13:512, 64:1), %4767 : Float(17:6656, 8:64, 13:512, 64:1), %4768 : Float(17:6656, 8:64, 13:512, 64:1), %4769 : Float(17:6656, 8:64, 13:512, 64:1), %4770 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%4772)
    %3363 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4746, %4747, %4748, %4749)
    %3364 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4750, %4751, %4752, %4753)
    %3365 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4754, %4755, %4756, %4757)
    %3366 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4758, %4759, %4760, %4761)
    %3367 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4762, %4763, %4764, %4765)
    %3368 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%4766, %4767, %4768, %4769)
    %3369 : ((Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1))) = prim::TupleConstruct(%3363, %3364, %3365, %3366, %3367, %3368)
    %3370 : (Float(17:6656, 13:512, 512:1), ((Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)), (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1))), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%4770, %3369, %4771)
    return (%3370)

T5Model.decoder
T5Stack._actual_script_module
  graph(%self.104 : __torch__.transformers.modeling_t5.T5Stack,
        %weight.102 : Float(32128:512, 512:1),
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1),
        %149 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.104)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %4 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="5"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %6 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="4"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %8 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="3"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %10 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="2"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %12 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="1"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.104)
    %14 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="0"](%13)
    %15 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.104)
    %16 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%self.104)
    %17 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:693:0
    %18 : int = aten::size(%input_ids, %17), scope: __module.decoder # transformers/modeling_t5.py:693:0
    %batch_size : Long() = prim::NumToTensor(%18), scope: __module.decoder
    %21 : int = aten::Int(%batch_size), scope: __module.decoder
    %22 : int = aten::Int(%batch_size), scope: __module.decoder
    %23 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_t5.py:693:0
    %24 : int = aten::size(%input_ids, %23), scope: __module.decoder # transformers/modeling_t5.py:693:0
    %seq_length : Long() = prim::NumToTensor(%24), scope: __module.decoder
    %26 : int = aten::Int(%seq_length), scope: __module.decoder
    %27 : Scalar = aten::ScalarImplicit(%seq_length), scope: __module.decoder
    %28 : int = aten::Int(%seq_length), scope: __module.decoder
    %29 : int = aten::Int(%seq_length), scope: __module.decoder
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder # transformers/modeling_t5.py:694:0
    %31 : int[] = prim::ListConstruct(%30, %29), scope: __module.decoder
    %input.65 : Long(17:13, 13:1) = aten::view(%input_ids, %31), scope: __module.decoder # transformers/modeling_t5.py:694:0
    %190 : Tensor = prim::CallMethod[name="forward1"](%16, %weight.102, %input.65)
    %34 : int[] = prim::ListConstruct(%22, %28), scope: __module.decoder
    %35 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %36 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %37 : Device = prim::Constant[value="cpu"](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %38 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %39 : Float(17:13, 13:1) = aten::ones(%34, %35, %36, %37, %38), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %40 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %41 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %42 : Device = prim::Constant[value="cpu"](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %43 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %44 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %45 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %46 : None = prim::Constant(), scope: __module.decoder
    %attention_mask : Float(17:13, 13:1) = aten::to(%39, %40, %41, %42, %43, %44, %45, %46), scope: __module.decoder # transformers/modeling_t5.py:723:0
    %48 : None = prim::Constant(), scope: __module.decoder
    %49 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:238:0
    %50 : Device = prim::Constant[value="cpu"](), scope: __module.decoder # transformers/modeling_utils.py:238:0
    %51 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:238:0
    %seq_ids : Long(13:1) = aten::arange(%27, %48, %49, %50, %51), scope: __module.decoder # transformers/modeling_utils.py:238:0
    %53 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %54 : Long(1:13, 13:1) = aten::unsqueeze(%seq_ids, %53), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %56 : Long(1:13, 1:13, 13:1) = aten::unsqueeze(%54, %55), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %57 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %58 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %59 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %60 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %61 : Long(1:13, 1:13, 13:1) = aten::slice(%56, %57, %58, %59, %60), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %62 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %63 : int[] = prim::ListConstruct(%21, %26, %62), scope: __module.decoder
    %64 : Long(17:169, 13:13, 13:1) = aten::repeat(%61, %63), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %65 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %66 : Long(1:13, 13:1) = aten::unsqueeze(%seq_ids, %65), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %67 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %68 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %69 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %71 : Long(1:13, 13:1) = aten::slice(%66, %67, %68, %69, %70), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %72 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %73 : Long(1:13, 13:1, 1:1) = aten::unsqueeze(%71, %72), scope: __module.decoder # transformers/modeling_utils.py:239:0
    %causal_mask.1 : Bool(17:169, 13:13, 13:1) = aten::le(%64, %73), scope: __module.decoder # torch/tensor.py:22:0
    %75 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_utils.py:241:0
    %76 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:241:0
    %77 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:241:0
    %78 : None = prim::Constant(), scope: __module.decoder
    %causal_mask : Float(17:169, 13:13, 13:1) = aten::to(%causal_mask.1, %75, %76, %77, %78), scope: __module.decoder # transformers/modeling_utils.py:241:0
    %80 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %81 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %83 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %84 : Float(17:169, 13:13, 13:1) = aten::slice(%causal_mask, %80, %81, %82, %83), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %85 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %86 : Float(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%84, %85), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %87 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %88 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %89 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %90 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %91 : Float(17:169, 1:169, 13:13, 13:1) = aten::slice(%86, %87, %88, %89, %90), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %92 : int = prim::Constant[value=3](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %93 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %94 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %95 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %96 : Float(17:169, 1:169, 13:13, 13:1) = aten::slice(%91, %92, %93, %94, %95), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %97 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %98 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %99 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %100 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %101 : Float(17:13, 13:1) = aten::slice(%attention_mask, %97, %98, %99, %100), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %102 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %103 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%101, %102), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %104 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %105 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%103, %104), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %106 : int = prim::Constant[value=3](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %107 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %108 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %109 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %110 : Float(17:13, 1:13, 1:13, 13:1) = aten::slice(%105, %106, %107, %108, %109), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %extended_attention_mask : Float(17:169, 1:169, 13:13, 13:1) = aten::mul(%96, %110), scope: __module.decoder # transformers/modeling_utils.py:242:0
    %112 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_utils.py:257:0
    %113 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:257:0
    %114 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:257:0
    %115 : None = prim::Constant(), scope: __module.decoder
    %116 : Float(17:169, 1:169, 13:13, 13:1) = aten::to(%extended_attention_mask, %112, %113, %114, %115), scope: __module.decoder # transformers/modeling_utils.py:257:0
    %117 : float = prim::Constant[value=1.](), scope: __module.decoder # torch/tensor.py:396:0
    %118 : int = prim::Constant[value=1](), scope: __module.decoder # torch/tensor.py:396:0
    %119 : Float(17:169, 1:169, 13:13, 13:1) = aten::rsub(%116, %117, %118), scope: __module.decoder # torch/tensor.py:396:0
    %120 : Double() = prim::Constant[value={-10000}](), scope: __module.decoder # transformers/modeling_utils.py:258:0
    %mask.2 : Float(17:169, 1:169, 13:13, 13:1) = aten::mul(%119, %120), scope: __module.decoder # transformers/modeling_utils.py:258:0
    %122 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %123 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %124 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %125 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %126 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %122, %123, %124, %125), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %128 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %129 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%126, %128), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %130 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %131 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%129, %130), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %132 : int = prim::Constant[value=3](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %133 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %134 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %135 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %encoder_extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%131, %132, %133, %134, %135), scope: __module.decoder # transformers/modeling_utils.py:192:0
    %137 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_utils.py:198:0
    %138 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:198:0
    %139 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_utils.py:198:0
    %140 : None = prim::Constant(), scope: __module.decoder
    %141 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%encoder_extended_attention_mask, %137, %138, %139, %140), scope: __module.decoder # transformers/modeling_utils.py:198:0
    %142 : float = prim::Constant[value=1.](), scope: __module.decoder # torch/tensor.py:396:0
    %143 : int = prim::Constant[value=1](), scope: __module.decoder # torch/tensor.py:396:0
    %144 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%141, %142, %143), scope: __module.decoder # torch/tensor.py:396:0
    %145 : Double() = prim::Constant[value={-1e+09}](), scope: __module.decoder # transformers/modeling_utils.py:203:0
    %mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%144, %145), scope: __module.decoder # transformers/modeling_utils.py:203:0
    %191 : Tensor = prim::CallMethod[name="forward"](%15, %190)
    %192 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%14, %191, %mask.2, %149, %mask)
    %150 : Float(17:6656, 13:512, 512:1), %151 : Float(17:1352, 8:1, 13:104, 13:8), %152 : Float(17:1352, 8:1, 13:104, 13:8), %153 : Float(17:6656, 8:64, 13:512, 64:1), %154 : Float(17:6656, 8:64, 13:512, 64:1), %155 : Float(17:6656, 8:64, 13:512, 64:1), %156 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%192)
    %193 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %150, %151, %149, %152)
    %158 : Float(17:6656, 13:512, 512:1), %159 : Float(17:6656, 8:64, 13:512, 64:1), %160 : Float(17:6656, 8:64, 13:512, 64:1), %161 : Float(17:6656, 8:64, 13:512, 64:1), %162 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%193)
    %194 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %158, %151, %149, %152)
    %164 : Float(17:6656, 13:512, 512:1), %165 : Float(17:6656, 8:64, 13:512, 64:1), %166 : Float(17:6656, 8:64, 13:512, 64:1), %167 : Float(17:6656, 8:64, 13:512, 64:1), %168 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%194)
    %195 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %164, %151, %149, %152)
    %170 : Float(17:6656, 13:512, 512:1), %171 : Float(17:6656, 8:64, 13:512, 64:1), %172 : Float(17:6656, 8:64, 13:512, 64:1), %173 : Float(17:6656, 8:64, 13:512, 64:1), %174 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%195)
    %196 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %170, %151, %149, %152)
    %176 : Float(17:6656, 13:512, 512:1), %177 : Float(17:6656, 8:64, 13:512, 64:1), %178 : Float(17:6656, 8:64, 13:512, 64:1), %179 : Float(17:6656, 8:64, 13:512, 64:1), %180 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%196)
    %197 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %176, %151, %149, %152)
    %182 : Float(17:6656, 13:512, 512:1), %183 : Float(17:6656, 8:64, 13:512, 64:1), %184 : Float(17:6656, 8:64, 13:512, 64:1), %185 : Float(17:6656, 8:64, 13:512, 64:1), %186 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%197)
    %198 : Tensor = prim::CallMethod[name="forward"](%2, %182)
    %199 : Tensor = prim::CallMethod[name="forward1"](%15, %198)
    %189 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%153, %154, %155, %156, %159, %160, %161, %162, %165, %166, %167, %168, %171, %172, %173, %174, %177, %178, %179, %180, %183, %184, %185, %186, %199)
    return (%189)

T5Model.encoder
T5Stack._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_t5.T5Stack,
        %1 : __torch__.torch.nn.modules.sparse.Embedding,
        %weight.100 : Float(32128:512, 512:1),
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %3 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="final_layer_norm"](%self.2)
    %4 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %5 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="5"](%4)
    %6 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %7 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="4"](%6)
    %8 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %9 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="3"](%8)
    %10 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %11 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="2"](%10)
    %12 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %13 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="1"](%12)
    %14 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="block"](%self.2)
    %15 : __torch__.transformers.modeling_t5.T5Block = prim::GetAttr[name="0"](%14)
    %16 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.2)
    %21 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_t5.py:693:0
    %22 : int = aten::size(%input_ids, %21), scope: __module.encoder # transformers/modeling_t5.py:693:0
    %23 : Long() = prim::NumToTensor(%22), scope: __module.encoder
    %24 : int = aten::Int(%23), scope: __module.encoder
    %25 : int = prim::Constant[value=-1](), scope: __module.encoder # transformers/modeling_t5.py:694:0
    %26 : int[] = prim::ListConstruct(%25, %24), scope: __module.encoder
    %input.1 : Long(17:13, 13:1) = aten::view(%input_ids, %26), scope: __module.encoder # transformers/modeling_t5.py:694:0
    %65 : Tensor = prim::CallMethod[name="forward"](%1, %weight.100, %input.1)
    %29 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %30 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %31 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %32 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %33 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %29, %30, %31, %32), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %36 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%33, %35), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %37 : int = prim::Constant[value=2](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %38 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%36, %37), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %39 : int = prim::Constant[value=3](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %40 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %41 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %42 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %extended_attention_mask.1 : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%38, %39, %40, %41, %42), scope: __module.encoder # transformers/modeling_utils.py:244:0
    %44 : int = prim::Constant[value=6](), scope: __module.encoder # transformers/modeling_utils.py:257:0
    %45 : bool = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_utils.py:257:0
    %46 : bool = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_utils.py:257:0
    %47 : None = prim::Constant(), scope: __module.encoder
    %48 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask.1, %44, %45, %46, %47), scope: __module.encoder # transformers/modeling_utils.py:257:0
    %49 : float = prim::Constant[value=1.](), scope: __module.encoder # torch/tensor.py:396:0
    %50 : int = prim::Constant[value=1](), scope: __module.encoder # torch/tensor.py:396:0
    %51 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%48, %49, %50), scope: __module.encoder # torch/tensor.py:396:0
    %52 : Double() = prim::Constant[value={-10000}](), scope: __module.encoder # transformers/modeling_utils.py:258:0
    %mask.1 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%51, %52), scope: __module.encoder # transformers/modeling_utils.py:258:0
    %66 : Tensor = prim::CallMethod[name="forward"](%16, %65)
    %67 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%15, %66, %mask.1)
    %56 : Float(17:6656, 13:512, 512:1), %57 : Float(17:1352, 8:1, 13:104, 13:8) = prim::TupleUnpack(%67)
    %68 : Tensor = prim::CallMethod[name="forward"](%13, %56, %57)
    %69 : Tensor = prim::CallMethod[name="forward"](%11, %68, %57)
    %70 : Tensor = prim::CallMethod[name="forward"](%9, %69, %57)
    %71 : Tensor = prim::CallMethod[name="forward"](%7, %70, %57)
    %72 : Tensor = prim::CallMethod[name="forward"](%5, %71, %57)
    %73 : Tensor = prim::CallMethod[name="forward"](%3, %72)
    %74 : Tensor = prim::CallMethod[name="forward1"](%16, %73)
    return (%74)

Embedding.*
ModuleList.*
  module had no methods with graph attrs.

T5Stack.dropout
Dropout._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.dropout # torch/nn/functional.py:973:0
    %x.1 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.dropout # torch/nn/functional.py:973:0
    return (%x.1)

T5Stack.final_layer_norm
T5LayerNorm._actual_script_module
  graph(%self.102 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.final_layer_norm
    %x.62 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.62, %8), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.final_layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.final_layer_norm
    %variance.13 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.13, %15, %16), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %x.63 : Float(17:6656, 13:512, 512:1) = aten::div(%x.62, %18), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %input.64 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.63), scope: __module.encoder/__module.encoder.final_layer_norm # transformers/modeling_t5.py:172:0
    return (%input.64)

T5Block._actual_script_module
  graph(%self.5 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.1 : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.5)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.5)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1, %mask.1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:1352, 8:1, 13:104, 13:8) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%4, %8)
    %11 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8)) = prim::TupleConstruct(%13, %9)
    return (%11)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.8 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.1 : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.8)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="relative_attention_bias"](%self.8)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.8)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.8)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.8)
    %8 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.1 : Long() = prim::NumToTensor(%9), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %14 : int = aten::Int(%bs.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %15 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %16 : int = aten::size(%1, %15), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %qlen.1 : Long() = prim::NumToTensor(%16), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %18 : Scalar = aten::ScalarImplicit(%qlen.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %19 : Scalar = aten::ScalarImplicit(%qlen.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %171 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %24 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %25 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int[] = prim::ListConstruct(%14, %24, %25, %26), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %28 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%171, %27), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %29 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.1 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%28, %29, %30), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %172 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %33 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %34 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int[] = prim::ListConstruct(%13, %33, %34, %35), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %37 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%172, %36), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %38 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.1 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%37, %38, %39), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %173 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %42 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %43 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int[] = prim::ListConstruct(%12, %42, %43, %44), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %46 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%173, %45), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.1 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%46, %47, %48), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %50 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %51 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %52 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.1, %50, %51), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.1 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.1, %52), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %54 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %55 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %56 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %57 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %58 : Long(13:1) = aten::arange(%19, %54, %55, %56, %57), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %59 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %62 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %63 : Long(13:1) = aten::slice(%58, %59, %60, %61, %62), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %context_position.1 : Long(13:1, 1:1) = aten::unsqueeze(%63, %64), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %66 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %67 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %68 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %69 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %70 : Long(13:1) = aten::arange(%18, %66, %67, %68, %69), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %71 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %72 : Long(1:13, 13:1) = aten::unsqueeze(%70, %71), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %73 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %74 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %75 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %76 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %memory_position.1 : Long(1:13, 13:1) = aten::slice(%72, %73, %74, %75, %76), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %78 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:293:0
    %relative_position.1 : Long(13:13, 13:1) = aten::sub(%memory_position.1, %context_position.1, %78), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:293:0
    %n.1 : Long(13:13, 13:1) = aten::neg(%relative_position.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:267:0
    %81 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %82 : Bool(13:13, 13:1) = aten::lt(%n.1, %81), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %83 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %84 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %85 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %86 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %87 : Long(13:13, 13:1) = aten::to(%82, %83, %84, %85, %86), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %88 : Long() = prim::Constant[value={16}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %89 : Long(13:13, 13:1) = aten::mul(%87, %88), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %90 : Long() = prim::Constant[value={0}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %91 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %ret : Long(13:13, 13:1) = aten::add(%89, %90, %91), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:270:0
    %n.2 : Long(13:13, 13:1) = aten::abs(%n.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:271:0
    %94 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %is_small.1 : Bool(13:13, 13:1) = aten::lt(%n.2, %94), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %96 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %97 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %98 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %99 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %100 : Float(13:13, 13:1) = aten::to(%n.2, %96, %97, %98, %99), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %101 : Long() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %102 : Float(13:13, 13:1) = aten::div(%100, %101), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %103 : Float(13:13, 13:1) = aten::log(%102), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %104 : Double() = prim::Constant[value={2.77259}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %105 : Float(13:13, 13:1) = aten::div(%103, %104), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %106 : Long() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %107 : Float(13:13, 13:1) = aten::mul(%105, %106), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %108 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %109 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %110 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %111 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %112 : Long(13:13, 13:1) = aten::to(%107, %108, %109, %110, %111), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %113 : Long() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %val_if_large.1 : Long(13:13, 13:1) = aten::add(%112, %113, %114), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %116 : int = prim::Constant[value=15](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %117 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %118 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %119 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %120 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %121 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %122 : Long(13:13, 13:1) = aten::full_like(%val_if_large.1, %116, %117, %118, %119, %120, %121), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %val_if_large.2 : Long(13:13, 13:1) = aten::min(%val_if_large.1, %122), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %124 : Long(13:13, 13:1) = aten::where(%is_small.1, %n.2, %val_if_large.2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %125 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %rp_bucket.1 : Long(13:13, 13:1) = aten::add_(%ret, %124, %125), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %127 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %128 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %129 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %130 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %131 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %132 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %133 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %input.4 : Long(13:13, 13:1) = aten::to(%rp_bucket.1, %127, %128, %129, %130, %131, %132, %133), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %174 : Tensor = prim::CallMethod[name="forward"](%4, %input.4)
    %136 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %137 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %138 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %139 : int[] = prim::ListConstruct(%136, %137, %138), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %140 : Float(8:1, 13:104, 13:8) = aten::permute(%174, %139), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %141 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %position_bias.1 : Float(1:8, 8:1, 13:104, 13:8) = aten::unsqueeze(%140, %141), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %143 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:387:0
    %position_bias.2 : Float(17:1352, 8:1, 13:104, 13:8) = aten::add(%position_bias.1, %mask.1, %143), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:387:0
    %145 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.2 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.1, %position_bias.2, %145), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %147 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %148 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %149 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %150 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %input.5 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.2, %147, %148, %149, %150), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %152 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %153 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %154 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.5, %152, %153), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.6 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%154, %input.5), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %156 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %157 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.1 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.6, %156, %157), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.7 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.1, %v.1), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %161 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %162 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.7, %160, %161), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %163 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %164 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%162, %163), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %165 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %166 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %167 : int[] = prim::ListConstruct(%11, %165, %166), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention
    %input.7 : Float(17:6656, 13:512, 512:1) = aten::view(%164, %167), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %175 : Tensor = prim::CallMethod[name="forward"](%3, %input.7)
    %170 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8)) = prim::TupleConstruct(%175, %position_bias.2)
    return (%170)

T5LayerSelfAttention._actual_script_module
  graph(%self.6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.1 : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.6)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.6)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.6)
    %16 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%16)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %mask.1)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:1352, 8:1, 13:104, 13:8) = prim::TupleUnpack(%17)
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %13 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0 # transformers/modeling_t5.py:439:0
    %x.8 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %18, %13), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0 # transformers/modeling_t5.py:439:0
    %15 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8)) = prim::TupleConstruct(%x.8, %11)
    return (%15)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    %y.1 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.1)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm
    %x.2 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.2, %8), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm
    %variance.1 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.1, %15, %16), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.3 : Float(17:6656, 13:512, 512:1) = aten::div(%x.2, %18), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.3 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.3, %x.2)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.5 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.5)

T5Attention.o
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.8 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.7, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.8)

T5Attention.q
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.4 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.4)

T5Attention.relative_attention_bias
Embedding._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.4 : Long(13:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %3 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %values.1 : Float(13:104, 13:8, 8:1) = aten::embedding(%2, %input.4, %3, %4, %5), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    return (%values.1)

T5Attention.v
Linear._actual_script_module
  graph(%self.11 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.11)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.6 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.0/__module.encoder.block.0.layer.0.SelfAttention/__module.encoder.block.0.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.6)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.17)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.17)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.17)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.11 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.11)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.15 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.15)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.15)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.15)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1 # transformers/modeling_t5.py:200:0
    %x.11 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.11)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    %y.2 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.2)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.16 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm
    %x.9 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.9, %8), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm
    %variance.2 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.2, %15, %16), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.10 : Float(17:6656, 13:512, 512:1) = aten::div(%x.9, %18), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.9 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.10), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.9, %x.9)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.11 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.11, %2, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.12)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.10)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.13 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.0/__module.encoder.block.0.layer.1/__module.encoder.block.0.layer.1.DenseReluDense/__module.encoder.block.0.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.13)

T5Block._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.22)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.22)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %10 : Tensor = prim::CallMethod[name="forward"](%4, %9)
    return (%10)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.25 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.25)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.25)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.25)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.25)
    %7 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.2 : Long() = prim::NumToTensor(%8), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %76 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%76, %24), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.2 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %77 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %33), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.2 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %42), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.2 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.2, %47, %48), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.3 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.2, %49), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.4 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.3, %2, %51), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %input.15 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.4, %53, %54, %55, %56), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.15, %58, %59), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.16 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.15), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.2 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.16, %62, %63), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.17 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.2, %v.2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.17, %66, %67), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention
    %input.17 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %79 : Tensor = prim::CallMethod[name="forward"](%3, %input.17)
    return (%79)

T5LayerSelfAttention._actual_script_module
  graph(%self.23 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.23)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.23)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.23)
    %13 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%13)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %7, %2)
    %15 : Tensor = prim::CallMethod[name="forward"](%3, %14)
    %11 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0 # transformers/modeling_t5.py:439:0
    %x.18 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %15, %11), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0 # transformers/modeling_t5.py:439:0
    return (%x.18)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    %y.3 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.3)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.24 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm
    %x.12 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.12, %8), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm
    %variance.3 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.3, %15, %16), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.13 : Float(17:6656, 13:512, 512:1) = aten::div(%x.12, %18), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.14 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.13), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.14, %x.12)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.15 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.15)

T5Attention.o
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.18 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.17, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.18)

T5Attention.q
Linear._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.14 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.14)

T5Attention.v
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.16 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.0/__module.encoder.block.1.layer.0.SelfAttention/__module.encoder.block.1.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.16)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.33 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.33)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.33)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.33)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.21 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.21)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.31 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.31)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.31)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.31)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1 # transformers/modeling_t5.py:200:0
    %x.21 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.21)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    %y.4 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.4)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.32 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm
    %x.19 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.19, %8), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm
    %variance.4 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.4, %15, %16), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.20 : Float(17:6656, 13:512, 512:1) = aten::div(%x.19, %18), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.19 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.20), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.19, %x.19)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.21 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.21, %2, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.22)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.20)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.23 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.1/__module.encoder.block.1.layer.1/__module.encoder.block.1.layer.1.DenseReluDense/__module.encoder.block.1.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.23)

T5Block._actual_script_module
  graph(%self.38 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.38)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.38)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %10 : Tensor = prim::CallMethod[name="forward"](%4, %9)
    return (%10)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.41)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.41)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.41)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.41)
    %7 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.3 : Long() = prim::NumToTensor(%8), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %76 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%76, %24), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.3 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %77 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %33), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.3 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %42), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.3 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.3, %47, %48), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.5 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.3, %49), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.6 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.5, %2, %51), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %input.25 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.6, %53, %54, %55, %56), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.25, %58, %59), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.26 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.25), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.3 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.26, %62, %63), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.27 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.3, %v.3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.27, %66, %67), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention
    %input.27 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %79 : Tensor = prim::CallMethod[name="forward"](%3, %input.27)
    return (%79)

T5LayerSelfAttention._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.39)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.39)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.39)
    %13 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%13)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %7, %2)
    %15 : Tensor = prim::CallMethod[name="forward"](%3, %14)
    %11 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0 # transformers/modeling_t5.py:439:0
    %x.28 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %15, %11), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0 # transformers/modeling_t5.py:439:0
    return (%x.28)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    %y.5 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.5)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.40 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm
    %x.22 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.22, %8), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm
    %variance.5 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.5, %15, %16), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.23 : Float(17:6656, 13:512, 512:1) = aten::div(%x.22, %18), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.24 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.23), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.24, %x.22)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.25 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.25)

T5Attention.o
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %input.27 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.28 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.27, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.28)

T5Attention.q
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.24 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.24)

T5Attention.v
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.26 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.0/__module.encoder.block.2.layer.0.SelfAttention/__module.encoder.block.2.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.26)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.49 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.49)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.49)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.49)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.31 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.31)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.47 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.47)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.47)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.47)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1 # transformers/modeling_t5.py:200:0
    %x.31 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.31)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    %y.6 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.6)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.48 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm
    %x.29 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.29, %8), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm
    %variance.6 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.6, %15, %16), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.30 : Float(17:6656, 13:512, 512:1) = aten::div(%x.29, %18), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.29 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.30), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.29, %x.29)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.31 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.31, %2, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.32)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.30)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.33 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.2/__module.encoder.block.2.layer.1/__module.encoder.block.2.layer.1.DenseReluDense/__module.encoder.block.2.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.33)

T5Block._actual_script_module
  graph(%self.54 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.54)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.54)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %10 : Tensor = prim::CallMethod[name="forward"](%4, %9)
    return (%10)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.57 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.57)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.57)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.57)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.57)
    %7 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.4 : Long() = prim::NumToTensor(%8), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.4), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.4), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.4), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.4), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %76 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%76, %24), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.4 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %77 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %33), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.4 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %42), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.4 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.4, %47, %48), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.7 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.4, %49), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.8 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.7, %2, %51), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %input.35 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.8, %53, %54, %55, %56), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.35, %58, %59), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.36 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.35), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.4 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.36, %62, %63), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.37 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.4, %v.4), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.37, %66, %67), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention
    %input.37 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %79 : Tensor = prim::CallMethod[name="forward"](%3, %input.37)
    return (%79)

T5LayerSelfAttention._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.55)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.55)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.55)
    %13 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%13)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %7, %2)
    %15 : Tensor = prim::CallMethod[name="forward"](%3, %14)
    %11 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0 # transformers/modeling_t5.py:439:0
    %x.38 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %15, %11), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0 # transformers/modeling_t5.py:439:0
    return (%x.38)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    %y.7 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.7)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm
    %x.32 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.32, %8), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm
    %variance.7 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.7, %15, %16), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.33 : Float(17:6656, 13:512, 512:1) = aten::div(%x.32, %18), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.34 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.33), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.34, %x.32)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.35 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.35)

T5Attention.o
Linear._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.linear.Linear,
        %input.37 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.38 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.37, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.38)

T5Attention.q
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.34 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.34)

T5Attention.v
Linear._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.36 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.0/__module.encoder.block.3.layer.0.SelfAttention/__module.encoder.block.3.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.36)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.65 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.65)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.65)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.65)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.41 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.41)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.63 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.63)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.63)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.63)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1 # transformers/modeling_t5.py:200:0
    %x.41 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.41)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    %y.8 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.8)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.64 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm
    %x.39 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.39, %8), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm
    %variance.8 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.8, %15, %16), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.40 : Float(17:6656, 13:512, 512:1) = aten::div(%x.39, %18), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.39 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.40), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.39, %x.39)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.41 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.41, %2, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.42)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.40)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.43 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.3/__module.encoder.block.3.layer.1/__module.encoder.block.3.layer.1.DenseReluDense/__module.encoder.block.3.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.43)

T5Block._actual_script_module
  graph(%self.70 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.70)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.70)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %10 : Tensor = prim::CallMethod[name="forward"](%4, %9)
    return (%10)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.73)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.73)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.73)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.73)
    %7 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.5 : Long() = prim::NumToTensor(%8), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.5), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.5), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.5), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.5), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %76 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%76, %24), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.5 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %77 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %33), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.5 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %42), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.5 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.5, %47, %48), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.9 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.5, %49), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.10 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.9, %2, %51), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %input.45 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.10, %53, %54, %55, %56), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.45, %58, %59), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.46 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.45), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.5 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.46, %62, %63), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.47 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.5, %v.5), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.47, %66, %67), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention
    %input.47 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %79 : Tensor = prim::CallMethod[name="forward"](%3, %input.47)
    return (%79)

T5LayerSelfAttention._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.71)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.71)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.71)
    %13 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%13)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %7, %2)
    %15 : Tensor = prim::CallMethod[name="forward"](%3, %14)
    %11 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0 # transformers/modeling_t5.py:439:0
    %x.48 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %15, %11), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0 # transformers/modeling_t5.py:439:0
    return (%x.48)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    %y.9 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.9)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm
    %x.42 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.42, %8), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm
    %variance.9 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.9, %15, %16), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.43 : Float(17:6656, 13:512, 512:1) = aten::div(%x.42, %18), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.44 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.43), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.44, %x.42)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.75)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.45 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.45)

T5Attention.o
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %input.47 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.48 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.47, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.48)

T5Attention.q
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.44 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.44)

T5Attention.v
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.46 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.0/__module.encoder.block.4.layer.0.SelfAttention/__module.encoder.block.4.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.46)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.81 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.81)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.81)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.81)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.51 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.51)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.79 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.79)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.79)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.79)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1 # transformers/modeling_t5.py:200:0
    %x.51 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.51)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    %y.10 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.10)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.80 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm
    %x.49 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.49, %8), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm
    %variance.10 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.10, %15, %16), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.50 : Float(17:6656, 13:512, 512:1) = aten::div(%x.49, %18), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.49 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.50), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.49, %x.49)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.51 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.51, %2, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.52)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.50)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.53 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.4/__module.encoder.block.4.layer.1/__module.encoder.block.4.layer.1.DenseReluDense/__module.encoder.block.4.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.53)

T5Block._actual_script_module
  graph(%self.86 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.86)
    %4 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="1"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.86)
    %6 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%5)
    %9 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %10 : Tensor = prim::CallMethod[name="forward"](%4, %9)
    return (%10)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.89 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.89)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.89)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.89)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.89)
    %7 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.6 : Long() = prim::NumToTensor(%8), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %76 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%76, %24), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.6 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %77 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %33), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.6 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %42), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.6 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.6, %47, %48), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.11 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.6, %49), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.12 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.11, %2, %51), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %input.55 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.12, %53, %54, %55, %56), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.55, %58, %59), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.56 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.55), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.6 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.56, %62, %63), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.57 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.6, %v.6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.57, %66, %67), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention
    %input.57 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %79 : Tensor = prim::CallMethod[name="forward"](%3, %input.57)
    return (%79)

T5LayerSelfAttention._actual_script_module
  graph(%self.87 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.87)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.87)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.87)
    %13 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%13)
    %14 : Tensor = prim::CallMethod[name="forward"](%4, %7, %2)
    %15 : Tensor = prim::CallMethod[name="forward"](%3, %14)
    %11 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0 # transformers/modeling_t5.py:439:0
    %x.58 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %15, %11), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0 # transformers/modeling_t5.py:439:0
    return (%x.58)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    %y.11 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.11)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm
    %x.52 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.52, %8), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm
    %variance.11 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.11, %15, %16), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.53 : Float(17:6656, 13:512, 512:1) = aten::div(%x.52, %18), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.54 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.53), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.54, %x.52)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.55 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.55)

T5Attention.o
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %input.57 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.58 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.57, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.58)

T5Attention.q
Linear._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.54 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.54)

T5Attention.v
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.56 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.0/__module.encoder.block.5.layer.0.SelfAttention/__module.encoder.block.5.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.56)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.97 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.97)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.97)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.97)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.61 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.61)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.95)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.95)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.95)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1 # transformers/modeling_t5.py:200:0
    %x.61 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1 # transformers/modeling_t5.py:200:0
    return (%x.61)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    %y.12 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.12)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.96 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.96)
    %3 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm
    %x.59 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.59, %8), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm
    %variance.12 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.12, %15, %16), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.60 : Float(17:6656, 13:512, 512:1) = aten::div(%x.59, %18), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.59 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.60), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.59, %x.59)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.61 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.61, %2, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.62)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.60)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.63 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.encoder/__module.encoder.block.5/__module.encoder.block.5.layer.1/__module.encoder.block.5.layer.1.DenseReluDense/__module.encoder.block.5.layer.1.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.63)

T5Stack.dropout
Dropout._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.dropout # torch/nn/functional.py:973:0
    %x.64 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.dropout # torch/nn/functional.py:973:0
    return (%x.64)

T5Stack.embed_tokens
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %weight.1 : Float(32128:512, 512:1),
        %input.1 : Long(17:13, 13:1)):
    %3 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
    %input.2 : Float(17:6656, 13:512, 512:1) = aten::embedding(%weight.1, %input.1, %3, %4, %5), scope: __module.encoder/__module.decoder.embed_tokens # torch/nn/functional.py:1814:0
    return (%input.2)

T5Stack.final_layer_norm
T5LayerNorm._actual_script_module
  graph(%self.253 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.253)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.final_layer_norm
    %x.167 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.167, %8), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.final_layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.final_layer_norm
    %variance : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance, %15, %16), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %x : Float(17:6656, 13:512, 512:1) = aten::div(%x.167, %18), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:168:0
    %input : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x), scope: __module.decoder/__module.decoder.final_layer_norm # transformers/modeling_t5.py:172:0
    return (%input)

T5Block._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.2 : Float(17:169, 1:169, 13:13, 13:1),
        %3 : Float(17:6656, 13:512, 512:1),
        %mask : Float(17:13, 1:13, 1:13, 13:1)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.107)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.107)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.107)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %35 : (Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %mask.2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:1352, 8:1, 13:104, 13:8), %15 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%35)
    %36 : (Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %mask)
    %29 : Float(17:6656, 13:512, 512:1), %30 : Float(17:1352, 8:1, 13:104, 13:8), %31 : Float(17:6656, 8:64, 13:512, 64:1), %32 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%36)
    %37 : Tensor = prim::CallMethod[name="forward"](%6, %29)
    %34 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8), Float(17:1352, 8:1, 13:104, 13:8), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%37, %14, %30, %12, %15, %31, %32)
    return (%34)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.110 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.2 : Float(17:169, 1:169, 13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.110)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="relative_attention_bias"](%self.110)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.110)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.110)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.110)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.7 : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.7), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.7), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.7), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %14 : int = aten::Int(%bs.7), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %16 : int = aten::size(%1, %15), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %qlen.2 : Long() = prim::NumToTensor(%16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %18 : Scalar = aten::ScalarImplicit(%qlen.2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %19 : Scalar = aten::ScalarImplicit(%qlen.2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %166 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %24 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %25 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int[] = prim::ListConstruct(%14, %24, %25, %26), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %28 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%166, %27), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %29 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.7 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%28, %29, %30), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %167 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %33 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %34 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int[] = prim::ListConstruct(%13, %33, %34, %35), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %37 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%167, %36), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %38 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.7 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%37, %38, %39), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %168 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %42 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %43 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int[] = prim::ListConstruct(%12, %42, %43, %44), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %46 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%168, %45), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.7 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%46, %47, %48), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %50 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %51 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %52 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.7, %50, %51), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.13 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.7, %52), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %54 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %55 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %56 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %57 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %58 : Long(13:1) = aten::arange(%19, %54, %55, %56, %57), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %59 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %60 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %62 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %63 : Long(13:1) = aten::slice(%58, %59, %60, %61, %62), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %64 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %context_position.2 : Long(13:1, 1:1) = aten::unsqueeze(%63, %64), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:291:0
    %66 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %67 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %68 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %69 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %70 : Long(13:1) = aten::arange(%18, %66, %67, %68, %69), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %71 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %72 : Long(1:13, 13:1) = aten::unsqueeze(%70, %71), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %73 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %74 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %75 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %76 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %memory_position.2 : Long(1:13, 13:1) = aten::slice(%72, %73, %74, %75, %76), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:292:0
    %78 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:293:0
    %relative_position.2 : Long(13:13, 13:1) = aten::sub(%memory_position.2, %context_position.2, %78), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:293:0
    %n.3 : Long(13:13, 13:1) = aten::neg(%relative_position.2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:267:0
    %81 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %82 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %83 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %84 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %85 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %86 : Long(13:13, 13:1) = aten::zeros_like(%n.3, %81, %82, %83, %84, %85), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %n.4 : Long(13:13, 13:1) = aten::max(%n.3, %86), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:273:0
    %88 : int = prim::Constant[value=16](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %is_small.2 : Bool(13:13, 13:1) = aten::lt(%n.4, %88), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/tensor.py:22:0
    %90 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %91 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %92 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %93 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %94 : Float(13:13, 13:1) = aten::to(%n.4, %90, %91, %92, %93), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %95 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %96 : Float(13:13, 13:1) = aten::div(%94, %95), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %97 : Float(13:13, 13:1) = aten::log(%96), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %98 : Double() = prim::Constant[value={2.07944}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %99 : Float(13:13, 13:1) = aten::div(%97, %98), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %100 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %101 : Float(13:13, 13:1) = aten::mul(%99, %100), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:282:0
    %102 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %103 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %104 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %105 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %106 : Long(13:13, 13:1) = aten::to(%101, %102, %103, %104, %105), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %107 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %108 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %val_if_large.3 : Long(13:13, 13:1) = aten::add(%106, %107, %108), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:281:0
    %110 : int = prim::Constant[value=31](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %111 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %112 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %113 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %114 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %115 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %116 : Long(13:13, 13:1) = aten::full_like(%val_if_large.3, %110, %111, %112, %113, %114, %115), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %val_if_large.4 : Long(13:13, 13:1) = aten::min(%val_if_large.3, %116), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:284:0
    %118 : Long(13:13, 13:1) = aten::where(%is_small.2, %n.4, %val_if_large.4), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %119 : Long() = prim::Constant[value={0}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %120 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %rp_bucket.2 : Long(13:13, 13:1) = aten::add(%118, %119, %120), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:286:0
    %122 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %123 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %124 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %125 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %126 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %127 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %128 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %input.68 : Long(13:13, 13:1) = aten::to(%rp_bucket.2, %122, %123, %124, %125, %126, %127, %128), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:299:0
    %169 : Tensor = prim::CallMethod[name="forward"](%4, %input.68)
    %131 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %132 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %133 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %134 : int[] = prim::ListConstruct(%131, %132, %133), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %135 : Float(8:1, 13:104, 13:8) = aten::permute(%169, %134), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %136 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %position_bias.3 : Float(1:8, 8:1, 13:104, 13:8) = aten::unsqueeze(%135, %136), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:301:0
    %138 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:387:0
    %position_bias.4 : Float(17:1352, 8:1, 13:104, 13:8) = aten::add(%position_bias.3, %mask.2, %138), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:387:0
    %140 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.14 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.13, %position_bias.4, %140), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %142 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %143 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %144 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %145 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %input.69 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.14, %142, %143, %144, %145), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %147 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %148 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %149 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.69, %147, %148), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.70 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%149, %input.69), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %151 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %152 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.7 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.70, %151, %152), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.70 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.7, %v.7), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %155 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %156 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %157 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.70, %155, %156), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %158 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %159 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%157, %158), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %160 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %161 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %162 : int[] = prim::ListConstruct(%11, %160, %161), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention
    %input.71 : Float(17:6656, 13:512, 512:1) = aten::view(%159, %162), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %170 : Tensor = prim::CallMethod[name="forward"](%3, %input.71)
    %165 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:1352, 8:1, 13:104, 13:8), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%170, %k.7, %position_bias.4, %v.7)
    return (%165)

T5LayerSelfAttention._actual_script_module
  graph(%self.108 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %mask.2 : Float(17:169, 1:169, 13:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.108)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.108)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.108)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %mask.2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:1352, 8:1, 13:104, 13:8), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0 # transformers/modeling_t5.py:439:0
    %x.71 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %20, %15), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0 # transformers/modeling_t5.py:439:0
    %17 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.71, %12, %13)
    return (%17)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    %y.13 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.13)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.109 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm
    %x.65 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.65, %8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm
    %variance.14 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.14, %15, %16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.66 : Float(17:6656, 13:512, 512:1) = aten::div(%x.65, %18), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.67 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.66), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.67, %x.65)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.68 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.68)

T5Attention.o
Linear._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.linear.Linear,
        %input.71 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.72 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.71, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.72)

T5Attention.q
Linear._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.111)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.67 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.67)

T5Attention.relative_attention_bias
Embedding._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.68 : Long(13:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %3 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %values.2 : Float(13:104, 13:8, 8:1) = aten::embedding(%2, %input.68, %3, %4, %5), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    return (%values.2)

T5Attention.v
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.69 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.0/__module.decoder.block.0.layer.0.SelfAttention/__module.decoder.block.0.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.69)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.119 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.119)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="relative_attention_bias"](%self.119)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.119)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.119)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.119)
    %9 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %10 : int = aten::size(%1, %9), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs.8 : Long() = prim::NumToTensor(%10), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs.8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs.8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs.8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %15 : int = aten::Int(%bs.8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %17 : int = aten::size(%1, %16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %qlen : Long() = prim::NumToTensor(%17), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %19 : Scalar = aten::ScalarImplicit(%qlen), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %23 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:338:0
    %24 : int = aten::size(%2, %23), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:338:0
    %klen : Long() = prim::NumToTensor(%24), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %26 : Scalar = aten::ScalarImplicit(%klen), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %170 : Tensor = prim::CallMethod[name="forward"](%8, %1)
    %28 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %29 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int[] = prim::ListConstruct(%15, %28, %29, %30), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %32 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%170, %31), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %33 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %34 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q.8 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%32, %33, %34), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %171 : Tensor = prim::CallMethod[name="forward"](%7, %2)
    %37 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %38 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int[] = prim::ListConstruct(%14, %37, %38, %39), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %41 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%171, %40), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %42 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %43 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k.8 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%41, %42, %43), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %172 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %46 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int[] = prim::ListConstruct(%13, %46, %47, %48), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %50 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%172, %49), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v.8 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%50, %51, %52), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %54 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %55 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %56 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.8, %54, %55), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.15 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.8, %56), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %58 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %59 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %60 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %61 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %62 : Long(13:1) = aten::arange(%19, %58, %59, %60, %61), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %63 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %64 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %65 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %67 : Long(13:1) = aten::slice(%62, %63, %64, %65, %66), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %68 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %context_position : Long(13:1, 1:1) = aten::unsqueeze(%67, %68), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:291:0
    %70 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %71 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %72 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %73 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %74 : Long(13:1) = aten::arange(%26, %70, %71, %72, %73), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %75 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %76 : Long(1:13, 13:1) = aten::unsqueeze(%74, %75), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %77 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %78 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %79 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %80 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %memory_position : Long(1:13, 13:1) = aten::slice(%76, %77, %78, %79, %80), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:292:0
    %82 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:293:0
    %relative_position : Long(13:13, 13:1) = aten::sub(%memory_position, %context_position, %82), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:293:0
    %n.5 : Long(13:13, 13:1) = aten::neg(%relative_position), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:267:0
    %85 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %86 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %87 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %88 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %89 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %90 : Long(13:13, 13:1) = aten::zeros_like(%n.5, %85, %86, %87, %88, %89), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %n : Long(13:13, 13:1) = aten::max(%n.5, %90), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:273:0
    %92 : int = prim::Constant[value=16](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/tensor.py:22:0
    %is_small : Bool(13:13, 13:1) = aten::lt(%n, %92), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/tensor.py:22:0
    %94 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %95 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %96 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %97 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %98 : Float(13:13, 13:1) = aten::to(%n, %94, %95, %96, %97), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %99 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %100 : Float(13:13, 13:1) = aten::div(%98, %99), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %101 : Float(13:13, 13:1) = aten::log(%100), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %102 : Double() = prim::Constant[value={2.07944}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %103 : Float(13:13, 13:1) = aten::div(%101, %102), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %104 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %105 : Float(13:13, 13:1) = aten::mul(%103, %104), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:282:0
    %106 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %107 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %108 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %109 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %110 : Long(13:13, 13:1) = aten::to(%105, %106, %107, %108, %109), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %111 : Long() = prim::Constant[value={16}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %112 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %val_if_large.5 : Long(13:13, 13:1) = aten::add(%110, %111, %112), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:281:0
    %114 : int = prim::Constant[value=31](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %115 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %116 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %117 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %118 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %119 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %120 : Long(13:13, 13:1) = aten::full_like(%val_if_large.5, %114, %115, %116, %117, %118, %119), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %val_if_large : Long(13:13, 13:1) = aten::min(%val_if_large.5, %120), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:284:0
    %122 : Long(13:13, 13:1) = aten::where(%is_small, %n, %val_if_large), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:286:0
    %123 : Long() = prim::Constant[value={0}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:286:0
    %124 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:286:0
    %rp_bucket : Long(13:13, 13:1) = aten::add(%122, %123, %124), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:286:0
    %126 : int = prim::Constant[value=4](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %127 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %128 : Device = prim::Constant[value="cpu"](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %129 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %130 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %131 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %132 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %input.74 : Long(13:13, 13:1) = aten::to(%rp_bucket, %126, %127, %128, %129, %130, %131, %132), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:299:0
    %173 : Tensor = prim::CallMethod[name="forward"](%5, %input.74)
    %135 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %136 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %137 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %138 : int[] = prim::ListConstruct(%135, %136, %137), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %139 : Float(8:1, 13:104, 13:8) = aten::permute(%173, %138), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %140 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %position_bias.5 : Float(1:8, 8:1, 13:104, 13:8) = aten::unsqueeze(%139, %140), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:301:0
    %142 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:387:0
    %position_bias : Float(17:1352, 8:1, 13:104, 13:8) = aten::add(%position_bias.5, %mask, %142), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:387:0
    %144 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores.16 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.15, %position_bias, %144), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %146 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %147 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %148 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %149 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %input.75 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.16, %146, %147, %148, %149), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %151 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %152 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %153 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.75, %151, %152), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.76 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%153, %input.75), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %155 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %156 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights.8 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.76, %155, %156), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.77 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.8, %v.8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %159 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %160 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %161 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.77, %159, %160), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %162 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %163 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%161, %162), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %164 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %165 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %166 : int[] = prim::ListConstruct(%12, %164, %165), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention
    %input.77 : Float(17:6656, 13:512, 512:1) = aten::view(%163, %166), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %174 : Tensor = prim::CallMethod[name="forward"](%4, %input.77)
    %169 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%174, %position_bias, %k.8, %v.8)
    return (%169)

T5LayerCrossAttention._actual_script_module
  graph(%self.117 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %mask : Float(17:13, 1:13, 1:13, 13:1)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.117)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.117)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.117)
    %19 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%19)
    %20 : (Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %mask)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:1352, 8:1, 13:104, 13:8), %13 : Float(17:6656, 8:64, 13:512, 64:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%20)
    %21 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1 # transformers/modeling_t5.py:476:0
    %x.78 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %21, %16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1 # transformers/modeling_t5.py:476:0
    %18 : (Float(17:6656, 13:512, 512:1), Float(17:1352, 8:1, 13:104, 13:8), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.78, %12, %13, %14)
    return (%18)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    %y.14 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.14)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.118 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm
    %x.72 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.72, %8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm
    %variance.15 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.15, %15, %16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.73 : Float(17:6656, 13:512, 512:1) = aten::div(%x.72, %18), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.73 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.73), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.73, %x.72)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.75 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.75)

T5Attention.o
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %input.77 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.78 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.77, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.78)

T5Attention.q
Linear._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.74 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.74)

T5Attention.relative_attention_bias
Embedding._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.74 : Long(13:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %3 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    %values : Float(13:104, 13:8, 8:1) = aten::embedding(%2, %input.74, %3, %4, %5), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias # torch/nn/functional.py:1814:0
    return (%values)

T5Attention.v
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.76 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.1/__module.decoder.block.0.layer.1.EncDecAttention/__module.decoder.block.0.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.76)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.128)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.128)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.128)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.81 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.81)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.126 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.126)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.126)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.126)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2 # transformers/modeling_t5.py:200:0
    %x.81 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.81)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.132 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.dropout # torch/nn/functional.py:973:0
    %y.15 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y.15)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.127)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm
    %x.79 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.79, %8), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm
    %variance.16 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.16, %15, %16), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.80 : Float(17:6656, 13:512, 512:1) = aten::div(%x.79, %18), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.79 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.80), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.79, %x.79)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.130 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.81 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.81, %2, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.82)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.129)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.80 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.80)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.131 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.131)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.83 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.0/__module.decoder.block.0.layer.2/__module.decoder.block.0.layer.2.DenseReluDense/__module.decoder.block.0.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.83)

T5Block._actual_script_module
  graph(%self.133 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8),
        %3 : Float(17:6656, 13:512, 512:1),
        %4 : Float(17:1352, 8:1, 13:104, 13:8)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.133)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.133)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.133)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %33 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%33)
    %34 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %4)
    %28 : Float(17:6656, 13:512, 512:1), %29 : Float(17:6656, 8:64, 13:512, 64:1), %30 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%34)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %28)
    %32 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%35, %12, %14, %29, %30)
    return (%32)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.136 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.136)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.136)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.136)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.136)
    %7 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.9 : Long() = prim::NumToTensor(%8), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %24), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.9 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %33), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.9 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%79, %42), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.9 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.9, %47, %48), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.17 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.9, %49), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.18 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.17, %2, %51), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %input.85 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.18, %53, %54, %55, %56), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.85, %58, %59), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.86 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.85), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.9 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.86, %62, %63), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.87 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.9, %v.9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.87, %66, %67), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention
    %input.87 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.87)
    %76 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%80, %k.9, %v.9)
    return (%76)

T5LayerSelfAttention._actual_script_module
  graph(%self.134 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.134)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.134)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.134)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%17)
    %18 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%18)
    %19 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %14 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0 # transformers/modeling_t5.py:439:0
    %x.88 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %19, %14), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0 # transformers/modeling_t5.py:439:0
    %16 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.88, %12)
    return (%16)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.141 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    %y.16 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.16)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.135 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.135)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm
    %x.82 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.82, %8), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm
    %variance.17 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.17, %15, %16), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.83 : Float(17:6656, 13:512, 512:1) = aten::div(%x.82, %18), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.84 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.83), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.84, %x.82)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.138 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.138)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.85 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.85)

T5Attention.o
Linear._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.linear.Linear,
        %input.87 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.88 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.87, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.88)

T5Attention.q
Linear._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.137)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.84 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.84)

T5Attention.v
Linear._actual_script_module
  graph(%self.139 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.139)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.86 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.0/__module.decoder.block.1.layer.0.SelfAttention/__module.decoder.block.1.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.86)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.144 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.144)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.144)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.144)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.144)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs.10 : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %11 : int = aten::Int(%bs.10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs.10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs.10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs.10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %81 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %25 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %28 : int[] = prim::ListConstruct(%14, %25, %26, %27), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %29 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%81, %28), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q.10 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%29, %30, %31), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %82 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %34 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %37 : int[] = prim::ListConstruct(%13, %34, %35, %36), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %38 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%82, %37), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k.10 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%38, %39, %40), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %83 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %43 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %46 : int[] = prim::ListConstruct(%12, %43, %44, %45), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %47 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%83, %46), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v.10 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%47, %48, %49), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %53 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.10, %51, %52), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.19 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.10, %53), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores.20 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.19, %3, %55), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %57 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %58 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %59 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %60 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %input.90 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.20, %57, %58, %59, %60), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %62 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %63 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %64 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.90, %62, %63), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.91 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%64, %input.90), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %67 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights.10 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.91, %66, %67), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.94 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.10, %v.10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %72 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.94, %70, %71), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %73 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %74 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%72, %73), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %75 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %76 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %77 : int[] = prim::ListConstruct(%11, %75, %76), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention
    %input.92 : Float(17:6656, 13:512, 512:1) = aten::view(%74, %77), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %84 : Tensor = prim::CallMethod[name="forward"](%4, %input.92)
    %80 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%84, %k.10, %v.10)
    return (%80)

T5LayerCrossAttention._actual_script_module
  graph(%self.142 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.142)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.142)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.142)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %3)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1 # transformers/modeling_t5.py:476:0
    %x.95 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %20, %15), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1 # transformers/modeling_t5.py:476:0
    %17 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.95, %12, %13)
    return (%17)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.149 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    %y.17 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.17)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.143)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm
    %x.89 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.89, %8), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm
    %variance.18 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.18, %15, %16), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.90 : Float(17:6656, 13:512, 512:1) = aten::div(%x.89, %18), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.89 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.90), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.89, %x.89)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.92 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.92)

T5Attention.o
Linear._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.linear.Linear,
        %input.92 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.93 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.92, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.93)

T5Attention.q
Linear._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.145)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.91 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.91)

T5Attention.v
Linear._actual_script_module
  graph(%self.147 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.147)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.93 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.1/__module.decoder.block.1.layer.1.EncDecAttention/__module.decoder.block.1.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.93)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.152 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.152)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.152)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.152)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.96 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.96)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.150 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.150)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.150)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.150)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2 # transformers/modeling_t5.py:200:0
    %x.98 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.98)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.dropout # torch/nn/functional.py:973:0
    %y.18 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y.18)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.151 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.151)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm
    %x.96 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.96, %8), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm
    %variance.19 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.19, %15, %16), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.97 : Float(17:6656, 13:512, 512:1) = aten::div(%x.96, %18), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.94 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.97), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.94, %x.96)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.154 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.96 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.96, %2, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.97)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.95 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.95)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.98 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.1/__module.decoder.block.1.layer.2/__module.decoder.block.1.layer.2.DenseReluDense/__module.decoder.block.1.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.98)

T5Block._actual_script_module
  graph(%self.157 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8),
        %3 : Float(17:6656, 13:512, 512:1),
        %4 : Float(17:1352, 8:1, 13:104, 13:8)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.157)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.157)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.157)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %33 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%33)
    %34 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %4)
    %28 : Float(17:6656, 13:512, 512:1), %29 : Float(17:6656, 8:64, 13:512, 64:1), %30 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%34)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %28)
    %32 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%35, %12, %14, %29, %30)
    return (%32)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.160)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.160)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.160)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.160)
    %7 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.11 : Long() = prim::NumToTensor(%8), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.11), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.11), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.11), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.11), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %24), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.11 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %33), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.11 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%79, %42), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.11 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.11, %47, %48), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.21 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.11, %49), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.22 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.21, %2, %51), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %input.100 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.22, %53, %54, %55, %56), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.100, %58, %59), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.101 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.100), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.11 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.101, %62, %63), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.104 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.11, %v.11), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.104, %66, %67), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention
    %input.102 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.102)
    %76 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%80, %k.11, %v.11)
    return (%76)

T5LayerSelfAttention._actual_script_module
  graph(%self.158 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.158)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.158)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.158)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%17)
    %18 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%18)
    %19 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %14 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0 # transformers/modeling_t5.py:439:0
    %x.105 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %19, %14), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0 # transformers/modeling_t5.py:439:0
    %16 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.105, %12)
    return (%16)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.165 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    %y.19 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.19)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.159 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm
    %x.99 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.99, %8), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm
    %variance.20 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.20, %15, %16), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.100 : Float(17:6656, 13:512, 512:1) = aten::div(%x.99, %18), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.99 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.100), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.99, %x.99)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.162)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.102 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.102)

T5Attention.o
Linear._actual_script_module
  graph(%self.164 : __torch__.torch.nn.modules.linear.Linear,
        %input.102 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.103 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.102, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.103)

T5Attention.q
Linear._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.161)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.101 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.101)

T5Attention.v
Linear._actual_script_module
  graph(%self.163 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.163)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.103 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.0/__module.decoder.block.2.layer.0.SelfAttention/__module.decoder.block.2.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.103)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.168 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.168)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.168)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.168)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.168)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs.12 : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %11 : int = aten::Int(%bs.12), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs.12), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs.12), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs.12), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %81 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %25 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %28 : int[] = prim::ListConstruct(%14, %25, %26, %27), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %29 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%81, %28), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q.12 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%29, %30, %31), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %82 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %34 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %37 : int[] = prim::ListConstruct(%13, %34, %35, %36), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %38 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%82, %37), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k.12 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%38, %39, %40), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %83 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %43 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %46 : int[] = prim::ListConstruct(%12, %43, %44, %45), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %47 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%83, %46), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v.12 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%47, %48, %49), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %53 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.12, %51, %52), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.23 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.12, %53), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores.24 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.23, %3, %55), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %57 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %58 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %59 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %60 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %input.105 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.24, %57, %58, %59, %60), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %62 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %63 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %64 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.105, %62, %63), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.106 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%64, %input.105), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %67 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights.12 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.106, %66, %67), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.111 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.12, %v.12), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %72 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.111, %70, %71), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %73 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %74 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%72, %73), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %75 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %76 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %77 : int[] = prim::ListConstruct(%11, %75, %76), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention
    %input.107 : Float(17:6656, 13:512, 512:1) = aten::view(%74, %77), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %84 : Tensor = prim::CallMethod[name="forward"](%4, %input.107)
    %80 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%84, %k.12, %v.12)
    return (%80)

T5LayerCrossAttention._actual_script_module
  graph(%self.166 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.166)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.166)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.166)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %3)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1 # transformers/modeling_t5.py:476:0
    %x.112 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %20, %15), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1 # transformers/modeling_t5.py:476:0
    %17 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.112, %12, %13)
    return (%17)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    %y.20 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.20)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.167 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm
    %x.106 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.106, %8), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm
    %variance.21 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.21, %15, %16), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.107 : Float(17:6656, 13:512, 512:1) = aten::div(%x.106, %18), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.104 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.107), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.104, %x.106)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.109 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.109)

T5Attention.o
Linear._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.linear.Linear,
        %input.107 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.108 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.107, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.108)

T5Attention.q
Linear._actual_script_module
  graph(%self.169 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.169)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.108 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.108)

T5Attention.v
Linear._actual_script_module
  graph(%self.171 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.171)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.110 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.1/__module.decoder.block.2.layer.1.EncDecAttention/__module.decoder.block.2.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.110)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.176 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.176)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.176)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.176)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.111 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.111)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.174 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.174)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.174)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.174)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2 # transformers/modeling_t5.py:200:0
    %x.115 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.115)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.dropout # torch/nn/functional.py:973:0
    %y.21 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y.21)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.175)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm
    %x.113 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.113, %8), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm
    %variance.22 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.22, %15, %16), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.114 : Float(17:6656, 13:512, 512:1) = aten::div(%x.113, %18), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.109 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.114), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.109, %x.113)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.111 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.111, %2, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.112)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.177)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.110 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.110)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.179 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.179)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.113 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.2/__module.decoder.block.2.layer.2/__module.decoder.block.2.layer.2.DenseReluDense/__module.decoder.block.2.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.113)

T5Block._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8),
        %3 : Float(17:6656, 13:512, 512:1),
        %4 : Float(17:1352, 8:1, 13:104, 13:8)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.181)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.181)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.181)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %33 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%33)
    %34 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %4)
    %28 : Float(17:6656, 13:512, 512:1), %29 : Float(17:6656, 8:64, 13:512, 64:1), %30 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%34)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %28)
    %32 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%35, %12, %14, %29, %30)
    return (%32)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.184 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.184)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.184)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.184)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.184)
    %7 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.13 : Long() = prim::NumToTensor(%8), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %24), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.13 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %33), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.13 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%79, %42), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.13 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.13, %47, %48), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.25 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.13, %49), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.26 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.25, %2, %51), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %input.115 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.26, %53, %54, %55, %56), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.115, %58, %59), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.116 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.115), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.13 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.116, %62, %63), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.121 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.13, %v.13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.121, %66, %67), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention
    %input.117 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.117)
    %76 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%80, %k.13, %v.13)
    return (%76)

T5LayerSelfAttention._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.182)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.182)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.182)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%17)
    %18 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%18)
    %19 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %14 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0 # transformers/modeling_t5.py:439:0
    %x.122 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %19, %14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0 # transformers/modeling_t5.py:439:0
    %16 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.122, %12)
    return (%16)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    %y.22 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.22)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.183 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.183)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm
    %x.116 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.116, %8), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm
    %variance.23 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.23, %15, %16), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.117 : Float(17:6656, 13:512, 512:1) = aten::div(%x.116, %18), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.114 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.117), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.114, %x.116)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.186 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.186)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.119 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.119)

T5Attention.o
Linear._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.linear.Linear,
        %input.117 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.188)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.118 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.117, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.118)

T5Attention.q
Linear._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.118 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.118)

T5Attention.v
Linear._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.187)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.120 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.0/__module.decoder.block.3.layer.0.SelfAttention/__module.decoder.block.3.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.120)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.192 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.192)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.192)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.192)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.192)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs.14 : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %11 : int = aten::Int(%bs.14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs.14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs.14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs.14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %81 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %25 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %28 : int[] = prim::ListConstruct(%14, %25, %26, %27), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %29 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%81, %28), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q.14 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%29, %30, %31), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %82 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %34 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %37 : int[] = prim::ListConstruct(%13, %34, %35, %36), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %38 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%82, %37), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k.14 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%38, %39, %40), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %83 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %43 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %46 : int[] = prim::ListConstruct(%12, %43, %44, %45), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %47 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%83, %46), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v.14 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%47, %48, %49), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %53 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.14, %51, %52), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.27 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.14, %53), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores.28 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.27, %3, %55), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %57 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %58 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %59 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %60 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %input.120 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.28, %57, %58, %59, %60), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %62 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %63 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %64 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.120, %62, %63), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.121 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%64, %input.120), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %67 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights.14 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.121, %66, %67), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.128 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.14, %v.14), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %72 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.128, %70, %71), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %73 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %74 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%72, %73), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %75 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %76 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %77 : int[] = prim::ListConstruct(%11, %75, %76), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention
    %input.122 : Float(17:6656, 13:512, 512:1) = aten::view(%74, %77), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %84 : Tensor = prim::CallMethod[name="forward"](%4, %input.122)
    %80 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%84, %k.14, %v.14)
    return (%80)

T5LayerCrossAttention._actual_script_module
  graph(%self.190 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.190)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.190)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.190)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %3)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1 # transformers/modeling_t5.py:476:0
    %x.129 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %20, %15), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1 # transformers/modeling_t5.py:476:0
    %17 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.129, %12, %13)
    return (%17)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    %y.23 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.23)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.191 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.191)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm
    %x.123 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.123, %8), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm
    %variance.24 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.24, %15, %16), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.124 : Float(17:6656, 13:512, 512:1) = aten::div(%x.123, %18), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.119 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.124), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.119, %x.123)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.194 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.194)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.126 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.126)

T5Attention.o
Linear._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.linear.Linear,
        %input.122 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.196)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.123 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.122, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.123)

T5Attention.q
Linear._actual_script_module
  graph(%self.193 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.193)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.125 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.125)

T5Attention.v
Linear._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.195)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.127 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.1/__module.decoder.block.3.layer.1.EncDecAttention/__module.decoder.block.3.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.127)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.200 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.200)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.200)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.200)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.126 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.126)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.198 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.198)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.198)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.198)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2 # transformers/modeling_t5.py:200:0
    %x.132 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.132)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.204 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.dropout # torch/nn/functional.py:973:0
    %y.24 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y.24)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.199 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm
    %x.130 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.130, %8), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm
    %variance.25 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.25, %15, %16), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.131 : Float(17:6656, 13:512, 512:1) = aten::div(%x.130, %18), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.124 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.131), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.124, %x.130)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.202 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.126 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.127 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.126, %2, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.127)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.125 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.125)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.128 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.3/__module.decoder.block.3.layer.2/__module.decoder.block.3.layer.2.DenseReluDense/__module.decoder.block.3.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.128)

T5Block._actual_script_module
  graph(%self.205 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8),
        %3 : Float(17:6656, 13:512, 512:1),
        %4 : Float(17:1352, 8:1, 13:104, 13:8)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.205)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.205)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.205)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %33 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%33)
    %34 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %4)
    %28 : Float(17:6656, 13:512, 512:1), %29 : Float(17:6656, 8:64, 13:512, 64:1), %30 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%34)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %28)
    %32 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%35, %12, %14, %29, %30)
    return (%32)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.208 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.208)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.208)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.208)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.208)
    %7 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.15 : Long() = prim::NumToTensor(%8), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %24), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.15 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %33), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.15 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%79, %42), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.15 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.15, %47, %48), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.29 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.15, %49), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.30 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.29, %2, %51), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %input.130 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.30, %53, %54, %55, %56), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.130, %58, %59), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.131 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.130), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.15 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.131, %62, %63), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.138 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.15, %v.15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.138, %66, %67), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention
    %input.132 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.132)
    %76 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%80, %k.15, %v.15)
    return (%76)

T5LayerSelfAttention._actual_script_module
  graph(%self.206 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.206)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.206)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.206)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%17)
    %18 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%18)
    %19 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %14 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0 # transformers/modeling_t5.py:439:0
    %x.139 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %19, %14), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0 # transformers/modeling_t5.py:439:0
    %16 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.139, %12)
    return (%16)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    %y.25 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.25)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.207 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.207)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm
    %x.133 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.133, %8), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm
    %variance.26 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.26, %15, %16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.134 : Float(17:6656, 13:512, 512:1) = aten::div(%x.133, %18), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.129 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.134), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.129, %x.133)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.136 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.136)

T5Attention.o
Linear._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.linear.Linear,
        %input.132 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.133 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.132, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.133)

T5Attention.q
Linear._actual_script_module
  graph(%self.209 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.209)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.135 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.135)

T5Attention.v
Linear._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.211)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.137 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.0/__module.decoder.block.4.layer.0.SelfAttention/__module.decoder.block.4.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.137)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.216 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.216)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.216)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.216)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.216)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs.16 : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %11 : int = aten::Int(%bs.16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs.16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs.16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs.16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %81 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %25 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %28 : int[] = prim::ListConstruct(%14, %25, %26, %27), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %29 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%81, %28), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q.16 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%29, %30, %31), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %82 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %34 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %37 : int[] = prim::ListConstruct(%13, %34, %35, %36), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %38 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%82, %37), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k.16 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%38, %39, %40), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %83 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %43 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %46 : int[] = prim::ListConstruct(%12, %43, %44, %45), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %47 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%83, %46), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v.16 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%47, %48, %49), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %53 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.16, %51, %52), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.31 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.16, %53), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores.32 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.31, %3, %55), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %57 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %58 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %59 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %60 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %input.135 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.32, %57, %58, %59, %60), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %62 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %63 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %64 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.135, %62, %63), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.136 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%64, %input.135), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %67 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights.16 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.136, %66, %67), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.145 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.16, %v.16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %72 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.145, %70, %71), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %73 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %74 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%72, %73), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %75 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %76 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %77 : int[] = prim::ListConstruct(%11, %75, %76), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention
    %input.137 : Float(17:6656, 13:512, 512:1) = aten::view(%74, %77), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %84 : Tensor = prim::CallMethod[name="forward"](%4, %input.137)
    %80 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%84, %k.16, %v.16)
    return (%80)

T5LayerCrossAttention._actual_script_module
  graph(%self.214 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.214)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.214)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.214)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %3)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1 # transformers/modeling_t5.py:476:0
    %x.146 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %20, %15), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1 # transformers/modeling_t5.py:476:0
    %17 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.146, %12, %13)
    return (%17)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    %y.26 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.26)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.215)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm
    %x.140 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.140, %8), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm
    %variance.27 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.27, %15, %16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.141 : Float(17:6656, 13:512, 512:1) = aten::div(%x.140, %18), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.134 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.141), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.134, %x.140)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.218 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.218)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.143 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.143)

T5Attention.o
Linear._actual_script_module
  graph(%self.220 : __torch__.torch.nn.modules.linear.Linear,
        %input.137 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.220)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.138 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.137, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.138)

T5Attention.q
Linear._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.217)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.142 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.142)

T5Attention.v
Linear._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.219)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.144 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.1/__module.decoder.block.4.layer.1.EncDecAttention/__module.decoder.block.4.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.144)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.224 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.224)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.224)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.224)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.141 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.141)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.222 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.222)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.222)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.222)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2 # transformers/modeling_t5.py:200:0
    %x.149 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.149)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.228 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.dropout # torch/nn/functional.py:973:0
    %y.27 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y.27)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.223 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.223)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm
    %x.147 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.147, %8), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm
    %variance.28 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.28, %15, %16), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.148 : Float(17:6656, 13:512, 512:1) = aten::div(%x.147, %18), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.139 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.148), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.139, %x.147)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.226 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.141 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.142 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.141, %2, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.142)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.225 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.140 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.140)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.227 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.227)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.143 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.4/__module.decoder.block.4.layer.2/__module.decoder.block.4.layer.2.DenseReluDense/__module.decoder.block.4.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.143)

T5Block._actual_script_module
  graph(%self.229 : __torch__.transformers.modeling_t5.T5Block,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8),
        %3 : Float(17:6656, 13:512, 512:1),
        %4 : Float(17:1352, 8:1, 13:104, 13:8)):
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.229)
    %6 : __torch__.transformers.modeling_t5.T5LayerFF = prim::GetAttr[name="2"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.229)
    %8 : __torch__.transformers.modeling_t5.T5LayerCrossAttention = prim::GetAttr[name="1"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.229)
    %10 : __torch__.transformers.modeling_t5.T5LayerSelfAttention = prim::GetAttr[name="0"](%9)
    %33 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %1, %2)
    %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 13:512, 512:1), %14 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%33)
    %34 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %13, %3, %4)
    %28 : Float(17:6656, 13:512, 512:1), %29 : Float(17:6656, 8:64, 13:512, 64:1), %30 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%34)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %28)
    %32 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%35, %12, %14, %29, %30)
    return (%32)

T5LayerSelfAttention.SelfAttention
T5Attention._actual_script_module
  graph(%self.232 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.232)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.232)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.232)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.232)
    %7 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %8 : int = aten::size(%1, %7), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:322:0
    %bs.17 : Long() = prim::NumToTensor(%8), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %10 : int = aten::Int(%bs.17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %11 : int = aten::Int(%bs.17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %12 : int = aten::Int(%bs.17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %13 : int = aten::Int(%bs.17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %77 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %21 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %22 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %23 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %24 : int[] = prim::ListConstruct(%13, %21, %22, %23), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %25 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%77, %24), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %q.17 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%25, %26, %27), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %78 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %30 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %32 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %33 : int[] = prim::ListConstruct(%12, %30, %31, %32), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %34 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%78, %33), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %k.17 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%34, %35, %36), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %79 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %39 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %41 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %42 : int[] = prim::ListConstruct(%11, %39, %40, %41), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %43 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%79, %42), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %v.17 : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%43, %44, %45), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:342:0
    %47 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %48 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %49 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k.17, %47, %48), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:373:0
    %scores.33 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q.17, %49), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:372:0
    %51 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %scores.34 : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.33, %2, %51), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:389:0
    %53 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %54 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %55 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %56 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %input.145 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores.34, %53, %54, %55, %56), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %58 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %59 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %60 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.145, %58, %59), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:1498:0
    %input.146 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%60, %input.145), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:390:0
    %62 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %63 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %weights.17 : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.146, %62, %63), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # torch/nn/functional.py:973:0
    %x.155 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights.17, %v.17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:397:0
    %66 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %67 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %68 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.155, %66, %67), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %69 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %70 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%68, %69), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %72 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %73 : int[] = prim::ListConstruct(%10, %71, %72), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention
    %input.147 : Float(17:6656, 13:512, 512:1) = aten::view(%70, %73), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention # transformers/modeling_t5.py:346:0
    %80 : Tensor = prim::CallMethod[name="forward"](%3, %input.147)
    %76 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%80, %k.17, %v.17)
    return (%76)

T5LayerSelfAttention._actual_script_module
  graph(%self.230 : __torch__.transformers.modeling_t5.T5LayerSelfAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:1352, 8:1, 13:104, 13:8)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.230)
    %4 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="SelfAttention"](%self.230)
    %5 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.230)
    %17 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1)
    %7 : Float(17:6656, 13:512, 512:1), %8 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%17)
    %18 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %7, %2)
    %10 : Float(17:6656, 13:512, 512:1), %11 : Float(17:6656, 8:64, 13:512, 64:1), %12 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%18)
    %19 : Tensor = prim::CallMethod[name="forward"](%3, %10)
    %14 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0 # transformers/modeling_t5.py:439:0
    %x.156 : Float(17:6656, 13:512, 512:1) = aten::add(%8, %19, %14), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0 # transformers/modeling_t5.py:439:0
    %16 : (Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%11, %x.156, %12)
    return (%16)

T5LayerSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.237 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    %y.28 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.dropout # torch/nn/functional.py:973:0
    return (%y.28)

T5LayerSelfAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.231 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.231)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm
    %x.150 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.150, %8), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm
    %variance.29 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.29, %15, %16), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %x.151 : Float(17:6656, 13:512, 512:1) = aten::div(%x.150, %18), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:168:0
    %input.144 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.151), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.144, %x.150)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.234 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.234)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    %x.153 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.k # torch/nn/functional.py:1676:0
    return (%x.153)

T5Attention.o
Linear._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.linear.Linear,
        %input.147 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.236)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    %input.148 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.147, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.o # torch/nn/functional.py:1676:0
    return (%input.148)

T5Attention.q
Linear._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.233)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    %x.152 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.q # torch/nn/functional.py:1676:0
    return (%x.152)

T5Attention.v
Linear._actual_script_module
  graph(%self.235 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.235)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    %x.154 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.0/__module.decoder.block.5.layer.0.SelfAttention/__module.decoder.block.5.layer.0.SelfAttention.v # torch/nn/functional.py:1676:0
    return (%x.154)

T5LayerCrossAttention.EncDecAttention
T5Attention._actual_script_module
  graph(%self.240 : __torch__.transformers.modeling_t5.T5Attention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="o"](%self.240)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v"](%self.240)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k"](%self.240)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q"](%self.240)
    %8 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %9 : int = aten::size(%1, %8), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:322:0
    %bs : Long() = prim::NumToTensor(%9), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %11 : int = aten::Int(%bs), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %12 : int = aten::Int(%bs), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %13 : int = aten::Int(%bs), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %14 : int = aten::Int(%bs), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %81 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %25 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %26 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %27 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %28 : int[] = prim::ListConstruct(%14, %25, %26, %27), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %29 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%81, %28), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %30 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %31 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %q : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%29, %30, %31), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %82 : Tensor = prim::CallMethod[name="forward"](%6, %2)
    %34 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %35 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %36 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %37 : int[] = prim::ListConstruct(%13, %34, %35, %36), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %38 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%82, %37), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %39 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %40 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %k : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%38, %39, %40), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %83 : Tensor = prim::CallMethod[name="forward"](%5, %2)
    %43 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %44 : int = prim::Constant[value=8](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %45 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %46 : int[] = prim::ListConstruct(%12, %43, %44, %45), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %47 : Float(17:6656, 13:512, 8:64, 64:1) = aten::view(%83, %46), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %48 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %49 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %v : Float(17:6656, 8:64, 13:512, 64:1) = aten::transpose(%47, %48, %49), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:342:0
    %51 : int = prim::Constant[value=3](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %52 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %53 : Float(17:6656, 8:64, 64:1, 13:512) = aten::transpose(%k, %51, %52), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:373:0
    %scores.35 : Float(17:1352, 8:169, 13:13, 13:1) = aten::matmul(%q, %53), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:372:0
    %55 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %scores : Float(17:1352, 8:169, 13:13, 13:1) = aten::add_(%scores.35, %3, %55), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:389:0
    %57 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %58 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %59 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %60 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %input.150 : Float(17:1352, 8:169, 13:13, 13:1) = aten::to(%scores, %57, %58, %59, %60), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %62 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %63 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %64 : Float(17:1352, 8:169, 13:13, 13:1) = aten::softmax(%input.150, %62, %63), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # torch/nn/functional.py:1498:0
    %input.151 : Float(17:1352, 8:169, 13:13, 13:1) = aten::type_as(%64, %input.150), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:390:0
    %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %67 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %weights : Float(17:1352, 8:169, 13:13, 13:1) = aten::dropout(%input.151, %66, %67), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # torch/nn/functional.py:973:0
    %x.162 : Float(17:6656, 8:832, 13:64, 64:1) = aten::matmul(%weights, %v), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:397:0
    %70 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %71 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %72 : Float(17:6656, 13:64, 8:832, 64:1) = aten::transpose(%x.162, %70, %71), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %73 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %74 : Float(17:6656, 13:512, 8:64, 64:1) = aten::contiguous(%72, %73), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %75 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %76 : int = prim::Constant[value=512](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %77 : int[] = prim::ListConstruct(%11, %75, %76), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention
    %input.152 : Float(17:6656, 13:512, 512:1) = aten::view(%74, %77), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention # transformers/modeling_t5.py:346:0
    %84 : Tensor = prim::CallMethod[name="forward"](%4, %input.152)
    %80 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%84, %k, %v)
    return (%80)

T5LayerCrossAttention._actual_script_module
  graph(%self.238 : __torch__.transformers.modeling_t5.T5LayerCrossAttention,
        %1 : Float(17:6656, 13:512, 512:1),
        %2 : Float(17:6656, 13:512, 512:1),
        %3 : Float(17:1352, 8:1, 13:104, 13:8)):
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.238)
    %5 : __torch__.transformers.modeling_t5.T5Attention = prim::GetAttr[name="EncDecAttention"](%self.238)
    %6 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.238)
    %18 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %1)
    %8 : Float(17:6656, 13:512, 512:1), %9 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%18)
    %19 : (Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %8, %2, %3)
    %11 : Float(17:6656, 13:512, 512:1), %12 : Float(17:6656, 8:64, 13:512, 64:1), %13 : Float(17:6656, 8:64, 13:512, 64:1) = prim::TupleUnpack(%19)
    %20 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %15 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1 # transformers/modeling_t5.py:476:0
    %x.163 : Float(17:6656, 13:512, 512:1) = aten::add(%9, %20, %15), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1 # transformers/modeling_t5.py:476:0
    %17 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 8:64, 13:512, 64:1), Float(17:6656, 8:64, 13:512, 64:1)) = prim::TupleConstruct(%x.163, %12, %13)
    return (%17)

T5LayerCrossAttention.dropout
Dropout._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    %y.29 : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.dropout # torch/nn/functional.py:973:0
    return (%y.29)

T5LayerCrossAttention.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.239 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.239)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm
    %x.157 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.157, %8), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm
    %variance.30 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.30, %15, %16), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %x.158 : Float(17:6656, 13:512, 512:1) = aten::div(%x.157, %18), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:168:0
    %input.149 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.158), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.149, %x.157)
    return (%21)

T5Attention.k
Linear._actual_script_module
  graph(%self.242 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.242)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    %x.160 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.k # torch/nn/functional.py:1676:0
    return (%x.160)

T5Attention.o
Linear._actual_script_module
  graph(%self.244 : __torch__.torch.nn.modules.linear.Linear,
        %input.152 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.244)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    %input.153 : Float(17:6656, 13:512, 512:1) = aten::matmul(%input.152, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.o # torch/nn/functional.py:1676:0
    return (%input.153)

T5Attention.q
Linear._actual_script_module
  graph(%self.241 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.241)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    %x.159 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.q # torch/nn/functional.py:1676:0
    return (%x.159)

T5Attention.v
Linear._actual_script_module
  graph(%self.243 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.243)
    %3 : Float(512:1, 512:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    %x.161 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.1/__module.decoder.block.5.layer.1.EncDecAttention/__module.decoder.block.5.layer.1.EncDecAttention.v # torch/nn/functional.py:1676:0
    return (%x.161)

T5LayerFF.DenseReluDense
T5DenseReluDense._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_t5.T5DenseReluDense,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wo"](%self.248)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.248)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="wi"](%self.248)
    %9 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.156 : Float(17:26624, 13:2048, 2048:1) = aten::relu(%9), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense # torch/nn/functional.py:1119:0
    %10 : Tensor = prim::CallMethod[name="forward"](%3, %input.156)
    %11 : Tensor = prim::CallMethod[name="forward"](%2, %10)
    return (%11)

T5LayerFF._actual_script_module
  graph(%self.246 : __torch__.transformers.modeling_t5.T5LayerFF,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.246)
    %3 : __torch__.transformers.modeling_t5.T5DenseReluDense = prim::GetAttr[name="DenseReluDense"](%self.246)
    %4 : __torch__.transformers.modeling_t5.T5LayerNorm = prim::GetAttr[name="layer_norm"](%self.246)
    %12 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %1)
    %6 : Float(17:6656, 13:512, 512:1), %7 : Float(17:6656, 13:512, 512:1) = prim::TupleUnpack(%12)
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %6)
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %13)
    %10 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2 # transformers/modeling_t5.py:200:0
    %x.166 : Float(17:6656, 13:512, 512:1) = aten::add(%7, %14, %10), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2 # transformers/modeling_t5.py:200:0
    return (%x.166)

T5LayerFF.dropout
Dropout._actual_script_module
  graph(%self.252 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.dropout # torch/nn/functional.py:973:0
    %y : Float(17:6656, 13:512, 512:1) = aten::dropout(%1, %2, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.dropout # torch/nn/functional.py:973:0
    return (%y)

T5LayerFF.layer_norm
T5LayerNorm._actual_script_module
  graph(%self.247 : __torch__.transformers.modeling_t5.T5LayerNorm,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.247)
    %3 : int = prim::Constant[value=6](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %4 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %5 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %6 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm
    %x.164 : Float(17:6656, 13:512, 512:1) = aten::to(%1, %3, %4, %5, %6), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %8 : int = prim::Constant[value=2](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %9 : Float(17:6656, 13:512, 512:1) = aten::pow(%x.164, %8), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %10 : int = prim::Constant[value=-1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %11 : int[] = prim::ListConstruct(%10), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm
    %12 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %13 : None = prim::Constant(), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm
    %variance.31 : Float(17:13, 13:1, 1:1) = aten::mean(%9, %11, %12, %13), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:167:0
    %15 : Double() = prim::Constant[value={1e-06}](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %16 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %17 : Float(17:13, 13:1, 1:1) = aten::add(%variance.31, %15, %16), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %18 : Float(17:13, 13:1, 1:1) = aten::sqrt(%17), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %x.165 : Float(17:6656, 13:512, 512:1) = aten::div(%x.164, %18), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:168:0
    %input.154 : Float(17:6656, 13:512, 512:1) = aten::mul(%2, %x.165), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.layer_norm # transformers/modeling_t5.py:172:0
    %21 : (Float(17:6656, 13:512, 512:1), Float(17:6656, 13:512, 512:1)) = prim::TupleConstruct(%input.154, %x.164)
    return (%21)

T5DenseReluDense.dropout
Dropout._actual_script_module
  graph(%self.250 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.156 : Float(17:26624, 13:2048, 2048:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    %input.157 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.156, %2, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.dropout # torch/nn/functional.py:973:0
    return (%input.157)

T5DenseReluDense.wi
Linear._actual_script_module
  graph(%self.249 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:6656, 13:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.249)
    %3 : Float(512:1, 2048:512) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    %input.155 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.wi # torch/nn/functional.py:1676:0
    return (%input.155)

T5DenseReluDense.wo
Linear._actual_script_module
  graph(%self.251 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.251)
    %3 : Float(2048:1, 512:2048) = aten::t(%2), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    %input.158 : Float(17:6656, 13:512, 512:1) = aten::matmul(%1, %3), scope: __module.decoder/__module.decoder.block.5/__module.decoder.block.5.layer.2/__module.decoder.block.5.layer.2.DenseReluDense/__module.decoder.block.5.layer.2.DenseReluDense.wo # torch/nn/functional.py:1676:0
    return (%input.158)

