XLMModel(
  (position_embeddings): Embedding(512, 2048)
  (embeddings): Embedding(30145, 2048, padding_idx=2)
  (layer_norm_emb): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (6): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (7): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (8): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (9): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (10): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (11): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (6): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (7): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (8): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (9): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (10): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (11): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  )
)

XLMModel._actual_script_module
XLMModel.forward
  graph(%self.1 : __torch__.transformers.modeling_xlm.XLMModel,
        %input_ids : Long(17:13, 13:1),
        %padding_mask : Long(17:13, 13:1)):
    %2829 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2830 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%2829)
    %2825 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2826 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="11"](%2825)
    %2817 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2818 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%2817)
    %2813 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2814 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="11"](%2813)
    %2799 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2800 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%2799)
    %2795 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2796 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="10"](%2795)
    %2787 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2788 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%2787)
    %2783 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2784 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="10"](%2783)
    %2769 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2770 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%2769)
    %2765 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2766 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="9"](%2765)
    %2757 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2758 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%2757)
    %2753 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2754 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="9"](%2753)
    %2739 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2740 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%2739)
    %2735 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2736 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="8"](%2735)
    %2727 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2728 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%2727)
    %2723 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2724 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="8"](%2723)
    %2709 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2710 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%2709)
    %2705 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2706 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="7"](%2705)
    %2697 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2698 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%2697)
    %2693 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2694 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="7"](%2693)
    %2679 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2680 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%2679)
    %2675 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2676 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="6"](%2675)
    %2667 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2668 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%2667)
    %2663 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2664 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="6"](%2663)
    %2649 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2650 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%2649)
    %2645 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2646 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="5"](%2645)
    %2637 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2638 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%2637)
    %2633 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2634 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="5"](%2633)
    %2619 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2620 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%2619)
    %2615 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2616 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="4"](%2615)
    %2607 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2608 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%2607)
    %2603 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2604 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="4"](%2603)
    %2589 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2590 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%2589)
    %2585 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2586 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="3"](%2585)
    %2577 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2578 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%2577)
    %2573 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2574 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="3"](%2573)
    %2559 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2560 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%2559)
    %2555 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2556 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="2"](%2555)
    %2547 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2548 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%2547)
    %2543 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2544 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="2"](%2543)
    %2529 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2530 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%2529)
    %2525 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2526 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="1"](%2525)
    %2517 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2518 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%2517)
    %2513 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2514 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="1"](%2513)
    %2499 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2500 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%2499)
    %2495 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2496 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="0"](%2495)
    %2487 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2488 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%2487)
    %2483 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2484 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="0"](%2483)
    %2470 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%self.1)
    %2467 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.1)
    %2465 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embeddings"](%self.1)
    %2463 : Tensor = prim::GetAttr[name="position_ids"](%self.1)
    %458 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:517:0
    %459 : int = aten::size(%input_ids, %458) # transformers/modeling_xlm.py:517:0
    %slen : Long() = prim::NumToTensor(%459)
    %502 : int = aten::Int(%slen)
    %497 : int = prim::Constant[value=0]() # transformers/modeling_xlm.py:546:0
    %498 : int = prim::Constant[value=0]() # transformers/modeling_xlm.py:546:0
    %499 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlm.py:546:0
    %500 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:546:0
    %501 : Long(1:512, 512:1) = aten::slice(%2463, %497, %498, %499, %500) # transformers/modeling_xlm.py:546:0
    %503 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:546:0
    %504 : int = prim::Constant[value=0]() # transformers/modeling_xlm.py:546:0
    %505 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:546:0
    %input.1 : Long(1:512, 13:1) = aten::slice(%501, %503, %504, %502, %505) # transformers/modeling_xlm.py:546:0
    %2954 : Tensor = prim::CallMethod[name="forward"](%2465, %input_ids)
    %2955 : Tensor = prim::CallMethod[name="forward"](%2467, %input.1)
    %520 : Float(17:0, 13:2048, 2048:1) = aten::expand_as(%2955, %2954) # transformers/modeling_xlm.py:573:0
    %521 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:573:0
    %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2954, %520, %521) # transformers/modeling_xlm.py:573:0
    %2956 : Tensor = prim::CallMethod[name="forward"](%2470, %input.2)
    %528 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %529 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2956, %528, %529) # torch/nn/functional.py:973:0
    %531 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:580:0
    %532 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %531) # transformers/modeling_xlm.py:580:0
    %533 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:580:0
    %534 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:580:0
    %535 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:580:0
    %536 : None = prim::Constant()
    %537 : Float(17:13, 13:1, 1:1) = aten::to(%532, %533, %534, %535, %536) # transformers/modeling_xlm.py:580:0
    %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %537) # transformers/modeling_xlm.py:580:0
    %2957 : Tensor = prim::CallMethod[name="forward"](%2484, %input.4, %padding_mask)
    %631 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %632 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2957, %631, %632) # torch/nn/functional.py:973:0
    %634 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %634) # transformers/modeling_xlm.py:601:0
    %2958 : Tensor = prim::CallMethod[name="forward"](%2488, %input.9)
    %2959 : Tensor = prim::CallMethod[name="forward"](%2496, %2958)
    %674 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2958, %2959, %674) # transformers/modeling_xlm.py:612:0
    %2960 : Tensor = prim::CallMethod[name="forward"](%2500, %input.13)
    %681 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %682 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %681) # transformers/modeling_xlm.py:614:0
    %683 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %684 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %685 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %686 : None = prim::Constant()
    %687 : Float(17:13, 13:1, 1:1) = aten::to(%682, %683, %684, %685, %686) # transformers/modeling_xlm.py:614:0
    %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2960, %687) # transformers/modeling_xlm.py:614:0
    %2961 : Tensor = prim::CallMethod[name="forward"](%2514, %input.14, %padding_mask)
    %781 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %782 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2961, %781, %782) # torch/nn/functional.py:973:0
    %784 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %784) # transformers/modeling_xlm.py:601:0
    %2962 : Tensor = prim::CallMethod[name="forward"](%2518, %input.19)
    %2963 : Tensor = prim::CallMethod[name="forward"](%2526, %2962)
    %824 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2962, %2963, %824) # transformers/modeling_xlm.py:612:0
    %2964 : Tensor = prim::CallMethod[name="forward"](%2530, %input.23)
    %831 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %832 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %831) # transformers/modeling_xlm.py:614:0
    %833 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %834 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %835 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %836 : None = prim::Constant()
    %837 : Float(17:13, 13:1, 1:1) = aten::to(%832, %833, %834, %835, %836) # transformers/modeling_xlm.py:614:0
    %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2964, %837) # transformers/modeling_xlm.py:614:0
    %2965 : Tensor = prim::CallMethod[name="forward"](%2544, %input.24, %padding_mask)
    %931 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %932 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2965, %931, %932) # torch/nn/functional.py:973:0
    %934 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %934) # transformers/modeling_xlm.py:601:0
    %2966 : Tensor = prim::CallMethod[name="forward"](%2548, %input.29)
    %2967 : Tensor = prim::CallMethod[name="forward"](%2556, %2966)
    %974 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2966, %2967, %974) # transformers/modeling_xlm.py:612:0
    %2968 : Tensor = prim::CallMethod[name="forward"](%2560, %input.33)
    %981 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %982 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %981) # transformers/modeling_xlm.py:614:0
    %983 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %984 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %985 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %986 : None = prim::Constant()
    %987 : Float(17:13, 13:1, 1:1) = aten::to(%982, %983, %984, %985, %986) # transformers/modeling_xlm.py:614:0
    %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2968, %987) # transformers/modeling_xlm.py:614:0
    %2969 : Tensor = prim::CallMethod[name="forward"](%2574, %input.34, %padding_mask)
    %1081 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1082 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2969, %1081, %1082) # torch/nn/functional.py:973:0
    %1084 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %1084) # transformers/modeling_xlm.py:601:0
    %2970 : Tensor = prim::CallMethod[name="forward"](%2578, %input.39)
    %2971 : Tensor = prim::CallMethod[name="forward"](%2586, %2970)
    %1124 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2970, %2971, %1124) # transformers/modeling_xlm.py:612:0
    %2972 : Tensor = prim::CallMethod[name="forward"](%2590, %input.43)
    %1131 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1132 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1131) # transformers/modeling_xlm.py:614:0
    %1133 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1134 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1135 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1136 : None = prim::Constant()
    %1137 : Float(17:13, 13:1, 1:1) = aten::to(%1132, %1133, %1134, %1135, %1136) # transformers/modeling_xlm.py:614:0
    %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2972, %1137) # transformers/modeling_xlm.py:614:0
    %2973 : Tensor = prim::CallMethod[name="forward"](%2604, %input.44, %padding_mask)
    %1231 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1232 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2973, %1231, %1232) # torch/nn/functional.py:973:0
    %1234 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %1234) # transformers/modeling_xlm.py:601:0
    %2974 : Tensor = prim::CallMethod[name="forward"](%2608, %input.49)
    %2975 : Tensor = prim::CallMethod[name="forward"](%2616, %2974)
    %1274 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2974, %2975, %1274) # transformers/modeling_xlm.py:612:0
    %2976 : Tensor = prim::CallMethod[name="forward"](%2620, %input.53)
    %1281 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1282 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1281) # transformers/modeling_xlm.py:614:0
    %1283 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1284 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1285 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1286 : None = prim::Constant()
    %1287 : Float(17:13, 13:1, 1:1) = aten::to(%1282, %1283, %1284, %1285, %1286) # transformers/modeling_xlm.py:614:0
    %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2976, %1287) # transformers/modeling_xlm.py:614:0
    %2977 : Tensor = prim::CallMethod[name="forward"](%2634, %input.54, %padding_mask)
    %1381 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1382 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2977, %1381, %1382) # torch/nn/functional.py:973:0
    %1384 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %1384) # transformers/modeling_xlm.py:601:0
    %2978 : Tensor = prim::CallMethod[name="forward"](%2638, %input.59)
    %2979 : Tensor = prim::CallMethod[name="forward"](%2646, %2978)
    %1424 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2978, %2979, %1424) # transformers/modeling_xlm.py:612:0
    %2980 : Tensor = prim::CallMethod[name="forward"](%2650, %input.63)
    %1431 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1432 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1431) # transformers/modeling_xlm.py:614:0
    %1433 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1434 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1435 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1436 : None = prim::Constant()
    %1437 : Float(17:13, 13:1, 1:1) = aten::to(%1432, %1433, %1434, %1435, %1436) # transformers/modeling_xlm.py:614:0
    %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2980, %1437) # transformers/modeling_xlm.py:614:0
    %2981 : Tensor = prim::CallMethod[name="forward"](%2664, %input.64, %padding_mask)
    %1531 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1532 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2981, %1531, %1532) # torch/nn/functional.py:973:0
    %1534 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %1534) # transformers/modeling_xlm.py:601:0
    %2982 : Tensor = prim::CallMethod[name="forward"](%2668, %input.69)
    %2983 : Tensor = prim::CallMethod[name="forward"](%2676, %2982)
    %1574 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2982, %2983, %1574) # transformers/modeling_xlm.py:612:0
    %2984 : Tensor = prim::CallMethod[name="forward"](%2680, %input.73)
    %1581 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1582 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1581) # transformers/modeling_xlm.py:614:0
    %1583 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1584 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1585 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1586 : None = prim::Constant()
    %1587 : Float(17:13, 13:1, 1:1) = aten::to(%1582, %1583, %1584, %1585, %1586) # transformers/modeling_xlm.py:614:0
    %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2984, %1587) # transformers/modeling_xlm.py:614:0
    %2985 : Tensor = prim::CallMethod[name="forward"](%2694, %input.74, %padding_mask)
    %1681 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1682 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2985, %1681, %1682) # torch/nn/functional.py:973:0
    %1684 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %1684) # transformers/modeling_xlm.py:601:0
    %2986 : Tensor = prim::CallMethod[name="forward"](%2698, %input.79)
    %2987 : Tensor = prim::CallMethod[name="forward"](%2706, %2986)
    %1724 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2986, %2987, %1724) # transformers/modeling_xlm.py:612:0
    %2988 : Tensor = prim::CallMethod[name="forward"](%2710, %input.83)
    %1731 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1732 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1731) # transformers/modeling_xlm.py:614:0
    %1733 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1734 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1735 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1736 : None = prim::Constant()
    %1737 : Float(17:13, 13:1, 1:1) = aten::to(%1732, %1733, %1734, %1735, %1736) # transformers/modeling_xlm.py:614:0
    %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2988, %1737) # transformers/modeling_xlm.py:614:0
    %2989 : Tensor = prim::CallMethod[name="forward"](%2724, %input.84, %padding_mask)
    %1831 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1832 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2989, %1831, %1832) # torch/nn/functional.py:973:0
    %1834 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %1834) # transformers/modeling_xlm.py:601:0
    %2990 : Tensor = prim::CallMethod[name="forward"](%2728, %input.89)
    %2991 : Tensor = prim::CallMethod[name="forward"](%2736, %2990)
    %1874 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2990, %2991, %1874) # transformers/modeling_xlm.py:612:0
    %2992 : Tensor = prim::CallMethod[name="forward"](%2740, %input.93)
    %1881 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %1882 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1881) # transformers/modeling_xlm.py:614:0
    %1883 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %1884 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1885 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %1886 : None = prim::Constant()
    %1887 : Float(17:13, 13:1, 1:1) = aten::to(%1882, %1883, %1884, %1885, %1886) # transformers/modeling_xlm.py:614:0
    %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2992, %1887) # transformers/modeling_xlm.py:614:0
    %2993 : Tensor = prim::CallMethod[name="forward"](%2754, %input.94, %padding_mask)
    %1981 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1982 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2993, %1981, %1982) # torch/nn/functional.py:973:0
    %1984 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %1984) # transformers/modeling_xlm.py:601:0
    %2994 : Tensor = prim::CallMethod[name="forward"](%2758, %input.99)
    %2995 : Tensor = prim::CallMethod[name="forward"](%2766, %2994)
    %2024 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2994, %2995, %2024) # transformers/modeling_xlm.py:612:0
    %2996 : Tensor = prim::CallMethod[name="forward"](%2770, %input.103)
    %2031 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %2032 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2031) # transformers/modeling_xlm.py:614:0
    %2033 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %2034 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2035 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2036 : None = prim::Constant()
    %2037 : Float(17:13, 13:1, 1:1) = aten::to(%2032, %2033, %2034, %2035, %2036) # transformers/modeling_xlm.py:614:0
    %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2996, %2037) # transformers/modeling_xlm.py:614:0
    %2997 : Tensor = prim::CallMethod[name="forward"](%2784, %input.104, %padding_mask)
    %2131 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %2132 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2997, %2131, %2132) # torch/nn/functional.py:973:0
    %2134 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %2134) # transformers/modeling_xlm.py:601:0
    %2998 : Tensor = prim::CallMethod[name="forward"](%2788, %input.109)
    %2999 : Tensor = prim::CallMethod[name="forward"](%2796, %2998)
    %2174 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2998, %2999, %2174) # transformers/modeling_xlm.py:612:0
    %3000 : Tensor = prim::CallMethod[name="forward"](%2800, %input.113)
    %2181 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %2182 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2181) # transformers/modeling_xlm.py:614:0
    %2183 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %2184 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2185 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2186 : None = prim::Constant()
    %2187 : Float(17:13, 13:1, 1:1) = aten::to(%2182, %2183, %2184, %2185, %2186) # transformers/modeling_xlm.py:614:0
    %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%3000, %2187) # transformers/modeling_xlm.py:614:0
    %3001 : Tensor = prim::CallMethod[name="forward"](%2814, %input.114, %padding_mask)
    %2281 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %2282 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%3001, %2281, %2282) # torch/nn/functional.py:973:0
    %2284 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:601:0
    %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %2284) # transformers/modeling_xlm.py:601:0
    %3002 : Tensor = prim::CallMethod[name="forward"](%2818, %input.119)
    %3003 : Tensor = prim::CallMethod[name="forward"](%2826, %3002)
    %2324 : int = prim::Constant[value=1]() # transformers/modeling_xlm.py:612:0
    %input : Float(17:26624, 13:2048, 2048:1) = aten::add(%3002, %3003, %2324) # transformers/modeling_xlm.py:612:0
    %3004 : Tensor = prim::CallMethod[name="forward"](%2830, %input)
    %2331 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:614:0
    %2332 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2331) # transformers/modeling_xlm.py:614:0
    %2333 : int = prim::Constant[value=6]() # transformers/modeling_xlm.py:614:0
    %2334 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2335 : bool = prim::Constant[value=0]() # transformers/modeling_xlm.py:614:0
    %2336 : None = prim::Constant()
    %2337 : Float(17:13, 13:1, 1:1) = aten::to(%2332, %2333, %2334, %2335, %2336) # transformers/modeling_xlm.py:614:0
    %2338 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%3004, %2337) # transformers/modeling_xlm.py:614:0
    %2339 : (Float(17:26624, 13:2048, 2048:1)) = prim::TupleConstruct(%2338)
    return (%2339)

XLMModel.embeddings
Embedding._actual_script_module
  graph(%self.2 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %1 : Tensor = prim::GetAttr[name="weight"](%self.2)
    %7 : int = prim::Constant[value=2](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %8 : bool = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%1, %input_ids, %7, %8, %9), scope: __module.embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

XLMModel.layer_norm_emb
LayerNorm._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.4)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm_emb
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %4, %2, %1, %5, %6), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    return (%input.3)

XLMModel.position_embeddings
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(1:512, 13:1)):
    %1 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %2 : int = prim::Constant[value=-1](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %3 : bool = prim::Constant[value=0](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %5 : Float(1:26624, 13:2048, 2048:1) = aten::embedding(%1, %input.1, %2, %3, %4), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    return (%5)

ModuleList.*
  module had no methods with graph attrs.

MultiHeadAttention._actual_script_module
  graph(%self.5 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.4 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.5)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.5)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.5)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.5)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.4, %5), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %bs.1 : Long() = prim::NumToTensor(%6), scope: __module.attentions.0
    %9 : int = aten::Int(%bs.1), scope: __module.attentions.0
    %10 : int = aten::Int(%bs.1), scope: __module.attentions.0
    %11 : int = aten::Int(%bs.1), scope: __module.attentions.0
    %12 : int = aten::Int(%bs.1), scope: __module.attentions.0
    %13 : int = aten::Int(%bs.1), scope: __module.attentions.0
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.4, %14), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %qlen.1 : Long() = prim::NumToTensor(%15), scope: __module.attentions.0
    %17 : int = aten::Int(%qlen.1), scope: __module.attentions.0
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.4)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.0
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.4)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.0
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.4)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.0
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
    %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %48), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %50, %51), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %52), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.0 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.0 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.0
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
    %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %62), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.0
    %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %64, %65, %66, %67), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.0
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %69, %70), scope: __module.attentions.0 # torch/nn/functional.py:1498:0
    %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.5), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %73, %74), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %77, %78), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.0
    %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.7)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.7)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1678:0
    %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %2, %6), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1678:0
    return (%x.2)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %4), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1678:0
    %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %2, %6), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1678:0
    return (%input.8)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1678:0
    %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %2, %6), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1678:0
    return (%x.1)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.8)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.8)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1678:0
    %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %2, %6), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1678:0
    return (%x.3)

MultiHeadAttention._actual_script_module
  graph(%self.15 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.14 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.15)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.15)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.15)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.15)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.14, %5), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %bs.2 : Long() = prim::NumToTensor(%6), scope: __module.attentions.1
    %9 : int = aten::Int(%bs.2), scope: __module.attentions.1
    %10 : int = aten::Int(%bs.2), scope: __module.attentions.1
    %11 : int = aten::Int(%bs.2), scope: __module.attentions.1
    %12 : int = aten::Int(%bs.2), scope: __module.attentions.1
    %13 : int = aten::Int(%bs.2), scope: __module.attentions.1
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.14, %14), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %qlen.2 : Long() = prim::NumToTensor(%15), scope: __module.attentions.1
    %17 : int = aten::Int(%qlen.2), scope: __module.attentions.1
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.14)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.1
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.14)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.1
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.14)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.1
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
    %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %48), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %50, %51), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %52), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.1 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.1 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.1
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.3), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
    %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %62), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.1
    %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %64, %65, %66, %67), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.1
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %69, %70), scope: __module.attentions.1 # torch/nn/functional.py:1498:0
    %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.15), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %73, %74), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.attentions.1 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %77, %78), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.1
    %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.17)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.17)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.17)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1678:0
    %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %2, %6), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1678:0
    return (%x.6)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %4), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1678:0
    %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %2, %6), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1678:0
    return (%input.18)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1678:0
    %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %2, %6), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1678:0
    return (%x.5)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1678:0
    %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %2, %6), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1678:0
    return (%x.7)

MultiHeadAttention._actual_script_module
  graph(%self.25 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.24 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.25)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.25)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.25)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.25)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.24, %5), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %bs.3 : Long() = prim::NumToTensor(%6), scope: __module.attentions.2
    %9 : int = aten::Int(%bs.3), scope: __module.attentions.2
    %10 : int = aten::Int(%bs.3), scope: __module.attentions.2
    %11 : int = aten::Int(%bs.3), scope: __module.attentions.2
    %12 : int = aten::Int(%bs.3), scope: __module.attentions.2
    %13 : int = aten::Int(%bs.3), scope: __module.attentions.2
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.24, %14), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %qlen.3 : Long() = prim::NumToTensor(%15), scope: __module.attentions.2
    %17 : int = aten::Int(%qlen.3), scope: __module.attentions.2
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.2
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.24)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.2
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.24)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.2
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
    %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %48), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %50, %51), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %52), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.2 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.2 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.2
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.5), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
    %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %62), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.2
    %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %64, %65, %66, %67), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.2
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %69, %70), scope: __module.attentions.2 # torch/nn/functional.py:1498:0
    %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.25), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %73, %74), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.attentions.2 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %77, %78), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.2
    %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.27)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1678:0
    %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %2, %6), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1678:0
    return (%x.10)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %input.27 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %4), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1678:0
    %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %2, %6), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1678:0
    return (%input.28)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1678:0
    %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %2, %6), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1678:0
    return (%x.9)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1678:0
    %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %2, %6), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1678:0
    return (%x.11)

MultiHeadAttention._actual_script_module
  graph(%self.35 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.34 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.35)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.35)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.35)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.35)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.34, %5), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %bs.4 : Long() = prim::NumToTensor(%6), scope: __module.attentions.3
    %9 : int = aten::Int(%bs.4), scope: __module.attentions.3
    %10 : int = aten::Int(%bs.4), scope: __module.attentions.3
    %11 : int = aten::Int(%bs.4), scope: __module.attentions.3
    %12 : int = aten::Int(%bs.4), scope: __module.attentions.3
    %13 : int = aten::Int(%bs.4), scope: __module.attentions.3
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.34, %14), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %qlen.4 : Long() = prim::NumToTensor(%15), scope: __module.attentions.3
    %17 : int = aten::Int(%qlen.4), scope: __module.attentions.3
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.34)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.3
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.34)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.3
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.34)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.3
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
    %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %48), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %50, %51), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %52), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.3 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.3 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.3
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.7), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
    %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %62), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.3
    %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %64, %65, %66, %67), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.3
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %69, %70), scope: __module.attentions.3 # torch/nn/functional.py:1498:0
    %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.35), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %73, %74), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.attentions.3 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %77, %78), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.3
    %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.37)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1678:0
    %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %2, %6), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1678:0
    return (%x.14)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.linear.Linear,
        %input.37 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %4), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1678:0
    %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %2, %6), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1678:0
    return (%input.38)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1678:0
    %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %2, %6), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1678:0
    return (%x.13)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1678:0
    %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %2, %6), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1678:0
    return (%x.15)

MultiHeadAttention._actual_script_module
  graph(%self.45 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.44 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.45)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.45)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.45)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.45)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.44, %5), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %bs.5 : Long() = prim::NumToTensor(%6), scope: __module.attentions.4
    %9 : int = aten::Int(%bs.5), scope: __module.attentions.4
    %10 : int = aten::Int(%bs.5), scope: __module.attentions.4
    %11 : int = aten::Int(%bs.5), scope: __module.attentions.4
    %12 : int = aten::Int(%bs.5), scope: __module.attentions.4
    %13 : int = aten::Int(%bs.5), scope: __module.attentions.4
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.44, %14), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %qlen.5 : Long() = prim::NumToTensor(%15), scope: __module.attentions.4
    %17 : int = aten::Int(%qlen.5), scope: __module.attentions.4
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.4
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.44)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.4
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.44)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.4
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
    %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %48), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %50, %51), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %52), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.4 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.4 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.4
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.9), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
    %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %62), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.4
    %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %64, %65, %66, %67), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.4
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %69, %70), scope: __module.attentions.4 # torch/nn/functional.py:1498:0
    %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.45), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %73, %74), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.attentions.4 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %77, %78), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.4
    %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.47)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1678:0
    %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %2, %6), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1678:0
    return (%x.18)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.linear.Linear,
        %input.47 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %4), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1678:0
    %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %2, %6), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1678:0
    return (%input.48)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1678:0
    %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %2, %6), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1678:0
    return (%x.17)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1678:0
    %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %2, %6), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1678:0
    return (%x.19)

MultiHeadAttention._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.54 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.55)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.55)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.55)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.55)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.54, %5), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %bs.6 : Long() = prim::NumToTensor(%6), scope: __module.attentions.5
    %9 : int = aten::Int(%bs.6), scope: __module.attentions.5
    %10 : int = aten::Int(%bs.6), scope: __module.attentions.5
    %11 : int = aten::Int(%bs.6), scope: __module.attentions.5
    %12 : int = aten::Int(%bs.6), scope: __module.attentions.5
    %13 : int = aten::Int(%bs.6), scope: __module.attentions.5
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.54, %14), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %qlen.6 : Long() = prim::NumToTensor(%15), scope: __module.attentions.5
    %17 : int = aten::Int(%qlen.6), scope: __module.attentions.5
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.54)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.5
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.54)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.5
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.54)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.5
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
    %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %48), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %50, %51), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %52), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.5 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.5 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.5
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.11), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
    %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %62), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.5
    %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %64, %65, %66, %67), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.5
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %69, %70), scope: __module.attentions.5 # torch/nn/functional.py:1498:0
    %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.55), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %73, %74), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.attentions.5 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %77, %78), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.5
    %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.57)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1678:0
    %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %2, %6), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1678:0
    return (%x.22)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %input.57 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %4), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1678:0
    %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %2, %6), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1678:0
    return (%input.58)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1678:0
    %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %2, %6), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1678:0
    return (%x.21)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1678:0
    %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %2, %6), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1678:0
    return (%x.23)

MultiHeadAttention._actual_script_module
  graph(%self.65 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.64 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.65)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.65)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.65)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.65)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.64, %5), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %bs.7 : Long() = prim::NumToTensor(%6), scope: __module.attentions.6
    %9 : int = aten::Int(%bs.7), scope: __module.attentions.6
    %10 : int = aten::Int(%bs.7), scope: __module.attentions.6
    %11 : int = aten::Int(%bs.7), scope: __module.attentions.6
    %12 : int = aten::Int(%bs.7), scope: __module.attentions.6
    %13 : int = aten::Int(%bs.7), scope: __module.attentions.6
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.64, %14), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %qlen.7 : Long() = prim::NumToTensor(%15), scope: __module.attentions.6
    %17 : int = aten::Int(%qlen.7), scope: __module.attentions.6
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.64)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.6
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.64)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.6
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.64)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.6
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
    %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %48), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %50, %51), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %52), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.6 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.6 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.6
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.13), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
    %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %62), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.6
    %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %64, %65, %66, %67), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.6
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %69, %70), scope: __module.attentions.6 # torch/nn/functional.py:1498:0
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.65), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %73, %74), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.attentions.6 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %77, %78), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.6
    %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.67)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1678:0
    %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %2, %6), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1678:0
    return (%x.26)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %4), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1678:0
    %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %2, %6), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1678:0
    return (%input.68)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1678:0
    %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %2, %6), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1678:0
    return (%x.25)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1678:0
    %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %2, %6), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1678:0
    return (%x.27)

MultiHeadAttention._actual_script_module
  graph(%self.75 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.74 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.75)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.75)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.75)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.75)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.74, %5), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %bs.8 : Long() = prim::NumToTensor(%6), scope: __module.attentions.7
    %9 : int = aten::Int(%bs.8), scope: __module.attentions.7
    %10 : int = aten::Int(%bs.8), scope: __module.attentions.7
    %11 : int = aten::Int(%bs.8), scope: __module.attentions.7
    %12 : int = aten::Int(%bs.8), scope: __module.attentions.7
    %13 : int = aten::Int(%bs.8), scope: __module.attentions.7
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.74, %14), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %qlen.8 : Long() = prim::NumToTensor(%15), scope: __module.attentions.7
    %17 : int = aten::Int(%qlen.8), scope: __module.attentions.7
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.74)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.7
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.74)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.7
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.74)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.7
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
    %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %48), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %50, %51), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %52), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.7 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.7 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.7
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.15), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
    %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %62), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.7
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %64, %65, %66, %67), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.7
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %69, %70), scope: __module.attentions.7 # torch/nn/functional.py:1498:0
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.75), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %73, %74), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.attentions.7 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %77, %78), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.7
    %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.77)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1678:0
    %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %2, %6), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1678:0
    return (%x.30)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %input.77 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %4), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1678:0
    %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %2, %6), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1678:0
    return (%input.78)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1678:0
    %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %2, %6), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1678:0
    return (%x.29)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1678:0
    %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %2, %6), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1678:0
    return (%x.31)

MultiHeadAttention._actual_script_module
  graph(%self.85 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.84 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.85)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.85)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.85)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.85)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.84, %5), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %bs.9 : Long() = prim::NumToTensor(%6), scope: __module.attentions.8
    %9 : int = aten::Int(%bs.9), scope: __module.attentions.8
    %10 : int = aten::Int(%bs.9), scope: __module.attentions.8
    %11 : int = aten::Int(%bs.9), scope: __module.attentions.8
    %12 : int = aten::Int(%bs.9), scope: __module.attentions.8
    %13 : int = aten::Int(%bs.9), scope: __module.attentions.8
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.84, %14), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %qlen.9 : Long() = prim::NumToTensor(%15), scope: __module.attentions.8
    %17 : int = aten::Int(%qlen.9), scope: __module.attentions.8
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.84)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.8
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.84)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.8
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.84)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.8
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
    %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %48), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %50, %51), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %52), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.8 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.8 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.8
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.17), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
    %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %62), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.8
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %64, %65, %66, %67), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.8
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %69, %70), scope: __module.attentions.8 # torch/nn/functional.py:1498:0
    %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.85), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %73, %74), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.attentions.8 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %77, %78), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.8
    %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.87)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1678:0
    %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %2, %6), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1678:0
    return (%x.34)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %input.87 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %4), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1678:0
    %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %2, %6), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1678:0
    return (%input.88)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1678:0
    %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %2, %6), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1678:0
    return (%x.33)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1678:0
    %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %2, %6), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1678:0
    return (%x.35)

MultiHeadAttention._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.94 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.95)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.95)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.95)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.95)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.94, %5), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %bs.10 : Long() = prim::NumToTensor(%6), scope: __module.attentions.9
    %9 : int = aten::Int(%bs.10), scope: __module.attentions.9
    %10 : int = aten::Int(%bs.10), scope: __module.attentions.9
    %11 : int = aten::Int(%bs.10), scope: __module.attentions.9
    %12 : int = aten::Int(%bs.10), scope: __module.attentions.9
    %13 : int = aten::Int(%bs.10), scope: __module.attentions.9
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.94, %14), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %qlen.10 : Long() = prim::NumToTensor(%15), scope: __module.attentions.9
    %17 : int = aten::Int(%qlen.10), scope: __module.attentions.9
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.94)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.9
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.94)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.9
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.94)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.9
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
    %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %48), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %50, %51), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %52), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.9 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.9 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.9
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.19), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
    %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %62), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.9
    %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %64, %65, %66, %67), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.9
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %69, %70), scope: __module.attentions.9 # torch/nn/functional.py:1498:0
    %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.95), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %73, %74), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.attentions.9 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %77, %78), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.9
    %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.97)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1678:0
    %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %2, %6), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1678:0
    return (%x.38)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %input.97 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %4), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1678:0
    %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %2, %6), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1678:0
    return (%input.98)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.96)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.96)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1678:0
    %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %2, %6), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1678:0
    return (%x.37)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1678:0
    %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %2, %6), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1678:0
    return (%x.39)

MultiHeadAttention._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.104 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.105)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.105)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.105)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.105)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.104, %5), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %bs.11 : Long() = prim::NumToTensor(%6), scope: __module.attentions.10
    %9 : int = aten::Int(%bs.11), scope: __module.attentions.10
    %10 : int = aten::Int(%bs.11), scope: __module.attentions.10
    %11 : int = aten::Int(%bs.11), scope: __module.attentions.10
    %12 : int = aten::Int(%bs.11), scope: __module.attentions.10
    %13 : int = aten::Int(%bs.11), scope: __module.attentions.10
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.104, %14), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %qlen.11 : Long() = prim::NumToTensor(%15), scope: __module.attentions.10
    %17 : int = aten::Int(%qlen.11), scope: __module.attentions.10
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.104)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.10
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.104)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.10
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.104)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.10
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
    %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %48), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %50, %51), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %52), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.10 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.10 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.10
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.21), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
    %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %62), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.10
    %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %64, %65, %66, %67), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.10
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %69, %70), scope: __module.attentions.10 # torch/nn/functional.py:1498:0
    %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.105), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %73, %74), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.attentions.10 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %77, %78), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.10
    %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.107)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.107)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.107)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1678:0
    %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %2, %6), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1678:0
    return (%x.42)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %input.107 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %4), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1678:0
    %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %2, %6), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1678:0
    return (%input.108)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1678:0
    %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %2, %6), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1678:0
    return (%x.41)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1678:0
    %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %2, %6), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1678:0
    return (%x.43)

MultiHeadAttention._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.114 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.115)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.115)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.115)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.115)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.114, %5), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %bs : Long() = prim::NumToTensor(%6), scope: __module.attentions.11
    %9 : int = aten::Int(%bs), scope: __module.attentions.11
    %10 : int = aten::Int(%bs), scope: __module.attentions.11
    %11 : int = aten::Int(%bs), scope: __module.attentions.11
    %12 : int = aten::Int(%bs), scope: __module.attentions.11
    %13 : int = aten::Int(%bs), scope: __module.attentions.11
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.114, %14), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %qlen : Long() = prim::NumToTensor(%15), scope: __module.attentions.11
    %17 : int = aten::Int(%qlen), scope: __module.attentions.11
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.11
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.114)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.11
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.114)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.11
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
    %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %48), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %50, %51), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %52), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.11 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.11 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.11
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.23), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
    %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %62), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.11
    %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %64, %65, %66, %67), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.11
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %69, %70), scope: __module.attentions.11 # torch/nn/functional.py:1498:0
    %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.115), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %73, %74), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.attentions.11 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %77, %78), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.11
    %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.117)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.117)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1678:0
    %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %2, %6), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1678:0
    return (%x.46)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %input.117 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %4), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1678:0
    %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %2, %6), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1678:0
    return (%input.118)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1678:0
    %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %2, %6), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1678:0
    return (%x.45)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1678:0
    %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %2, %6), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1678:0
    return (%x.47)

LayerNorm._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.9 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.0
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %4, %2, %1, %5, %6), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

LayerNorm._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.1
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %4, %2, %1, %5, %6), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

LayerNorm._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.2
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %4, %2, %1, %5, %6), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

LayerNorm._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.39 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.3
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %4, %2, %1, %5, %6), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

LayerNorm._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.49 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.4
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %4, %2, %1, %5, %6), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

LayerNorm._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.59 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.5
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %4, %2, %1, %5, %6), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

LayerNorm._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.6
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %4, %2, %1, %5, %6), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

LayerNorm._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.79 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.7
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %4, %2, %1, %5, %6), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

LayerNorm._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.89 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.8
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %4, %2, %1, %5, %6), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

LayerNorm._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.99 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.9
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %4, %2, %1, %5, %6), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

LayerNorm._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.109 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.10
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %4, %2, %1, %5, %6), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

LayerNorm._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.11
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %4, %2, %1, %5, %6), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    return (%input_tensor)

TransformerFFN._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.11)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.11)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.0 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.11)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
    %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %2, %6), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
    return (%input.10)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %input.11 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %4), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1678:0
    %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %2, %6), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1678:0
    return (%input.12)

TransformerFFN._actual_script_module
  graph(%self.21 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.21)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.21)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.1 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.21)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
    %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %2, %6), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
    return (%input.20)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %input.21 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %4), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1678:0
    %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %2, %6), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1678:0
    return (%input.22)

TransformerFFN._actual_script_module
  graph(%self.31 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.31)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.31)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.2 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.31)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
    %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %2, %6), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
    return (%input.30)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %4), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1678:0
    %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %2, %6), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1678:0
    return (%input.32)

TransformerFFN._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.41)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.41)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.3 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.41)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
    %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %2, %6), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
    return (%input.40)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %input.41 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %4), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1678:0
    %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %2, %6), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1678:0
    return (%input.42)

TransformerFFN._actual_script_module
  graph(%self.51 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.51)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.51)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.4 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.51)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
    %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %2, %6), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
    return (%input.50)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.linear.Linear,
        %input.51 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %4), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1678:0
    %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %2, %6), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1678:0
    return (%input.52)

TransformerFFN._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.61)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.61)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.5 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.61)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.62)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.62)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
    %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %2, %6), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
    return (%input.60)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.linear.Linear,
        %input.61 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %4), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1678:0
    %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %2, %6), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1678:0
    return (%input.62)

TransformerFFN._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.71)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.71)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.6 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.71)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
    %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %2, %6), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
    return (%input.70)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.linear.Linear,
        %input.71 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %4), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1678:0
    %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %2, %6), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1678:0
    return (%input.72)

TransformerFFN._actual_script_module
  graph(%self.81 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.81)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.81)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.7 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.81)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
    %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %2, %6), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
    return (%input.80)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.linear.Linear,
        %input.81 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %4), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1678:0
    %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %2, %6), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1678:0
    return (%input.82)

TransformerFFN._actual_script_module
  graph(%self.91 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.91)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.91)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.8 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.91)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
    %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %2, %6), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
    return (%input.90)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %4), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1678:0
    %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %2, %6), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1678:0
    return (%input.92)

TransformerFFN._actual_script_module
  graph(%self.101 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.101)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.101)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.9 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.101)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
    %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %2, %6), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
    return (%input.100)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.linear.Linear,
        %input.101 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %4), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1678:0
    %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %2, %6), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1678:0
    return (%input.102)

TransformerFFN._actual_script_module
  graph(%self.111 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.111)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.111)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.10 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.111)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
    %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %2, %6), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
    return (%input.110)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %input.111 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %4), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1678:0
    %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %2, %6), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1678:0
    return (%input.112)

TransformerFFN._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.121)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.121)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.11 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.121)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.122)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
    %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %2, %6), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
    return (%input.120)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %input.121 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %output : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %4), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1678:0
    %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output, %2, %6), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1678:0
    return (%input.122)

LayerNorm._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.13 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.0
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %4, %2, %1, %5, %6), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    return (%tensor.2)

LayerNorm._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.1
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %4, %2, %1, %5, %6), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    return (%tensor.3)

LayerNorm._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.2
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %4, %2, %1, %5, %6), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    return (%tensor.4)

LayerNorm._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.43 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.3
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %4, %2, %1, %5, %6), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    return (%tensor.5)

LayerNorm._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.53 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.4
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %4, %2, %1, %5, %6), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    return (%tensor.6)

LayerNorm._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.63 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.5
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %4, %2, %1, %5, %6), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    return (%tensor.7)

LayerNorm._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.73 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.6
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %4, %2, %1, %5, %6), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    return (%tensor.8)

LayerNorm._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.7
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %4, %2, %1, %5, %6), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    return (%tensor.9)

LayerNorm._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.93 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.94)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.94)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.8
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %4, %2, %1, %5, %6), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    return (%tensor.10)

LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.103 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.9
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %4, %2, %1, %5, %6), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    return (%tensor.11)

LayerNorm._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.113 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.10
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %4, %2, %1, %5, %6), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    return (%tensor.12)

LayerNorm._actual_script_module
  graph(%self : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.11
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input, %4, %2, %1, %5, %6), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    return (%tensor)

